abstract,cours
"  We explore the emergence of persistent infection in a closed region where the
disease progression of the individuals is given by the SIRS model, with an
individual becoming infected on contact with another infected individual within
a given range. We focus on the role of synchronization in the persistence of
contagion. Our key result is that higher degree of synchronization, both
globally in the population and locally in the neighborhoods, hinders
persistence of infection. Importantly, we find that early short-time asynchrony
appears to be a consistent precursor to future persistence of infection, and
can potentially provide valuable early warnings for sustained contagion in a
population patch. Thus transient synchronization can help anticipate the
long-term persistence of infection. Further we demonstrate that when the range
of influence of an infected individual is wider, one obtains lower persistent
infection. This counter-intuitive observation can also be understood through
the relation of synchronization to infection burn-out.
",quantitative-biology
"  This paper analyzes the coexistence performance of Wi-Fi and cellular
networks conditioned on non-saturated traffic in the unlicensed spectrum. Under
the condition, the time-domain behavior of a cellular small-cell base station
(SCBS) with a listen-before-talk (LBT) procedure is modeled as a Markov chain,
and it is combined with a Markov chain which describes the time-domain behavior
of a Wi-Fi access point. Using the proposed model, this study finds the optimal
contention window size of cellular SCBSs in which total throughput of both
networks is maximized while satisfying the required throughput of each network,
under the given traffic densities of both networks. This will serve as a
guideline for cellular operators with respect to performing LBT at cellular
SCBSs according to the changes of traffic volumes of both networks over time.
",computer-science
"  Let $G$ be a finite solvable or symmetric group and let $B$ be a $2$-block of
$G$. We construct a canonical correspondence between the irreducible characters
of height zero in $B$ and those in its Brauer first main correspondent. For
symmetric groups our bijection is compatible with restriction of characters.
",mathematics
"  Let $\phi$ be a spherical Hecke-Maass cusp form on the non-compact space
$\mathrm{PGL}_3(\mathbb{Z})\backslash\mathrm{PGL}_3(\mathbb{R})$. We establish
various pointwise upper bounds for $\phi$ in terms of its Laplace eigenvalue
$\lambda_\phi$. These imply, for $\phi$ arithmetically normalized and tempered
at the archimedean place, the bound $\|\phi\|_\infty\ll_\epsilon
\lambda_{\phi}^{39/40+\epsilon}$ for the global sup-norm (without restriction
to a compact subset). On the way, we derive a new uniform upper bound for the
$\mathrm{GL}_3$ Jacquet-Whittaker function.
",mathematics
"  Using a new and general method, we prove the existence of random attractor
for the three dimensional stochastic primitive equations defined on a manifold
$\D\subset\R^3$ improving the existence of weak attractor for the deterministic
model. Furthermore, we show the existence of the invariant measure.
",mathematics
"  Deep Reinforcement Learning (DRL) has been applied successfully to many
robotic applications. However, the large number of trials needed for training
is a key issue. Most of existing techniques developed to improve training
efficiency (e.g. imitation) target on general tasks rather than being tailored
for robot applications, which have their specific context to benefit from. We
propose a novel framework, Assisted Reinforcement Learning, where a classical
controller (e.g. a PID controller) is used as an alternative, switchable policy
to speed up training of DRL for local planning and navigation problems. The
core idea is that the simple control law allows the robot to rapidly learn
sensible primitives, like driving in a straight line, instead of random
exploration. As the actor network becomes more advanced, it can then take over
to perform more complex actions, like obstacle avoidance. Eventually, the
simple controller can be discarded entirely. We show that not only does this
technique train faster, it also is less sensitive to the structure of the DRL
network and consistently outperforms a standard Deep Deterministic Policy
Gradient network. We demonstrate the results in both simulation and real-world
experiments.
",computer-science
"  Random scattering is usually viewed as a serious nuisance in optical imaging,
and needs to be prevented in the conventional imaging scheme based on
single-photon interference. Here we proposed a two-photon imaging scheme with
the widely used lens replaced by a dynamic random medium. In contrast to
destroying imaging process, the dynamic random medium in our scheme works as a
crucial imaging element to bring constructive interference, and allows us to
image an object from light field scattered by this dynamic random medium. On
the one hand, our imaging scheme with incoherent two-photon illumination
enables us to achieve super-resolution imaging with the resolution reaching
Heisenberg limit. On the other hand, with coherent two-photon illumination, the
image of a pure-phase object can be obtained in our imaging scheme. These
results show new possibilities to overcome bottleneck of widely used
single-photon imaging by developing imaging method based on multi-photon
interference.
",physics
"  A Fourier-Chebyshev spectral method is proposed in this paper for solving the
cavitation problem in nonlinear elasticity. The interpolation error for the
cavitation solution is analyzed, the elastic energy error estimate for the
discrete cavitation solution is obtained, and the convergence of the method is
proved. An algorithm combined a gradient type method with a damped quasi-Newton
method is applied to solve the discretized nonlinear equilibrium equations.
Numerical experiments show that the Fourier-Chebyshev spectral method is
efficient and capable of producing accurate numerical cavitation solutions.
",mathematics
"  Over the course of last decade, the Nice model has dramatically changed our
view of the solar system's formation and early evolution. Within the context of
this model, a transient period of planet-planet scattering is triggered by
gravitational interactions between the giant planets and a massive primordial
planetesimal disk, leading to a successful reproduction of the solar system's
present-day architecture. In typical realizations of the Nice model,
self-gravity of the planetesimal disk is routinely neglected, as it poses a
computational bottleneck to the calculations. Recent analyses have shown,
however, that a self-gravitating disk can exhibit behavior that is dynamically
distinct, and this disparity may have significant implications for the solar
system's evolutionary path. In this work, we explore this discrepancy utilizing
a large suite of Nice odel simulations with and without a self-gravitating
planetesimal disk, taking advantage of the inherently parallel nature of
graphic processing units. Our simulations demonstrate that self-consistent
modeling of particle interactions does not lead to significantly different
final planetary orbits from those obtained within conventional simulations.
Moreover, self-gravitating calculations show similar planetesimal evolution to
non-self-gravitating numerical experiments after dynamical instability is
triggered, suggesting that the orbital clustering observed in the distant
Kuiper belt is unlikely to have a self-gravitational origin.
",physics
"  Researchers at the National Institute of Standards and Technology(NIST) have
measured the value of the Planck constant to be $h =6.626\,069\,934(89)\times
10^{-34}\,$J$\,$s (relative standard uncertainty $13\times 10^{-9}$). The
result is based on over 10$\,$000 weighings of masses with nominal values
ranging from 0.5$\,$kg to 2$\,$kg with the Kibble balance NIST-4. The
uncertainty has been reduced by more than twofold relative to a previous
determination because of three factors: (1) a much larger data set than
previously available, allowing a more realistic, and smaller, Type A
evaluation; (2) a more comprehensive measurement of the back action of the
weighing current on the magnet by weighing masses up to 2$\,$kg, decreasing the
uncertainty associated with magnet non-linearity; (3) a rigorous investigation
of the dependence of the geometric factor on the coil velocity reducing the
uncertainty assigned to time-dependent leakage of current in the coil.
",physics
"  Computer programs do not always work as expected. In fact, ominous warnings
about the desperate state of the software industry continue to be released with
almost ritualistic regularity. In this paper, we look at the 60 years history
of programming and at the different practical methods that software community
developed to live with programming errors. We do so by observing a class of
students discussing different approaches to programming errors. While learning
about the different methods for dealing with errors, we uncover basic
assumptions that proponents of different paradigms follow. We learn about the
mathematical attempt to eliminate errors through formal methods, scientific
method based on testing, a way of building reliable systems through engineering
methods, as well as an artistic approach to live coding that accepts errors as
a creative inspiration. This way, we can explore the differences and
similarities among the different paradigms. By inviting proponents of different
methods into a single discussion, we hope to open potential for new thinking
about errors. When should we use which of the approaches? And what can software
development learn from mathematics, science, engineering and art? When
programming or studying programming, we are often enclosed in small communities
and we take our basic assumptions for granted. Through the discussion in this
paper, we attempt to map the large and rich space of programming ideas and
provide reference points for exploring, perhaps foreign, ideas that can
challenge some of our assumptions.
",computer-science
"  Community detection is a key data analysis problem across different fields.
During the past decades, numerous algorithms have been proposed to address this
issue. However, most work on community detection does not address the issue of
statistical significance. Although some research efforts have been made towards
mining statistically significant communities, deriving an analytical solution
of p-value for one community under the configuration model is still a
challenging mission that remains unsolved. To partially fulfill this void, we
present a tight upper bound on the p-value of a single community under the
configuration model, which can be used for quantifying the statistical
significance of each community analytically. Meanwhile, we present a local
search method to detect statistically significant communities in an iterative
manner. Experimental results demonstrate that our method is comparable with the
competing methods on detecting statistically significant communities.
",computer-science
"  We consider the problem of designing risk-sensitive optimal control policies
for scheduling packet transmissions in a stochastic wireless network. A single
client is connected to an access point (AP) through a wireless channel. Packet
transmission incurs a cost $C$, while packet delivery yields a reward of $R$
units. The client maintains a finite buffer of size $B$, and a penalty of $L$
units is imposed upon packet loss which occurs due to finite queueing buffer.
We show that the risk-sensitive optimal control policy for such a simple
set-up is of threshold type, i.e., it is optimal to carry out packet
transmissions only when $Q(t)$, i.e., the queue length at time $t$ exceeds a
certain threshold $\tau$. It is also shown that the value of threshold $\tau$
increases upon increasing the cost per unit packet transmission $C$.
Furthermore, it is also shown that a threshold policy with threshold equal to
$\tau$ is optimal for a set of problems in which cost $C$ lies within an
interval $[C_l,C_u]$. Equations that need to be solved in order to obtain
$C_l,C_u$ are also provided.
",computer-science
"  We give a complete formula for the characteristic polynomial of hyperplane
arrangements $\mathcal J_n$ consisting of the hyperplanes $x_i+x_j=1$, $x_k=0$,
$x_l=1$, $ 1\leq i, j, k, l\leq n$. The formula is obtained by associating
hyperplane arrangements with graphs, and then enumerating central graphs via
generating functions for the number of bipartite graphs of given order, size
and number of connected components.
",mathematics
"  The radially outward flow of fluid through a porous medium occurs in many
practical problems, from transport across vascular walls to the pressurisation
of boreholes in the subsurface. When the driving pressure is non-negligible
relative to the stiffness of the solid structure, the poromechanical coupling
between the fluid and the solid can control both the steady-state and the
transient mechanics of the system. Very large pressures or very soft materials
lead to large deformations of the solid skeleton, which introduce kinematic and
constitutive nonlinearity that can have a nontrivial impact on these mechanics.
Here, we study the transient response of a poroelastic cylinder to sudden fluid
injection. We consider the impacts of kinematic and constitutive nonlinearity,
both separately and in combination, and we highlight the central role of
driving method in the evolution of the response. We show that the various
facets of nonlinearity may either accelerate or decelerate the transient
response relative to linear poroelasticity, depending on the boundary
conditions and the initial geometry, and that an imposed fluid pressure leads
to a much faster response than an imposed fluid flux.
",physics
"  Modern industrial automatic machines and robotic cells are equipped with
highly complex human-machine interfaces (HMIs) that often prevent human
operators from an effective use of the automatic systems. In particular, this
applies to vulnerable users, such as those with low experience or education
level, the elderly and the disabled. To tackle this issue, it becomes necessary
to design user-oriented HMIs, which adapt to the capabilities and skills of
users, thus compensating their limitations and taking full advantage of their
knowledge. In this paper, we propose a methodological approach to the design of
complex adaptive human-machine systems that might be inclusive of all users, in
particular the vulnerable ones. The proposed approach takes into account both
the technical requirements and the requirements for ethical, legal and social
implications (ELSI) for the design of automatic systems. The technical
requirements derive from a thorough analysis of three use cases taken from the
European project INCLUSIVE. To achieve the ELSI requirements, the MEESTAR
approach is combined with the specific legal issues for occupational systems
and requirements of the target users.
",computer-science
"  We study the phase space dynamics of cosmological models in the theoretical
formulations of non-minimal metric-torsion couplings with a scalar field, and
investigate in particular the critical points which yield stable solutions
exhibiting cosmic acceleration driven by the {\em dark energy}. The latter is
defined in a way that it effectively has no direct interaction with the
cosmological fluid, although in an equivalent scalar-tensor cosmological setup
the scalar field interacts with the fluid (which we consider to be the
pressureless dust). Determining the conditions for the existence of the stable
critical points we check their physical viability, in both Einstein and Jordan
frames. We also verify that in either of these frames, the evolution of the
universe at the corresponding stable points matches with that given by the
respective exact solutions we have found in an earlier work (arXiv: 1611.00654
[gr-qc]). We not only examine the regions of physical relevance for the
trajectories in the phase space when the coupling parameter is varied, but also
demonstrate the evolution profiles of the cosmological parameters of interest
along fiducial trajectories in the effectively non-interacting scenarios, in
both Einstein and Jordan frames.
",physics
"  In multi-object tracking applications, model parameter tuning is a
prerequisite for reliable performance. In particular, it is difficult to know
statistics of false measurements due to various sensing conditions and changes
in the field of views. In this paper we are interested in designing a
multi-object tracking algorithm that handles unknown false measurement rate.
Recently proposed robust multi-Bernoulli filter is employed for clutter
estimation while generalized labeled multi-Bernoulli filter is considered for
target tracking. Performance evaluation with real videos demonstrates the
effectiveness of the tracking algorithm for real-world scenarios.
",computer-science
"  With the proliferation of social media, fashion inspired from celebrities,
reputed designers as well as fashion influencers has shortened the cycle of
fashion design and manufacturing. However, with the explosion of fashion
related content and large number of user generated fashion photos, it is an
arduous task for fashion designers to wade through social media photos and
create a digest of trending fashion. This necessitates deep parsing of fashion
photos on social media to localize and classify multiple fashion items from a
given fashion photo. While object detection competitions such as MSCOCO have
thousands of samples for each of the object categories, it is quite difficult
to get large labeled datasets for fast fashion items. Moreover,
state-of-the-art object detectors do not have any functionality to ingest large
amount of unlabeled data available on social media in order to fine tune object
detectors with labeled datasets. In this work, we show application of a generic
object detector, that can be pretrained in an unsupervised manner, on 24
categories from recently released Open Images V4 dataset. We first train the
base architecture of the object detector using unsupervisd learning on 60K
unlabeled photos from 24 categories gathered from social media, and then
subsequently fine tune it on 8.2K labeled photos from Open Images V4 dataset.
On 300 X 300 image inputs, we achieve 72.7% mAP on a test dataset of 2.4K
photos while performing 11% to 17% better as compared to the state-of-the-art
object detectors. We show that this improvement is due to our choice of
architecture that lets us do unsupervised learning and that performs
significantly better in identifying small objects.
",statistics
"  Crystal plasticity is mediated through dislocations, which form knotted
configurations in a complex energy landscape. Once they disentangle and move,
they may also be impeded by permanent obstacles with finite energy barriers or
frustrating long-range interactions. The outcome of such complexity is the
emergence of dislocation avalanches as the basic mechanism of plastic flow in
solids at the nanoscale. While the deformation behavior of bulk materials
appears smooth, a predictive model should clearly be based upon the character
of these dislocation avalanches and their associated strain bursts. We provide
here a comprehensive overview of experimental observations, theoretical models
and computational approaches that have been developed to unravel the multiple
aspects of dislocation avalanche physics and the phenomena leading to strain
bursts in crystal plasticity.
",physics
"  The introduction of serious games as pedagogical supports in the field of
education is a process gaining in popularity amongst the teaching community.
This article creates a link between the integration of new pedagogical
solutions in first-year primary class and the fundamental research on the
motivation of the players/learners, detailing an experiment based on a game
specifically developed, named QCM. QCM considers the learning worksheets issued
from the Freinet pedagogy using various gameplay mechanisms. The main
contribution of QCM in relation to more traditional games is the dissociation
of immersion mechanisms, in order to improve the understanding of the user
experience. This game also contains a system of gameplay metrics, the analysis
of which shows a relative increase in the motivation of students using QCM
instead of paper worksheets, while revealing large differences in students
behavior in conjunction with the mechanisms of gamification employed. Keywords
: Serious games, learning analytics, gamification, flow.
",computer-science
"  While an increasing number of two-dimensional (2D) materials, including
graphene and silicene, have already been realized, others have only been
predicted. An interesting example is the two-dimensional form of silicon
carbide (2D-SiC). Here, we present an observation of atomically thin and
hexagonally bonded nanosized grains of SiC assembling temporarily in graphene
oxide pores during an atomic resolution scanning transmission electron
microscopy experiment. Even though these small grains do not fully represent
the bulk crystal, simulations indicate that their electronic structure already
approaches that of 2D-SiC. This is predicted to be flat, but some doubts have
remained regarding the preference of Si for sp$^{3}$ hybridization. Exploring a
number of corrugated morphologies, we find completely flat 2D-SiC to have the
lowest energy. We further compute its phonon dispersion, with a Raman-active
transverse optical mode, and estimate the core level binding energies. Finally,
we study the chemical reactivity of 2D-SiC, suggesting it is like silicene
unstable against molecular absorption or interlayer linking. Nonetheless, it
can form stable van der Waals-bonded bilayers with either graphene or hexagonal
boron nitride, promising to further enrich the family of two-dimensional
materials once bulk synthesis is achieved.
",physics
"  The thermal stability of most electronic and photo-electronic devices
strongly depends on the relationship between Schottky Barrier Height (SBH) and
temperature. In this paper, the possible of thermionic current depicted via
correct and reliability relationship between forward current and voltage is
consequently discussed, the intrinsic SBH insensitive to temperature can be
calculated by modification on Richardson- Dushman`s formula suggested in this
paper. The results of application on four hetero-junctions prove that the
method proposed is credible in this paper, this suggests that the I/V/T method
is a feasible alternative to characterize these heterojunctions.
",physics
"  We present a robust deep learning based 6 degrees-of-freedom (DoF)
localization system for endoscopic capsule robots. Our system mainly focuses on
localization of endoscopic capsule robots inside the GI tract using only visual
information captured by a mono camera integrated to the robot. The proposed
system is a 23-layer deep convolutional neural network (CNN) that is capable to
estimate the pose of the robot in real time using a standard CPU. The dataset
for the evaluation of the system was recorded inside a surgical human stomach
model with realistic surface texture, softness, and surface liquid properties
so that the pre-trained CNN architecture can be transferred confidently into a
real endoscopic scenario. An average error of 7:1% and 3:4% for translation and
rotation has been obtained, respectively. The results accomplished from the
experiments demonstrate that a CNN pre-trained with raw 2D endoscopic images
performs accurately inside the GI tract and is robust to various challenges
posed by reflection distortions, lens imperfections, vignetting, noise, motion
blur, low resolution, and lack of unique landmarks to track.
",computer-science
"  Using process algebra, this paper describes the formalisation of the
process/semantics behind the purely event-driven programming language.
",computer-science
"  Many structured prediction problems (particularly in vision and language
domains) are ambiguous, with multiple outputs being correct for an input - e.g.
there are many ways of describing an image, multiple ways of translating a
sentence; however, exhaustively annotating the applicability of all possible
outputs is intractable due to exponentially large output spaces (e.g. all
English sentences). In practice, these problems are cast as multi-class
prediction, with the likelihood of only a sparse set of annotations being
maximized - unfortunately penalizing for placing beliefs on plausible but
unannotated outputs. We make and test the following hypothesis - for a given
input, the annotations of its neighbors may serve as an additional supervisory
signal. Specifically, we propose an objective that transfers supervision from
neighboring examples. We first study the properties of our developed method in
a controlled toy setup before reporting results on multi-label classification
and two image-grounded sequence modeling tasks - captioning and question
generation. We evaluate using standard task-specific metrics and measures of
output diversity, finding consistent improvements over standard maximum
likelihood training and other baselines.
",statistics
"  Simple finite dimensional Kantor triple systems over the complex numbers are
classified in terms of Satake diagrams. We prove that every simple and linearly
compact Kantor triple system has finite dimension and give an explicit
presentation of all the classical and exceptional systems.
",mathematics
"  We give a counterexample to the vector generalization of Costa's entropy
power inequality (EPI) due to Liu, Liu, Poor and Shamai. In particular, the
claimed inequality can fail if the matix-valued parameter in the convex
combination does not commute with the covariance of the additive Gaussian
noise. Conversely, the inequality holds if these two matrices commute.
",computer-science
"  We establish a functional weak law of large numbers for observable
macroscopic state variables of interacting particle systems (e.g., voter and
contact processes) over fast time-varying sparse random networks of
interactions. We show that, as the number of agents $N$ grows large, the
proportion of agents $\left(\overline{Y}_{k}^{N}(t)\right)$ at a certain state
$k$ converges in distribution -- or, more precisely, weakly with respect to the
uniform topology on the space of \emph{càdlàg} sample paths -- to the
solution of an ordinary differential equation over any compact interval
$\left[0,T\right]$. Although the limiting process is Markov, the prelimit
processes, i.e., the normalized macrostate vector processes
$\left(\mathbf{\overline{Y}}^{N}(t)\right)=\left(\overline{Y}_{1}^{N}(t),\ldots,\overline{Y}_{K}^{N}(t)\right)$,
are non-Markov as they are tied to the \emph{high-dimensional} microscopic
state of the system, which precludes the direct application of standard
arguments for establishing weak convergence. The techniques developed in the
paper for establishing weak convergence might be of independent interest.
",mathematics
"  We present a generalisation of C. Bishop and P. Jones' result in [BJ1], where
they give a characterisation of the tangent points of a Jordan curve in terms
of $\beta$ numbers. Instead of the $L^\infty$ Jones' $\beta$ numbers, we use an
averaged version of them, firstly introduced by J. Azzam and R. Schul in [AS1].
A fundamental tool in the proof will be the Reifenberg parameterisation Theorem
of G. David and T. Toro (see [DT1]).
",mathematics
"  Path integrals describing quantum many-body systems can be calculated with
Monte Carlo sampling techniques, but average quantities are often subject to
signal-to-noise ratios that degrade exponentially with time. A
phase-reweighting technique inspired by recent observations of random walk
statistics in correlation functions is proposed that allows energy levels to be
extracted from late-time correlation functions with time-independent
signal-to-noise ratios. Phase reweighting effectively includes dynamical
refinement of source magnitudes but introduces a bias associated with the
phase. This bias can be removed by performing an extrapolation, but at the
expense of re-introducing a signal-to-noise problem. Lattice Quantum
Chromodynamics calculations of the $\rho$ and nucleon masses and of the
$\Xi\Xi$ binding energy show consistency between standard results obtained
using earlier-time correlation functions and phase-reweighted results using
late-time correlation functions inaccessible to standard statistical analysis
methods.
",physics
"  Motion planning classically concerns the problem of accomplishing a goal
configuration while avoiding obstacles. However, the need for more
sophisticated motion planning methodologies, taking temporal aspects into
account, has emerged. To address this issue, temporal logics have recently been
used to formulate such advanced specifications. This paper will consider Signal
Temporal Logic in combination with Model Predictive Control. A robustness
metric, called Discrete Average Space Robustness, is introduced and used to
maximize the satisfaction of specifications which results in a natural
robustness against noise. The comprised optimization problem is convex and
formulated as a Linear Program.
",computer-science
"  Deep convolutional neural networks (CNNs) based approaches are the
state-of-the-art in various computer vision tasks, including face recognition.
Considerable research effort is currently being directed towards further
improving deep CNNs by focusing on more powerful model architectures and better
learning techniques. However, studies systematically exploring the strengths
and weaknesses of existing deep models for face recognition are still
relatively scarce in the literature. In this paper, we try to fill this gap and
study the effects of different covariates on the verification performance of
four recent deep CNN models using the Labeled Faces in the Wild (LFW) dataset.
Specifically, we investigate the influence of covariates related to: image
quality -- blur, JPEG compression, occlusion, noise, image brightness,
contrast, missing pixels; and model characteristics -- CNN architecture, color
information, descriptor computation; and analyze their impact on the face
verification performance of AlexNet, VGG-Face, GoogLeNet, and SqueezeNet. Based
on comprehensive and rigorous experimentation, we identify the strengths and
weaknesses of the deep learning models, and present key areas for potential
future research. Our results indicate that high levels of noise, blur, missing
pixels, and brightness have a detrimental effect on the verification
performance of all models, whereas the impact of contrast changes and
compression artifacts is limited. It has been found that the descriptor
computation strategy and color information does not have a significant
influence on performance.
",statistics
"  We introduce the coherent state mapping ring-polymer molecular dynamics
(CS-RPMD), a new method that accurately describes electronic non-adiabatic
dynamics with explicit nuclear quantization. This new approach is derived by
using coherent state mapping representation for the electronic degrees of
freedom (DOF) and the ring-polymer path-integral representation for the nuclear
DOF. CS-RPMD Hamiltonian does not contain any inter-bead coupling term in the
state-dependent potential, which is a key feature that ensures correct
electronic Rabi oscillations. Hamilton's equation of motion is used to sample
initial configurations and propagate the trajectories, preserving the
distribution with classical symplectic evolution. In the special one-bead limit
for mapping variables, CS-RPMD preserves the detailed balance. Numerical tests
of this method with a two-state model system show a very good agreement with
exact quantum results over a broad range of electronic couplings.
",physics
"  Hybrid unmanned aircraft, that combine hover capability with a wing for fast
and efficient forward flight, have attracted a lot of attention in recent
years. Many different designs are proposed, but one of the most promising is
the tailsitter concept. However, tailsitters are difficult to control across
the entire flight envelope, which often includes stalled flight. Additionally,
their wing surface makes them susceptible to wind gusts. In this paper, we
propose incremental nonlinear dynamic inversion control for the attitude and
position control. The result is a single, continuous controller, that is able
to track the acceleration of the vehicle across the flight envelope. The
proposed controller is implemented on the Cyclone hybrid UAV. Multiple outdoor
experiments are performed, showing that unmodeled forces and moments are
effectively compensated by the incremental control structure, and that
accelerations can be tracked across the flight envelope. Finally, we provide a
comprehensive procedure for the implementation of the controller on other types
of hybrid UAVs.
",computer-science
"  A 1-ended finitely presented group has semistable fundamental group at
$\infty$ if it acts geometrically on some (equivalently any) simply connected
and locally finite complex $X$ with the property that any two proper rays in
$X$ are properly homotopic. If $G$ has semistable fundamental group at $\infty$
then one can unambiguously define the fundamental group at $\infty$ for $G$.
The problem, asking if all finitely presented groups have semistable
fundamental group at $\infty$ has been studied for over 40 years. If $G$ is an
ascending HNN extension of a finitely presented group then indeed, $G$ has
semistable fundamental group at $\infty$, but since the early 1980's it has
been suggested that the finitely presented groups that are ascending HNN
extensions of {\it finitely generated} groups may include a group with
non-semistable fundamental group at $\infty$. Ascending HNN extensions
naturally break into two classes, those with bounded depth and those with
unbounded depth. Our main theorem shows that bounded depth finitely presented
ascending HNN extensions of finitely generated groups have semistable
fundamental group at $\infty$. Semistability is equivalent to two weaker
asymptotic conditions on the group holding simultaneously. We show one of these
conditions holds for all ascending HNN extensions, regardless of depth. We give
a technique for constructing ascending HNN extensions with unbounded depth.
This work focuses attention on a class of groups that may contain a group with
non-semistable fundamental group at $\infty$.
",mathematics
"  The Extreme Ultraviolet Variability Experiment (EVE) on the Solar Dynamics
Observatory obtains extreme-ultraviolet (EUV) spectra of the full-disk Sun at a
spectral resolution of ~1 A and cadence of 10 s. Such a spectral resolution
would normally be considered to be too low for the reliable determination of
electron density (N_e) sensitive emission line intensity ratios, due to
blending. However, previous work has shown that a limited number of Fe XXI
features in the 90-60 A wavelength region of EVE do provide useful
N_e-diagnostics at relatively low flare densities (N_e ~ 10^11-10^12 cm^-3).
Here we investigate if additional highly ionised Fe line ratios in the EVE
90-160 A range may be reliably employed as N_e-diagnostics. In particular, the
potential for such diagnostics to provide density estimates for high N_e
(~10^13 cm^-3) flare plasmas is assessed. Our study employs EVE spectra for
X-class flares, combined with observations of highly active late-type stars
from the Extreme Ultraviolet Explorer (EUVE) satellite plus experimental data
for well-diagnosed tokamak plasmas, both of which are similar in wavelength
coverage and spectral resolution to those from EVE. Several ratios are
identified in EVE data which yield consistent values of electron density,
including Fe XX 113.35/121.85 and Fe XXII 114.41/135.79, with confidence in
their reliability as N_e-diagnostics provided by the EUVE and tokamak results.
These ratios also allow the determination of density in solar flare plasmas up
to values of ~10^13 cm^-3.
",physics
"  In many modern machine learning applications, the outcome is expensive or
time-consuming to collect while the predictor information is easy to obtain.
Semi-supervised learning (SSL) aims at utilizing large amounts of `unlabeled'
data along with small amounts of `labeled' data to improve the efficiency of a
classical supervised approach. Though numerous SSL classification and
prediction procedures have been proposed in recent years, no methods currently
exist to evaluate the prediction performance of a working regression model. In
the context of developing phenotyping algorithms derived from electronic
medical records (EMR), we present an efficient two-step estimation procedure
for evaluating a binary classifier based on various prediction performance
measures in the semi-supervised (SS) setting. In step I, the labeled data is
used to obtain a non-parametrically calibrated estimate of the conditional risk
function. In step II, SS estimates of the prediction accuracy parameters are
constructed based on the estimated conditional risk function and the unlabeled
data. We demonstrate that under mild regularity conditions the proposed
estimators are consistent and asymptotically normal. Importantly, the
asymptotic variance of the SS estimators is always smaller than that of the
supervised counterparts under correct model specification. We also correct for
potential overfitting bias in the SS estimators in finite sample with
cross-validation and develop a perturbation resampling procedure to approximate
their distributions. Our proposals are evaluated through extensive simulation
studies and illustrated with two real EMR studies aiming to develop phenotyping
algorithms for rheumatoid arthritis and multiple sclerosis.
",statistics
"  We study planted problems---finding hidden structures in random noisy
inputs---through the lens of the sum-of-squares semidefinite programming
hierarchy (SoS). This family of powerful semidefinite programs has recently
yielded many new algorithms for planted problems, often achieving the best
known polynomial-time guarantees in terms of accuracy of recovered solutions
and robustness to noise. One theme in recent work is the design of spectral
algorithms which match the guarantees of SoS algorithms for planted problems.
Classical spectral algorithms are often unable to accomplish this: the twist in
these new spectral algorithms is the use of spectral structure of matrices
whose entries are low-degree polynomials of the input variables. We prove that
for a wide class of planted problems, including refuting random constraint
satisfaction problems, tensor and sparse PCA, densest-k-subgraph, community
detection in stochastic block models, planted clique, and others, eigenvalues
of degree-d matrix polynomials are as powerful as SoS semidefinite programs of
roughly degree d. For such problems it is therefore always possible to match
the guarantees of SoS without solving a large semidefinite program. Using
related ideas on SoS algorithms and low-degree matrix polynomials (and inspired
by recent work on SoS and the planted clique problem by Barak et al.), we prove
new nearly-tight SoS lower bounds for the tensor and sparse principal component
analysis problems. Our lower bounds for sparse principal component analysis are
the first to suggest that going beyond existing algorithms for this problem may
require sub-exponential time.
",computer-science
"  Graph games with {\omega}-regular winning conditions provide a mathematical
framework to analyze a wide range of problems in the analysis of reactive
systems and programs (such as the synthesis of reactive systems, program
repair, and the verification of branching time properties). Parity conditions
are canonical forms to specify {\omega}-regular winning conditions. Graph games
with parity conditions are equivalent to {\mu}-calculus model checking, and
thus a very important algorithmic problem. Symbolic algorithms are of great
significance because they provide scalable algorithms for the analysis of large
finite-state systems, as well as algorithms for the analysis of infinite-state
systems with finite quotient. A set-based symbolic algorithm uses the basic set
operations and the one-step predecessor operators. We consider graph games with
$n$ vertices and parity conditions with $c$ priorities. While many explicit
algorithms exist for graph games with parity conditions, for set-based symbolic
algorithms there are only two algorithms (notice that we use space to refer to
the number of sets stored by a symbolic algorithm): (a) the basic algorithm
that requires $O(n^c)$ symbolic operations and linear space; and (b) an
improved algorithm that requires $O(n^{c/2+1})$ symbolic operations but also
$O(n^{c/2+1})$ space (i.e., exponential space). In this work we present two
set-based symbolic algorithms for parity games: (a) our first algorithm
requires $O(n^{c/2+1})$ symbolic operations and only requires linear space; and
(b) developing on our first algorithm, we present an algorithm that requires
$O(n^{c/3+1})$ symbolic operations and only linear space. We also present the
first linear space set-based symbolic algorithm for parity games that requires
at most a sub-exponential number of symbolic operations.
",computer-science
"  Spatially explicit capture recapture (SECR) models have gained enormous
popularity to solve abundance estimation problems in ecology. In this study, we
develop a novel Bayesian SECR model that disentangles the process of animal
movement through a detector from the process of recording data by a detector in
the face of imperfect detection. We integrate this complexity into an advanced
version of a recent SECR model involving partially identified individuals
(Royle, 2015). We assess the performance of our model over a range of realistic
simulation scenarios and demonstrate that estimates of population size $N$
improve when we utilize the proposed model relative to the model that does not
explicitly estimate trap detection probability (Royle, 2015). We confront and
investigate the proposed model with a spatial capture-recapture data set from a
camera trapping survey on tigers (\textit{Panthera tigris}) in Nagarahole,
southern India. Trap detection probability is estimated at 0.489 and therefore
justifies the necessity to utilize our model in field situations. We discuss
possible extensions, future work and relevance of our model to other
statistical applications beyond ecology.
",statistics
"  Analysis and quantification of brain structural changes, using Magnetic
resonance imaging (MRI), are increasingly used to define novel biomarkers of
brain pathologies, such as Alzheimer's disease (AD). Network-based models of
the brain have shown that both local and global topological properties can
reveal patterns of disease propagation. On the other hand, intra-subject
descriptions cannot exploit the whole information context, accessible through
inter-subject comparisons. To address this, we developed a novel approach,
which models brain structural connectivity atrophy with a multiplex network and
summarizes it within a classification score. On an independent dataset
multiplex networks were able to correctly segregate, from normal controls (NC),
AD patients and subjects with mild cognitive impairment that will convert to AD
(cMCI) with an accuracy of, respectively, $0.86 \pm 0.01$ and $0.84 \pm 0.01$.
The model also shows that illness effects are maximally detected by parceling
the brain in equal volumes of $3000$ $mm^3$ (""patches""), without any $a$
$priori$ segmentation based on anatomical features. A direct comparison to
standard voxel-based morphometry on the same dataset showed that the multiplex
network approach had higher sensitivity. This method is general and can have
twofold potential applications: providing a reliable tool for clinical trials
and a disease signature of neurodegenerative pathologies.
",physics
"  As the core issue of blockchain, the mining requires solving a proof-of-work
puzzle, which is resource expensive to implement in mobile devices due to high
computing power needed. Thus, the development of blockchain in mobile
applications is restricted. In this paper, we consider the edge computing as
the network enabler for mobile blockchain. In particular, we study optimal
pricing-based edge computing resource management to support mobile blockchain
applications where the mining process can be offloaded to an Edge computing
Service Provider (ESP). We adopt a two-stage Stackelberg game to jointly
maximize the profit of the ESP and the individual utilities of different
miners. In Stage I, the ESP sets the price of edge computing services. In Stage
II, the miners decide on the service demand to purchase based on the observed
prices. We apply the backward induction to analyze the sub-game perfect
equilibrium in each stage for uniform and discriminatory pricing schemes.
Further, the existence and uniqueness of Stackelberg game are validated for
both pricing schemes. At last, the performance evaluation shows that the ESP
intends to set the maximum possible value as the optimal price for profit
maximization under uniform pricing. In addition, the discriminatory pricing
helps the ESP encourage higher total service demand from miners and achieve
greater profit correspondingly.
",computer-science
"  Hierarchical clustering is a class of algorithms that seeks to build a
hierarchy of clusters. It has been the dominant approach to constructing
embedded classification schemes since it outputs dendrograms, which capture the
hierarchical relationship among members at all levels of granularity,
simultaneously. Being greedy in the algorithmic sense, a hierarchical
clustering partitions data at every step solely based on a similarity /
dissimilarity measure. The clustering results oftentimes depend on not only the
distribution of the underlying data, but also the choice of dissimilarity
measure and the clustering algorithm. In this paper, we propose a method to
incorporate prior domain knowledge about entity relationship into the
hierarchical clustering. Specifically, we use a distance function in
ultrametric space to encode the external ontological information. We show that
popular linkage-based algorithms can faithfully recover the encoded structure.
Similar to some regularized machine learning techniques, we add this distance
as a penalty term to the original pairwise distance to regulate the final
structure of the dendrogram. As a case study, we applied this method on real
data in the building of a customer behavior based product taxonomy for an
Amazon service, leveraging the information from a larger Amazon-wide browse
structure. The method is useful when one wants to leverage the relational
information from external sources, or the data used to generate the distance
matrix is noisy and sparse. Our work falls in the category of semi-supervised
or constrained clustering.
",statistics
"  We present a new experimental approach to investigate the magnetic properties
of the anisotropic heavy-fermion system YbRh$_2$Si$_2$ as a function of
crystallographic orientation. Angle-dependent electron spin resonance (ESR)
measurements are performed at a low temperature of 1.6 K and at an ESR
frequency of 4.4 GHz utilizing a superconducting planar microwave resonator in
a $^4$He-cryostat in combination with in-situ sample rotation. The obtained ESR
g-factor of YbRh$_2$Si$_2$ as a function of the crystallographic angle is
consistent with results of previous measurements using conventional ESR
spectrometers at higher frequencies and fields. Perspectives to implement this
experimental approach into a dilution refrigerator and to reach the
magnetically ordered phase of YbRh$_2$Si$_2$ are discussed.
",physics
"  In this paper we propose definitions and examples of categorical enhancements
of the data involved in the $2d$-$4d$ wall-crossing formulas which generalize
both Cecotti-Vafa and Kontsevich-Soibelman motivic wall-crossing formulas.
",mathematics
"  Despite a well-ordered pyrochlore crystal structure and strong magnetic
interactions between the Dy$^{3+}$ or Ho$^{3+}$ ions, no long range magnetic
order has been detected in the pyrochlore titanates Ho$_2$Ti$_2$O$_7$ and
Dy$_2$Ti$_2$O$_7$. To explore the actual magnetic phase formed by cooling these
materials, we measure their magnetization dynamics using toroidal,
boundary-free magnetization transport techniques. We find that the dynamical
magnetic susceptibility of both compounds has the same distinctive
phenomenology, that is indistinguishable in form from that of the dielectric
permittivity of dipolar glass-forming liquids. Moreover, Ho$_2$Ti$_2$O$_7$ and
Dy$_2$Ti$_2$O$_7$ both exhibit microscopic magnetic relaxation times that
increase along the super-Arrhenius trajectories analogous to those observed in
glass-forming dipolar liquids. Thus, upon cooling below about 2K,
Dy$_2$Ti$_2$O$_7$ and Ho$_2$Ti$_2$O$_7$ both appear to enter the same magnetic
state exhibiting the characteristics of a glass-forming spin-liquid.
",physics
"  Despite the effectiveness of convolutional neural networks (CNNs) especially
in image classification tasks, the effect of convolution features on learned
representations is still limited. It mostly focuses on the salient object of
the images, but ignores the variation information on clutter and local. In this
paper, we propose a special framework, which is the multiple VLAD encoding
method with the CNNs features for image classification. Furthermore, in order
to improve the performance of the VLAD coding method, we explore the
multiplicity of VLAD encoding with the extension of three kinds of encoding
algorithms, which are the VLAD-SA method, the VLAD-LSA and the VLAD-LLC method.
Finally, we equip the spatial pyramid patch (SPM) on VLAD encoding to add the
spatial information of CNNs feature. In particular, the power of SPM leads our
framework to yield better performance compared to the existing method.
",computer-science
"  In this article, we study the problem of controlling a highway segment facing
stochastic perturbations, such as recurrent incidents and moving bottlenecks.
To model traffic flow under perturbations, we use the cell-transmission model
with Markovian capacities. The control inputs are: (i) the inflows that are
sent to various on-ramps to the highway (for managing traffic demand), and (ii)
the priority levels assigned to the on-ramp traffic relative to the mainline
traffic (for allocating highway capacity). The objective is to maximize the
throughput while ensuring that on-ramp queues remain bounded in the long-run.
We develop a computational approach to solving this stability-constrained,
throughput-maximization problem. Firstly, we use the classical drift condition
in stability analysis of Markov processes to derive a sufficient condition for
boundedness of on-ramp queues. Secondly, we show that our control design
problem can be formulated as a mixed integer program with linear or bilinear
constraints, depending on the complexity of Lyapunov function involved in the
stability condition. Finally, for specific types of capacity perturbations, we
derive intuitive criteria for managing demand and/or selecting priority levels.
These criteria suggest that inflows and priority levels should be determined
simultaneously such that traffic queues are placed at locations that discharge
queues fast. We illustrate the performance benefits of these criteria through a
computational study of a segment on Interstate 210 in California, USA.
",computer-science
"  The present online social media platform is afflicted with several issues,
with hate speech being on the predominant forefront. The prevalence of online
hate speech has fueled horrific real-world hate-crime such as the mass-genocide
of Rohingya Muslims, communal violence in Colombo and the recent massacre in
the Pittsburgh synagogue. Consequently, It is imperative to understand the
diffusion of such hateful content in an online setting. We conduct the first
study that analyses the flow and dynamics of posts generated by hateful and
non-hateful users on Gab (gab.com) over a massive dataset of 341K users and 21M
posts. Our observations confirms that hateful content diffuse farther, wider
and faster and have a greater outreach than those of non-hateful users. A
deeper inspection into the profiles and network of hateful and non-hateful
users reveals that the former are more influential, popular and cohesive. Thus,
our research explores the interesting facets of diffusion dynamics of hateful
users and broadens our understanding of hate speech in the online world.
",computer-science
"  Recent progress in deep learning for audio synthesis opens the way to models
that directly produce the waveform, shifting away from the traditional paradigm
of relying on vocoders or MIDI synthesizers for speech or music generation.
Despite their successes, current state-of-the-art neural audio synthesizers
such as WaveNet and SampleRNN suffer from prohibitive training and inference
times because they are based on autoregressive models that generate audio
samples one at a time at a rate of 16kHz. In this work, we study the more
computationally efficient alternative of generating the waveform frame-by-frame
with large strides. We present SING, a lightweight neural audio synthesizer for
the original task of generating musical notes given desired instrument, pitch
and velocity. Our model is trained end-to-end to generate notes from nearly
1000 instruments with a single decoder, thanks to a new loss function that
minimizes the distances between the log spectrograms of the generated and
target waveforms. On the generalization task of synthesizing notes for pairs of
pitch and instrument not seen during training, SING produces audio with
significantly improved perceptual quality compared to a state-of-the-art
autoencoder based on WaveNet as measured by a Mean Opinion Score (MOS), and is
about 32 times faster for training and 2, 500 times faster for inference.
",computer-science
"  In this work, we provide theoretical guarantees for reward decomposition in
deterministic MDPs. Reward decomposition is a special case of Hierarchical
Reinforcement Learning, that allows one to learn many policies in parallel and
combine them into a composite solution. Our approach builds on mapping this
problem into a Reward Discounted Traveling Salesman Problem, and then deriving
approximate solutions for it. In particular, we focus on approximate solutions
that are local, i.e., solutions that only observe information about the current
state. Local policies are easy to implement and do not require substantial
computational resources as they do not perform planning. While local
deterministic policies, like Nearest Neighbor, are being used in practice for
hierarchical reinforcement learning, we propose three stochastic policies that
guarantee better performance than any deterministic policy.
",statistics
"  There is a great need to stock materials for production, but storing
materials comes at a cost. Lack of organization in the inventory can result in
a very high cost for the final product, in addition to generating other
problems in the production chain. In this work we present mathematical and
statistical methods applicable to stock management. The stock analysis using
ABC curves serves to identify which are the priority items, the most expensive
and with the highest turnover (demand), and thus determine, through stock
control models, the purchase lot size and the periodicity that minimize the
total costs of storing these materials. Using the Economic Order Quantity (EOQ)
model and the (Q,R) model, the inventory costs of a company were minimized. The
comparison of the results provided by the models was performed.
",statistics
"  The atomic norm provides a generalization of the $\ell_1$-norm to continuous
parameter spaces. When applied as a sparse regularizer for line spectral
estimation the solution can be obtained by solving a convex optimization
problem. This problem is known as atomic norm soft thresholding (AST). It can
be cast as a semidefinite program and solved by standard methods. In the
semidefinite formulation there are $O(N^2)$ dual variables and a standard
primal-dual interior point method requires at least $O(N^6)$ flops per
iteration. That has lead researcher to consider alternating direction method of
multipliers (ADMM) for the solution of AST, but this method is still somewhat
slow for large problem sizes. To obtain a faster algorithm we reformulate AST
as a non-symmetric conic program. That has two properties of key importance to
its numerical solution: the conic formulation has only $O(N)$ dual variables
and the Toeplitz structure inherent to AST is preserved. Based on it we derive
FastAST which is a primal-dual interior point method for solving AST. Two
variants are considered with the fastest one requiring only $O(N^2)$ flops per
iteration. Extensive numerical experiments demonstrate that FastAST solves AST
significantly faster than a state-of-the-art solver based on ADMM.
",computer-science
"  In this paper we introduce a new mathematical model for the active
contraction of cardiac muscle, featuring different thermo-electric and
nonlinear conductivity properties. The passive hyperelastic response of the
tissue is described by an orthotropic exponential model, whereas the ionic
activity dictates active contraction incorporated through the concept of
orthotropic active strain. We use a fully incompressible formulation, and the
generated strain modifies directly the conductivity mechanisms in the medium
through the pull-back transformation. We also investigate the influence of
thermo-electric effects in the onset of multiphysics emergent spatiotemporal
dynamics, using nonlinear diffusion. It turns out that these ingredients have a
key role in reproducing pathological chaotic dynamics such as ventricular
fibrillation during inflammatory events, for instance. The specific structure
of the governing equations suggests to cast the problem in mixed-primal form
and we write it in terms of Kirchhoff stress, displacements, solid pressure,
electric potential, activation generation, and ionic variables. We also propose
a new mixed-primal finite element method for its numerical approximation, and
we use it to explore the properties of the model and to assess the importance
of coupling terms, by means of a few computational experiments in 3D.
",quantitative-biology
"  We study thick subcategories defined by modules of complexity one in
$\underline{\md}R$, where $R$ is the exterior algebra in $n+1$ indeterminates.
",mathematics
"  Enabling artificial agents to automatically learn complex, versatile and
high-performing behaviors is a long-lasting challenge. This paper presents a
step in this direction with hierarchical behavioral repertoires that stack
several behavioral repertoires to generate sophisticated behaviors. Each
repertoire of this architecture uses the lower repertoires to create complex
behaviors as sequences of simpler ones, while only the lowest repertoire
directly controls the agent's movements. This paper also introduces a novel
approach to automatically define behavioral descriptors thanks to an
unsupervised neural network that organizes the produced high-level behaviors.
The experiments show that the proposed architecture enables a robot to learn
how to draw digits in an unsupervised manner after having learned to draw lines
and arcs. Compared to traditional behavioral repertoires, the proposed
architecture reduces the dimensionality of the optimization problems by orders
of magnitude and provides behaviors with a twice better fitness. More
importantly, it enables the transfer of knowledge between robots: a
hierarchical repertoire evolved for a robotic arm to draw digits can be
transferred to a humanoid robot by simply changing the lowest layer of the
hierarchy. This enables the humanoid to draw digits although it has never been
trained for this task.
",computer-science
"  The ablation of solid tin surfaces by an 800-nanometer-wavelength laser is
studied for a pulse length range from 500 fs to 4.5 ps and a fluence range
spanning 0.9 to 22 J/cm^2. The ablation depth and volume are obtained employing
a high-numerical-aperture optical microscope, while the ion yield and energy
distributions are obtained from a set of Faraday cups set up under various
angles. We found a slight increase of the ion yield for an increasing pulse
length, while the ablation depth is slightly decreasing. The ablation volume
remained constant as a function of pulse length. The ablation depth follows a
two-region logarithmic dependence on the fluence, in agreement with the
available literature and theory. In the examined fluence range, the ion yield
angular distribution is sharply peaked along the target normal at low fluences
but rapidly broadens with increasing fluence. The total ionization fraction
increases monotonically with fluence to a 5-6% maximum, which is substantially
lower than the typical ionization fractions obtained with nanosecond-pulse
ablation. The angular distribution of the ions does not depend on the laser
pulse length within the measurement uncertainty. These results are of
particular interest for the possible utilization of fs-ps laser systems in
plasma sources of extreme ultraviolet light for nanolithography.
",physics
"  Octahedral tilting is most common distortion process observed in
centrosymmetric perovskite compounds (ABO$_{3}$). Indeed, crucial physical
properties of this oxide stem from the tilts of BO$_{6}$ rigid octahedra. In
microwave ceramics with perovskite-type structure, there is a close relation
between the temperature coefficient of resonant frequency and tilt system of
the perovskite structure. However, in many cases, limited access facilities are
needed to assign correctly the space group, including neutron scattering and
transmission electron microscopy. Here, we combine the Raman scattering and
group-theory calculations to probe the structural distortion in the perovskite
(Ba$_{1-x}$Sr$_{x}$)$_{3}$CaNb$_{2}$O$_{9}$ solid solution, which exhibits a
structural phase transition at $x$ $\geq$ 0.7, from D$_{3d}^{3}$ trigonal to
C$_{2h}^{3}$ monoclinic cell. Both phases are related by an octahedral tilting
distortion ($a^{0}b^{-}b^{-}$ in Glazer notation). Low temperature Raman
spectra corroborate the group-theoretical predictions for
Sr$_{3}$CaNb$_{2}$O$_{9}$ compound, since 36 modes detected at 25 K agree well
with those 42 (25A$_{g}$ $\oplus$ 17B$_{g}$) predicted ones.
",physics
"  Twin Support Vector Machines (TWSVMs) have emerged an efficient alternative
to Support Vector Machines (SVM) for learning from imbalanced datasets. The
TWSVM learns two non-parallel classifying hyperplanes by solving a couple of
smaller sized problems. However, it is unsuitable for large datasets, as it
involves matrix operations. In this paper, we discuss a Twin Neural Network
(Twin NN) architecture for learning from large unbalanced datasets. The Twin NN
also learns an optimal feature map, allowing for better discrimination between
classes. We also present an extension of this network architecture for
multiclass datasets. Results presented in the paper demonstrate that the Twin
NN generalizes well and scales well on large unbalanced datasets.
",computer-science
"  The famous pentagon identity for quantum dilogarithms has a generalization
for every Dynkin quiver, due to Reineke. A more advanced generalization is
associated with a pair of alternating Dynkin quivers, due to Keller. The
description and proof of Keller's identities involves cluster algebras and
cluster categories, and the statement of the identity is implicit. In this
paper we describe Keller's identities explicitly, and prove them by a dimension
counting argument. Namely, we consider quiver representations
$\boldsymbol{\mathrm{Rep}}_\gamma$ together with a superpotential function
$W_\gamma$, and calculate the Betti numbers of the equivariant $W_\gamma$ rapid
decay cohomology algebra of $\boldsymbol{\mathrm{Rep}}_\gamma$ in two different
ways corresponding to two natural stratifications of
$\boldsymbol{\mathrm{Rep}}_\gamma$. This approach is suggested by Kontsevich
and Soibelman in relation with the Cohomological Hall Algebra of quivers, and
the associated Donaldson-Thomas invariants.
",mathematics
"  We simplify the construction of projection complexes due to
Bestvina-Bromberg-Fujiwara. To do so, we introduce a sharper version of the
Behrstock inequality, and show that it can always be enforced. Furthermore, we
use the new setup to prove acylindricity results for the action on the
projection complexes. We also treat quasi-trees of metric spaces associated to
projection complexes, and prove an acylindricity criterion in that context as
well.
",mathematics
"  Changes to network structure can substantially affect when and how widely new
ideas, products, and conventions are adopted. In models of biological
contagion, interventions that randomly rewire edges (making them ""longer"")
accelerate spread. However, there are other models relevant to social
contagion, such as those motivated by myopic best-response in games with
strategic complements, in which individual's behavior is described by a
threshold number of adopting neighbors above which adoption occurs (i.e.,
complex contagions). Recent work has argued that highly clustered, rather than
random, networks facilitate spread of these complex contagions. Here we show
that minor modifications of prior analyses, which make them more realistic,
reverse this result. The modification is that we allow very rarely below
threshold adoption, i.e., very rarely adoption occurs, where there is only one
adopting neighbor. To model the trade-off between long and short edges we
consider networks that are the union of cycle-power-$k$ graphs and random
graphs on $n$ nodes. We study how the time to global spread changes as we
replace the cycle edges with (random) long ties. Allowing adoptions below
threshold to occur with order $1/\sqrt{n}$ probability is enough to ensure that
random rewiring accelerates spread. Simulations illustrate the robustness of
these results to other commonly-posited models for noisy best-response
behavior. We then examine empirical social networks, where we find that
hypothetical interventions that (a) randomly rewire existing edges or (b) add
random edges reduce time to spread compared with the original network or
addition of ""short"", triad-closing edges, respectively. This substantially
revises conclusions about how interventions change the spread of behavior,
suggesting that those wanting to increase spread should induce formation of
long ties, rather than triad-closing ties.
",computer-science
"  Finding the exact integrality gap $\alpha$ for the LP relaxation of the
metric Travelling Salesman Problem (TSP) has been an open problem for over
thirty years, with little progress made. It is known that $4/3 \leq \alpha \leq
3/2$, and a famous conjecture states $\alpha = 4/3$. For this problem,
essentially two ""fundamental"" classes of instances have been proposed. This
fundamental property means that in order to show that the integrality gap is at
most $\rho$ for all instances of metric TSP, it is sufficient to show it only
for the instances in the fundamental class. However, despite the importance and
the simplicity of such classes, no apparent effort has been deployed for
improving the integrality gap bounds for them. In this paper we take a natural
first step in this endeavour, and consider the $1/2$-integer points of one such
class. We successfully improve the upper bound for the integrality gap from
$3/2$ to $10/7$ for a superclass of these points, as well as prove a lower
bound of $4/3$ for the superclass. Our methods involve innovative applications
of tools from combinatorial optimization which have the potential to be more
broadly applied.
",computer-science
"  Pore space characteristics of biochars may vary depending on the used raw
material and processing technology. Pore structure has significant effects on
the water retention properties of biochar amended soils. In this work, several
biochars were characterized with three-dimensional imaging and image analysis.
X-ray computed microtomography was used to image biochars at resolution of 1.14
$\mu$m and the obtained images were analysed for porosity, pore-size
distribution, specific surface area and structural anisotropy. In addition,
random walk simulations were used to relate structural anisotropy to diffusive
transport. Image analysis showed that considerable part of the biochar volume
consist of pores in size range relevant to hydrological processes and storage
of plant available water. Porosity and pore-size distribution were found to
depend on the biochar type and the structural anisotopy analysis showed that
used raw material considerably affects the pore characteristics at micrometre
scale. Therefore attention should be paid to raw material selection and quality
in applications requiring optimized pore structure.
",physics
"  We determine which of the modular curves $X_\Delta(N)$, that is, curves lying
between $X_0(N)$ and $X_1(N)$, are bielliptic. Somewhat surprisingly, we find
that one of these curves has exceptional automorphisms. Finally we find all
$X_\Delta(N)$ that have infinitely many quadratic points over $\mathbb{Q}$.
",mathematics
"  We prove that certain conditions on multigraded Betti numbers of a simplicial
complex $K$ imply existence of a higher Massey product in cohomology of a
moment-angle-complex $\mathcal Z_K$, which contains a unique element (a
strictly defined product). Using the simplicial multiwedge construction, we
find a family $\mathcal{F}$ of polyhedral products being smooth closed
manifolds such that for any $l,r\geq 2$ there exists an $l$-connected manifold
$M\in\mathcal F$ with a nontrivial strictly defined $r$-fold Massey product in
$H^{*}(M)$. As an application to homological algebra, we determine a wide class
of triangulated spheres $K$ such that a nontrivial higher Massey product of any
order may exist in Koszul homology of their Stanley--Reisner rings. As an
application to rational homotopy theory, we establish a combinatorial criterion
for a simple graph $\Gamma$ to provide a (rationally) formal generalized
moment-angle manifold $\mathcal Z_{P}^{J}=(D^{2j_{i}},S^{2j_{i}-1})^{\partial
P^*}$, $J=(j_{1},\ldots,j_m)$ over a graph-associahedron $P=P_{\Gamma}$ and
compute all the diffeomorphism types of formal moment-angle manifolds over
graph-associahedra.
",mathematics
"  Wireless rechargeable sensor networks, consisting of sensor nodes with
rechargeable batteries and mobile chargers to replenish their batteries, have
gradually become a promising solution to the bottleneck of energy limitation
that hinders the wide deployment of wireless sensor networks (WSN). In this
paper, we focus on the mobile charger scheduling and path optimization scenario
in which the $k$-coverage ability of a network system needs to be maintained.
We formulate the optimal $k$-coverage charging problem of finding a feasible
path for a mobile charger to charge a set of sensor nodes within their
estimated charging time windows under the constraint of maintaining the
$k$-coverage ability of the network system, with an objective of minimizing the
energy consumption on traveling per tour. We show the hardness of the problem
that even finding a feasible path for the trivial case of the problem is an
NP-complete one with no polytime constant-factor approximation algorithm.
",computer-science
"  This work is motivated by a particular problem of a modern paper
manufacturing industry, in which maximum efficiency of the fiber-filler
recovery process is desired. A lot of unwanted materials along with valuable
fibers and fillers come out as a by-product of the paper manufacturing process
and mostly goes as waste. The job of an efficient Krofta supracell is to
separate the unwanted materials from the valuable ones so that fibers and
fillers can be collected from the waste materials and reused in the
manufacturing process. The efficiency of Krofta depends on several crucial
process parameters and monitoring them is a difficult proposition. To solve
this problem, we propose a novel hybridization of regression trees (RT) and
artificial neural networks (ANN), hybrid RT-ANN model, to solve the problem of
low recovery percentage of the supracell. This model is used to achieve the
goal of improving supracell efficiency, viz., gain in percentage recovery. In
addition, theoretical results for the universal consistency of the proposed
model are given with the optimal value of a vital model parameter. Experimental
findings show that the proposed hybrid RT-ANN model achieves higher accuracy in
predicting Krofta recovery percentage than other conventional regression models
for solving the Krofta efficiency problem. This work will help the paper
manufacturing company to become environmentally friendly with minimal
ecological damage and improved waste recovery.
",statistics
"  We simulate a rotating 2D BEC to study the melting of a vortex lattice in
presence of random impurities. Impurities are introduced either through a
protocol in which vortex lattice is produced in an impurity potential or first
creating the vortex lattice in the absence of random pinning and then cranking
up the (co-rotating) impurity potential. We find that for a fixed strength,
pinning of vortices at randomly distributed impurities leads to the new states
of vortex lattice. It is unearthed that the vortex lattice follow a two-step
melting via loss of positional and orientational order. Also, the comparisons
between the states obtained in two protocols show that the vortex lattice
states are metastable states when impurities are introduced after the formation
of an ordered vortex lattice. We also show the existence of metastable states
which depend on the history of how the vortex lattice is created.
",physics
"  Graphs are a commonly used construct for representing relationships between
elements in complex high dimensional datasets. Many real-world phenomenon are
dynamic in nature, meaning that any graph used to represent them is inherently
temporal. However, many of the machine learning models designed to capture
knowledge about the structure of these graphs ignore this rich temporal
information when creating representations of the graph. This results in models
which do not perform well when used to make predictions about the future state
of the graph -- especially when the delta between time stamps is not small. In
this work, we explore a novel training procedure and an associated unsupervised
model which creates graph representations optimised to predict the future state
of the graph. We make use of graph convolutional neural networks to encode the
graph into a latent representation, which we then use to train our temporal
offset reconstruction method, inspired by auto-encoders, to predict a later
time point -- multiple time steps into the future. Using our method, we
demonstrate superior performance for the task of future link prediction
compared with none-temporal state-of-the-art baselines. We show our approach to
be capable of outperforming non-temporal baselines by 38% on a real world
dataset.
",computer-science
"  We prove the orbifold version of Zvonkine's $r$-ELSV formula in two special
cases: the case of $r=2$ (complete $3$-cycles) for any genus $g\geq 0$ and the
case of any $r\geq 1$ for genus $g=0$.
",mathematics
"  We introduce multi-modal, attention-based neural machine translation (NMT)
models which incorporate visual features into different parts of both the
encoder and the decoder. We utilise global image features extracted using a
pre-trained convolutional neural network and incorporate them (i) as words in
the source sentence, (ii) to initialise the encoder hidden state, and (iii) as
additional data to initialise the decoder hidden state. In our experiments, we
evaluate how these different strategies to incorporate global image features
compare and which ones perform best. We also study the impact that adding
synthetic multi-modal, multilingual data brings and find that the additional
data have a positive impact on multi-modal models. We report new
state-of-the-art results and our best models also significantly improve on a
comparable phrase-based Statistical MT (PBSMT) model trained on the Multi30k
data set according to all metrics evaluated. To the best of our knowledge, it
is the first time a purely neural model significantly improves over a PBSMT
model on all metrics evaluated on this data set.
",computer-science
"  We propose a fast and accurate numerical method for pricing European
swaptions in multi-factor Gaussian term structure models. Our method can be
used to accelerate the calibration of such models to the volatility surface.
The pricing of an interest rate option in such a model involves evaluating a
multi-dimensional integral of the payoff of the claim on a domain where the
payoff is positive. In our method, we approximate the exercise boundary of the
state space by a hyperplane tangent to the maximum probability point on the
boundary and simplify the multi-dimensional integration into an analytical
form. The maximum probability point can be determined using the gradient
descent method. We demonstrate that our method is superior to previous methods
by comparing the results to the price obtained by numerical integration.
",quantitative-finance
"  A number of high-level languages and libraries have been proposed that offer
novel and simple to use abstractions for concurrent, asynchronous, and
distributed programming. The execution models that realise them, however, often
change over time---whether to improve performance, or to extend them to new
language features---potentially affecting behavioural and safety properties of
existing programs. This is exemplified by SCOOP, a message-passing approach to
concurrent object-oriented programming that has seen multiple changes proposed
and implemented, with demonstrable consequences for an idiomatic usage of its
core abstraction. We propose a semantics comparison workbench for SCOOP with
fully and semi-automatic tools for analysing and comparing the state spaces of
programs with respect to different execution models or semantics. We
demonstrate its use in checking the consistency of properties across semantics
by applying it to a set of representative programs, and highlighting a
deadlock-related discrepancy between the principal execution models of SCOOP.
Furthermore, we demonstrate the extensibility of the workbench by generalising
the formalisation of an execution model to support recently proposed extensions
for distributed programming. Our workbench is based on a modular and
parameterisable graph transformation semantics implemented in the GROOVE tool.
We discuss how graph transformations are leveraged to atomically model
intricate language abstractions, how the visual yet algebraic nature of the
model can be used to ascertain soundness, and highlight how the approach could
be applied to similar languages.
",computer-science
"  Let $f$ be a primitive cusp form of weight $k$ and level $N,$ let $\chi$ be a
Dirichlet character of conductor coprime with $N,$ and let
$\mathfrak{L}(f\otimes \chi, s)$ denote either $\log L(f\otimes \chi, s)$ or
$(L'/L)(f\otimes \chi, s).$ In this article we study the distribution of the
values of $\mathfrak{L}$ when either $\chi$ or $f$ vary. First, for a
quasi-character $\psi\colon \mathbb{C} \to \mathbb{C}^\times$ we find the limit
for the average $\mathrm{Avg}\_\chi \psi(L(f\otimes\chi, s)),$ when $f$ is
fixed and $\chi$ varies through the set of characters with prime conductor that
tends to infinity. Second, we prove an equidistribution result for the values
of $\mathfrak{L}(f\otimes \chi,s)$ by establishing analytic properties of the
above limit function. Third, we study the limit of the harmonic average
$\mathrm{Avg}^h\_f \psi(L(f, s)),$ when $f$ runs through the set of primitive
cusp forms of given weight $k$ and level $N\to \infty.$ Most of the results are
obtained conditionally on the Generalized Riemann Hypothesis for
$L(f\otimes\chi, s).$
",mathematics
"  We study Shimura curves of PEL type in $\mathsf{A}_g$ generically contained
in the Prym locus. We study both the unramified Prym locus, obtained using
étale double covers, and the ramified Prym locus, corresponding to double
covers ramified at two points. In both cases we consider the family of all
double covers compatible with a fixed group action on the base curve. We
restrict to the case where the family is 1-dimensional and the quotient of the
base curve by the group is $\mathbb{P}^1$. We give a simple criterion for the
image of these families under the Prym map to be a Shimura curve. Using
computer algebra we check all the examples gotten in this way up to genus 28.
We obtain 43 Shimura curves generically contained in the unramified Prym locus
and 9 families generically contained in the ramified Prym locus. Most of these
curves are not generically contained in the Jacobian locus.
",mathematics
"  Drivable free space information is vital for autonomous vehicles that have to
plan evasive maneuvers in real-time. In this paper, we present a new efficient
method for environmental free space detection with laser scanner based on 2D
occupancy grid maps (OGM) to be used for Advanced Driving Assistance Systems
(ADAS) and Collision Avoidance Systems (CAS). Firstly, we introduce an enhanced
inverse sensor model tailored for high-resolution laser scanners for building
OGM. It compensates the unreflected beams and deals with the ray casting to
grid cells accuracy and computational effort problems. Secondly, we introduce
the 'vehicle on a circle for grid maps' map alignment algorithm that allows
building more accurate local maps by avoiding the computationally expensive
inaccurate operations of image sub-pixel shifting and rotation. The resulted
grid map is more convenient for ADAS features than existing methods, as it
allows using less memory sizes, and hence, results into a better real-time
performance. Thirdly, we present an algorithm to detect what we call the
'in-sight edges'. These edges guarantee modeling the free space area with a
single polygon of a fixed number of vertices regardless the driving situation
and map complexity. The results from real world experiments show the
effectiveness of our approach.
",computer-science
"  In this paper we describe EasyInterface, an open-source toolkit for rapid
development of web-based graphical user interfaces (GUIs). This toolkit
addresses the need of researchers to make their research prototype tools
available to the community, and integrating them in a common environment,
rapidly and without being familiar with web programming or GUI libraries in
general. If a tool can be executed from a command-line and its output goes to
the standard output, then in few minutes one can make it accessible via a
web-interface or within Eclipse. Moreover, the toolkit defines a text-based
language that can be used to get more sophisticated GUIs, e.g., syntax
highlighting, dialog boxes, user interactions, etc. EasyInterface was
originally developed for building a common frontend for tools developed in the
Envisage project.
",computer-science
"  The best summary of a long video differs among different people due to its
highly subjective nature. Even for the same person, the best summary may change
with time or mood. In this paper, we introduce the task of generating
customized video summaries through simple text. First, we train a deep
architecture to effectively learn semantic embeddings of video frames by
leveraging the abundance of image-caption data via a progressive and residual
manner. Given a user-specific text description, our algorithm is able to select
semantically relevant video segments and produce a temporally aligned video
summary. In order to evaluate our textually customized video summaries, we
conduct experimental comparison with baseline methods that utilize ground-truth
information. Despite the challenging baselines, our method still manages to
show comparable or even exceeding performance. We also show that our method is
able to generate semantically diverse video summaries by only utilizing the
learned visual embeddings.
",computer-science
"  Some explanations to Kaldi's PLDA implementation to make formula derivation
easier to catch.
",statistics
"  Cold load pick-up (CLPU) has been a critical concern to utilities.
Researchers and industry practitioners have underlined the impact of CLPU on
distribution system design and service restoration. The recent large-scale
deployment of smart meters has provided the industry with a huge amount of data
that is highly granular, both temporally and spatially. In this paper, a
data-driven framework is proposed for assessing CLPU demand of residential
customers using smart meter data. The proposed framework consists of two
interconnected layers: 1) At the feeder level, a nonlinear auto-regression
model is applied to estimate the diversified demand during the system
restoration and calculate the CLPU demand ratio. 2) At the customer level,
Gaussian Mixture Models (GMM) and probabilistic reasoning are used to quantify
the CLPU demand increase. The proposed methodology has been verified using real
smart meter data and outage cases.
",computer-science
"  Strengthening or destroying a network is a very important issue in designing
resilient networks or in planning attacks against networks including planning
strategies to immunize a network against diseases, viruses etc.. Here we
develop a method for strengthening or destroying a random network with a
minimum cost. We assume a correlation between the cost required to strengthen
or destroy a node and the degree of the node. Accordingly, we define a cost
function c(k), which is the cost of strengthening or destroying a node with
degree k. Using the degrees $k$ in a network and the cost function c(k), we
develop a method for defining a list of priorities of degrees, and for choosing
the right group of degrees to be strengthened or destroyed that minimizes the
total price of strengthening or destroying the entire network. We find that the
list of priorities of degrees is universal and independent of the network's
degree distribution, for all kinds of random networks. The list of priorities
is the same for both strengthening a network and for destroying a network with
minimum cost. However, in spite of this similarity there is a difference
between their p_c - the critical fraction of nodes that has to be functional,
to guarantee the existence of a giant component in the network.
",physics
"  Chimera states, namely complex spatiotemporal patterns that consist of
coexisting domains of spatially coherent and incoherent dynamics, are
investigated in a network of coupled identical oscillators. These intriguing
spatiotemporal patterns were first reported in nonlocally coupled phase
oscillators, and it was shown that such mixed type behavior occurs only for
specific initial conditions in nonlocally and globally coupled networks. The
influence of initial conditions on chimera states has remained a fundamental
problem since their discovery. In this report, we investigate the robustness of
chimera states together with incoherent and coherent states in dependence on
the initial conditions. For this, we use the basin stability method which is
related to the volume of the basin of attraction, and we consider nonlocally
and globally coupled time-delayed Mackey-Glass oscillators as example.
Previously, it was shown that the existence of chimera states can be
characterized by mean phase velocity and a statistical measure, such as the
strength of incoherence, by using well prepared initial conditions. Here we
show further how the coexistence of different dynamical states can be
identified and quantified by means of the basin stability measure over a wide
range of the parameter space.
",physics
"  The paper focuses on considering some special precessional motions as the
spin motions, separating the octonion angular momentum of a proton into six
components, elucidating the proton angular momentum in the proton spin puzzle,
especially the proton spin, decomposition, quarks and gluons, and polarization
and so forth. J. C. Maxwell was the first to use the quaternions to study the
electromagnetic fields. Subsequently the complex octonions are utilized to
depict the electromagnetic field, gravitational field, and quantum mechanics
and so forth. In the complex octonion space, the precessional equilibrium
equation infers the angular velocity of precession. The external
electromagnetic strength may induce a new precessional motion, generating a new
term of angular momentum, even if the orbital angular momentum is zero. This
new term of angular momentum can be regarded as the spin angular momentum, and
its angular velocity of precession is different from the angular velocity of
revolution. The study reveals that the angular momentum of the proton must be
separated into more components than ever before. In the proton spin puzzle, the
orbital angular momentum and magnetic dipole moment are independent of each
other, and they should be measured and calculated respectively.
",physics
"  Strong disorder in interacting quantum systems can give rise to the
phenomenon of Many-Body Localization (MBL), which defies thermalization due to
the formation of an extensive number of quasi local integrals of motion. The
one particle operator content of these integrals of motion is related to the
one particle orbitals of the one particle density matrix and shows a strong
signature across the MBL transition as recently pointed out by Bera et al.
[Phys. Rev. Lett. 115, 046603 (2015); Ann. Phys. 529, 1600356 (2017)]. We study
the properties of the one particle orbitals of many-body eigenstates of an MBL
system in one dimension. Using shift-and-invert MPS (SIMPS), a matrix product
state method to target highly excited many-body eigenstates introduced in
[Phys. Rev. Lett. 118, 017201 (2017)], we are able to obtain accurate results
for large systems of sizes up to L = 64. We find that the one particle orbitals
drawn from eigenstates at different energy densities have high overlap and
their occupations are correlated with the energy of the eigenstates. Moreover,
the standard deviation of the inverse participation ratio of these orbitals is
maximal at the nose of the mobility edge. Also, the one particle orbitals decay
exponentially in real space, with a correlation length that increases at low
disorder. In addition, we find a 1/f distribution of the coupling constants of
a certain range of the number operators of the OPOs, which is related to their
exponential decay.
",physics
"  Deep learning has been demonstrated to achieve excellent results for image
classification and object detection. However, the impact of deep learning on
video analysis (e.g. action detection and recognition) has been limited due to
complexity of video data and lack of annotations. Previous convolutional neural
networks (CNN) based video action detection approaches usually consist of two
major steps: frame-level action proposal detection and association of proposals
across frames. Also, these methods employ two-stream CNN framework to handle
spatial and temporal feature separately. In this paper, we propose an
end-to-end deep network called Tube Convolutional Neural Network (T-CNN) for
action detection in videos. The proposed architecture is a unified network that
is able to recognize and localize action based on 3D convolution features. A
video is first divided into equal length clips and for each clip a set of tube
proposals are generated next based on 3D Convolutional Network (ConvNet)
features. Finally, the tube proposals of different clips are linked together
employing network flow and spatio-temporal action detection is performed using
these linked video proposals. Extensive experiments on several video datasets
demonstrate the superior performance of T-CNN for classifying and localizing
actions in both trimmed and untrimmed videos compared to state-of-the-arts.
",computer-science
"  We present the tomographic cross-correlation between galaxy lensing measured
in the Kilo Degree Survey (KiDS-450) with overlapping lensing measurements of
the cosmic microwave background (CMB), as detected by Planck 2015. We compare
our joint probe measurement to the theoretical expectation for a flat
$\Lambda$CDM cosmology, assuming the best-fitting cosmological parameters from
the KiDS-450 cosmic shear and Planck CMB analyses. We find that our results are
consistent within $1\sigma$ with the KiDS-450 cosmology, with an amplitude
re-scaling parameter $A_{\rm KiDS} = 0.86 \pm 0.19$. Adopting a Planck
cosmology, we find our results are consistent within $2\sigma$, with $A_{\it
Planck} = 0.68 \pm 0.15$. We show that the agreement is improved in both cases
when the contamination to the signal by intrinsic galaxy alignments is
accounted for, increasing $A$ by $\sim 0.1$. This is the first tomographic
analysis of the galaxy lensing -- CMB lensing cross-correlation signal, and is
based on five photometric redshift bins. We use this measurement as an
independent validation of the multiplicative shear calibration and of the
calibrated source redshift distribution at high redshifts. We find that
constraints on these two quantities are strongly correlated when obtained from
this technique, which should therefore not be considered as a stand-alone
competitive calibration tool.
",physics
"  Recently, the authors and de Wolff introduced the imaginary projection of a
polynomial $f\in\mathbb{C}[\mathbf{z}]$ as the projection of the variety of $f$
onto its imaginary part, $\mathcal{I}(f) \ = \ \{\text{Im}(\mathbf{z}) \, : \,
\mathbf{z} \in \mathcal{V}(f) \}$. Since a polynomial $f$ is stable if and only
if $\mathcal{I}(f) \cap \mathbb{R}_{>0}^n \ = \ \emptyset$, the notion offers a
novel geometric view underlying stability questions of polynomials. In this
article, we study the relation between the imaginary projections and
hyperbolicity cones, where the latter ones are only defined for homogeneous
polynomials. Building upon this, for homogeneous polynomials we provide a tight
upper bound for the number of components in the complement $\mathcal{I}(f)^{c}$
and thus for the number of hyperbolicity cones of $f$. And we show that for $n
\ge 2$, a polynomial $f$ in $n$ variables can have an arbitrarily high number
of strictly convex and bounded components in $\mathcal{I}(f)^{c}$.
",mathematics
"  Applications which use human speech as an input require a speech interface
with high recognition accuracy. The words or phrases in the recognised text are
annotated with a machine-understandable meaning and linked to knowledge graphs
for further processing by the target application. These semantic annotations of
recognised words can be represented as a subject-predicate-object triples which
collectively form a graph often referred to as a knowledge graph. This type of
knowledge representation facilitates to use speech interfaces with any spoken
input application, since the information is represented in logical, semantic
form, retrieving and storing can be followed using any web standard query
languages. In this work, we develop a methodology for linking speech input to
knowledge graphs and study the impact of recognition errors in the overall
process. We show that for a corpus with lower WER, the annotation and linking
of entities to the DBpedia knowledge graph is considerable. DBpedia Spotlight,
a tool to interlink text documents with the linked open data is used to link
the speech recognition output to the DBpedia knowledge graph. Such a
knowledge-based speech recognition interface is useful for applications such as
question answering or spoken dialog systems.
",computer-science
"  Certain systems of inviscid fluid dynamics have the property that for
solutions that are only slightly better than differentiable in Eulerian
variables, the corresponding Lagrangian trajectories are analytic in time. We
elucidate the mechanisms in fluid dynamics systems that give rise to this
automatic Lagrangian analyticity, as well as mechanisms in some particular
fluids systems which prevent it from occurring.
We give a conceptual argument for a general fluids model which shows that the
fulfillment of a basic set of criteria results in the analyticity of the
trajectory maps in time. We then apply this to the incompressible Euler
equations to prove analyticity of trajectories for vortex patch solutions. We
also use the method to prove the Lagrangian trajectories are analytic for
solutions to the pressureless Euler-Poisson equations, for initial data with
moderate regularity.
We then examine the compressible Euler equations, and find that the finite
speed of propagation in the system is incompatible with the Lagrangian
analyticity property. By taking advantage of this finite speed we are able to
construct smooth initial data with the property that some corresponding
Lagrangian trajectory is not analytic in time. We also study the Vlasov-Poisson
system, uncovering another mechanism that deters the analyticity of
trajectories. In this instance, we find that a key nonlocal operator does not
preserve analytic dependence in time. For this system we can also construct
smooth initial data for which the corresponding solution has some non-analytic
Lagrangian trajectory. This provides a counterexample to Lagrangian analyticity
for a system in which there is an infinite speed of propagation, unlike the
compressible Euler equations.
",mathematics
"  Societies are complex systems which tend to polarize into sub-groups of
individuals with dramatically opposite perspectives. This phenomenon is
reflected -- and often amplified -- in online social networks where, however,
humans are no more the only players, and co-exist alongside with social bots,
i.e. software-controlled accounts. Analyzing large-scale social data collected
during the Catalan referendum for independence on October 1 2017, consisting of
nearly 4 millions Twitter posts generated by almost 1 million users, we
identify the two polarized groups of Independentists and Constitutionalists and
quantify the structural and emotional roles played by social bots. We show that
bots act from peripheral areas of the social system to target influential
humans of both groups, mostly bombarding Independentists with negative and
violent contents, sustaining and inflating instability in this online society.
These results quantify the potential dangerous influence of political bots
during voting processes.
",computer-science
"  We show that if a noncollapsed $CD(K,n)$ space $X$ with $n\ge 2$ has
curvature bounded above by $\kappa$ in the sense of Alexandrov then $K\le
(n-1)\kappa$ and $X$ is an Alexandrov space of curvature bounded below by
$K-\kappa (n-2)$. We also show that if a $CD(K,n)$ space $Y$ with finite $n$
has curvature bounded above then it is infinitesimally Hilbertian.
",mathematics
"  We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where
an agent is spawned at a random location in a 3D environment and asked a
question (""What color is the car?""). In order to answer, the agent must first
intelligently navigate to explore the environment, gather information through
first-person (egocentric) vision, and then answer the question (""orange"").
This challenging task requires a range of AI skills -- active perception,
language understanding, goal-driven navigation, commonsense reasoning, and
grounding of language into actions. In this work, we develop the environments,
end-to-end-trained reinforcement learning agents, and evaluation protocols for
EmbodiedQA.
",computer-science
"  We present a novel view of nonlinear manifold learning using derivative-free
optimization techniques. Specifically, we propose an extension of the classical
multi-dimensional scaling (MDS) method, where instead of performing gradient
descent, we sample and evaluate possible ""moves"" in a sphere of fixed radius
for each point in the embedded space. A fixed-point convergence guarantee can
be shown by formulating the proposed algorithm as an instance of General
Pattern Search (GPS) framework. Evaluation on both clean and noisy synthetic
datasets shows that pattern search MDS can accurately infer the intrinsic
geometry of manifolds embedded in high-dimensional spaces. Additionally,
experiments on real data, even under noisy conditions, demonstrate that the
proposed pattern search MDS yields state-of-the-art results.
",statistics
"  It is well known, thanks to Lax-Wendroff theorem, that the local conservation
of a numerical scheme for a conservative hyperbolic system is a simple and
systematic way to guarantee that, if stable, a scheme will provide a sequence
of solutions that will converge to a weak solution of the continuous problem.
In [1], it is shown that a nonconservative scheme will not provide a good
solution. The question of using, nevertheless, a nonconservative formulation of
the system and getting the correct solution has been a long-standing debate. In
this paper, we show how get a relevant weak solution from a pressure-based
formulation of the Euler equations of fluid mechanics. This is useful when
dealing with nonlinear equations of state because it is easier to compute the
internal energy from the pressure than the opposite. This makes it possible to
get oscillation free solutions, contrarily to classical conservative methods.
An extension to multiphase flows is also discussed, as well as a
multidimensional extension.
",mathematics
"  Analysis of a Bayesian mixture model for the Matrix Langevin distribution on
the Stiefel manifold is presented. The model exploits a particular
parametrization of the Matrix Langevin distribution, various aspects of which
are elaborated on. A general, and novel, family of conjugate priors, and an
efficient Markov chain Monte Carlo (MCMC) sampling scheme for the corresponding
posteriors is then developed for the mixture model. Theoretical properties of
the prior and posterior distributions, including posterior consistency, are
explored in detail. Extensive simulation experiments are presented to validate
the efficacy of the framework. Real-world examples, including a large scale
neuroimaging dataset, are analyzed to demonstrate the computational
tractability of the approach.
",statistics
"  Vehicle-to-infrastructure (V2I) communication may provide high data rates to
vehicles via millimeter-wave (mmWave) microcellular networks. This paper uses
stochastic geometry to analyze the coverage of urban mmWave microcellular
networks. Prior work used a pathloss model with a line-of-sight probability
function based on randomly oriented buildings, to determine whether a link was
line-of-sight or non-line-of-sight. In this paper, we use a pathloss model
inspired by measurements, which uses a Manhattan distance pathloss model and
accounts for differences in pathloss exponents and losses when turning corners.
In our model, streets are randomly located as a Manhattan Poisson line process
(MPLP) and the base stations (BSs) are distributed according to a Poisson point
process. Our model is well suited for urban microcellular networks where the
BSs are deployed at street level. Based on this new approach, we derive the
coverage probability under certain BS association rules to obtain closed-form
solutions without much complexity. In addition, we draw two main conclusions
from our work. First, non-line-of-sight BSs are not a major benefit for
association or source of interference most of the time. Second, there is an
ultra-dense regime where deploying active BSs does not enhance coverage.
",computer-science
"  DFT is used throughout nanoscience, especially when modeling spin-dependent
properties that are important in spintronics. But standard quantum chemical
methods (both CCSD(T) and self-consistent semilocal density functional
calculations) fail badly for the spin adiabatic energy difference in Fe(II)
spin-crossover complexes. We show that all-electron fixed-node diffusion Monte
Carlo can be converged at significant computational cost, and that the B3LYP
single-determinant has sufficiently accurate nodes, providing benchmarks for
these systems. We also find that density-corrected DFT, using Hartree-Fock
densities (HF-DFT), greatly improves accuracy and reduces dependence on
approximations for these calculations. The small gap in the self-consistent DFT
calculations for the high-spin state is consistent with this. For the spin
adiabatic energy differences in these complexes, HF-DFT is both accurate and
reliable, and we make a strong prediction for the Fe-Porphyrin complex. The
""parameter-dilemma"" of needing different amounts of mixing for different
properties is eliminated by HF-DFT.
",physics
"  Recently, a wide range of smart devices are deployed in a variety of
environments to improve the quality of human life. One of the important
IoT-based applications is smart homes for healthcare, especially for elders.
IoT-based smart homes enable elders' health to be properly monitored and taken
care of. However, elders' privacy might be disclosed from smart homes due to
non-fully protected network communication or other reasons. To demonstrate how
serious this issue is, we introduce in this paper a Privacy Mining Approach
(PMA) to mine privacy from smart homes by conducting a series of deductions and
analyses on sensor datasets generated by smart homes. The experimental results
demonstrate that PMA is able to deduce a global sensor topology for a smart
home and disclose elders' privacy in terms of their house layouts.
",statistics
"  We report the first observation of the magnon-polariton bistability in a
cavity magnonics system consisting of cavity photons strongly interacting with
the magnons in a small yttrium iron garnet (YIG) sphere. The bistable behaviors
are emerged as sharp frequency switchings of the cavity magnon-polaritons
(CMPs) and related to the transition between states with large and small number
of polaritons. In our experiment, we align, respectively, the [100] and [110]
crystallographic axes of the YIG sphere parallel to the static magnetic field
and find very different bistable behaviors (e.g., clockwise and
counter-clockwise hysteresis loops) in these two cases. The experimental
results are well fitted and explained as being due to the Kerr nonlinearity
with either positive or negative coefficient. Moreover, when the magnetic field
is tuned away from the anticrossing point of CMPs, we observe simultaneous
bistability of both magnons and cavity photons by applying a drive field on the
lower branch.
",physics
"  The micro-local Gevrey regularity of a class of ""sums of squares"" with real
analytic coefficients is studied in detail. Some partial regularity result is
also given.
",mathematics
"  It is well known that the addition of noise in a multistable system can
induce random transitions between stable states. The rate of transition can be
characterised in terms of the noise-free system's dynamics and the added noise:
for potential systems in the presence of asymptotically low noise the
well-known Kramers' escape time gives an expression for the mean escape time.
This paper examines some general properties and examples of transitions between
local steady and oscillatory attractors within networks: the transition rates
at each node may be affected by the dynamics at other nodes. We use first
passage time theory to explain some properties of scalings noted in the
literature for an idealised model of initiation of epileptic seizures in small
systems of coupled bistable systems with both steady and oscillatory
attractors. We focus on the case of sequential escapes where a steady attractor
is only marginally stable but all nodes start in this state. As the nodes
escape to the oscillatory regime, we assume that the transitions back are very
infrequent in comparison. We quantify and characterise the resulting sequences
of noise-induced escapes. For weak enough coupling we show that a master
equation approach gives a good quantitative understanding of sequential
escapes, but for strong coupling this description breaks down.
",physics
"  Biomedical events describe complex interactions between various biomedical
entities. Event trigger is a word or a phrase which typically signifies the
occurrence of an event. Event trigger identification is an important first step
in all event extraction methods. However many of the current approaches either
rely on complex hand-crafted features or consider features only within a
window. In this paper we propose a method that takes the advantage of recurrent
neural network (RNN) to extract higher level features present across the
sentence. Thus hidden state representation of RNN along with word and entity
type embedding as features avoid relying on the complex hand-crafted features
generated using various NLP toolkits. Our experiments have shown to achieve
state-of-art F1-score on Multi Level Event Extraction (MLEE) corpus. We have
also performed category-wise analysis of the result and discussed the
importance of various features in trigger identification task.
",computer-science
"  An alternative to Density Functional Theory are wavefunction based electronic
structure calculations for solids. In order to perform them the Exponential
Wall (EW) problem has to be resolved. It is caused by an exponential increase
of the number of configurations with increasing electron number N. There are
different routes one may follow. One is to characterize a many-electron
wavefunction by a vector in Liouville space with a cumulant metric rather than
in Hilbert space. This removes the EW problem. Another is to model the solid by
an {\it impurity} or {\it fragment} embedded in a {\it bath} which is treated
at a much lower level than the former. This is the case in Density Matrix
Embedding Theory (DMET) or Density Embedding Theory (DET). The latter are
closely related to a Schmidt decomposition of a system and to the determination
of the associated entanglement. We show here the connection between the two
approaches. It turns out that the DMET (or DET) has an identical active space
as a previously used Local Ansatz, based on a projection and partitioning
approach. Yet, the EW problem is resolved differently in the two cases. By
studying a $H_{10}$ ring these differences are analyzed with the help of the
method of increments.
",physics
"  The admittance of two types of Josephson weak links is calculated, i.e., of a
one-dimensional superconducting wire with a local suppression of the order
parameter, and the second is a short S-c-S structure, where S denotes a
superconductor and c---a constriction. The systems of the first type are
analyzed on the basis of time-dependent Ginzburg-Landau equations. We show that
the impedance $Z(\Omega)$ has a maximum as a function of the frequency
$\Omega$, and the electric field $E_{\Omega}$ is determined by two
gauge-invariant quantities---the condensate momentum $Q_{\Omega}$ and the
potential $\mu$ related to charge imbalance. The structures of the second type
are studied on the basis of microscopic equations for quasiclassical Green's
functions in the Keldysh technique. For short S-c-S contacts (the Thouless
energy ${E_{\text{Th}} = D/L^{2} \gg \Delta}$) we present a formula for
admittance $Y$ valid at frequencies $\Omega$ and temperatures $T$ less than the
Thouless energy but arbitrary with respect to the energy gap $\Delta$. It is
shown that, at low temperatures, the absorption is absent [${\mathrm{Re}(Y) =
0}$] if the frequency does not exceed the energy gap in the center of the
constriction (${\Omega < \Delta \cos \varphi_{0}}$, where $2 \varphi_{0}$ is
the phase difference between the S reservoirs). The absorption gradually
increases with increasing the difference ${(\Omega - \Delta \cos \varphi_{0})}$
if $2 \varphi_{0}$ is less than the phase difference $2 \varphi_{\text{c}}$
corresponding to the critical Josephson current. In the interval ${2
\varphi_{\text{c}} < 2 \varphi_{0} < \pi}$, the absorption has a maximum. This
interval of the phase difference is achievable in phase-biased Josephson
junctions. Close to $T_{\text{c}}$ the admittance has a maximum at low $\Omega$
which is described by an analytical formula.
",physics
"  Bayesian nonparametrics are a class of probabilistic models in which the
model size is inferred from data. A recently developed methodology in this
field is small-variance asymptotic analysis, a mathematical technique for
deriving learning algorithms that capture much of the flexibility of Bayesian
nonparametric inference algorithms, but are simpler to implement and less
computationally expensive. Past work on small-variance analysis of Bayesian
nonparametric inference algorithms has exclusively considered batch models
trained on a single, static dataset, which are incapable of capturing time
evolution in the latent structure of the data. This work presents a
small-variance analysis of the maximum a posteriori filtering problem for a
temporally varying mixture model with a Markov dependence structure, which
captures temporally evolving clusters within a dataset. Two clustering
algorithms result from the analysis: D-Means, an iterative clustering algorithm
for linearly separable, spherical clusters; and SD-Means, a spectral clustering
algorithm derived from a kernelized, relaxed version of the clustering problem.
Empirical results from experiments demonstrate the advantages of using D-Means
and SD-Means over contemporary clustering algorithms, in terms of both
computational cost and clustering accuracy.
",statistics
"  We derive a priori estimates for the incompressible free-boundary Euler
equations with surface tension in three spatial dimensions. Working in
Lagrangian coordinates, we provide a priori estimates for the local existence
when the initial velocity, which is rotational, belongs to $H^3$ and the trace
of initial velocity on the free boundary to $H^{3.5}$, thus lowering the
requirement on the regularity of initial data in the Lagrangian setting. Our
methods are direct and involve three key elements: estimates for the pressure,
the boundary regularity provided by the mean curvature, and the Cauchy
invariance.
",mathematics
"  Manual annotations of temporal bounds for object interactions (i.e. start and
end times) are typical training input to recognition, localization and
detection algorithms. For three publicly available egocentric datasets, we
uncover inconsistencies in ground truth temporal bounds within and across
annotators and datasets. We systematically assess the robustness of
state-of-the-art approaches to changes in labeled temporal bounds, for object
interaction recognition. As boundaries are trespassed, a drop of up to 10% is
observed for both Improved Dense Trajectories and Two-Stream Convolutional
Neural Network.
We demonstrate that such disagreement stems from a limited understanding of
the distinct phases of an action, and propose annotating based on the Rubicon
Boundaries, inspired by a similarly named cognitive model, for consistent
temporal bounds of object interactions. Evaluated on a public dataset, we
report a 4% increase in overall accuracy, and an increase in accuracy for 55%
of classes when Rubicon Boundaries are used for temporal annotations.
",computer-science
"  In spite of decades of research, much remains to be discovered about folding:
the detailed structure of the initial (unfolded) state, vestigial folding
instructions remaining only in the unfolded state, the interaction of the
molecule with the solvent, instantaneous power at each point within the
molecule during folding, the fact that the process is stable in spite of myriad
possible disturbances, potential stabilization of trajectory by chaos, and, of
course, the exact physical mechanism (code or instructions) by which the
folding process is specified in the amino acid sequence. Simulations based upon
microscopic physics have had some spectacular successes and continue to
improve, particularly as super-computer capabilities increase. The simulations,
exciting as they are, are still too slow and expensive to deal with the
enormous number of molecules of interest. In this paper, we introduce an
approximate model based upon physics, empirics, and information science which
is proposed for use in machine learning applications in which very large
numbers of sub-simulations must be made. In particular, we focus upon machine
learning applications in the learning phase and argue that our model is
sufficiently close to the physics that, in spite of its approximate nature, can
facilitate stepping through machine learning solutions to explore the mechanics
of folding mentioned above. We particularly emphasize the exploration of energy
flow (power) within the molecule during folding, the possibility of energy
scale invariance (above a threshold), vestigial information in the unfolded
state as attractive targets for such machine language analysis, and statistical
analysis of an ensemble of folding micro-steps.
",quantitative-biology
"  mlpack is an open-source C++ machine learning library with an emphasis on
speed and flexibility. Since its original inception in 2007, it has grown to be
a large project implementing a wide variety of machine learning algorithms,
from standard techniques such as decision trees and logistic regression to
modern techniques such as deep neural networks as well as other
recently-published cutting-edge techniques not found in any other library.
mlpack is quite fast, with benchmarks showing mlpack outperforming other
libraries' implementations of the same methods. mlpack has an active community,
with contributors from around the world---including some from PUST. This short
paper describes the goals and design of mlpack, discusses how the open-source
community functions, and shows an example usage of mlpack for a simple data
science problem.
",computer-science
"  We explore the power of spatial context as a self-supervisory signal for
learning visual representations. In particular, we propose spatial context
networks that learn to predict a representation of one image patch from another
image patch, within the same image, conditioned on their real-valued relative
spatial offset. Unlike auto-encoders, that aim to encode and reconstruct
original image patches, our network aims to encode and reconstruct intermediate
representations of the spatially offset patches. As such, the network learns a
spatially conditioned contextual representation. By testing performance with
various patch selection mechanisms we show that focusing on object-centric
patches is important, and that using object proposal as a patch selection
mechanism leads to the highest improvement in performance. Further, unlike
auto-encoders, context encoders [21], or other forms of unsupervised feature
learning, we illustrate that contextual supervision (with pre-trained model
initialization) can improve on existing pre-trained model performance. We build
our spatial context networks on top of standard VGG_19 and CNN_M architectures
and, among other things, show that we can achieve improvements (with no
additional explicit supervision) over the original ImageNet pre-trained VGG_19
and CNN_M models in object categorization and detection on VOC2007.
",computer-science
"  Systems subject to uncertain inputs produce uncertain responses. Uncertainty
quantification (UQ) deals with the estimation of statistics of the system
response, given a computational model of the system and a probabilistic model
of its inputs. In engineering applications it is common to assume that the
inputs are mutually independent or coupled by a Gaussian or elliptical
dependence structure (copula). In this paper we overcome such limitations by
modelling the dependence structure of multivariate inputs as vine copulas. Vine
copulas are models of multivariate dependence built from simpler pair-copulas.
The vine representation is flexible enough to capture complex dependencies.
This paper formalises the framework needed to build vine copula models of
multivariate inputs and to combine them with virtually any UQ method. The
framework allows for a fully automated, data-driven inference of the
probabilistic input model on available input data. The procedure is exemplified
on two finite element models of truss structures, both subject to inputs with
non-Gaussian dependence structures. For each case, we analyse the moments of
the model response (using polynomial chaos expansions), and perform a
structural reliability analysis to calculate the probability of failure of the
system (using the first order reliability method and importance sampling).
Reference solutions are obtained by Monte Carlo simulation. The results show
that, while the Gaussian assumption yields biased statistics, the vine copula
representation achieves significantly more precise estimates, even when its
structure needs to be fully inferred from a limited amount of observations.
",statistics
"  We obtain a structure theorem for the group of holomorphic automorphisms of a
conformally Kähler, Einstein-Maxwell metric, extending the classical results
of Matsushima, Licherowicz and Calabi in the Kähler-Einstein, cscK, and
extremal Kähler cases. Combined with previous results of LeBrun,
Apostolov-Maschler and Futaki-Ono, this completes the classification of the
conformally Kähler, Einstein--Maxwell metrics on $\mathbb{CP}^1 \times
\mathbb{CP}^1$. We also use our result in order to introduce a (relative)
Mabuchi energy in the more general context of $(K, q, a)$-extremal Kähler
metrics in a given Kähler class, and show that the existence of $(K, q,
a)$-extremal Kähler metrics is stable under small deformation of the Kähler
class, the Killing vector field $K$ and the normalization constant $a$.
",mathematics
"  We investigate core-collapse supernova (CCSN) nucleosynthesis with
self-consistent, axisymmetric (2D) simulations performed using the
radiation-hydrodynamics code Chimera. Computational costs have traditionally
constrained the evolution of the nuclear composition within multidimensional
CCSN models to, at best, a 14-species $\alpha$-network capable of tracking only
$(\alpha,\gamma)$ reactions from $^{4}$He to $^{60}$Zn. Such a simplified
network limits the ability to accurately evolve detailed composition and
neutronization or calculate the nuclear energy generation rate. Lagrangian
tracer particles are commonly used to extend the nuclear network evolution by
incorporating more realistic networks in post-processing nucleosynthesis
calculations. However, limitations such as poor spatial resolution of the
tracer particles, inconsistent thermodynamic evolution, including misestimation
of expansion timescales, and uncertain determination of the multidimensional
mass-cut at the end of the simulation impose uncertainties inherent to this
approach. We present a detailed analysis of the impact of such uncertainties
for four self-consistent axisymmetric CCSN models initiated from stellar
metallicity, non-rotating progenitors of 12 $M_\odot$, 15 $M_\odot$, 20
$M_\odot$, and 25 $M_\odot$ and evolved with the smaller $\alpha$-network to
more than 1 s after the launch of an explosion.
",physics
"  Recently, G. A. Freiman, M. Herzog, P. Longobardi, M. Maj proved two
`structure theorems' for ordered groups \cite{FHLM}. We give elementary proof
of these two theorems.
",mathematics
"  The general procedure underlying Hartree-Fock and Kohn-Sham density
functional theory calculations consists in optimizing orbitals for a
self-consistent solution of the Roothaan-Hall equations in an iterative
process. It is often ignored that multiple self-consistent solutions can exist,
several of which may correspond to minima of the energy functional. In addition
to the difficulty sometimes encountered to converge the calculation to a
self-consistent solution, one must ensure that the correct self-consistent
solution was found, typically the one with the lowest electronic energy.
Convergence to an unwanted solution is in general not trivial to detect and
will deliver incorrect energy and molecular properties, and accordingly a
misleading description of chemical reactivity. Wrong conclusions based on
incorrect self-consistent field convergence are particularly cumbersome in
automated calculations met in high-throughput virtual screening, structure
optimizations, ab initio molecular dynamics, and in real-time explorations of
chemical reactivity, where the vast amount of data can hardly be manually
inspected. Here, we introduce a fast and automated approach to detect and cure
incorrect orbital convergence, which is especially suited for electronic
structure calculations on sequences of molecular structures. Our approach
consists of a randomized perturbation of the converged electron density
(matrix) intended to push orbital convergence to solutions that correspond to
another stationary point (of potentially lower electronic energy) in the
variational parameter space of an electronic wave function approximation.
",physics
"  In this paper we analyse the benefits of incorporating interval-valued fuzzy
sets into the Bousi-Prolog system. A syntax, declarative semantics and im-
plementation for this extension is presented and formalised. We show, by using
potential applications, that fuzzy logic programming frameworks enhanced with
them can correctly work together with lexical resources and ontologies in order
to improve their capabilities for knowledge representation and reasoning.
",computer-science
"  Automatic compiler phase selection/ordering has traditionally been focused on
CPUs and, to a lesser extent, FPGAs. We present experiments regarding compiler
phase ordering specialization of OpenCL kernels targeting a GPU. We use
iterative exploration to specialize LLVM phase orders on 15 OpenCL benchmarks
to an NVIDIA GPU. We analyze the generated NVIDIA PTX code for the various
versions to identify the main causes of the most significant improvements and
present results of a set of experiments that demonstrate the importance of
using specific phase orders. Using specialized compiler phase orders, we were
able to achieve geometric mean improvements of 1.54x (up to 5.48x) and 1.65x
(up to 5.7x) over PTX generated by the NVIDIA CUDA compiler from CUDA versions
of the same kernels, and over execution of the OpenCL kernels compiled from
source with the NVIDIA OpenCL driver, respectively. We also evaluate the use of
code-features in the OpenCL kernels. More specifically, we evaluate an approach
that achieves geometric mean improvements of 1.49x and 1.56x over the same
OpenCL baseline, by using the compiler sequences of the 1 or 3 most similar
benchmarks, respectively.
",computer-science
"  Fundamental questions on the nature of matter and energy have found answers
thanks to the use of particle accelerators. Societal applications, such as
cancer treatment or cancer imaging, illustrate the impact of accelerators in
our current life. Today, accelerators use metallic cavities that sustain
electricfields with values limited to about 100 MV/m. Because of their ability
to support extreme accelerating gradients, the plasma medium has recently been
proposed for future cavity-like accelerating structures. This contribution
highlights the tremendous evolution of plasma accelerators driven by either
laser or particle beams that allow the production of high quality particle
beams with a degree of tunability and a set of parameters that make them very
pertinent for many applications.
",physics
"  The competition between spin-orbit coupling, bandwidth ($W$) and
electron-electron interaction ($U$) makes iridates highly susceptible to small
external perturbations, which can trigger the onset of novel types of
electronic and magnetic states. Here we employ {\em first principles}
calculations based on density functional theory and on the constrained random
phase approximation to study how dimensionality and strain affect the strength
of $U$ and $W$ in (SrIrO$_3$)$_m$/(SrTiO$_3$) superlattices. The result is a
phase diagram explaining two different types of controllable magnetic and
electronic transitions, spin-flop and insulator-to-metal, connected with the
disruption of the $J_{eff}=1/2$ state which cannnot be understood within a
simplified local picture.
",physics
"  Recently, there is a series of reports by Wang et al. on the
superconductivity in K-doped p-terphenyl (KxC18H14) with the transition
temperatures range from 7 to 123 Kelvin. Identifying the structural and bonding
character is the key to understand the superconducting phases and the related
properties. Therefore we carried out an extensive study on the crystal
structures with different doping levels and investigate the thermodynamic
stability, structural, electronic, and magnetic properties by the
first-principles calculations. Our calculated structures capture most features
of the experimentally observed X-ray diffraction patterns. The K doping
concentration is constrained to within the range of 2 and 3. The obtained
formation energy indicates that the system at x = 2.5 is more stable. The
strong ionic bonding interaction is found in between K atoms and organic
molecules. The charge transfer accounts for the metallic feature of the doped
materials. For a small amount of charge transferred, the tilting force between
the two successive benzenes drives the system to stabilize at the
antiferromagnetic ground state, while the system exhibits non-magnetic behavior
with increasing charge transfer. The multiformity of band structures near the
Fermi level indicates that the driving force for superconductivity is
complicated.
",physics
"  Halide perovskite (HaP) semiconductors are revolutionizing photovoltaic (PV)
solar energy conversion by showing remarkable performance of solar cells made
with esp. tetragonal methylammonium lead tri-iodide (MAPbI3). In particular,
the low voltage loss of these cells implies a remarkably low recombination rate
of photogenerated carriers. It was suggested that low recombination can be due
to spatial separation of electrons and holes, a possibility if MAPbI3 is a
semiconducting ferroelectric, which, however, requires clear experimental
evidence. As a first step we show that, in operando, MAPbI3 (unlike MAPbBr3) is
pyroelectric, which implies it can be ferroelectric. The next step, proving it
is (not) ferroelectric, is challenging, because of the material s relatively
high electrical conductance (a consequence of an optical band gap suitable for
PV conversion!) and low stability under high applied bias voltage. This
excludes normal measurements of a ferroelectric hysteresis loop to prove
ferroelctricity s hallmark for switchable polarization. By adopting an approach
suitable for electrically leaky materials as MAPbI3, we show here ferroelectric
hysteresis from well-characterized single crystals at low temperature (still
within the tetragonal phase, which is the room temperature stable phase). Using
chemical etching, we also image polar domains, the structural fingerprint for
ferroelectricity, periodically stacked along the polar axis of the crystal,
which, as predicted by theory, scale with the overall crystal size. We also
succeeded in detecting clear second-harmonic generation, direct evidence for
the material s non-centrosymmetry. We note that the material s ferroelectric
nature, can, but not obviously need to be important in a PV cell, operating
around room temperature.
",physics
"  This paper studies optimal time-bounded control in multi-mode systems with
discrete costs. Multi-mode systems are an important subclass of linear hybrid
systems, in which there are no guards on transitions and all invariants are
global. Each state has a continuous cost attached to it, which is linear in the
sojourn time, while a discrete cost is attached to each transition taken. We
show that an optimal control for this model can be computed in NEXPTIME and
approximated in PSPACE. We also show that the one-dimensional case is simpler:
although the problem is NP-complete (and in LOGSPACE for an infinite time
horizon), we develop an FPTAS for finding an approximate solution.
",computer-science
"  We present a neural architecture that takes as input a 2D or 3D shape and
outputs a program that generates the shape. The instructions in our program are
based on constructive solid geometry principles, i.e., a set of boolean
operations on shape primitives defined recursively. Bottom-up techniques for
this shape parsing task rely on primitive detection and are inherently slow
since the search space over possible primitive combinations is large. In
contrast, our model uses a recurrent neural network that parses the input shape
in a top-down manner, which is significantly faster and yields a compact and
easy-to-interpret sequence of modeling instructions. Our model is also more
effective as a shape detector compared to existing state-of-the-art detection
techniques. We finally demonstrate that our network can be trained on novel
datasets without ground-truth program annotations through policy gradient
techniques.
",computer-science
"  Advanced satellite-based frequency transfers by TWCP and IPPP have been
performed between NICT and KRISS. We confirm that the disagreement between them
is less than 1x10^{-16} at an averaging time of several days. Additionally, an
intercontinental frequency ratio measurement of Sr and Yb optical lattice
clocks was directly performed by TWCP. We achieved an uncertainty at the
mid-10^{-16} level after a total measurement time of 12 hours. The frequency
ratio was consistent with the recently reported values within the uncertainty.
",physics
"  Statistical analyses of urban environments have been recently improved
through publicly available high resolution data and mapping technologies that
have been adopted across industries. These technologies allow us to create
metrics to empirically investigate urban design principles of the past
half-century. Philadelphia is an interesting case study for this work, with its
rapid urban development and population increase in the last decade. We outline
a data analysis pipeline for exploring the association between safety and local
neighborhood features such as population, economic health and the built
environment. As a particular example of our analysis pipeline, we focus on
quantitative measures of the built environment that serve as proxies for
vibrancy: the amount of human activity in a local area. Historically, vibrancy
has been very challenging to measure empirically. Measures based on land use
zoning are not an adequate description of local vibrancy and so we construct a
database and set of measures of business activity in each neighborhood. We
employ several matching analyses to explore the relationship between
neighborhood vibrancy and safety, such as comparing high crime versus low crime
locations within the same neighborhood. As additional sources of urban data
become available, our analysis pipeline can serve as the template for further
investigations into the relationships between safety, economic factors and the
built environment at the local neighborhood level.
",statistics
"  With the advent of public access to small gate-based quantum processors, it
becomes necessary to develop a benchmarking methodology such that independent
researchers can validate the operation of these processors. We explore the
usefulness of a number of simple quantum circuits as benchmarks for gate-based
quantum computing devices and show that circuits performing identity operations
are very simple, scalable and sensitive to gate errors and are therefore very
well suited for this task. We illustrate the procedure by presenting benchmark
results for the IBM Quantum Experience, a cloud-based platform for gate-based
quantum computing.
",physics
"  We study the multi-armed bandit problem where the rewards are realizations of
general non-stationary stochastic processes, a setting that generalizes many
existing lines of work and analyses. In particular, we present a theoretical
analysis and derive regret guarantees for rested bandits in which the reward
distribution of each arm changes only when we pull that arm. Remarkably, our
regret bounds are logarithmic in the number of rounds under several natural
conditions. We introduce a new algorithm based on classical UCB ideas combined
with the notion of weighted discrepancy, a useful tool for measuring the
non-stationarity of a stochastic process. We show that the notion of
discrepancy can be used to design very general algorithms and a unified
framework for the analysis of multi-armed rested bandit problems with
non-stationary rewards. In particular, we show that we can recover the regret
guarantees of many specific instances of bandit problems with non-stationary
rewards that have been studied in the literature. We also provide experiments
demonstrating that our algorithms can enjoy a significant improvement in
practice compared to standard benchmarks.
",computer-science
"  In the Sachdev-Ye-Kitaev model, we argue that the entanglement entropy of any
eigenstate (including the ground state) obeys a volume law, whose coefficient
can be calculated analytically from the energy and subsystem size. We expect
that the argument applies to a broader class of chaotic models with all-to-all
interactions.
",physics
"  Oral Disintegrating Tablets (ODTs) is a novel dosage form that can be
dissolved on the tongue within 3min or less especially for geriatric and
pediatric patients. Current ODT formulation studies usually rely on the
personal experience of pharmaceutical experts and trial-and-error in the
laboratory, which is inefficient and time-consuming. The aim of current
research was to establish the prediction model of ODT formulations with direct
compression process by Artificial Neural Network (ANN) and Deep Neural Network
(DNN) techniques. 145 formulation data were extracted from Web of Science. All
data sets were divided into three parts: training set (105 data), validation
set (20) and testing set (20). ANN and DNN were compared for the prediction of
the disintegrating time. The accuracy of the ANN model has reached 85.60%,
80.00% and 75.00% on the training set, validation set and testing set
respectively, whereas that of the DNN model was 85.60%, 85.00% and 80.00%,
respectively. Compared with the ANN, DNN showed the better prediction for ODT
formulations. It is the first time that deep neural network with the improved
dataset selection algorithm is applied to formulation prediction on small data.
The proposed predictive approach could evaluate the critical parameters about
quality control of formulation, and guide research and process development. The
implementation of this prediction model could effectively reduce drug product
development timeline and material usage, and proactively facilitate the
development of a robust drug product.
",statistics
"  We consider a large market model of defaultable assets in which the asset
price processes are modelled as Heston-type stochastic volatility models with
default upon hitting a lower boundary. We assume that both the asset prices and
their volatilities are correlated through systemic Brownian motions. We are
interested in the loss process that arises in this setting and we prove the
existence of a large portfolio limit for the empirical measure process of this
system. This limit evolves as a measure valued process and we show that it will
have a density given in terms of a solution to a stochastic partial
differential equation of filtering type in the two-dimensional half-space, with
a Dirichlet boundary condition. We employ Malliavin calculus to establish the
existence of a regular density for the volatility component, and an
approximation by models of piecewise constant volatilities combined with a
kernel smoothing technique to obtain existence and regularity for the full
two-dimensional filtering problem. We are able to establish good regularity
properties for solutions, however uniqueness remains an open problem.
",mathematics
"  The composition of web services is a promising approach enabling flexible and
loose integration of business applications. Numerous approaches related to web
services composition have been developed usually following three main phases:
the service discovery is based on the semantic description of advertised
services, i.e. the functionality of the service, meanwhile the service
selection is based on non- functional quality dimensions of service, and
finally the service composition aims to support an underlying process. Most of
those approaches explore techniques of static or dynamic design for an optimal
service composition. One important aspect so far is mostly neglected, focusing
on the output produced of composite web services. In this paper, in contrast to
many prominent approaches we introduce a data quality perspective on web
services. Based on a data quality management approach, we propose a framework
for analyzing data produced by the composite service execution. Utilising
process information together with data in service logs, our approach allows
identifying problems in service composition and execution. Analyzing the
service execution history our approach helps to improve common approaches of
service selection and composition.
",computer-science
"  State space models in which the system state is a finite set--called the
multi-object state--have generated considerable interest in recent years.
Smoothing for state space models provides better estimation performance than
filtering by using the full posterior rather than the filtering density. In
multi-object state estimation, the Bayes multi-object filtering recursion
admits an analytic solution known as the Generalized Labeled Multi-Bernoulli
(GLMB) filter. In this work, we extend the analytic GLMB recursion to propagate
the multi-object posterior. We also propose an implementation of this so-called
multi-scan GLMB posterior recursion using a similar approach to the GLMB filter
implementation.
",statistics
"  In spite of the close connection between the evaluation of quantified Boolean
formulas (QBF) and propositional satisfiability (SAT), tools and techniques
which exploit structural properties of SAT instances are known to fail for QBF.
This is especially true for the structural parameter treewidth, which has
allowed the design of successful algorithms for SAT but cannot be
straightforwardly applied to QBF since it does not take into account the
interdependencies between quantified variables.
In this work we introduce and develop dependency treewidth, a new structural
parameter based on treewidth which allows the efficient solution of QBF
instances. Dependency treewidth pushes the frontiers of tractability for QBF by
overcoming the limitations of previously introduced variants of treewidth for
QBF. We augment our results by developing algorithms for computing the
decompositions that are required to use the parameter.
",computer-science
"  Finding the dense regions of a graph and relations among them is a
fundamental problem in network analysis. Core and truss decompositions reveal
dense subgraphs with hierarchical relations. The incremental nature of
algorithms for computing these decompositions and the need for global
information at each step of the algorithm hinders scalable parallelization and
approximations since the densest regions are not revealed until the end. In a
previous work, Lu et al. proposed to iteratively compute the $h$-indices of
neighbor vertex degrees to obtain the core numbers and prove that the
convergence is obtained after a finite number of iterations. This work
generalizes the iterative $h$-index computation for truss decomposition as well
as nucleus decomposition which leverages higher-order structures to generalize
core and truss decompositions. In addition, we prove convergence bounds on the
number of iterations. We present a framework of local algorithms to obtain the
core, truss, and nucleus decompositions. Our algorithms are local, parallel,
offer high scalability, and enable approximations to explore time and quality
trade-offs. Our shared-memory implementation verifies the efficiency,
scalability, and effectiveness of our local algorithms on real-world networks.
",computer-science
"  This paper investigates to what extent cognitive biases may affect human
understanding of interpretable machine learning models, in particular of rules
discovered from data. Twenty cognitive biases are covered, as are possible
debiasing techniques that can be adopted by designers of machine learning
algorithms and software. Our review transfers results obtained in cognitive
psychology to the domain of machine learning, aiming to bridge the current gap
between these two areas. It needs to be followed by empirical studies
specifically aimed at the machine learning domain.
",statistics
"  Partial differential equations with random inputs have become popular models
to characterize physical systems with uncertainty coming from, e.g., imprecise
measurement and intrinsic randomness. In this paper, we perform asymptotic rare
event analysis for such elliptic PDEs with random inputs. In particular, we
consider the asymptotic regime that the noise level converges to zero
suggesting that the system uncertainty is low, but does exists. We develop
sharp approximations of the probability of a large class of rare events.
",mathematics
"  In this article we us the mean curvature flow with surgery to derive
regularity estimates going past Brakke regularity for the level set flow. We
also show a stability result for the plane under the level set flow.
",mathematics
"  The object of this paper is to study $\eta$-Ricci solitons on
$(\varepsilon)$-almost paracontact metric manifolds. We investigate
$\eta$-Ricci solitons in the case when its potential vector field is exactly
the characteristic vector field $\xi$ of the $(\varepsilon)$-almost paracontact
metric manifold and when the potential vector field is torse-forming. We also
study Einstein-like and $(\varepsilon)$-para Sasakian manifolds admitting
$\eta$-Ricci solitons. Finally we obtain some results for $\eta$-Ricci solitons
on $(\varepsilon)$-almost paracontact metric manifolds with a special view
towards parallel symmetric (0,2)-tensor fields.
",mathematics
"  Future observations of terrestrial exoplanet atmospheres will occur for
planets at different stages of geological evolution. We expect to observe a
wide variety of atmospheres and planets with alternative evolutionary paths,
with some planets resembling Earth at different epochs. For an Earth-like
atmospheric time trajectory, we simulate planets from prebiotic to current
atmosphere based on geological data. We use a stellar grid F0V to M8V
($T_\mathrm{eff}$ = 7000$\mskip3mu$K to 2400$\mskip3mu$K) to model four
geological epochs of Earth's history corresponding to a prebiotic world
(3.9$\mskip3mu$Ga), the rise of oxygen at 2.0$\mskip3mu$Ga and at
0.8$\mskip3mu$Ga, and the modern Earth. We show the VIS - IR spectral features,
with a focus on biosignatures through geological time for this grid of Sun-like
host stars and the effect of clouds on their spectra.
We find that the observability of biosignature gases reduces with increasing
cloud cover and increases with planetary age. The observability of the visible
O$_2$ feature for lower concentrations will partly depend on clouds, which
while slightly reducing the feature increase the overall reflectivity thus the
detectable flux of a planet. The depth of the IR ozone feature contributes
substantially to the opacity at lower oxygen concentrations especially for the
high near-UV stellar environments around F stars. Our results are a grid of
model spectra for atmospheres representative of Earth's geological history to
inform future observations and instrument design and are publicly available
online.
",physics
"  Context: Visual aesthetics is increasingly seen as an essential factor in
perceived usability, interaction, and overall appraisal of user interfaces
especially with respect to mobile applications. Yet, a question that remains is
how to assess and to which extend users agree on visual aesthetics. Objective:
This paper analyzes the inter-rater agreement on visual aesthetics of user
interfaces of Android apps as a basis for guidelines and evaluation models.
Method: We systematically collected ratings on the visual aesthetics of 100
user interfaces of Android apps from 10 participants and analyzed the frequency
distribution, reliability and influencing design aspects. Results: In general,
user interfaces of Android apps are perceived more ugly than beautiful. Yet,
raters only moderately agree on the visual aesthetics. Disagreements seem to be
related to subtle differences with respect to layout, shapes, colors,
typography, and background images. Conclusion: Visual aesthetics is a key
factor for the success of apps. However, the considerable disagreement of
raters on the perceived visual aesthetics indicates the need for a better
understanding of this software quality with respect to mobile apps.
",computer-science
"  We introduce the Self-Annotated Reddit Corpus (SARC), a large corpus for
sarcasm research and for training and evaluating systems for sarcasm detection.
The corpus has 1.3 million sarcastic statements -- 10 times more than any
previous dataset -- and many times more instances of non-sarcastic statements,
allowing for learning in both balanced and unbalanced label regimes. Each
statement is furthermore self-annotated -- sarcasm is labeled by the author,
not an independent annotator -- and provided with user, topic, and conversation
context. We evaluate the corpus for accuracy, construct benchmarks for sarcasm
detection, and evaluate baseline methods.
",computer-science
"  Determining the redshift distribution $n(z)$ of galaxy samples is essential
for several cosmological probes including weak lensing. For imaging surveys,
this is usually done using photometric redshifts estimated on an
object-by-object basis. We present a new approach for directly measuring the
global $n(z)$ of cosmological galaxy samples, including uncertainties, using
forward modeling. Our method relies on image simulations produced using UFig
(Ultra Fast Image Generator) and on ABC (Approximate Bayesian Computation)
within the $MCCL$ (Monte-Carlo Control Loops) framework. The galaxy population
is modeled using parametric forms for the luminosity functions, spectral energy
distributions, sizes and radial profiles of both blue and red galaxies. We
apply exactly the same analysis to the real data and to the simulated images,
which also include instrumental and observational effects. By adjusting the
parameters of the simulations, we derive a set of acceptable models that are
statistically consistent with the data. We then apply the same cuts to the
simulations that were used to construct the target galaxy sample in the real
data. The redshifts of the galaxies in the resulting simulated samples yield a
set of $n(z)$ distributions for the acceptable models. We demonstrate the
method by determining $n(z)$ for a cosmic shear like galaxy sample from the
4-band Subaru Suprime-Cam data in the COSMOS field. We also complement this
imaging data with a spectroscopic calibration sample from the VVDS survey. We
compare our resulting posterior $n(z)$ distributions to the one derived from
photometric redshifts estimated using 36 photometric bands in COSMOS and find
good agreement. This offers good prospects for applying our approach to current
and future large imaging surveys.
",physics
"  Scattering for the mass-critical fractional Schrödinger equation with a
cubic Hartree-type nonlinearity for initial data in a small ball in the
scale-invariant space of three-dimensional radial and square-integrable initial
data is established. For this, we prove a bilinear estimate for free solutions
and extend it to perturbations of bounded quadratic variation. This result is
shown to be sharp by proving the unboundedness of a third order derivative of
the flow map in the super-critical range.
",mathematics
"  Matrix divisors are introduced in the work by A.Weil (1938) which is
considered as a starting point of the theory of holomorphic vector bundles on
Riemann surfaces. In this theory matrix divisors play the role similar to the
role of usual divisors in the theory of line bundles. Moreover, they provide
explicit coordinates (Tyurin parameters) in an open subset of the moduli space
of stable vector bundles. These coordinates turned out to be helpful in
integration of soliton equations.
We would like to gain attention to one more relationship between matrix
divisors of vector G-bundles (where G is a complex semi-simple Lie group) and
the theory of integrable systems, namely to the relationship with Lax operator
algebras. The result we obtain can be briefly formulated as follows: the moduli
space of matrix divisors with certain discrete invariants and fixed support is
a homogeneous space. Its tangent space at the unit is naturally isomorphic to
the quotient space of M-operators by L-operators, both spaces essentially
defined by the same invariants (the result goes back to Krichever, 2001). We
give one more description of the same space in terms of root systems.
",mathematics
"  Automatic sleep staging is a challenging problem and state-of-the-art
algorithms have not yet reached satisfactory performance to be used instead of
manual scoring by a sleep technician. Much research has been done to find good
feature representations that extract the useful information to correctly
classify each epoch into the correct sleep stage. While many useful features
have been discovered, the amount of features have grown to an extent that a
feature reduction step is necessary in order to avoid the curse of
dimensionality. One reason for the need of such a large feature set is that
many features are good for discriminating only one of the sleep stages and are
less informative during other stages. This paper explores how a second feature
representation over a large set of pre-defined features can be learned using an
auto-encoder with a selective attention for the current sleep stage in the
training batch. This selective attention allows the model to learn feature
representations that focuses on the more relevant inputs without having to
perform any dimensionality reduction of the input data. The performance of the
proposed algorithm is evaluated on a large data set of polysomnography (PSG)
night recordings of patients with sleep-disordered breathing. The performance
of the auto-encoder with selective attention is compared with a regular
auto-encoder and previous works using a deep belief network (DBN).
",quantitative-biology
"  In the context of commutative differential graded algebras over $\mathbb Q$,
we show that an iteration of ""odd spherical fibration"" creates a ""total space""
commutative differential graded algebra with only odd degree cohomology. Then
we show for such a commutative differential graded algebra that, for any of its
""fibrations"" with ""fiber"" of finite cohomological dimension, the induced map on
cohomology is injective.
",mathematics
"  A basic goal in complexity theory is to understand the communication
complexity of number-on-the-forehead problems
$f\colon(\{0,1\}^n)^{k}\to\{0,1\}$ with $k\gg\log n$ parties. We study the
problems of inner product and set disjointness and determine their randomized
communication complexity for every $k\geq\log n$, showing in both cases that
$\Theta(1+\lceil\log n\rceil/\log\lceil1+k/\log n\rceil)$ bits are necessary
and sufficient. In particular, these problems admit constant-cost protocols if
and only if the number of parties is $k\geq n^{\epsilon}$ for some constant
$\epsilon>0.$
",computer-science
"  Computer science would not be the same without personal computers. In the
West the so called PC revolution started in the late '70s and has its roots in
hobbyists and do-it-yourself clubs. In the following years the diffusion of
home and personal computers has made the discipline closer to many people. A
bit later, to a lesser extent, yet in a similar way, the revolution took place
also in East European countries. Today, the scenario of personal computing has
completely changed, however the computers of the '80s are still objects of
fascination for a number of retrocomputing fans who enjoy using, programming
and hacking the old 8-bits. The paper highlights the continuity between
yesterday's hobbyists and today's retrocomputing enthusiasts, particularly
focusing on East European PCs. Besides the preservation of old hardware and
software, the community is engaged in the development of emulators and cross
compilers. Such tools can be used for historical investigation, for example to
trace the origins of the BASIC interpreters loaded in the ROMs of East European
PCs.
",computer-science
"  This paper presents the design of a nonlinear control law for a typical
electromagnetic actuator system. Electromagnetic actuators are widely
implemented in industrial applications, and especially as linear positioning
system. In this work, we aim at taking into account a magnetic phenomenon that
is usually neglected: flux fringing. This issue is addressed with an uncertain
modeling approach. The proposed control law consists of two steps, a
backstepping control regulates the mechanical part and a sliding mode approach
controls the coil current and the magnetic force implicitly. An illustrative
example shows the effectiveness of the presented approach.
",computer-science
"  We present an investigation into the intrinsic magnetic properties of the
compounds YCo5 and GdCo5, members of the RETM5 class of permanent magnets (RE =
rare earth, TM = transition metal). Focusing on Y and Gd provides direct
insight into both the TM magnetization and RE-TM interactions without the
complication of strong crystal field effects. We synthesize single crystals of
YCo5 and GdCo5 using the optical floating zone technique and measure the
magnetization from liquid helium temperatures up to 800 K. These measurements
are interpreted through calculations based on a Green's function formulation of
density-functional theory, treating the thermal disorder of the local magnetic
moments within the coherent potential approximation. The rise in the
magnetization of GdCo5 with temperature is shown to arise from a faster
disordering of the Gd magnetic moments compared to the antiferromagnetically
aligned Co sublattice. We use the calculations to analyze the different Curie
temperatures of the compounds and also compare the molecular (Weiss) fields at
the RE site with previously published neutron scattering experiments. To gain
further insight into the RE-TM interactions, we perform substitutional doping
on the TM site, studying the compounds RECo4.5Ni0.5, RECo4Ni, and RECo4.5Fe0.5.
Both our calculations and experiments on powdered samples find an
increased/decreased magnetization with Fe/Ni doping, respectively. The
calculations further reveal a pronounced dependence on the location of the
dopant atoms of both the Curie temperatures and the Weiss field at the RE site.
",physics
"  Light-shining-through-a-wall experiments represent a new experimental
approach in the search for undiscovered elementary particles not accessible
with accelerator based experiments. The next generation of these experiments,
such as ALPS~II, require high finesse, long baseline optical cavities with fast
length control. In this paper we report on a length stabilization control loop
used to keep a 9.2\,m cavity resonant. It achieves a unity-gain-frequency of
4\,kHz and actuates on a mirror with a diameter of 50.8\,mm. The finesse of
this cavity was measured to be 101,304$\pm$540 for 1064\,nm light. The
differential cavity length noise between 1064\,nm and 532\,nm light was also
measured since 532\,nm light will be used to sense the length of the
regeneration cavity. Out-of-loop noise sources and different control strategies
are discussed, in order to fulfill the length stability requirements for
ALPS~II.
",physics
"  This paper develops a randomized approach for incrementally building deep
neural networks, where a supervisory mechanism is proposed to constrain the
random assignment of the weights and biases, and all the hidden layers have
direct links to the output layer. A fundamental result on the universal
approximation property is established for such a class of randomized leaner
models, namely deep stochastic configuration networks (DeepSCNs). A learning
algorithm is presented to implement DeepSCNs with either specific architecture
or self-organization. The read-out weights attached with all direct links from
each hidden layer to the output layer are evaluated by the least squares
method. Given a set of training examples, DeepSCNs can speedily produce a
learning representation, that is, a collection of random basis functions with
the cascaded inputs together with the read-out weights. An empirical study on a
function approximation is carried out to demonstrate some properties of the
proposed deep learner model.
",computer-science
"  We study the dynamics of the Bogoliubov wave packet in superconductors and
calculate the supercurrent carried by the wave packet. We discover an anomalous
contribution to the supercurrent, related to the quantum metric of the Bloch
wave function. This anomalous contribution is most important for flat or
quasiflat bands, as exemplified by the attractive Hubbard models on the Creutz
ladder and sawtooth lattice. Our theoretical framework is general and can be
used to study a wide variety of phenomena, such as spin transport and exciton
transport.
",physics
"  We use globular cluster kinematics data, primarily from the SLUGGS survey, to
measure the dark matter fraction ($f_{\rm DM}$) and the average dark matter
density ($\left< \rho_{\rm DM} \right>$) within the inner 5 effective radii
($R_{\rm e}$) for 32 nearby early--type galaxies (ETGs) with stellar mass log
$(M_*/\rm M_\odot)$ ranging from $10.1$ to $11.8$. We compare our results with
a simple galaxy model based on scaling relations as well as with cosmological
hydrodynamical simulations where the dark matter profile has been modified
through various physical processes.
We find a high $f_{\rm DM}$ ($\geq0.6$) within 5~$R_{\rm e}$ in most of our
sample, which we interpret as a signature of a late mass assembly history that
is largely devoid of gas-rich major mergers. However, around log $(M_*/M_\odot)
\sim 11$, there is a wide range of $f_{\rm DM}$ which may be challenging to
explain with any single cosmological model. We find tentative evidence that
lenticulars (S0s), unlike ellipticals, have mass distributions that are similar
to spiral galaxies, with decreasing $f_{\rm DM}$ within 5~$R_{\rm e}$ as galaxy
luminosity increases. However, we do not find any difference between the
$\left< \rho_{\rm DM} \right>$ of S0s and ellipticals in our sample, despite
the differences in their stellar populations. We have also used $\left<
\rho_{\rm DM} \right>$ to infer the epoch of halo assembly ($z{\sim}2-4$). By
comparing the age of their central stars with the inferred epoch of halo
formation, we are able to gain more insight into their mass assembly histories.
Our results suggest a fundamental difference in the dominant late-phase mass
assembly channel between lenticulars and elliptical galaxies.
",physics
"  Bose-Einstein condensates with tunable interatomic interactions have been
studied intensely in recent experiments. The investigation of the collapse of a
condensate following a sudden change in the nature of the interaction from
repulsive to attractive has led to the observation of a remnant condensate that
did not undergo further collapse. We suggest that this high-density remnant is
in fact the absolute minimum of the energy, if the attractive atomic
interactions are nonlocal, and is therefore inherently stable. We show that a
variational trial function consisting of a superposition of two distinct
gaussians is an accurate representation of the wavefunction of the ground state
of the conventional local Gross-Pitaevskii field equation for an attractive
condensate and gives correctly the points of emergence of instability. We then
use such a superposition of two gaussians as a variational trial function in
order to calculate the minima of the energy when it includes a nonlocal
interaction term. We use experimental data in order to study the long range of
the nonlocal interaction, showing that they agree very well with a
dimensionally derived expression for this range.
",physics
"  The use of Laplacian eigenfunctions is ubiquitous in a wide range of computer
graphics and geometry processing applications. In particular, Laplacian
eigenbases allow generalizing the classical Fourier analysis to manifolds. A
key drawback of such bases is their inherently global nature, as the Laplacian
eigenfunctions carry geometric and topological structure of the entire
manifold. In this paper, we introduce a new framework for local spectral shape
analysis. We show how to efficiently construct localized orthogonal bases by
solving an optimization problem that in turn can be posed as the
eigendecomposition of a new operator obtained by a modification of the standard
Laplacian. We study the theoretical and computational aspects of the proposed
framework and showcase our new construction on the classical problems of shape
approximation and correspondence. We obtain significant improvement compared to
classical Laplacian eigenbases as well as other alternatives for constructing
localized bases.
",computer-science
"  We prove generalized weighted Ostrowski and Ostrowski--Grüss type
inequalities on time scales via a parameter function. In particular, our result
extends a result of Dragomir and Barnett. Furthermore, we apply our results to
the continuous, discrete, and quantum cases, to obtain some interesting new
inequalities.
",mathematics
"  In this paper, we propose a novel method to register football broadcast video
frames on the static top view model of the playing surface. The proposed method
is fully automatic in contrast to the current state of the art which requires
manual initialization of point correspondences between the image and the static
model. Automatic registration using existing approaches has been difficult due
to the lack of sufficient point correspondences. We investigate an alternate
approach exploiting the edge information from the line markings on the field.
We formulate the registration problem as a nearest neighbour search over a
synthetically generated dictionary of edge map and homography pairs. The
synthetic dictionary generation allows us to exhaustively cover a wide variety
of camera angles and positions and reduce this problem to a minimal per-frame
edge map matching procedure. We show that the per-frame results can be improved
in videos using an optimization framework for temporal camera stabilization. We
demonstrate the efficacy of our approach by presenting extensive results on a
dataset collected from matches of football World Cup 2014.
",computer-science
"  We present the characteristics and the performance of the new CCD camera
system, SNUCAM-II (Seoul National University CAMera system II) that was
installed on the Lee Sang Gak Telescope (LSGT) at the Siding Spring Observatory
in 2016. SNUCAM-II consists of a deep depletion chip covering a wide wavelength
from 0.3 {\mu}m to 1.1 {\mu}m with high sensitivity (QE at > 80% over 0.4 to
0.9 {\mu}m). It is equipped with the SDSS ugriz filters and 13 medium band
width (50 nm) filters, enabling us to study spectral energy distributions
(SEDs) of diverse objects from extragalactic sources to solar system objects.
On LSGT, SNUCAM-II offers 15.7 {\times} 15.7 arcmin field-of-view (FOV) at a
pixel scale of 0.92 arcsec and a limiting magnitude of g = 19.91 AB mag and
z=18.20 AB mag at 5{\sigma} with 180 sec exposure time for point source
detection.
",physics
"  We report the results of a pilot program to use the Magellan/M2FS
spectrograph to survey the galactic populations and internal kinematics of
galaxy clusters. For this initial study, we present spectroscopic measurements
for $223$ quiescent galaxies observed along the line of sight to the galaxy
cluster Abell 267 ($z\sim0.23$). We develop a Bayesian method for modeling the
integrated light from each galaxy as a simple stellar population, with free
parameters that specify redshift ($v_\mathrm{los}/c$) and characteristic age,
metallicity ($\mathrm{[Fe/H]}$), alpha-abundance ($[\alpha/\mathrm{Fe}]$), and
internal velocity dispersion ($\sigma_\mathrm{int}$) for individual galaxies.
Parameter estimates derived from our 1.5-hour observation of A267 have median
random errors of $\sigma_{v_\mathrm{los}}=20\ \mathrm{km\ s^{-1}}$,
$\sigma_{\mathrm{Age}}=1.2\ \mathrm{Gyr}$, $\sigma_{\mathrm{[Fe/H]}}=0.11\
\mathrm{dex}$, $\sigma_{[\alpha/\mathrm{Fe}]}=0.07\ \mathrm{dex}$, and
$\sigma_{\sigma_\mathrm{int}}=20\ \mathrm{km\ s^{-1}}$. In a companion paper,
we use these results to model the structure and internal kinematics of A267.
",physics
"  A problem of classification of local field potentials (LFPs), recorded from
the prefrontal cortex of a macaque monkey, is considered. An adult macaque
monkey is trained to perform a memory-based saccade. The objective is to decode
the eye movement goals from the LFP collected during a memory period. The LFP
classification problem is modeled as that of classification of smooth functions
embedded in Gaussian noise. It is then argued that using minimax function
estimators as features would lead to consistent LFP classifiers. The theory of
Gaussian sequence models allows us to represent minimax estimators as finite
dimensional objects. The LFP classifier resulting from this mathematical
endeavor is a spectrum based technique, where Fourier series coefficients of
the LFP data, followed by appropriate shrinkage and thresholding, are used as
features in a linear discriminant classifier. The classifier is then applied to
the LFP data to achieve high decoding accuracy. The function classification
approach taken in the paper also provides a systematic justification for using
Fourier series, with shrinkage and thresholding, as features for the problem,
as opposed to using the power spectrum. It also suggests that phase information
is crucial to the decision making.
",statistics
"  We introduce torchbearer, a model fitting library for pytorch aimed at
researchers working on deep learning or differentiable programming. The
torchbearer library provides a high level metric and callback API that can be
used for a wide range of applications. We also include a series of built in
callbacks that can be used for: model persistence, learning rate decay,
logging, data visualization and more. The extensive documentation includes an
example library for deep learning and dynamic programming problems and can be
found at this http URL. The code is licensed under the MIT
License and available at this https URL.
",statistics
"  We optimized the substrate temperature (Ts) and phosphorus concentration (x)
of BaFe2(As1-xPx)2 films on practical metal-tape substrates for pulsed laser
deposition from the viewpoints of crystallinity, superconductor critical
temperature (Tc), and critical current density (Jc). It was found that the
optimum Ts and x values are 1050 degree C and x = 0.28, respectively. The
optimized film exhibits Tc_onset = 26.6 and Tc_zero = 22.4 K along with a high
self-field Jc at 4 K (~1 MA/cm2) and relatively isotropic Jc under magnetic
fields up to 9 T. Unexpectedly, we found that lower crystallinity samples,
which were grown at a higher Ts of 1250 degree C than the optimized Ts = 1050
degree C, exhibit higher Jc along the ab plane under high magnetic fields than
the optimized samples. The presence of horizontal defects that act as strong
vortex pinning centers, such as stacking faults, are a possible origin of the
high Jc values in the poor crystallinity samples.
",physics
"  According to magnetohydrodynamics (MHD), the encounter of two collisional
magnetized plasmas at high velocity gives rise to shock waves. Investigations
conducted so far have found that the same conclusion still holds in the case of
collisionless plasmas. For the case of a flow-aligned field, MHD stipulates
that the field and the fluid are disconnected, so that the shock produced is
independent of the field. We present a violation of this MHD prediction when
considering the encounter of two cold pair plasmas along a flow-aligned
magnetic field. As the guiding magnetic field grows, isotropization is
progressively suppressed, resulting in a strong influence of the field on the
resulting structure. A micro-physics analysis allows to understand the
mechanisms at work. Particle-in-cell simulations also support our conclusions
and show that the results are not restricted to a strictly parallel field.
",physics
"  In the preceding paper (Efroimsky 2017), we derived an expression for the
tidal dissipation rate in a homogeneous near-spherical Maxwell body librating
in longitude. Now, by equating this expression to the outgoing energy flux due
to the vapour plumes, we estimate the mean tidal viscosity of Enceladus, under
the assumption that the Enceladean mantle behaviour is Maxwell. This method
yields a value of $\,0.24\times 10^{14}\;\mbox{Pa~s}\,$ for the mean tidal
viscosity, which is very close to the viscosity of ice near the melting point.
",physics
"  We consider the problem of estimating species trees from unrooted gene tree
topologies in the presence of incomplete lineage sorting, a common phenomenon
that creates gene tree heterogeneity in multilocus datasets. One popular class
of reconstruction methods in this setting is based on internode distances, i.e.
the average graph distance between pairs of species across gene trees. While
statistical consistency in the limit of large numbers of loci has been
established in some cases, little is known about the sample complexity of such
methods. Here we make progress on this question by deriving a lower bound on
the worst-case variance of internode distance which depends linearly on the
corresponding graph distance in the species tree. We also discuss some
algorithmic implications.
",quantitative-biology
"  We develop a commuting vector field method for a general class of radiating
spacetimes. The metrics considered are certain long range perturbations of
Minkowski space including those constructed from global stability problems in
general relativity. Our method provides sharp peeling estimates for solutions
to both linear and nonlinear (null form) scalar fields.
",mathematics
"  Security-critical tasks require proper isolation from untrusted software.
Chip manufacturers design and include trusted execution environments (TEEs) in
their processors to secure these tasks. The integrity and security of the
software in the trusted environment depend on the verification process of the
system.
We find a form of attack that can be performed on the current implementations
of the widely deployed ARM TrustZone technology. The attack exploits the fact
that the trustlet (TA) or TrustZone OS loading verification procedure may use
the same verification key and may lack proper rollback prevention across
versions. If an exploit works on an out-of-date version, but the vulnerability
is patched on the latest version, an attacker can still use the same exploit to
compromise the latest system by downgrading the software to an older and
exploitable version.
We did experiments on popular devices on the market including those from
Google, Samsung and Huawei, and found that all of them have the risk of being
attacked. Also, we show a real-world example to exploit Qualcomm's QSEE.
In addition, in order to find out which device images share the same
verification key, pattern matching schemes for different vendors are analyzed
and summarized.
",computer-science
"  Among the proposals for joint disease mapping, the shared component model has
become more popular. Another recent advance to strengthen inference of disease
data has been the extension of purely spatial models to include time and
space-time interaction. Such analyses have additional benefits over purely
spatial models. However, only a few proposed spatio-temporal models could
address analysing multiple diseases jointly.
In the proposed model, each component is shared by different subsets of
diseases, spatial and temporal trends are considered for each component, and
the relative weight of these trends for each component for each relevant
disease can be estimated. We present an application of the proposed method on
incidence rates of seven prevalent cancers in Iran. The effect of the shared
components on the individual cancer types can be identified. Regional and
temporal variation in relative risks is shown. We present a model which
combines the benefits of shared-components with spatio-temporal techniques for
multivariate data. We show, how the model allows to analyse geographical and
temporal variation among diseases beyond previous approaches.
",statistics
"  Rogue waves, and their periodic counterparts, have been shown to exist in a
number of integrable models. However, relatively little is known about the
existence of these objects in models where an exact formula is unattainable. In
this work, we develop a novel numerical perspective towards identifying such
states as localized solutions in space-time. Importantly, we illustrate that
this methodology in addition to benchmarking known solutions (and confirming
their numerical propagation under controllable error) enables the continuation
of such solutions over parametric variations to non-integrable models. As a
result, we can answer in the positive the question about the parametric
robustness of Peregrine-like waveforms and even of generalizations thereof on a
cnoidal wave background.
",physics
"  We construct two infinite series of Moufang loops of exponent $3$ whose
commutative center (i.e. the set of elements that commute with all elements of
the loop) is not a normal subloop. In particular, we obtain examples of such
loops of orders $3^8$ and $3^{11}$ one of which can be defined as the Moufang
triplication of the free Burnside group $B(3,3)$.
",mathematics
"  We propose the use of specific dynamical processes and more in general of
ideas from Physics to model the evolution in time of musical structures. We
apply this approach to two Études by F. Chopin, namely op.10 n.3 and op.25
n.1, proposing some original description based on concepts of symmetry
breaking/restoration and quantum coherence, which could be useful for
interpretation. In this analysis, we take advantage of colored musical scores,
obtained by implementing Scriabin's color code for sounds to musical notation.
",physics
"  The coupled exciton-vibrational dynamics of a three-site model of the FMO
complex is investigated using the Multi-layer Multi-configuration
Time-dependent Hartree (ML-MCTDH) approach. Emphasis is put on the effect of
the spectral density on the exciton state populations as well as on the
vibrational and vibronic non-equilibrium excitations. Models which use either a
single or site-specific spectral densities are contrasted to a spectral density
adapted from experiment. For the transfer efficiency, the total integrated
Huang-Rhys factor is found to be more important than details of the spectral
distributions. However, the latter are relevant for the obtained
non-equilibrium vibrational and vibronic distributions and thus influence the
actual pattern of population relaxation.
",physics
"  We extend the classic convergence rate theory for subgradient methods to
apply to non-Lipschitz functions. For the deterministic projected subgradient
method, we present a global $O(1/\sqrt{T})$ convergence rate for any convex
function which is locally Lipschitz around its minimizers. This approach is
based on Shor's classic subgradient analysis and implies generalizations of the
standard convergence rates for gradient descent on functions with Lipschitz or
Hölder continuous gradients. Further, we show a $O(1/\sqrt{T})$ convergence
rate for the stochastic projected subgradient method on convex functions with
at most quadratic growth, which improves to $O(1/T)$ under either strong
convexity or a weaker quadratic lower bound condition.
",computer-science
"  It is known that the essential spectrum of a Schrödinger operator $H$ on
$\ell^{2}\left(\mathbb{N}\right)$ is equal to the union of the spectra of right
limits of $H$. The natural generalization of this relation to $\mathbb{Z}^{n}$
is known to hold as well. In this paper we generalize the notion of right
limits to general infinite connected graphs and construct examples of graphs
for which the essential spectrum of the Laplacian is strictly bigger than the
union of the spectra of its right limits. As these right limits are trees, this
result is complemented by the fact that the equality still holds for general
bounded operators on regular trees. We prove this and characterize the
essential spectrum in the spherically symmetric case.
",mathematics
"  We study the class of rings $R$ with the property that for $x\in R$ at least
one of the elements $x$ and $1+x$ are tripotent.
",mathematics
"  In this paper, we present promising accurate prefix boosting (PAPB), a
discriminative training technique for attention based sequence-to-sequence
(seq2seq) ASR. PAPB is devised to unify the training and testing scheme in an
effective manner. The training procedure involves maximizing the score of each
partial correct sequence obtained during beam search compared to other
hypotheses. The training objective also includes minimization of token
(character) error rate. PAPB shows its efficacy by achieving 10.8\% and 3.8\%
WER with and without RNNLM respectively on Wall Street Journal dataset.
",computer-science
"  Historically, machine learning in computer security has prioritized defense:
think intrusion detection systems, malware classification, and botnet traffic
identification. Offense can benefit from data just as well. Social networks,
with their access to extensive personal data, bot-friendly APIs, colloquial
syntax, and prevalence of shortened links, are the perfect venues for spreading
machine-generated malicious content. We aim to discover what capabilities an
adversary might utilize in such a domain. We present a long short-term memory
(LSTM) neural network that learns to socially engineer specific users into
clicking on deceptive URLs. The model is trained with word vector
representations of social media posts, and in order to make a click-through
more likely, it is dynamically seeded with topics extracted from the target's
timeline. We augment the model with clustering to triage high value targets
based on their level of social engagement, and measure success of the LSTM's
phishing expedition using click-rates of IP-tracked links. We achieve state of
the art success rates, tripling those of historic email attack campaigns, and
outperform humans manually performing the same task.
",statistics
"  Route selection based on performance measurements is an essential task in
inter-domain Traffic Engineering. It can benefit from the detection of
significant changes in RTT measurements and the understanding on potential
causes of change. Among the extensive works on change detection methods and
their applications in various domains, few focus on RTT measurements. It is
thus unclear which approach works the best on such data.
In this paper, we present an evaluation framework for change detection on RTT
times series, consisting of: 1) a carefully labelled 34,008-hour RTT dataset as
ground truth; 2) a scoring method specifically tailored for RTT measurements.
Furthermore, we proposed a data transformation that improves the detection
performance of existing methods. Path changes are as well attended to. We fix
shortcomings of previous works by distinguishing path changes due to routing
protocols (IGP and BGP) from those caused by load balancing.
Finally, we apply our change detection methods to a large set of measurements
from RIPE Atlas. The characteristics of both RTT and path changes are analyzed;
the correlation between the two are also illustrated. We identify extremely
frequent AS path changes yet with few consequences on RTT, which has not been
reported before.
",computer-science
"  In this paper, an improved thermal lattice Boltzmann (LB) model is proposed
for simulating liquid-vapor phase change, which is aimed at improving an
existing thermal LB model for liquid-vapor phase change [S. Gong and P. Cheng,
Int. J. Heat Mass Transfer 55, 4923 (2012)]. First, we emphasize that the
replacement of \[{\left( {\rho {c_V}} \right)^{ - 1}}\nabla \cdot \left(
{\lambda \nabla T} \right)\] with \[\nabla \cdot \left( {\chi \nabla T}
\right)\] is an inappropriate treatment for diffuse interface modeling of
liquid-vapor phase change. Furthermore, the error terms ${\partial_{t0}}\left(
{Tv} \right) + \nabla \cdot \left( {Tvv} \right)$, which exist in the
macroscopic temperature equation recovered from the standard thermal LB
equation, are eliminated in the present model through a way that is consistent
with the philosophy of the LB method. In addition, the discrete effect of the
source term is also eliminated in the present model. Numerical simulations are
performed for droplet evaporation and bubble nucleation to validate the
capability of the improved model for simulating liquid-vapor phase change.
Numerical comparisons show that the aforementioned replacement leads to
significant numerical errors and the error terms in the recovered macroscopic
temperature equation also result in considerable errors.
",physics
"  Dense subgraph discovery is a key primitive in many graph mining
applications, such as detecting communities in social networks and mining gene
correlation from biological data. Most studies on dense subgraph mining only
deal with one graph. However, in many applications, we have more than one graph
describing relations among a same group of entities. In this paper, given two
graphs sharing the same set of vertices, we investigate the problem of
detecting subgraphs that contrast the most with respect to density. We call
such subgraphs Density Contrast Subgraphs, or DCS in short. Two widely used
graph density measures, average degree and graph affinity, are considered. For
both density measures, mining DCS is equivalent to mining the densest subgraph
from a ""difference"" graph, which may have both positive and negative edge
weights. Due to the existence of negative edge weights, existing dense subgraph
detection algorithms cannot identify the subgraph we need. We prove the
computational hardness of mining DCS under the two graph density measures and
develop efficient algorithms to find DCS. We also conduct extensive experiments
on several real-world datasets to evaluate our algorithms. The experimental
results show that our algorithms are both effective and efficient.
",computer-science
"  A fundamental challenge in multiagent systems is to design local control
algorithms to ensure a desirable collective behaviour. The information
available to the agents, gathered either through communication or sensing,
naturally restricts the achievable performance. Hence, it is fundamental to
identify what piece of information is valuable and can be exploited to design
control laws with enhanced performance guarantees. This paper studies the case
when such information is uncertain or inaccessible for a class of submodular
resource allocation problems termed covering problems. In the first part of
this work we pinpoint a fundamental risk-reward tradeoff faced by the system
operator when conditioning the control design on a valuable but uncertain piece
of information, which we refer to as the cardinality, that represents the
maximum number of agents that can simultaneously select any given resource.
Building on this analysis, we propose a distributed algorithm that allows
agents to learn the cardinality while adjusting their behaviour over time. This
algorithm is proved to perform on par or better to the optimal design obtained
when the exact cardinality is known a priori.
",computer-science
"  Designing a new drug is a lengthy and expensive process. As the space of
potential molecules is very large (10^23-10^60), a common technique during drug
discovery is to start from a molecule which already has some of the desired
properties. An interdisciplinary team of scientists generates hypothesis about
the required changes to the prototype. In this work, we develop an algorithmic
unsupervised-approach that automatically generates potential drug molecules
given a prototype drug. We show that the molecules generated by the system are
valid molecules and significantly different from the prototype drug. Out of the
compounds generated by the system, we identified 35 FDA-approved drugs. As an
example, our system generated Isoniazid - one of the main drugs for
Tuberculosis. The system is currently being deployed for use in collaboration
with pharmaceutical companies to further analyze the additional generated
molecules.
",statistics
"  We characterize a multi tier network with classical macro cells, and multi
radio access technology (RAT) small cells, which are able to operate in
microwave and millimeter-wave (mm-wave) bands. The small cells are assumed to
be deployed along roads modeled as a Poisson line process. This
characterization is more realistic as compared to the classical Poisson point
processes typically used in literature. In this context, we derive the
association and RAT selection probabilities of the typical user under various
system parameters such as the small cell deployment density and mm-wave antenna
gain, and with varying street densities. Finally, we calculate the signal to
interference plus noise ratio (SINR) coverage probability for the typical user
considering a tractable dominant interference based model for mm-wave
interference. Our analysis reveals the need of deploying more small cells per
street in cities with more streets to maintain coverage, and highlights that
mm-wave RAT in small cells can help to improve the SINR performance of the
users.
",computer-science
"  The Neo-Deterministic Seismic Hazard Assessment (NDSHA) method reliably and
realistically simulates the suite of earthquake ground motions that may impact
civil populations as well as their heritage buildings. The modeling technique
is developed from comprehensive physical knowledge of the seismic source
process, the propagation of earthquake waves and their combined interactions
with site effects. NDSHA effectively accounts for the tensor nature of
earthquake ground motions formally described as the tensor product of the
earthquake source functions and the Green Functions of the pathway. NDSHA uses
all available information about the space distribution of large magnitude
earthquake, including Maximum Credible Earthquake (MCE) and geological and
geophysical data. It does not rely on scalar empirical ground motion
attenuation models, as these are often both weakly constrained by available
observations and unable to account for the tensor nature of earthquake ground
motion. Standard NDSHA provides robust and safely conservative hazard estimates
for engineering design and mitigation decision strategies without requiring
(often faulty) assumptions about the probabilistic risk analysis model of
earthquake occurrence. If specific applications may benefit from temporal
information the definition of the Gutenberg-Richter (GR) relation is performed
according to the multi-scale seismicity model and occurrence rate is associated
to each modeled source. Observations from recent destructive earthquakes in
Italy and Nepal have confirmed the validity of NDSHA approach and application,
and suggest that more widespread application of NDSHA will enhance earthquake
safety and resilience of civil populations in all earthquake-prone regions,
especially in tectonically active areas where the historic earthquake record is
too short.
",physics
"  This paper analyzes the market impacts of expanding California's centralized
electricity market across the western United States and provides the first
statistical assessment of this issue. Using market data from 2015-2018, I
estimate the short-term effects of increasing regional electricity trade
between California and neighboring states on prices, emissions, and generation.
Consistent with economic theory, I find negative price impacts from regional
trade, with each 1 gigawatt-hour (GWh) increase in California electricity
imports associated with an average 0.15 dollar decrease in CAISO price. The
price effect yields significant consumer savings well in excess of
implementation costs required to set up a regional market. I find a short-term
decrease in California carbon dioxide emissions associated with trading that is
partially offset by increased emissions in neighboring regions. Specifically,
each 1 GWh increase in regional trade is associated with a net 70-ton average
decrease in CO2 emissions across the western U.S. A small amount of increased
SO2 and NOx emissions are also observed in neighboring states associated with
increased exports to California. This implies a small portion (less than 10
percent) of electricity exports to California are supplied by coal generation.
This study identifies substantial short-term monetary benefits from market
regionalization for California consumers. It also shows that California's cap
and trade program is relatively effective in limiting the carbon content of
imported electricity, even absent a regional cap on CO2. The conclusions
suggest efforts to reduce trade barriers should move forward in parallel with
strong greenhouse gas policies that cap emissions levels across the market
region.
",quantitative-finance
"  Microtubules (MTs) are filamentous protein polymers roughly 25 nm in
diameter. Ubiquitous in eukaryotes, MTs are well known for their structural
role but also act as actuators, sensors, and, in association with other
proteins, checkpoint regulators. The thin diameter and transparency of
microtubules classifies them as sub-resolution phase objects, with concomitant
imaging challenges. Label-free methods for imaging microtubules are preferred
when long exposure times would lead to phototoxicity in fluorescence, or for
retaining more native structure and activity.
This method approaches quantitative phase imaging of MTs as an inverse
problem based on the Transport of Intensity Equation. In a co-registered
comparison of MT signal-to-background-noise ratio, TIE Microscopy of MTs shows
an improvement of more than three times that of video-enhanced bright field
imaging.
This method avoids the anisotropy caused by prisms used in differential
interference contrast and takes only two defocused images as input. Unlike
other label-free techniques for imaging microtubules, in TIE microscopy
background removal is a natural consequence of taking the difference of two
defocused images, so the need to frequently update a background image is
eliminated.
",physics
"  Small $p$-values are often required to be accurately estimated in large scale
genomic studies for the adjustment of multiple hypothesis tests and the ranking
of genomic features based on their statistical significance. For those
complicated test statistics whose cumulative distribution functions are
analytically intractable, existing methods usually do not work well with small
$p$-values due to lack of accuracy or computational restrictions. We propose a
general approach for accurately and efficiently calculating small $p$-values
for a broad range of complicated test statistics based on the principle of the
cross-entropy method and Markov chain Monte Carlo sampling techniques. We
evaluate the performance of the proposed algorithm through simulations and
demonstrate its application to three real examples in genomic studies. The
results show that our approach can accurately evaluate small to extremely small
$p$-values (e.g. $10^{-6}$ to $10^{-100}$). The proposed algorithm is helpful
to the improvement of existing test procedures and the development of new test
procedures in genomic studies.
",statistics
"  We introduce a gradient flow formulation of linear Boltzmann equations. Under
a diffusive scaling we derive a diffusion equation by using the machinery of
gradient flows.
",mathematics
"  Of the roughly 3000 neutron stars known, only a handful have sub-stellar
companions. The most famous of these are the low-mass planets around the
millisecond pulsar B1257+12. New evidence indicates that observational biases
could still hide a wide variety of planetary systems around most neutron stars.
We consider the environment and physical processes relevant to neutron star
planets, in particular the effect of X-ray irradiation and the relativistic
pulsar wind on the planetary atmosphere. We discuss the survival time of planet
atmospheres and the planetary surface conditions around different classes of
neutron stars, and define a neutron star habitable zone. Depending on as-yet
poorly constrained aspects of the pulsar wind, both Super-Earths around
B1257+12 could lie within its habitable zone.
",physics
"  We address the problem of analyzing the radius of convergence of perturbative
expansion of non-equilibrium steady states of Lindblad driven spin chains. A
simple formal approach is developed for systematically computing the
perturbative expansion of small driven systems. We consider the paradigmatic
model of an open $XXZ$ spin 1/2 chain with boundary supported ultralocal
Lindblad dissipators and treat two different perturbative cases: (i) expansion
in system-bath coupling parameter and (ii) expansion in driving (bias)
parameter. In the first case (i) we find that the radius of convergence quickly
shrinks with increasing the system size, while in the second case (ii) we find
that the convergence radius is always larger than $1$, and in particular it
approaches $1$ from above as we change the anisotropy from easy plane ($XY$) to
easy axis (Ising) regime.
",physics
"  The complexity embedded in condensed matter fertilizes the discovery of new
states of matter, enriched by ingredients like frustration. Illustrating
examples in magnetic systems are Kitaev spin liquids, skyrmions phases, or spin
ices. These unconventional ground states support exotic excitations, for
example the magnetic charges in spin ices, also called monopoles. Beyond their
discovery, an important challenge is to be able to control and manipulate them.
Here, we propose a new mechanism to inject monopoles in a spin ice through a
staggered magnetic field. We show theoretically, and demonstrate experimentally
in the Ho$_2$Ir$_2$O$_7$ pyrochlore iridate, that it results in the
stabilization of a monopole crystal, which exhibits magnetic fragmentation. In
this new state of matter, the magnetic moment fragments into an ordered part
and a persistently fluctuating one. Compared to conventional spin ices, the
different nature of the excitations in this fragmented state opens the way to
novel tunable field-induced and dynamical behaviors.
",physics
"  Component-based development is a software engineering paradigm that can
facilitate the construction of embedded systems and tackle its complexities.
The modern embedded systems have more and more demanding requirements. One way
to cope with such versatile and growing set of requirements is to employ
heterogeneous processing power, i.e., CPU-GPU architectures. The new CPU-GPU
embedded boards deliver an increased performance but also introduce additional
complexity and challenges. In this work, we address the component-to-hardware
allocation for CPU-GPU embedded systems. The allocation for such systems is
much complex due to the increased amount of GPU-related information. For
example, while in traditional embedded systems the allocation mechanism may
consider only the CPU memory usage of components to find an appropriate
allocation scheme, in heterogeneous systems, the GPU memory usage needs also to
be taken into account in the allocation process. This paper aims at decreasing
the component-to-hardware allocation complexity by introducing a 2-layer
component-based architecture for heterogeneous embedded systems. The detailed
CPU-GPU information of the system is abstracted at a high-layer by compacting
connected components into single units that behave as regular components. The
allocator, based on the compacted information received from the high-level
layer, computes, with a decreased complexity, feasible allocation schemes. In
the last part of the paper, the 2-layer allocation method is evaluated using an
existing embedded system demonstrator; namely, an underwater robot.
",computer-science
"  We prove that the open Gromov-Witten invariants on K3 surfaces satisfy the
Kontsevich-Soibelman wall-crossing formula. One one hand, this gives a
geometric interpretation of the slab functions in Gross-Siebert program. On the
other hands, the open Gromov-Witten invariants coincide with the weighted
counting of tropical discs. This is an analog of the corresponding theorem on
toric varieties \cite{M2}\cite{NS} but on compact Calabi-Yau surfaces.
",mathematics
"  We formulate the Nambu-Goldstone theorem as a triangular relation between
pairs of Goldstone bosons with the degenerate vacuum. The vacuum degeneracy is
then a natural consequence of this relation. Inside the scenario of String
Theory, we then find that there is a correspondence between the way how the
$D$-branes interact and the properties of the Goldstone bosons.
",physics
"  The aim of this paper is to characterize the nonnegative functions $\varphi$
defined on $(0,\infty)$ for which the Hausdorff operator
$$\mathscr H_\varphi f(z)= \int_0^\infty
f\left(\frac{z}{t}\right)\frac{\varphi(t)}{t}dt$$ is bounded on the Hardy
spaces of the upper half-plane $\mathcal H_a^p(\mathbb C_+)$, $p\in[1,\infty]$.
The corresponding operator norms and their applications are also given.
",mathematics
"  2 Diabetes is a leading worldwide public health concern, and its increasing
prevalence has significant health and economic importance in all nations. The
condition is a multifactorial disorder with a complex aetiology. The genetic
determinants remain largely elusive, with only a handful of identified
candidate genes. Genome wide association studies (GWAS) promised to
significantly enhance our understanding of genetic based determinants of common
complex diseases. To date, 83 single nucleotide polymorphisms (SNPs) for type 2
diabetes have been identified using GWAS. Standard statistical tests for single
and multi-locus analysis such as logistic regression, have demonstrated little
effect in understanding the genetic architecture of complex human diseases.
Logistic regression is modelled to capture linear interactions but neglects the
non-linear epistatic interactions present within genetic data. There is an
urgent need to detect epistatic interactions in complex diseases as this may
explain the remaining missing heritability in such diseases. In this paper, we
present a novel framework based on deep learning algorithms that deal with
non-linear epistatic interactions that exist in genome wide association data.
Logistic association analysis under an additive genetic model, adjusted for
genomic control inflation factor, is conducted to remove statistically
improbable SNPs to minimize computational overheads.
",statistics
"  We propose Cooperative Training (CoT) for training generative models that
measure a tractable density for discrete data. CoT coordinately trains a
generator $G$ and an auxiliary predictive mediator $M$. The training target of
$M$ is to estimate a mixture density of the learned distribution $G$ and the
target distribution $P$, and that of $G$ is to minimize the Jensen-Shannon
divergence estimated through $M$. CoT achieves independent success without the
necessity of pre-training via Maximum Likelihood Estimation or involving
high-variance algorithms like REINFORCE. This low-variance algorithm is
theoretically proved to be unbiased for both generative and predictive tasks.
We also theoretically and empirically show the superiority of CoT over most
previous algorithms in terms of generative quality and diversity, predictive
generalization ability and computational cost.
",statistics
"  For each $n$, we construct a separable metric space $\mathbb{U}_n$ that is
universal in the coarse category of separable metric spaces with asymptotic
dimension ($\mathop{asdim}$) at most $n$ and universal in the uniform category
of separable metric spaces with uniform dimension ($\mathop{udim}$) at most
$n$. Thus, $\mathbb{U}_n$ serves as a universal space for dimension $n$ in both
the large-scale and infinitesimal topology. More precisely, we prove:
\[
\mathop{asdim} \mathbb{U}_n = \mathop{udim} \mathbb{U}_n = n
\] and such that for each separable metric space $X$,
a) if $\mathop{asdim} X \leq n$, then $X$ is coarsely equivalent to a subset
of $\mathbb{U}_n$;
b) if $\mathop{udim} X \leq n$, then $X$ is uniformly homeomorphic to a
subset of $\mathbb{U}_n$.
",mathematics
"  Let $G$ be a finite group and let $c(G)$ be the number of cyclic subgroups of
$G$. We study the function $\alpha(G) = c(G)/|G|$. We explore its basic
properties and we point out a connection with the probability of commutation.
For many families $\mathscr{F}$ of groups we characterize the groups $G \in
\mathscr{F}$ for which $\alpha(G)$ is maximal and we classify the groups $G$
for which $\alpha(G) > 3/4$. We also study the number of cyclic subgroups of a
direct power of a given group deducing an asymptotic result and we characterize
the equality $\alpha(G) = \alpha(G/N)$ when $G/N$ is a symmetric group.
",mathematics
"  The fast detection of terahertz radiation is of great importance for various
applications such as fast imaging, high speed communications, and spectroscopy.
Most commercial products capable of sensitively responding the terahertz
radiation are thermal detectors, i.e., pyroelectric sensors and bolometers.
This class of terahertz detectors is normally characterized by low modulation
frequency (dozens or hundreds of Hz). Here we demonstrate the first fast
semiconductor-based terahertz quantum well photodetectors by carefully
designing the device structure and microwave transmission line for high
frequency signal extraction. Modulation response bandwidth of gigahertz level
is obtained. As an example, the 6.2-GHz modulated terahertz light emitted from
a Fabry-Pérot terahertz quantum cascade laser is successfully detected
using the fast terahertz quantum well photodetector. In addition to the fast
terahertz detection, the technique presented in this work can also facilitate
the frequency stability or phase noise characterizations for terahertz quantum
cascade lasers.
",physics
"  Using Maple, we implement a SAT solver based on the principle of
inclusion-exclusion and the Bonferroni inequalities. Using randomly generated
input, we investigate the performance of our solver as a function of the number
of variables and number of clauses. We also test it against Maple's built-in
tautology procedure. Finally, we implement the Lovász local lemma with Maple
and discuss its applicability to SAT.
",computer-science
"  We prove various inequalities between the number of partitions with the bound
on the largest part and some restrictions on occurrences of parts. We explore
many interesting consequences of these partition inequalities. In particular,
we show that for $L\geq 1$, the number of partitions with $l-s \leq L$ and
$s=1$ is greater than the number of partitions with $l-s\leq L$ and $s>1$. Here
$l$ and $s$ are the largest part and the smallest part of the partition,
respectively.
",mathematics
"  The hour-glass-like dispersion of spin excitations is a common feature of
underdoped cuprates. It was qualitatively explained by the random phase
approximation based on various ordered states with some phenomenological
parameters; however, its origin remains elusive. Here, we present a numerical
study of spin dynamics in the $t$-$J$ model using the variational Monte Carlo
method. This parameter-free method satisfies the no double-occupancy constraint
of the model and thus provides a better evaluation on the spin dynamics with
respect to various mean-field trial states. We conclude that the lower branch
of the hour-glass dispersion is a collective mode and the upper branch is more
likely the consequence of the stripe state than the other candidates.
",physics
"  Deep learning has demonstrated tremendous potential for Automatic Text
Scoring (ATS) tasks. In this paper, we describe a new neural architecture that
enhances vanilla neural network models with auxiliary neural coherence
features. Our new method proposes a new \textsc{SkipFlow} mechanism that models
relationships between snapshots of the hidden representations of a long
short-term memory (LSTM) network as it reads. Subsequently, the semantic
relationships between multiple snapshots are used as auxiliary features for
prediction. This has two main benefits. Firstly, essays are typically long
sequences and therefore the memorization capability of the LSTM network may be
insufficient. Implicit access to multiple snapshots can alleviate this problem
by acting as a protection against vanishing gradients. The parameters of the
\textsc{SkipFlow} mechanism also acts as an auxiliary memory. Secondly,
modeling relationships between multiple positions allows our model to learn
features that represent and approximate textual coherence. In our model, we
call this \textit{neural coherence} features. Overall, we present a unified
deep learning architecture that generates neural coherence features as it reads
in an end-to-end fashion. Our approach demonstrates state-of-the-art
performance on the benchmark ASAP dataset, outperforming not only feature
engineering baselines but also other deep learning models.
",computer-science
"  We investigate proving properties of Curry programs using Agda. First, we
address the functional correctness of Curry functions that, apart from some
syntactic and semantic differences, are in the intersection of the two
languages. Second, we use Agda to model non-deterministic functions with two
distinct and competitive approaches incorporating the non-determinism. The
first approach eliminates non-determinism by considering the set of all
non-deterministic values produced by an application. The second approach
encodes every non-deterministic choice that the application could perform. We
consider our initial experiment a success. Although proving properties of
programs is a notoriously difficult task, the functional logic paradigm does
not seem to add any significant layer of difficulty or complexity to the task.
",computer-science
"  New index transforms with Weber type kernels, consisting of products of
Bessel functions of the first and second kind are investigated. Mapping
properties and inversion formulas are established for these transforms in
Lebesgue spaces. The results are applied to solve a boundary value problem on
the wedge for a fourth order partial differential equation.
",mathematics
"  GALEX detected a significant fraction of early-type galaxies showing Far-UV
bright structures. These features suggest the occurrence of recent star
formation episodes. We aim at understanding their evolutionary path[s] and the
mechanisms at the origin of their UV-bright structures. We investigate with a
multi-lambda approach 11 early-types selected because of their nearly passive
stage of evolution in the nuclear region. The paper, second of a series,
focuses on the comparison between UV features detected by Swift-UVOT, tracing
recent star formation, and the galaxy optical structure mapping older stellar
populations. We performed their UV surface photometry and used BVRI photometry
from other sources. Our integrated magnitudes have been analyzed and compared
with corresponding values in the literature. We characterize the overall galaxy
structure best fitting the UV and optical luminosity profiles using a single
Sersic law. NGC 1366, NGC 1426, NGC 3818, NGC 3962 and NGC 7192 show
featureless luminosity profiles. Excluding NGC 1366 which has a clear edge-on
disk , n~1-2, and NGC 3818, the remaining three have Sersic's indices n~3-4 in
optical and a lower index in the UV. Bright ring/arm-like structures are
revealed by UV images and luminosity profiles of NGC 1415, NGC 1533, NGC 1543,
NGC 2685, NGC 2974 and IC 2006. The ring/arm-like structures are different from
galaxy to galaxy. Sersic indices of UV profiles for those galaxies are in the
range n=1.5-3 both in S0s and in Es. In our sample optical Sersic indices are
usually larger than the UV ones. (M2-V) color profiles are bluer in
ring/arm-like structures with respect to the galaxy body. The lower values of
Sersic's indices in the UV bands with respect to optical ones, suggesting the
presence of a disk, point out that the role of the dissipation cannot be
neglected in recent evolutionary phases of these early-type galaxies.
",physics
"  The Poisson-Fermi model is an extension of the classical Poisson-Boltzmann
model to include the steric and correlation effects of ions and water treated
as nonuniform spheres in aqueous solutions. Poisson-Boltzmann electrostatic
calculations are essential but computationally very demanding for molecular
dynamics or continuum simulations of complex systems in molecular biophysics
and electrochemistry. The graphic processing unit (GPU) with enormous
arithmetic capability and streaming memory bandwidth is now a powerful engine
for scientific as well as industrial computing. We propose two parallel GPU
algorithms, one for linear solver and the other for nonlinear solver, for
solving the Poisson-Fermi equation approximated by the standard finite
difference method in 3D to study biological ion channels with crystallized
structures from the Protein Data Bank, for example. Numerical methods for both
linear and nonlinear solvers in the parallel algorithms are given in detail to
illustrate the salient features of the CUDA (compute unified device
architecture) software platform of GPU in implementation. It is shown that the
parallel algorithms on GPU over the sequential algorithms on CPU (central
processing unit) can achieve 22.8x and 16.9x speedups for the linear solver
time and total runtime, respectively.
",physics
"  In recent work, redressed warped frames have been introduced for the analysis
and synthesis of audio signals with non-uniform frequency and time resolutions.
In these frames, the allocation of frequency bands or time intervals of the
elements of the representation can be uniquely described by means of a warping
map. Inverse warping applied after time-frequency sampling provides the key to
reduce or eliminate dispersion of the warped frame elements in the conjugate
variable, making it possible, e.g., to construct frequency warped frames with
synchronous time alignment through frequency. The redressing procedure is
however exact only when the analysis and synthesis windows have compact support
in the domain where warping is applied. This implies that frequency warped
frames cannot have compact support in the time domain. This property is
undesirable when online computation is required. Approximations in which the
time support is finite are however possible, which lead to small reconstruction
errors. In this paper we study the approximation error for compactly supported
frequency warped analysis-synthesis elements, providing a few examples and case
studies.
",computer-science
"  We use a variant of the technique in [Lac17a] to give sparse L^p(log(L))^4
bounds for a class of model singular and maximal Radon transforms
",mathematics
"  This paper studies a mean-variance portfolio selection problem under partial
information with drift uncertainty. It is proved that all the contingent claims
in this model are attainable in the sense of Xiong and Zhou. Further, we
propose a numerical scheme to approximate the optimal portfolio. Malliavin
calculus and the strong law of large numbers play important roles in this
scheme.
",quantitative-finance
"  Mobile edge computing is a new computing paradigm, which pushes cloud
computing capabilities away from the centralized cloud to the network edge.
However, with the sinking of computing capabilities, the new challenge incurred
by user mobility arises: since end-users typically move erratically, the
services should be dynamically migrated among multiple edges to maintain the
service performance, i.e., user-perceived latency. Tackling this problem is
non-trivial since frequent service migration would greatly increase the
operational cost. To address this challenge in terms of the performance-cost
trade-off, in this paper we study the mobile edge service performance
optimization problem under long-term cost budget constraint. To address user
mobility which is typically unpredictable, we apply Lyapunov optimization to
decompose the long-term optimization problem into a series of real-time
optimization problems which do not require a priori knowledge such as user
mobility. As the decomposed problem is NP-hard, we first design an
approximation algorithm based on Markov approximation to seek a near-optimal
solution. To make our solution scalable and amenable to future 5G application
scenario with large-scale user devices, we further propose a distributed
approximation scheme with greatly reduced time complexity, based on the
technique of best response update. Rigorous theoretical analysis and extensive
evaluations demonstrate the efficacy of the proposed centralized and
distributed schemes.
",computer-science
"  We develop a systematic study of Jahn-Teller (JT) models with continuous
symmetries by explor- ing their algebraic properties. The compact symmetric
spaces corresponding to JT models carrying a Lie group symmetry are identified,
and their invariants used to reduce their adiabatic potential energy surfaces
into orbit spaces. Each orbit consists of a set of JT distorted molecular
structures with equal adiabatic electronic spectrum. Molecular motion may be
decomposed into pseudorota- tional and radial. The former preserves the orbit,
while the latter maps an orbit into another. The dimensionality and topology of
the internal space of each orbit depends on the number of degener- ate states
in its adiabatic electronic spectra. Furthermore, qualitatively different
pseudorotational modes occur in orbits of different types. We also provide a
simple proof that the electronic spectrum for the space of JT minimum-energy
structures (trough) displays a universality predicted by the epikernel
principle. This result is in turn used to prove the topological equivalence
between bosonic (fermionic) JT troughs and real (quaternionic) projective
spaces, a conclusion which has outstanding physical consequences, as explained
in our work. The relevance of our study for the more common case of JT systems
with only discrete point group symmetry, and for generic asymmetric molecular
systems with conical intersections involving more than two states is likewise
discussed. In particular, we show that JT models with continuous symmetries
present the simplest models of conical intersections among an arbitrary number
of electronic state crossings.
",physics
"  In recent years, research has been done on applying Recurrent Neural Networks
(RNNs) as recommender systems. Results have been promising, especially in the
session-based setting where RNNs have been shown to outperform state-of-the-art
models. In many of these experiments, the RNN could potentially improve the
recommendations by utilizing information about the user's past sessions, in
addition to its own interactions in the current session. A problem for
session-based recommendation, is how to produce accurate recommendations at the
start of a session, before the system has learned much about the user's current
interests. We propose a novel approach that extends a RNN recommender to be
able to process the user's recent sessions, in order to improve
recommendations. This is done by using a second RNN to learn from recent
sessions, and predict the user's interest in the current session. By feeding
this information to the original RNN, it is able to improve its
recommendations. Our experiments on two different datasets show that the
proposed approach can significantly improve recommendations throughout the
sessions, compared to a single RNN working only on the current session. The
proposed model especially improves recommendations at the start of sessions,
and is therefore able to deal with the cold start problem within sessions.
",computer-science
"  In this paper we define the notion of pullback lifting of a lifting crossed
module over a crossed module morphism and interpret this notion in the category
of group-groupoid actions as pullback action. Moreover, we give a criterion for
the lifting of homotopic crossed module morphisms to be homotopic, which will
be called homotopy lifting property for crossed module morphisms. Finally, we
investigate some properties of derivations of lifting crossed modules according
to base crossed module derivations.
",mathematics
"  We study the mutual alignment of radio sources within two surveys, FIRST and
TGSS. This is done by producing two position angle catalogues containing the
preferential directions of respectively $30\,059$ and $11\,674$ extended
sources distributed over more than $7\,000$ and $17\,000$ square degrees. The
identification of the sources in the FIRST sample was performed in advance by
volunteers of the Radio Galaxy Zoo project, while for the TGSS sample it is the
result of an automated process presented here. After taking into account
systematic effects, marginal evidence of a local alignment on scales smaller
than $2.5°$ is found in the FIRST sample. The probability of this happening
by chance is found to be less than $2$ per cent. Further study suggests that on
scales up to $1.5°$ the alignment is maximal. For one third of the sources,
the Radio Galaxy Zoo volunteers identified an optical counterpart. Assuming a
flat $\Lambda$CDM cosmology with $\Omega_m = 0.31, \Omega_\Lambda = 0.69$, we
convert the maximum angular scale on which alignment is seen into a physical
scale in the range $[19, 38]$ Mpc $h_{70}^{-1}$. This result supports recent
evidence reported by Taylor and Jagannathan of radio jet alignment in the $1.4$
deg$^2$ ELAIS N1 field observed with the Giant Metrewave Radio Telescope. The
TGSS sample is found to be too sparsely populated to manifest a similar signal.
",physics
"  The use of standard platforms in the field of humanoid robotics can
accelerate research, and lower the entry barrier for new research groups. While
many affordable humanoid standard platforms exist in the lower size ranges of
up to 60cm, beyond this the few available standard platforms quickly become
significantly more expensive, and difficult to operate and maintain. In this
paper, the igus Humanoid Open Platform is presented---a new, affordable,
versatile and easily customisable standard platform for humanoid robots in the
child-sized range. At 90cm, the robot is large enough to interact with a
human-scale environment in a meaningful way, and is equipped with enough torque
and computing power to foster research in many possible directions. The
structure of the robot is entirely 3D printed, allowing for a lightweight and
appealing design. The electrical and mechanical designs of the robot are
presented, and the main features of the corresponding open-source ROS software
are discussed. The 3D CAD files for all of the robot parts have been released
open-source in conjunction with this paper.
",computer-science
"  We prove the existence of singular harmonic ${\bf Z}_2$ spinors on
$3$-manifolds with $b_1 > 1$. The proof relies on a wall-crossing formula for
solutions to the Seiberg-Witten equation with two spinors. The existence of
singular harmonic ${\bf Z}_2$ spinors and the shape of our wall-crossing
formula shed new light on recent observations made by Joyce regarding Donaldson
and Segal's proposal for counting $G_2$-instantons.
",mathematics
"  A finite dimensional operator that commutes with some symmetry group admits
quotient operators, which are determined by the choice of associated
representation. Taking the quotient isolates the part of the spectrum
supporting the chosen representation and reduces the complexity of the problem,
however it is not uniquely defined. Here we present a computationally simple
way of choosing a special basis for the space of intertwiners, allowing us to
construct a quotient that reflects the structure of the original operator. This
quotient construction generalizes previous definitions for discrete graphs,
which either dealt with restricted group actions or only with the trivial
representation.
We also extend the method to quantum graphs, which simplifies previous
constructions within this context, answers an open question regarding
self-adjointness and offers alternative viewpoints in terms of a scattering
approach. Applications to isospectrality are discussed, together with numerous
examples and comparisons with previous results.
",mathematics
"  We present deep ALMA CO(5-4) observations of a main sequence, clumpy galaxy
at z=1.5 in the HUDF. Thanks to the ~0.5"" resolution of the ALMA data, we can
link stellar population properties to the CO(5-4) emission on scales of a few
kpc. We detect strong CO(5-4) emission from the nuclear region of the galaxy,
consistent with the observed $L_{\rm IR}$-$L^{\prime}_{\rm CO(5-4)}$
correlation and indicating on-going nuclear star formation. The CO(5-4) gas
component appears more concentrated than other star formation tracers or the
dust distribution in this galaxy. We discuss possible implications of this
difference in terms of star formation efficiency and mass build-up at the
galaxy centre. Conversely, we do not detect any CO(5-4) emission from the
UV-bright clumps. This might imply that clumps have a high star formation
efficiency (although they do not display unusually high specific star formation
rates) and are not entirely gas dominated, with gas fractions no larger than
that of their host galaxy (~50%). Stellar feedback and disk instability torques
funnelling gas towards the galaxy centre could contribute to the relatively low
gas content. Alternatively, clumps could fall in a more standard star formation
efficiency regime if their actual star-formation rates are lower than generally
assumed. We find that clump star-formation rates derived with several
different, plausible methods can vary by up to an order of magnitude. The
lowest estimates would be compatible with a CO(5-4) non-detection even for
main-sequence like values of star formation efficiency and gas content.
",physics
"  We study the observed relation between accretion rate (in terms of L/L_Edd)
and shape of the hard X-ray spectral energy distribution (namely the photon
index Gamma_X) for a large sample of 228 hard X-ray selected, low-redshift
active galactic nuclei (AGN), drawn from the Swift/BAT AGN Spectroscopic Survey
(BASS). This includes 30 AGN for which black hole mass (and therefore L/L_Edd)
is measured directly through masers, spatially resolved gas or stellar
dynamics, or reverberation mapping. The high quality and broad energy coverage
of the data provided through BASS allow us to examine several alternative
determinations of both Gamma_X and L/L_Edd. For the BASS sample as a whole, we
find a statistically significant, albeit very weak correlation between Gamma_X
and L/L_Edd. The best-fitting relations we find, Gamma_X=0.15
log(L/L_Edd)+const., are considerably shallower than those reported in previous
studies. Moreover, we find no corresponding correlations among the subsets of
AGN with different M_BH determination methodology. In particular, we find no
robust evidence for a correlation when considering only those AGN with direct
or single-epoch M_BH estimates. This latter finding is in contrast to several
previous studies which focused on z>0.5 broad-line AGN. We discuss this tension
and conclude that it can be partially accounted for if one adopts a simplified,
power-law X-ray spectral model, combined with L/L_Edd estimates that are based
on the continuum emission and on single-epoch broad line spectroscopy in the
optical regime. We finally highlight the limitations on using Gamma_X as a
probe of supermassive black hole evolution in deep extragalactic X-ray surveys.
",physics
"  Despite recent advances in face recognition using deep learning, severe
accuracy drops are observed for large pose variations in unconstrained
environments. Learning pose-invariant features is one solution, but needs
expensively labeled large-scale data and carefully designed feature learning
algorithms. In this work, we focus on frontalizing faces in the wild under
various head poses, including extreme profile views. We propose a novel deep 3D
Morphable Model (3DMM) conditioned Face Frontalization Generative Adversarial
Network (GAN), termed as FF-GAN, to generate neutral head pose face images. Our
framework differs from both traditional GANs and 3DMM based modeling.
Incorporating 3DMM into the GAN structure provides shape and appearance priors
for fast convergence with less training data, while also supporting end-to-end
training. The 3DMM-conditioned GAN employs not only the discriminator and
generator loss but also a new masked symmetry loss to retain visual quality
under occlusions, besides an identity loss to recover high frequency
information. Experiments on face recognition, landmark localization and 3D
reconstruction consistently show the advantage of our frontalization method on
faces in the wild datasets.
",computer-science
"  Given a direct system of Hilbert spaces $s\mapsto \mathcal H_s$ (with
isometric inclusion maps $\iota_s^t:\mathcal H_s\rightarrow \mathcal H_t$ for
$s\leq t$) corresponding to quantum systems on scales $s$, we define notions of
scale invariant and weakly scale invariant operators. Is some cases of quantum
spin chains we find conditions for transfer matrices and nearest neighbour
Hamiltonians to be scale invariant or weakly so. Scale invariance forces
spatial inhomogeneity of the spectral parameter. But weakly scale invariant
transfer matrices may be spatially homogeneous in which case the change of
spectral parameter from one scale to another is governed by a classical
dynamical system exhibiting fractal behaviour.
",mathematics
"  We have developed a system combining a back-illuminated
Complementary-Metal-Oxide-Semiconductor (CMOS) imaging sensor and Xilinx Zynq
System-on-Chip (SoC) device for a soft X-ray (0.5-10 keV) imaging spectroscopy
observation of the Sun to investigate the dynamics of the solar corona. Because
typical timescales of energy release phenomena in the corona span a few minutes
at most, we aim to obtain the corresponding energy spectra and derive the
physical parameters, i.e., temperature and emission measure, every few tens of
seconds or less for future solar X-ray observations. An X-ray photon-counting
technique, with a frame rate of a few hundred frames per second or more, can
achieve such results. We used the Zynq SoC device to achieve the requirements.
Zynq contains an ARM processor core, which is also known as the Processing
System (PS) part, and a Programmable Logic (PL) part in a single chip. We use
the PL and PS to control the sensor and seamless recording of data to a storage
system, respectively. We aim to use the system for the third flight of the
Focusing Optics Solar X-ray Imager (FOXSI-3) sounding rocket experiment for the
first photon-counting X-ray imaging and spectroscopy of the Sun.
",physics
"  Timings of human activities are marked by circadian clocks which in turn are
entrained to different environmental signals. In an urban environment the
presence of artificial lighting and various social cues tend to disrupt the
natural entrainment with the sunlight. However, it is not completely understood
to what extent this is the case. Here we exploit the large-scale data analysis
techniques to study the mobile phone calling activity of people in large cities
to infer the dynamics of urban daily rhythms. From the calling patterns of
about 1,000,000 users spread over different cities but lying inside the same
time-zone, we show that the onset and termination of the calling activity
synchronizes with the east-west progression of the sun. We also find that the
onset and termination of the calling activity of users follows a yearly
dynamics, varying across seasons, and that its timings are entrained to solar
midnight. Furthermore, we show that the average mid-sleep time of people living
in urban areas depends on the age and gender of each cohort as a result of
biological and social factors.
",physics
"  We recalculate the leading relativistic corrections for the ground electronic
state of the hydrogen molecule using variational method with explicitly
correlated functions which satisfy the interelectronic cusp condition. The new
computational approach allowed for the control of the numerical precision which
reached about 8 significant digits. More importantly, the updated theoretical
energies became discrepant with the known experimental values and we conclude
that the yet unknown relativistic recoil corrections might be larger than
previously anticipated.
",physics
"  Let $(X,d,\mu)$ be a doubling metric measure space endowed with a Dirichlet
form $\E$ deriving from a ""carré du champ"". Assume that $(X,d,\mu,\E)$
supports a scale-invariant $L^2$-Poincaré inequality. In this article, we
study the following properties of harmonic functions, heat kernels and Riesz
transforms for $p\in (2,\infty]$:
(i) $(G_p)$: $L^p$-estimate for the gradient of the associated heat
semigroup;
(ii) $(RH_p)$: $L^p$-reverse Hölder inequality for the gradients of
harmonic functions;
(iii) $(R_p)$: $L^p$-boundedness of the Riesz transform ($p<\infty$);
(iv) $(GBE)$: a generalised Bakry-Émery condition.
We show that, for $p\in (2,\infty)$, (i), (ii) (iii) are equivalent, while
for $p=\infty$, (i), (ii), (iv) are equivalent.
Moreover, some of these equivalences still hold under weaker conditions than
the $L^2$-Poincaré inequality.
Our result gives a characterisation of Li-Yau's gradient estimate of heat
kernels for $p=\infty$, while for $p\in (2,\infty)$ it is a substantial
improvement as well as a generalisation of earlier results by
Auscher-Coulhon-Duong-Hofmann [7] and Auscher-Coulhon [6]. Applications to
isoperimetric inequalities and Sobolev inequalities are given. Our results
apply to Riemannian and sub-Riemannian manifolds as well as to non-smooth
spaces, and to degenerate elliptic/parabolic equations in these settings.
",mathematics
"  This paper studies the heat equation $u_t=\Delta u$ in a bounded domain
$\Omega\subset\mathbb{R}^{n}(n\geq 2)$ with positive initial data and a local
nonlinear Neumann boundary condition: the normal derivative $\partial
u/\partial n=u^{q}$ on partial boundary $\Gamma_1\subseteq \partial\Omega$ for
some $q>1$, while $\partial u/\partial n=0$ on the other part. We investigate
the lower bound of the blow-up time $T^{*}$ of $u$ in several aspects. First,
$T^{*}$ is proved to be at least of order $(q-1)^{-1}$ as $q\rightarrow 1^{+}$.
Since the existing upper bound is of order $(q-1)^{-1}$, this result is sharp.
Secondly, if $\Omega$ is convex and $|\Gamma_{1}|$ denotes the surface area of
$\Gamma_{1}$, then $T^{*}$ is shown to be at least of order
$|\Gamma_{1}|^{-\frac{1}{n-1}}$ for $n\geq 3$ and
$|\Gamma_{1}|^{-1}\big/\ln\big(|\Gamma_{1}|^{-1}\big)$ for $n=2$ as
$|\Gamma_{1}|\rightarrow 0$, while the previous result is
$|\Gamma_{1}|^{-\alpha}$ for any $\alpha<\frac{1}{n-1}$. Finally, we generalize
the results for convex domains to the domains with only local convexity near
$\Gamma_{1}$.
",mathematics
"  The time evolution of the energy transport triggered in a strongly coupled
system by a temperature gradient is holographically related to the evolution of
an asymptotically AdS black brane. We study the far-from-equilibrium properties
of such a system by using the AdS/CFT correspondence. In particular, we
describe the appearance of a steady state, and study the information flow by
computing the time evolution of the holographic entanglement entropy. Some
universal properties of the quenching process are presented.
",physics
"  We present the evolution of the Cosmic Spectral Energy Distribution (CSED)
from $z = 1 - 0$. Our CSEDs originate from stacking individual spectral energy
distribution fits based on panchromatic photometry from the Galaxy and Mass
Assembly (GAMA) and COSMOS datasets in ten redshift intervals with completeness
corrections applied. Below $z = 0.45$, we have credible SED fits from 100 nm to
1 mm. Due to the relatively low sensitivity of the far-infrared data, our
far-infrared CSEDs contain a mix of predicted and measured fluxes above $z =
0.45$. Our results include appropriate errors to highlight the impact of these
corrections. We show that the bolometric energy output of the Universe has
declined by a factor of roughly four -- from $5.1 \pm 1.0$ at $z \sim 1$ to
$1.3 \pm 0.3 \times 10^{35}~h_{70}$~W~Mpc$^{-3}$ at the current epoch. We show
that this decrease is robust to cosmic variance, SED modelling and other
various types of error. Our CSEDs are also consistent with an increase in the
mean age of stellar populations. We also show that dust attenuation has
decreased over the same period, with the photon escape fraction at 150~nm
increasing from $16 \pm 3$ at $z \sim 1$ to $24 \pm 5$ per cent at the current
epoch, equivalent to a decrease in $A_\mathrm{FUV}$ of 0.4~mag. Our CSEDs
account for $68 \pm 12$ and $61 \pm 13$ per cent of the cosmic optical and
infrared backgrounds respectively as defined from integrated galaxy counts and
are consistent with previous estimates of the cosmic infrared background with
redshift.
",physics
"  Surface observations indicate that the speed of the solar meridional
circulation in the photosphere varies in anti-phase with the solar cycle. The
current explanation for the source of this variation is that inflows into
active regions alter the global surface pattern of the meridional circulation.
When these localized inflows are integrated over a full hemisphere, they
contribute to the slow down of the axisymmetric poleward horizontal component.
The behavior of this large scale flow deep inside the convection zone remains
largely unknown. Present helioseismic techniques are not sensitive enough to
capture the dynamics of this weak large scale flow. Moreover, the large time of
integration needed to map the meridional circulation inside the convection
zone, also masks some of the possible dynamics on shorter timescales. In this
work we examine the dynamics of the meridional circulation that emerges from a
3D MHD global simulation of the solar convection zone. Our aim is to assess and
quantify the behavior of meridional circulation deep inside the convection
zone, where the cyclic large-scale magnetic field can reach considerable
strength. Our analyses indicate that the meridional circulation morphology and
amplitude are both highly influenced by the magnetic field, via the impact of
magnetic torques on the global angular momentum distribution. A dynamic feature
induced by these magnetic torques is the development of a prominent upward flow
at mid latitudes in the lower convection zone that occurs near the equatorward
edge of the toroidal bands and that peaks during cycle maximum. Globally, the
dynamo-generated large-scale magnetic field drives variations in the meridional
flow, in stark contrast to the conventional kinematic flux transport view of
the magnetic field being advected passively by the flow.
",physics
"  We report on a versatile, highly controllable hybrid cold Rydberg atom fiber
interface, based on laser cooled atoms transported into a hollow core
Kagomé crystal fiber. Our experiments are the first to demonstrate the
feasibility of exciting cold Rydberg atoms inside a hollow core fiber and we
study the influence of the fiber on Rydberg electromagnetically induced
transparency (EIT) signals. Using a temporally resolved detection method to
distinguish between excitation and loss, we observe two different regimes of
the Rydberg excitations: one EIT regime and one regime dominated by atom loss.
These results are a substantial advancement towards future use of our system
for quantum simulation or information.
",physics
"  Many database columns contain string or numerical data that conforms to a
pattern, such as phone numbers, dates, addresses, product identifiers, and
employee ids. These patterns are useful in a number of data processing
applications, including understanding what a specific field represents when
field names are ambiguous, identifying outlier values, and finding similar
fields across data sets. One way to express such patterns would be to learn
regular expressions for each field in the database. Unfortunately, exist- ing
techniques on regular expression learning are slow, taking hundreds of seconds
for columns of just a few thousand values. In contrast, we develop XSystem, an
efficient method to learn patterns over database columns in significantly less
time. We show that these patterns can not only be built quickly, but are
expressive enough to capture a number of key applications, including detecting
outliers, measuring column similarity, and assigning semantic labels to columns
(based on a library of regular expressions). We evaluate these applications
with datasets that range from chemical databases (based on a collaboration with
a pharmaceutical company), our university data warehouse, and open data from
MassData.gov.
",computer-science
"  We study the relation between the microscopic properties of a many-body
system and the electron spectra, experimentally accessible by photoemission. In
a recent paper [Phys. Rev. Lett. 114, 236402 (2015)], we introduced the
""fluctuation diagnostics"" approach, to extract the dominant wave vector
dependent bosonic fluctuations from the electronic self-energy. Here, we first
reformulate the theory in terms of fermionic modes, to render its connection
with resonance valence bond (RVB) fluctuations more transparent. Secondly, by
using a large-U expansion, where U is the Coulomb interaction, we relate the
fluctuations to real space correlations. Therefore, it becomes possible to
study how electron spectra are related to charge, spin, superconductivity and
RVB-like real space correlations, broadening the analysis of an earlier work
[Phys. Rev. B 89, 245130 (2014)]. This formalism is applied to the pseudogap
physics of the two-dimensional Hubbard model, studied in the dynamical cluster
approximation. We perform calculations for embedded clusters with up to 32
sites, having three inequivalent K-points at the Fermi surface. We find that as
U is increased, correlation functions gradually attain values consistent with
an RVB state. This first happens for correlation functions involving the
antinodal point and gradually spreads to the nodal point along the Fermi
surface. Simultaneously a pseudogap opens up along the Fermi surface. We relate
this to a crossover from a Kondo-like state to an RVB-like localized cluster
state and to the presence of RVB and spin fluctuations. These changes are
caused by a strong momentum dependence in the cluster bath-couplings along the
Fermi surface. We also show, from a more algorithmic perspective, how the
time-consuming calculations in fluctuation diagnostics can be drastically
simplified.
",physics
"  In this paper we present results on dynamic multivariate scalar risk
measures, which arise in markets with transaction costs and systemic risk. Dual
representations of such risk measures are presented. These are then used to
obtain the main results of this paper on time consistency; namely, an
equivalent recursive formulation of multivariate scalar risk measures to
multiportfolio time consistency. We are motivated to study time consistency of
multivariate scalar risk measures as the superhedging risk measure in markets
with transaction costs (with a single eligible asset) (Jouini and Kallal
(1995), Roux and Zastawniak (2016), Loehne and Rudloff (2014)) does not satisfy
the usual scalar concept of time consistency. In fact, as demonstrated in
(Feinstein and Rudloff (2018)), scalar risk measures with the same
scalarization weight at all times would not be time consistent in general. The
deduced recursive relation for the scalarizations of multiportfolio time
consistent set-valued risk measures provided in this paper requires
consideration of the entire family of scalarizations. In this way we develop a
direct notion of a ""moving scalarization"" for scalar time consistency that
corroborates recent research on scalarizations of dynamic multi-objective
problems (Karnam, Ma, and Zhang (2017), Kovacova and Rudloff (2018)).
",quantitative-finance
"  It has previously been shown that a dye-filled microcavity can produce a
Bose-Einstein condensate of photons. Thermalization of photons is possible via
repeated absorption and re-emission by the dye molecules. In this paper, we
theoretically explore the behavior of the polarization of light in this system.
We find that in contrast to the near complete thermalization between different
spatial modes of light, thermalization of polarization states is expected to
generally be incomplete. We show that the polarization degree changes
significantly from below to above threshold, and explain the dependence of
polarization on all relevant material parameters.
",physics
"  Sylvester factor, an essential part of the asymptotic formula of Hardy and
Littlewood which is the extended Goldbach conjecture, regarded as strongly
multiplicative arithmetic function, has several remarkable properties.
",mathematics
"  Many recent studies of the motor system are divided into two distinct
approaches: Those that investigate how motor responses are encoded in cortical
neurons' firing rate dynamics and those that study the learning rules by which
mammals and songbirds develop reliable motor responses. Computationally, the
first approach is encapsulated by reservoir computing models, which can learn
intricate motor tasks and produce internal dynamics strikingly similar to those
of motor cortical neurons, but rely on biologically unrealistic learning rules.
The more realistic learning rules developed by the second approach are often
derived for simplified, discrete tasks in contrast to the intricate dynamics
that characterize real motor responses. We bridge these two approaches to
develop a biologically realistic learning rule for reservoir computing. Our
algorithm learns simulated motor tasks on which previous reservoir computing
algorithms fail, and reproduces experimental findings including those that
relate motor learning to Parkinson's disease and its treatment.
",quantitative-biology
"  By combining Shannon's cryptography model with an assumption to the lower
bound of adversaries' uncertainty to the queried dataset, we develop a secure
Bayesian inference-based privacy model and then in some extent answer Dwork et
al.'s question [1]: ""why Bayesian risk factors are the right measure for
privacy loss"".
This model ensures an adversary can only obtain little information of each
individual from the model's output if the adversary's uncertainty to the
queried dataset is larger than the lower bound. Importantly, the assumption to
the lower bound almost always holds, especially for big datasets. Furthermore,
this model is flexible enough to balance privacy and utility: by using four
parameters to characterize the assumption, there are many approaches to balance
privacy and utility and to discuss the group privacy and the composition
privacy properties of this model.
",computer-science
"  In this paper, we prove that positivity of denominator vectors holds for any
skew-symmetric cluster algebra.
",mathematics
"  The paper aims to apply the complex octonion to explore the influence of the
energy gradient on the Eotvos experiment, impacting the gravitational mass in
the ultra-strong magnetic fields. Until now the Eotvos experiment has never
been validated under the ultra-strong magnetic field. It is aggravating the
existing serious qualms about the Eotvos experiment. According to the
electromagnetic and gravitational theory described with the complex octonions,
the ultra-strong magnetic field must result in a tiny variation of the
gravitational mass. The magnetic field with the gradient distribution will
generate the energy gradient. These influencing factors will exert an influence
on the state of equilibrium in the Eotvos experiment. That is, the
gravitational mass will depart from the inertial mass to a certain extent, in
the ultra-strong magnetic fields. Only under exceptional circumstances,
especially in the case of the weak field strength, the gravitational mass may
be equal to the inertial mass approximately. The paper appeals intensely to
validate the Eotvos experiment in the ultra-strong electromagnetic strengths.
It is predicted that the physical property of gravitational mass will be
distinct from that of inertial mass.
",physics
"  For a unit vector field on a closed immersed Euclidean hypersurface
$M^{2n+1}$, $n\geq 1$, we exhibit a nontrivial lower bound for its energy which
depends on the degree of the Gauss map of the immersion. When the hypersurface
is the unit sphere $\mathbb{S}^{2n+1}$, immersed with degree one, this lower
bound corresponds to a well established value from the literature. We introduce
a list of functionals $\mathcal{B}_k$ on a compact Riemannian manifold $M^{m}$,
$1\leq k\leq m$, and show that, when the underlying manifold is a closed
hypersurface, these functionals possess similar properties regarding the degree
of the immersion. In addition, we prove that Hopf flows minimize
$\mathcal{B}_n$ on $\mathbb{S}^{2n+1}$.
",mathematics
"  We study the photoinduced breakdown of a two-orbital Mott insulator and
resulting metallic state. Using time-dependent density matrix renormalization
group, we scrutinize the real-time dynamics of the half-filled two-orbital
Hubbard model interacting with a resonant radiation field pulse. The breakdown,
caused by production of doublon-holon pairs, is enhanced by Hund's exchange,
which dynamically activates large orbital fluctuations. The melting of the Mott
insulator is accompanied by a high to low spin transition with a concomitant
reduction of antiferromagnetic spin fluctuations. Most notably, the overall
time response is driven by the photogeneration of excitons with orbital
character that are stabilized by Hund's coupling. These unconventional ""Hund
excitons"" correspond to bound spin-singlet orbital-triplet doublon-holon pairs.
We study exciton properties such as bandwidth, binding potential, and size
within a semiclassical approach. The photometallic state results from a
coexistence of Hund excitons and doublon-holon plasma.
",physics
"  The purpose of this paper is to carry out a classical construction of a
non-constant holomorphic disk with boundary on (the suspension of) a Lagrangian
submanifold in $\mathbb{R}^{2 n}$ in the case the Lagrangian is the lift of a
coisotropic (a.k.a. pre-Lagrangian) submanifold in (a subset $U$ of)
$\mathbb{R}^{2 n - 1}$. We show that the positive lower and finite upper bounds
for the area of such a disk (which are due to M. Gromov and J.-C. Sikorav and
F. Laudenbach-Sikorav for general Lagrangians) depend on the coisotropic
submanifold only but not on its lift to the symplectization. The main
application is to a $C^0$-characterization of contact embeddings in terms of
coisotropic embeddings in another paper by the present author. Moreover, we
prove a version of Gromov's non-existence of exact Lagrangian embeddings into
standard $\mathbb{R}^{2 n}$ for coisotropic embeddings into $S^1 \times
\mathbb{R}^{2 n}$. This allows us to distinguish different contact structures
on the latter by means of the (modified) contact shape invariant. As in the
general Lagrangian case, all of the existence results are based on Gromov's
theory of $J$-holomorphic curves and his compactness theorem (or persistence
principle). Analytical difficulties arise mainly at the ends of the cone
$\mathbb{R}_+ \times U$.
",mathematics
"  The round trip time of the light pulse limits the maximum detectable
frequency response range of vibration in phase-sensitive optical time domain
reflectometry ({\phi}-OTDR). We propose a method to break the frequency
response range restriction of {\phi}-OTDR system by modulating the light pulse
interval randomly which enables a random sampling for every vibration point in
a long sensing fiber. This sub-Nyquist randomized sampling method is suits for
detecting sparse-wideband-frequency vibration signals. Up to MHz resonance
vibration signal with over dozens of frequency components and 1.153MHz single
frequency vibration signal are clearly identified for a sensing range of 9.6km
with 10kHz maximum sampling rate.
",physics
"  We study the consistency of Lipschitz learning on graphs in the limit of
infinite unlabeled data and finite labeled data. Previous work has conjectured
that Lipschitz learning is well-posed in this limit, but is insensitive to the
distribution of the unlabeled data, which is undesirable for semi-supervised
learning. We first prove that this conjecture is true in the special case of a
random geometric graph model with kernel-based weights. Then we go on to show
that on a random geometric graph with self-tuning weights, Lipschitz learning
is in fact highly sensitive to the distribution of the unlabeled data, and we
show how the degree of sensitivity can be adjusted by tuning the weights. In
both cases, our results follow from showing that the sequence of learned
functions converges to the viscosity solution of an $\infty$-Laplace type
equation, and studying the structure of the limiting equation.
",computer-science
"  Let $\{ R_n, {\mathfrak m}_n \}_{n \ge 0}$ be an infinite sequence of regular
local rings with $R_{n+1}$ birationally dominating $R_n$ and ${\mathfrak
m}_nR_{n+1}$ a principal ideal of $R_{n+1}$ for each $n$. We examine properties
of the integrally closed local domain $S = \bigcup_{n \ge 0}R_n$.
",mathematics
"  In the field of software engineering there are many new archetypes are
introducing day to day Improve the efficiency and effectiveness of software
development. Due to dynamic environment organizations are frequently exchanging
their software constraint to meet their objectives. The propose research is a
new approach by integrating the traditional V model and agile methodology to
combining the strength of these models while minimizing their individual
weakness.The fluctuating requirements of emerging a carried software system and
accumulative cost of operational software are imposing researchers and experts
to determine innovative and superior means for emerging software application at
slight business or at enterprise level are viewing for. Agile methodology has
its own benefits but there are deficiency several of the features of
traditional software development methodologies that are essential for success.
Thats why an embedded approach will be the right answer for software industry
rather than a pure agile approach. This research shows how agile embedded
traditional can play a vital role in development of software. A survey
conducted to find the impact of this approach in industry. Both qualitative and
quantitative analysis performed.
",computer-science
"  In this work, we propose to train a deep neural network by distributed
optimization over a graph. Two nonlinear functions are considered: the
rectified linear unit (ReLU) and a linear unit with both lower and upper
cutoffs (DCutLU). The problem reformulation over a graph is realized by
explicitly representing ReLU or DCutLU using a set of slack variables. We then
apply the alternating direction method of multipliers (ADMM) to update the
weights of the network layerwise by solving subproblems of the reformulated
problem. Empirical results suggest that the ADMM-based method is less sensitive
to overfitting than the stochastic gradient descent (SGD) and Adam methods.
",computer-science
"  Context. The 4th release of the SDSS Moving Object Catalog (SDSSMOC) is
presently the largest photometric dataset of asteroids. Up to this point, the
release of large asteroid datasets has always been followed by a redefinition
of asteroid taxonomy. In the years that followed the release of the first
SDSSMOC, several classification schemes using its data were proposed, all using
the taxonomic classes from previous taxonomies. However, no successful attempt
has been made to derive a new taxonomic system directly from the SDSS dataset.
Aims. The scope of the work is to propose a different interpretation scheme for
gauging u0g0r0i0z0 asteroid observations based on the continuity of spectral
features. The scheme is integrated into previous taxonomic labeling, but is not
dependent on them. Methods. We analyzed the behavior of asteroid sampling
through principal components analysis to understand the role of uncertainties
in the SDSSMOC. We identified that asteroids in this space follow two separate
linear trends using reflectances in the visible, which is characteristic of
their spectrophotometric features. Results. Introducing taxonomic classes, we
are able to interpret both trends as representative of featured and featureless
spectra. The evolution within the trend is connected mainly to the band depth
for featured asteroids and to the spectral slope for featureless ones. We
defined a different taxonomic system that allowed us to only classify asteroids
by two labels. Conclusions. We have classified 69% of all SDSSMOC sample, which
is a robustness higher than reached by previous SDSS classifications.
Furthermore, as an example, we present the behavior of asteroid (5129) Groom,
whose taxonomic labeling changes according to one of the trends owing to phase
reddening. Now, such behavior can be characterized by the variation of one
single parameter, its position in the trend.
",physics
"  An exciting branch of machine learning research focuses on methods for
learning, optimizing, and integrating unknown functions that are difficult or
costly to evaluate. A popular Bayesian approach to this problem uses a Gaussian
process (GP) to construct a posterior distribution over the function of
interest given a set of observed measurements, and selects new points to
evaluate using the statistics of this posterior. Here we extend these methods
to exploit derivative information from the unknown function. We describe
methods for Bayesian optimization (BO) and Bayesian quadrature (BQ) in settings
where first and second derivatives may be evaluated along with the function
itself. We perform sampling-based inference in order to incorporate uncertainty
over hyperparameters, and show that both hyperparameter and function
uncertainty decrease much more rapidly when using derivative information.
Moreover, we introduce techniques for overcoming ill-conditioning issues that
have plagued earlier methods for gradient-enhanced Gaussian processes and
kriging. We illustrate the efficacy of these methods using applications to real
and simulated Bayesian optimization and quadrature problems, and show that
exploting derivatives can provide substantial gains over standard methods.
",statistics
"  The purpose of the present paper is to investigate a fusion rule algebra
arising from irreducible characters of a compact group $G$ and a closed
subgroup $G_0$ of $G$ with finite index. The convolution of this fusion rule
algebra is introduced by inducing irreducible representations of $G_0$ to $G$
and by restricting irreducible representations of $G$ to $G_0$.
",mathematics
"  More than 10^43 positrons annihilate every second in the centre of our Galaxy
yet, despite four decades of observations, their origin is still unknown. Many
candidates have been proposed, such as supernovae and low mass X-ray binaries.
However, these models are difficult to reconcile with the distribution of
positrons, which are highly concentrated in the Galactic bulge, and therefore
require specific propagation of the positrons through the interstellar medium.
Alternative sources include dark matter decay, or the supermassive black hole,
both of which would have a naturally high bulge-to-disc ratio.
The chief difficulty in reconciling models with the observations is the
intrinsically poor angular resolution of gamma-ray observations, which cannot
resolve point sources. Essentially all of the positrons annihilate via the
formation of positronium. This gives rise to the possibility of observing
recombination lines of positronium emitted before the atom annihilates. These
emission lines would be in the UV and the NIR, giving an increase in angular
resolution of a factor of 10^4 compared to gamma ray observations, and allowing
the discrimination between point sources and truly diffuse emission.
Analogously to the formation of positronium, it is possible to form atoms of
true muonium and true tauonium. Since muons and tauons are intrinsically
unstable, the formation of such leptonium atoms will be localised to their
places of origin. Thus observations of true muonium or true tauonium can
provide another way to distinguish between truly diffuse sources such as dark
matter decay, and an unresolved distribution of point sources.
",physics
"  What can we learn from a connectome? We constructed a simplified model of the
first two stages of the fly visual system, the lamina and medulla. The
resulting hexagonal lattice convolutional network was trained using
backpropagation through time to perform object tracking in natural scene
videos. Networks initialized with weights from connectome reconstructions
automatically discovered well-known orientation and direction selectivity
properties in T4 neurons and their inputs, while networks initialized at random
did not. Our work is the first demonstration, that knowledge of the connectome
can enable in silico predictions of the functional properties of individual
neurons in a circuit, leading to an understanding of circuit function from
structure alone.
",quantitative-biology
"  The exploitation of the excellent intrinsic electronic properties of graphene
for device applications is hampered by a large contact resistance between the
metal and graphene. The formation of edge contacts rather than top contacts is
one of the most promising solutions for realizing low ohmic contacts. In this
paper the fabrication and characterization of edge contacts to large area
CVD-grown monolayer graphene by means of optical lithography using CMOS
compatible metals, i.e. Nickel and Aluminum is reported. Extraction of the
contact resistance by Transfer Line Method (TLM) as well as the direct
measurement using Kelvin Probe Force Microscopy demonstrates a very low width
specific contact resistance.
",physics
"  Automatic machine learning performs predictive modeling with high performing
machine learning tools without human interference. This is achieved by making
machine learning applications parameter-free, i.e. only a dataset is provided
while the complete model selection and model building process is handled
internally through (often meta) optimization. Projects like Auto-WEKA and
auto-sklearn aim to solve the Combined Algorithm Selection and Hyperparameter
optimization (CASH) problem resulting in huge configuration spaces. However,
for most real-world applications, the optimization over only a few different
key learning algorithms can not only be sufficient, but also potentially
beneficial. The latter becomes apparent when one considers that models have to
be validated, explained, deployed and maintained. Here, less complex model are
often preferred, for validation or efficiency reasons, or even a strict
requirement. Automatic gradient boosting simplifies this idea one step further,
using only gradient boosting as a single learning algorithm in combination with
model-based hyperparameter tuning, threshold optimization and encoding of
categorical features. We introduce this general framework as well as a concrete
implementation called autoxgboost. It is compared to current AutoML projects on
16 datasets and despite its simplicity is able to achieve comparable results on
about half of the datasets as well as performing best on two.
",statistics
"  A finite-support constraint on the parameter space is used to derive a lower
bound on the error of an estimator of the correlation coefficient in the
bivariate exponential distribution. The bound is then exploited to examine
optimality of three estimators, each being a nonlinear function of moments of
exponential or Rayleigh observables. The estimator based on a measure of cosine
similarity is shown to be highly efficient for values of the correlation
coefficient greater than 0.35; for smaller values, however, it is the
transformed Pearson correlation coefficient that exhibits errors closer to the
derived bound.
",statistics
"  The kinetic effects of electrons are important to long wavelength
magnetohydrodynamic(MHD)instabilities and short wavelength drift-Alfvenic
instabilities responsible for turbulence transport in magnetized plasmas, since
the non-adiabatic electron can interact with, modify and drive the low
frequency instabilities. A novel conservative split weight scheme is proposed
for the electromagnetic simulation with drift kinetic electrons in tokamak
plasmas, which shows great computational advantages that there is no numerical
constrain of electron skin depth on the perpendicular grid size without
sacrificing any physics. Both kinetic Alfven wave and collision-less tearing
mode are verified by using this model, which has already been implemented into
the gyrokinetic toroidal code(GTC). This model will be used for the micro
tearing mode and neoclassical tearing mode simulation based on the first
principle in the future.
",physics
"  We study fairness in collaborative-filtering recommender systems, which are
sensitive to discrimination that exists in historical data. Biased data can
lead collaborative filtering methods to make unfair predictions against
minority groups of users. We identify the insufficiency of existing fairness
metrics and propose four new metrics that address different forms of
unfairness. These fairness metrics can be optimized by adding fairness terms to
the learning objective. Experiments on synthetic and real data show that our
new metrics can better measure fairness than the baseline, and that the
fairness objectives effectively help reduce unfairness.
",computer-science
"  Let H(q,p) = p^2/2 + V(q) be a 1-degree of freedom mechanical Hamiltonian
with a C^n periodic potential V where n>4. The Nosé-thermostated system
associated to H is shown to have invariant tori near the infinite temperature
limit. This is shown to be true for all thermostats similar to Nosé's. These
results complement the result of Legoll, Luskin and Moeckel who proved the
existence of such tori near the decoupling limit.
",mathematics
"  Transformation optics methods and gradient index electromagnetic structures
rely upon spatially varied arbitrary permittivity. This, along with recent
interest in millimeter-wave lens-based antennas demands high spatial resolution
dielectric variation. Perforated media have been used to fabricate gradient
index structures from microwaves to THz but are often limited in contrast. We
show that by employing regular polygon unit-cells (hexagon, square, and
triangle) on matched lattices we can realize very high contrast permittivity
ranging from 0.1-1.0 of the background permittivity. Silicon micromachining
(Bosch process) is performed on high resistivity Silicon wafers to achieve a
minimum permittivity of 1.25 (10% of Silicon) in the WR28 waveguide band,
specifically targeting the proposed 39 GHz 5G communications band. The method
is valid into the THz band.
",physics
"  Hydrogen peroxide (H2O2) is an important signaling molecule in cancer cells.
However, the significant secretion of H2O2 by cancer cells have been rarely
observed. Cold atmospheric plasma (CAP) is a near room temperature ionized gas
composed of neutral particles, charged particles, reactive species, and
electrons. Here, we first demonstrated that breast cancer cells and pancreatic
adenocarcinoma cells generated micromolar level H2O2 during just 1 min of
direct CAP treatment on these cells. The cell-based H2O2 generation is affected
by the medium volume, the cell confluence, as well as the discharge voltage.
The application of cold atmospheric plasma (CAP) in the cancer treatment has
been intensively investigated over the past decade. Several cellular responses
to the CAP treatment have been observed including the consumption of the
CAP-originated reactive species, the rise of intracellular reactive oxygen
species, the damage on DNA and mitochondria, as well as the activation of
apoptotic events. This is a new previously unknown cellular response to CAP,
which provides a new prospective to understand the interaction between CAP and
cells.
",physics
"  We have performed realistic atomistic simulations at finite temperatures
using Monte Carlo and atomistic spin dynamics simulations incorporating quantum
(Bose-Einstein) statistics. The description is much improved at low
temperatures compared to classical (Boltzmann) statistics normally used in
these kind of simulations, while at higher temperatures the classical
statistics are recovered. This corrected low-temperature description is
reflected in both magnetization and the magnetic specific heat, the latter
allowing for improved modeling of the magnetic contribution to free energies. A
central property in the method is the magnon density of states at finite
temperatures and we have compared several different implementations for
obtaining it. The method has no restrictions regarding chemical and magnetic
order of the considered materials. This is demonstrated by applying the method
to elemental ferromagnetic systems, including Fe and Ni, as well as Fe-Co
random alloys and the ferrimagnetic system GdFe$_3$ .
",physics
"  Motivated by truncated EM method introduced by Mao (2015), a new explicit
numerical method named modified truncated Euler-Maruyama method is developed in
this paper. Strong convergence rates of the given numerical scheme to the exact
solutions to stochastic differential equations are investigated under given
conditions in this paper. Compared with truncated EM method, the given
numerical simulation strongly converges to the exact solution at fixed time $T$
and over a time interval $[0,T]$ under weaker sufficient conditions. Meanwhile,
the convergence rates are also obtained for both cases. Two examples are
provided to support our conclusions.
",mathematics
"  We identify and describe the main dynamic regimes occurring during the
melting of the PCM n-octadecane in horizontal layers of several sizes heated
from below. This configuration allows to cover a wide range of effective
Rayleigh numbers on the liquid PCM phase, up to $\sim 10^9$, without changing
any external parameter control. We identify four different regimes as time
evolves: (i) the conductive regime, (ii) linear regime, (iii) coarsening regime
and (iv) turbulent regime. The first two regimes appear at all domain sizes.
However the third and fourth regime require a minimum advance of the
solid/liquid interface to develop, and we observe them only for large enough
domains.
The transition to turbulence takes places after a secondary instability that
forces the coarsening of the thermal plumes. Each one of the melting regimes
creates a distinct solid/liquid front that characterizes the internal state of
the melting process. We observe that most of the magnitudes of the melting
process are ruled by power laws, although not all of them. Thus the number of
plumes, some regimes of the Rayleigh number as a function of time, the number
of plumes after the primary and secondary instability, the thermal and kinetic
boundary layers follow simple power laws. In particular, we find that the
Nusselt number scales with the Rayleigh number as $Nu \sim Ra^{0.29}$ in the
turbulent regime, consistent with theories and experiments on Rayleigh-Bénard
convection that show an exponent $2/7$.
",physics
"  In this paper, we study the energy decay for the thermoelastic Bresse system
in the whole line with two different dissipative mechanism, given by heat
conduction (Types I and III). We prove that the decay rate of the solutions are
very slow. More precisely, we show that the solutions decay with the rate of
$(1+t)^{-\frac{1}{8}}$ in the $L^2$-norm, whenever the initial data belongs to
$L^1(R) \cap H^{s}(R)$ for a suitable $s$. The wave speeds of propagation have
influence on the decay rate with respect to the regularity of the initial data.
This phenomenon is known as \textit{regularity-loss}. The main tool used to
prove our results is the energy method in the Fourier space.
",mathematics
"  We study many-body localization properties of the disordered XXZ spin chain
in the Ising phase. Disorder is introduced via a random magnetic field in the
$z$-direction. We prove a strong form of dynamical exponential clustering for
eigenstates in the droplet spectrum: For any pair of local observables
separated by a distance $\ell$, the sum of the associated correlators over
these states decays exponentially in $\ell$, in expectation. This exponential
clustering persists under the time evolution in the droplet spectrum. Our
result applies to the large disorder regime as well as to the strong Ising
phase at fixed disorder, with bounds independent of the support of the
observables.
",mathematics
"  The mixture of factor analyzers model was first introduced over 20 years ago
and, in the meantime, has been extended to several non-Gaussian analogues. In
general, these analogues account for situations with heavy tailed and/or skewed
clusters. An approach is introduced that unifies many of these approaches into
one very general model: the mixture of hidden truncation hyperbolic factor
analyzers (MHTHFA) model. In the process of doing this, a hidden truncation
hyperbolic factor analysis model is also introduced. The MHTHFA model is
illustrated for clustering as well as semi-supervised classification using two
real datasets.
",statistics
"  This paper studies mathematical properties of reaction systems that was
introduced by Enrenfeucht and Rozenberg as computational models inspired by
biochemical reaction in the living cells. In particular, we continue the study
on the generative power of functions specified by minimal reaction systems
under composition initiated by Salomaa. Allowing degenerate reaction systems,
functions specified by minimal reaction systems over a quarternary alphabet
that are permutations generate the alternating group on the power set of the
background set.
",computer-science
"  Wind shear measured by Doppler tracking of the Huygens probe is evaluated,
and found to be within the range anticipated by pre-flight assessments (namely
less than two times the Brunt-Vaisala frequency). The strongest large-scale
shear encountered was ~5 m/s/km, a level associated with 'Light' turbulence in
terrestrial aviation. Near-surface winds (below 4km) have small-scale
fluctuations of ~0.2 m/s , indicated both by probe tilt and Doppler tracking,
and the characteristics of the fluctuation, of interest for future missions to
Titan, can be reproduced with a simple autoregressive (AR(1)) model. The
turbulent dissipation rate at an altitude of ~500m is found to be 16 cm2/sec3,
which may be a useful benchmark for atmospheric circulation models.
",physics
"  Using supercharacter theory, we identify the matrices that are diagonalized
by the discrete cosine and discrete sine transforms, respectively. Our method
affords a combinatorial interpretation for the matrix entries.
",mathematics
"  Syntax errors are generally easy to fix for humans, but not for parsers, in
general, and LR parsers, in particular. Traditional 'panic mode' error
recovery, though easy to implement and applicable to any grammar, often leads
to a cascading chain of errors that drown out the original. More advanced error
recovery techniques suffer less from this problem but have seen little
practical use because their typical performance was seen as poor, their worst
case unbounded, and the repairs they reported arbitrary. In this paper we show
two generic error recovery algorithms that fix all three problems. First, our
algorithms are the first to report the complete set of possible repair
sequences for a given location, allowing programmers to select the one that
best fits their intention. Second, on a corpus of 200,000 real-world
syntactically invalid Java programs, we show that our best performing algorithm
is able to repair 98.71% of files within a cut-off of 0.5s. Furthermore, we are
also able to use the complete set of repair sequences to reduce the cascading
error problem even further than previous approaches. Our best performing
algorithm reports 442,252.0 error locations in the corpus to the user, while
the panic mode algorithm reports 980,848.0 error locations: in other words, our
algorithms reduce the cascading error problem by well over half.
",computer-science
"  This paper addresses the problem of multi-view people occupancy map
estimation. Existing solutions for this problem either operate per-view, or
rely on a background subtraction pre-processing. Both approaches lessen the
detection performance as scenes become more crowded. The former does not
exploit joint information, whereas the latter deals with ambiguous input due to
the foreground blobs becoming more and more interconnected as the number of
targets increases.
Although deep learning algorithms have proven to excel on remarkably numerous
computer vision tasks, such a method has not been applied yet to this problem.
In large part this is due to the lack of large-scale multi-camera data-set.
The core of our method is an architecture which makes use of monocular
pedestrian data-set, available at larger scale then the multi-view ones,
applies parallel processing to the multiple video streams, and jointly utilises
it. Our end-to-end deep learning method outperforms existing methods by large
margins on the commonly used PETS 2009 data-set. Furthermore, we make publicly
available a new three-camera HD data-set. Our source code and trained models
will be made available under an open-source license.
",computer-science
"  Inverse Uncertainty Quantification (UQ), or Bayesian calibration, is the
process to quantify the uncertainties of random input parameters based on
experimental data. The introduction of model discrepancy term is significant
because ""over-fitting"" can theoretically be avoided. But it also poses
challenges in the practical applications. One of the mostly concerned and
unresolved problem is the ""lack of identifiability"" issue. With the presence of
model discrepancy, inverse UQ becomes ""non-identifiable"" in the sense that it
is difficult to precisely distinguish between the parameter uncertainties and
model discrepancy when estimating the calibration parameters. Previous research
to alleviate the non-identifiability issue focused on using informative priors
for the calibration parameters and the model discrepancy, which is usually not
a viable solution because one rarely has such accurate and informative prior
knowledge. In this work, we show that identifiability is largely related to the
sensitivity of the calibration parameters with regards to the chosen responses.
We adopted an improved modular Bayesian approach for inverse UQ that does not
require priors for the model discrepancy term. The relationship between
sensitivity and identifiability was demonstrated with a practical example in
nuclear engineering. It was shown that, in order for a certain calibration
parameter to be statistically identifiable, it should be significant to at
least one of the responses whose data are used for inverse UQ. Good
identifiability cannot be achieved for a certain calibration parameter if it is
not significant to any of the responses. It is also demonstrated that ""fake
identifiability"" is possible if model responses are not appropriately chosen,
or inaccurate but informative priors are specified.
",statistics
"  Given a closed oriented surface S we describe those cohomology classes which
appear as the period characters of abelian differentials for some choice of
complex structure on S consistent with the orientation. The proof is based upon
Ratner's solution of Raghunathan's conjecture.
",mathematics
"  We revisit a classical scenario in communication theory: a source is
generating a waveform which we sample at regular intervals; we wish to
transform the signal in such a way as to minimize distortion in its
reconstruction, despite noise. The transformation must be online (also called
causal), in order to enable real-time signaling. The noise model we consider is
adversarial $\ell_1$-bounded; this is the ""atomic norm"" convex relaxation of
the standard adversary model in discrete-alphabet communications, namely
sparsity (low Hamming weight). We require that our encoding not increase the
power of the original signal.
In the ""block coding"" setting such encoding is possible due to the existence
of large almost-Euclidean sections in $\ell_1$ spaces (established in the work
of Dvoretzky, Milman, Kašin, and Figiel, Lindenstrauss and Milman).
Our main result is that an analogous result is achievable even online.
Equivalently, we show a ""lower triangular"" version of $\ell_1$ Dvoretzky
theorems. In terms of communication, the result has the following form: If the
signal is a stream of reals $x_1,\ldots$, one per unit time, which we encode
causally into $\rho$ (a constant) reals per unit time (forming altogether an
output stream $\mathcal{E}(x)$), and if the adversarial noise added to this
encoded stream up to time $s$ is a vector $\vec{y}$, then at time $s$ the
decoder's reconstruction of the input prefix $x_{[s]}$ is accurate in a
time-weighted $\ell_2$ norm, to within $s^{-1/2+\delta}$ (any $\delta>0$) times
the adversary's noise as measured in a time-weighted $\ell_1$ norm. The
time-weighted decoding norm forces increasingly accurate reconstruction of the
distant past, while the time-weighted noise norm permits only vanishing effect
from noise in the distant past.
Encoding is linear, and decoding is performed by an LP analogous to those
used in compressed sensing.
",computer-science
"  We describe a neural network model that jointly learns distributed
representations of texts and knowledge base (KB) entities. Given a text in the
KB, we train our proposed model to predict entities that are relevant to the
text. Our model is designed to be generic with the ability to address various
NLP tasks with ease. We train the model using a large corpus of texts and their
entity annotations extracted from Wikipedia. We evaluated the model on three
important NLP tasks (i.e., sentence textual similarity, entity linking, and
factoid question answering) involving both unsupervised and supervised
settings. As a result, we achieved state-of-the-art results on all three of
these tasks. Our code and trained models are publicly available for further
academic research.
",computer-science
"  Deep Learning refers to a set of machine learning techniques that utilize
neural networks with many hidden layers for tasks, such as image
classification, speech recognition, language understanding. Deep learning has
been proven to be very effective in these domains and is pervasively used by
many Internet services. In this paper, we describe different automotive uses
cases for deep learning in particular in the domain of computer vision. We
surveys the current state-of-the-art in libraries, tools and infrastructures
(e.\,g.\ GPUs and clouds) for implementing, training and deploying deep neural
networks. We particularly focus on convolutional neural networks and computer
vision use cases, such as the visual inspection process in manufacturing plants
and the analysis of social media data. To train neural networks, curated and
labeled datasets are essential. In particular, both the availability and scope
of such datasets is typically very limited. A main contribution of this paper
is the creation of an automotive dataset, that allows us to learn and
automatically recognize different vehicle properties. We describe an end-to-end
deep learning application utilizing a mobile app for data collection and
process support, and an Amazon-based cloud backend for storage and training.
For training we evaluate the use of cloud and on-premises infrastructures
(including multiple GPUs) in conjunction with different neural network
architectures and frameworks. We assess both the training times as well as the
accuracy of the classifier. Finally, we demonstrate the effectiveness of the
trained classifier in a real world setting during manufacturing process.
",computer-science
"  Evidence accumulation models of simple decision-making have long assumed that
the brain estimates a scalar decision variable corresponding to the
log-likelihood ratio of the two alternatives. Typical neural implementations of
this algorithmic cognitive model assume that large numbers of neurons are each
noisy exemplars of the scalar decision variable. Here we propose a neural
implementation of the diffusion model in which many neurons construct and
maintain the Laplace transform of the distance to each of the decision bounds.
As in classic findings from brain regions including LIP, the firing rate of
neurons coding for the Laplace transform of net accumulated evidence grows to a
bound during random dot motion tasks. However, rather than noisy exemplars of a
single mean value, this approach makes the novel prediction that firing rates
grow to the bound exponentially, across neurons there should be a distribution
of different rates. A second set of neurons records an approximate inversion of
the Laplace transform, these neurons directly estimate net accumulated
evidence. In analogy to time cells and place cells observed in the hippocampus
and other brain regions, the neurons in this second set have receptive fields
along a ""decision axis."" This finding is consistent with recent findings from
rodent recordings. This theoretical approach places simple evidence
accumulation models in the same mathematical language as recent proposals for
representing time and space in cognitive models for memory.
",quantitative-biology
"  Preprocessing tools for automated text analysis have become more widely
available in major languages, but non-English tools are often still limited in
their functionality. When working with Spanish-language text, researchers can
easily find tools for tokenization and stemming, but may not have the means to
extract more complex word features like verb tense or mood. Yet Spanish is a
morphologically rich language in which such features are often identifiable
from word form. Conjugation rules are consistent, but many special verbs and
nouns take on different rules. While building a complete dictionary of known
words and their morphological rules would be labor intensive, resources to do
so already exist, in spell checkers designed to generate valid forms of known
words. This paper introduces a set of tools for Spanish-language morphological
analysis, built using the COES spell checking tools, to label person, mood,
tense, gender and number, derive a word's root noun or verb infinitive, and
convert verbs to their nominal form.
",computer-science
"  Given an integer base $b>1$, a set of integers is represented in base $b$ by
a language over $\{0,1,...,b-1\}$. The set is said to be $b$-recognisable if
its representation is a regular language. It is known that eventually periodic
sets are $b$-recognisable in every base $b$, and Cobham's theorem implies the
converse: no other set is $b$-recognisable in every base $b$.
We are interested in deciding whether a $b$-recognisable set of integers
(given as a finite automaton) is eventually periodic. Honkala showed that this
problem decidable in 1986 and recent developments give efficient decision
algorithms. However, they only work when the integers are written with the
least significant digit first.
In this work, we consider the natural order of digits (Most Significant Digit
First) and give a quasi-linear algorithm to solve the problem in this case.
",computer-science
"  Today's telecommunication networks have become sources of enormous amounts of
widely heterogeneous data. This information can be retrieved from network
traffic traces, network alarms, signal quality indicators, users' behavioral
data, etc. Advanced mathematical tools are required to extract meaningful
information from these data and take decisions pertaining to the proper
functioning of the networks from the network-generated data. Among these
mathematical tools, Machine Learning (ML) is regarded as one of the most
promising methodological approaches to perform network-data analysis and enable
automated network self-configuration and fault management. The adoption of ML
techniques in the field of optical communication networks is motivated by the
unprecedented growth of network complexity faced by optical networks in the
last few years. Such complexity increase is due to the introduction of a huge
number of adjustable and interdependent system parameters (e.g., routing
configurations, modulation format, symbol rate, coding schemes, etc.) that are
enabled by the usage of coherent transmission/reception technologies, advanced
digital signal processing and compensation of nonlinear effects in optical
fiber propagation. In this paper we provide an overview of the application of
ML to optical communications and networking. We classify and survey relevant
literature dealing with the topic, and we also provide an introductory tutorial
on ML for researchers and practitioners interested in this field. Although a
good number of research papers have recently appeared, the application of ML to
optical networks is still in its infancy: to stimulate further work in this
area, we conclude the paper proposing new possible research directions.
",statistics
"  We developed control and visualization programs, YUI and HANA, for High-
Resolution Chopper spectrometer (HRC) installed at BL12 in MLF, J-PARC. YUI is
a comprehensive program to control DAQ-middleware, the accessories, and sample
environment devices. HANA is a program for the data transformation and
visualization of inelastic neutron scattering spectra. In this paper, we
describe the basic system structures and unique functions of these programs
from the viewpoint of users.
",physics
"  One of the key aspects of the United States democracy is free and fair
elections that allow for a peaceful transfer of power from one President to the
next. The 2016 US presidential election stands out due to suspected foreign
influence before, during, and after the election. A significant portion of that
suspected influence was carried out via social media. In this paper, we look
specifically at 3,500 Facebook ads allegedly purchased by the Russian
government. These ads were released on May 10, 2018 by the US Congress House
Intelligence Committee. We analyzed the ads using natural language processing
techniques to determine textual and semantic features associated with the most
effective ones. We clustered the ads over time into the various campaigns and
the labeled parties associated with them. We also studied the effectiveness of
Ads on an individual, campaign and party basis. The most effective ads tend to
have less positive sentiment, focus on past events and are more specific and
personalized in nature. The more effective campaigns also show such similar
characteristics. The campaigns' duration and promotion of the Ads suggest a
desire to sow division rather than sway the election.
",computer-science
"  Under the generalized Lindelöf hypothesis, the exponent in the error term
of the prime geodesic theorem for the modular surface is reduced to
$\frac{5}{8}+\varepsilon $ outside a set of finite logarithmic measure.
",mathematics
"  We provide a method to solve optimization problem when objective function is
a complex stochastic simulator of an urban transportation system. To reach this
goal, a Bayesian optimization framework is introduced. We show how the choice
of prior and inference algorithm effect the outcome of our optimization
procedure. We develop dimensionality reduction techniques that allow for our
optimization techniques to be applicable for real-life problems. We develop a
distributed, Gaussian Process Bayesian regression and active learning models
that allow parallel execution of our algorithms and enable usage of high
performance computing. We present a fully Bayesian approach that is more sample
efficient and reduces computational budget. Our framework is supported by
theoretical analysis and an empirical study. We demonstrate our framework on
the problem of calibrating a multi-modal transportation network of city of
Bloomington, Illinois. Finally, we discuss directions for further research.
",statistics
"  In recent years an increasing number of observational studies have hinted at
the presence of warps in protoplanetary discs, however a general comprehensive
description of observational diagnostics of warped discs was missing. We
performed a series of 3D SPH hydrodynamic simulations and combined them with 3D
radiative transfer calculations to study the observability of warps in
circumbinary discs, whose plane is misaligned with respect to the orbital plane
of the central binary. Our numerical hydrodynamic simulations confirm previous
analytical results on the dependence of the warp structure on the viscosity and
the initial misalignment between the binary and the disc. To study the
observational signatures of warps we calculate images in the continuum at
near-infrared and sub-millimetre wavelengths and in the pure rotational
transition of CO in the sub-millimetre. Warped circumbinary discs show surface
brightness asymmetry in near-infrared scattered light images as well as in
optically thick gas lines at sub-millimetre wavelengths. The asymmetry is
caused by self-shadowing of the disc by the inner warped regions, thus the
strength of the asymmetry depends on the strength of the warp. The projected
velocity field, derived from line observations, shows characteristic
deviations, twists and a change in the slope of the rotation curve, from that
of an unperturbed disc. In extreme cases even the direction of rotation appears
to change in the disc inwards of a characteristic radius. The strength of the
kinematical signatures of warps decreases with increasing inclination. The
strength of all warp signatures decreases with decreasing viscosity.
",physics
"  Composition and lattice join (transitive closure of a union) of equivalence
relations are operations taking pairs of decidable equivalence relations to
relations that are semi-decidable, but not necessarily decidable. This article
addresses the question, is every semi-decidable equivalence relation obtainable
in those ways from a pair of decidable equivalence relations? It is shown that
every semi-decidable equivalence relation, of which every equivalence class is
infinite, is obtainable as both a composition and a lattice join of decidable
equivalence relations having infinite equivalence classes. An example is
constructed of a semi-decidable, but not decidable, equivalence relation having
finite equivalence classes that can be obtained from decidable equivalence
relations, both by composition and also by lattice join. Another example is
constructed, in which such a relation cannot be obtained from decidable
equivalence relations in either of the two ways.
",mathematics
"  This paper considers the problem of predicting the number of claims that have
already incurred in past exposure years, but which have not yet been reported
to the insurer. This is an important building block in the risk management
strategy of an insurer since the company should be able to fulfill its
liabilities with respect to such claims. Our approach puts emphasis on modeling
the time between the occurrence and reporting of claims, the so-called
reporting delay. Using data at a daily level we propose a micro-level model for
the heterogeneity in reporting delay caused by calendar day effects in the
reporting process, such as the weekday pattern and holidays. A simulation study
identifies the strengths and weaknesses of our approach in several scenarios
compared to traditional methods to predict the number of incurred but not
reported claims from aggregated data (i.e. the chain ladder method). We also
illustrate our model on a European general liability insurance data set and
conclude that the granular approach compared to the chain ladder method is more
robust with respect to volatility in the occurrence process. Our framework can
be extended to other predictive problems where interest goes to events that
incurred in the past but which are subject to an observation delay (e.g. the
number of infections during an epidemic).
",quantitative-finance
"  In this paper we generalize the main result of [4] for manifolds that are not
necessarily Einstein. In fact, we obtain an upper bound for the volume of a
locally volume-minimizing closed hypersurface $\Sigma$ of a Riemannian
5-manifold $M$ with scalar curvature bounded from below by a positive constant
in terms of the total traceless Ricci curvature of $\Sigma$. Furthermore, if
$\Sigma$ saturates the respective upper bound and $M$ has nonnegative Ricci
curvature, then $\Sigma$ is isometric to $\mathbb{S}^4$ up to scaling and $M$
splits in a neighborhood of $\Sigma$. Also, we obtain a rigidity result for the
Riemannian cover of $M$ when $\Sigma$ minimizes the volume in its homotopy
class and saturates the upper bound.
",mathematics
"  Fast timing capability in X-ray observation of astrophysical objects is one
of the key properties for the ASTRO-H (Hitomi) mission. Absolute timing
accuracies of 350 micro second or 35 micro second are required to achieve
nominal scientific goals or to study fast variabilities of specific sources.
The satellite carries a GPS receiver to obtain accurate time information, which
is distributed from the central onboard computer through the large and complex
SpaceWire network. The details on the time system on the hardware and software
design are described. In the distribution of the time information, the
propagation delays and jitters affect the timing accuracy. Six other items
identified within the timing system will also contribute to absolute time
error. These error items have been measured and checked on ground to ensure the
time error budgets meet the mission requirements. The overall timing
performance in combination with hardware performance, software algorithm, and
the orbital determination accuracies, etc, under nominal conditions satisfies
the mission requirements of 35 micro second. This work demonstrates key points
for space-use instruments in hardware and software designs and calibration
measurements for fine timing accuracy on the order of microseconds for
mid-sized satellites using the SpaceWire (IEEE1355) network.
",physics
"  For $n\in \mathbb{N}$ let $S_n$ be the smallest number $S>0$ satisfying the
inequality $$ \int_K f \le S \cdot |K|^{\frac 1n} \cdot \max_{\xi\in S^{n-1}}
\int_{K\cap \xi^\bot} f $$ for all centrally-symmetric convex bodies $K$ in
$\mathbb{R}^n$ and all even, continuous probability densities $f$ on $K$. Here
$|K|$ is the volume of $K$. It was proved by the second-named author that
$S_n\le 2\sqrt{n}$, and in analogy with Bourgain's slicing problem, it was
asked whether $S_n$ is bounded from above by a universal constant. In this note
we construct an example showing that $S_n\ge c\sqrt{n}/\sqrt{\log \log n},$
where $c > 0$ is an absolute constant. Additionally, for any $0 < \alpha < 2$
we describe a related example that satisfies the so-called
$\psi_{\alpha}$-condition.
",mathematics
"  Among the Milky Way satellites discovered in the past three years, Triangulum
II has presented the most difficulty in revealing its dynamical status. Kirby
et al. (2015a) identified it as the most dark matter-dominated galaxy known,
with a mass-to-light ratio within the half-light radius of 3600 +3500 -2100
M_sun/L_sun. On the other hand, Martin et al. (2016) measured an outer velocity
dispersion that is 3.5 +/- 2.1 times larger than the central velocity
dispersion, suggesting that the system might not be in equilibrium. From new
multi-epoch Keck/DEIMOS measurements of 13 member stars in Triangulum II, we
constrain the velocity dispersion to be sigma_v < 3.4 km/s (90% C.L.). Our
previous measurement of sigma_v, based on six stars, was inflated by the
presence of a binary star with variable radial velocity. We find no evidence
that the velocity dispersion increases with radius. The stars display a wide
range of metallicities, indicating that Triangulum II retained supernova ejecta
and therefore possesses or once possessed a massive dark matter halo. However,
the detection of a metallicity dispersion hinges on the membership of the two
most metal-rich stars. The stellar mass is lower than galaxies of similar mean
stellar metallicity, which might indicate that Triangulum II is either a star
cluster or a tidally stripped dwarf galaxy. Detailed abundances of one star
show heavily depressed neutron-capture abundances, similar to stars in most
other ultra-faint dwarf galaxies but unlike stars in globular clusters.
",physics
"  Logistic linear mixed model is widely used in experimental designs and
genetic analysis with binary traits. Motivated by modern applications, we
consider the case with many groups of random effects and each group corresponds
to a variance component. When the number of variance components is large,
fitting the logistic linear mixed model is challenging. We develop two
efficient and stable minorization-maximization (MM) algorithms for the
estimation of variance components based on the Laplace approximation of the
logistic model. One of them leads to a simple iterative soft-thresholding
algorithm for variance component selection using maximum penalized approximated
likelihood. We demonstrate the variance component estimation and selection
performance of our algorithms by simulation studies and a real data analysis.
",statistics
"  For an affine toric variety $\mathrm{Spec}(A)$, we give a convex geometric
description of the Hodge decomposition of its Hochschild cohomology. Under
certain assumptions we compute the dimensions of the Hodge summands
$T^1_{(i)}(A)$, generalizing the existing results about the Andre-Quillen
cohomology group $T^1_{(1)}(A)$. We prove that every Poisson structure on a
possibly singular affine toric variety can be quantized in the sense of
deformation quantization.
",mathematics
"  In single star systems like our own Solar system, comets dominate the mass
budget of bodies that are ejected into interstellar space, since they form
further away and are less tightly bound. However 1I/`Oumuamua, the first
interstellar object detected, appears asteroidal in its spectra and in its lack
of detectable activity. We argue that the galactic budget of interstellar
objects like 1I/`Oumuamua should be dominated by planetesimal material ejected
during planet formation in circumbinary systems, rather than in single star
systems or widely separated binaries. We further show that in circumbinary
systems, rocky bodies should be ejected in comparable numbers to icy ones. This
suggests that a substantial fraction of additional interstellar objects
discovered in the future should display an active coma. We find that the rocky
population, of which 1I/`Oumuamua seems to be a member, should be predominantly
sourced from A-type and late B-star binaries.
",physics
"  For over twenty years, the term 'cosmic web' has guided our understanding of
the large-scale arrangement of matter in the cosmos, accurately evoking the
concept of a network of galaxies linked by filaments. But the physical
correspondence between the cosmic web and structural-engineering or textile
'spiderwebs' is even deeper than previously known, and extends to origami
tessellations as well. Here we explain that in a good structure-formation
approximation known as the adhesion model, threads of the cosmic web form a
spiderweb, i.e. can be strung up to be entirely in tension. The correspondence
is exact if nodes sampling voids are included, and if structure is excluded
within collapsed regions (walls, filaments and haloes), where dark-matter
multistreaming and baryonic physics affect the structure. We also suggest how
concepts arising from this link might be used to test cosmological models: for
example, to test for large-scale anisotropy and rotational flows in the cosmos.
",physics
"  Convolutional Neural Networks (CNNs) are a popular deep learning architecture
widely applied in different domains, in particular in classifying over images,
for which the concept of convolution with a filter comes naturally.
Unfortunately, the requirement of a distance (or, at least, of a neighbourhood
function) in the input feature space has so far prevented its direct use on
data types such as omics data. However, a number of omics data are metrizable,
i.e., they can be endowed with a metric structure, enabling to adopt a
convolutional based deep learning framework, e.g., for prediction. We propose a
generalized solution for CNNs on omics data, implemented through a dedicated
Keras layer. In particular, for metagenomics data, a metric can be derived from
the patristic distance on the phylogenetic tree. For transcriptomics data, we
combine Gene Ontology semantic similarity and gene co-expression to define a
distance; the function is defined through a multilayer network where 3 layers
are defined by the GO mutual semantic similarity while the fourth one by gene
co-expression. As a general tool, feature distance on omics data is enabled by
OmicsConv, a novel Keras layer, obtaining OmicsCNN, a dedicated deep learning
framework. Here we demonstrate OmicsCNN on gut microbiota sequencing data, for
Inflammatory Bowel Disease (IBD) 16S data, first on synthetic data and then a
metagenomics collection of gut microbiota of 222 IBD patients.
",statistics
"  Model-free policy learning has enabled robust performance of complex tasks
with relatively simple algorithms. However, this simplicity comes at the cost
of requiring an Oracle and arguably very poor sample complexity. This renders
such methods unsuitable for physical systems. Variants of model-based methods
address this problem through the use of simulators, however, this gives rise to
the problem of policy transfer from simulated to the physical system. Model
mismatch due to systematic parameter shift and unmodelled dynamics error may
cause sub-optimal or unsafe behavior upon direct transfer. We introduce the
Adaptive Policy Transfer for Stochastic Dynamics (ADAPT) algorithm that
achieves provably safe and robust, dynamically-feasible zero-shot transfer of
RL-policies to new domains with dynamics error. ADAPT combines the strengths of
offline policy learning in a black-box source simulator with online tube-based
MPC to attenuate bounded model mismatch between the source and target dynamics.
ADAPT allows online transfer of policy, trained solely in a simulation offline,
to a family of unknown targets without fine-tuning. We also formally show that
(i) ADAPT guarantees state and control safety through state-action tubes under
the assumption of Lipschitz continuity of the divergence in dynamics and, (ii)
ADAPT results in a bounded loss of reward accumulation relative to a policy
trained and evaluated in the source environment. We evaluate ADAPT on 2
continuous, non-holonomic simulated dynamical systems with 4 different
disturbance models, and find that ADAPT performs between 50%-300% better on
mean reward accrual than direct policy transfer.
",computer-science
"  Many-body phenomena were always an integral part of physics comprising of
collective behaviors through self-organization, in systems consisting of many
components and degrees of freedom. We investigate the collective behaviors of
strongly interacting particles confined in one dimension. We show that
many-body orders with topological characteristics can be found at the Mott
insulator limit for hardcore bosons, at different fillings, without considering
the spin degree of freedom or long-range microscopic interactions. These orders
have unique properties like weak or strong quantum correlations (entanglement),
quantified by the entanglement entropy, edge excitations/modes and gapped
energy spectrum with highly degenerate ground state, bearing resemblance to
topologically ordered phases of matter.
",physics
"  In this article, we propound a question on the annihilator of Koszul
homologies of a system of parameters of an almost complete intersection $R$.
The question can be stated in terms of the acyclicity of certain (finite)
residual approximation complexes whose $0$-th homologies are the residue field
of $R$. We show that our question has an affirmative answer for certain almost
complete intersection rings with small multiplicities, as well as for the
$1$-th Koszul homology of any almost complete intersection. The statement about
the $1$-th Koszul homology is shown to be equivalent to the Monomial Conjecture
and thus follows from its validity.
",mathematics
"  We consider the following control problem on fair allocation of indivisible
goods. Given a set $I$ of items and a set of agents, each having strict linear
preference over the items, we ask for a minimum subset of the items whose
deletion guarantees the existence of a proportional allocation in the remaining
instance; we call this problem Proportionality by Item Deletion (PID). Our main
result is a polynomial-time algorithm that solves PID for three agents. By
contrast, we prove that PID is computationally intractable when the number of
agents is unbounded, even if the number $k$ of item deletions allowed is small,
since the problem turns out to be W[3]-hard with respect to the parameter $k$.
Additionally, we provide some tight lower and upper bounds on the complexity of
PID when regarded as a function of $|I|$ and $k$.
",computer-science
"  The model studied in this paper is a stochastic extension of the so-called
neuron model introduced by Hodgkin and Huxley. In the sense of rough paths, the
model is perturbed by a multiplicative noise driven by a fractional Brownian
motion, with a vector field satisfying the viability condition of Coutin and
Marie for $\mathbb R\times [0,1]^3$. An application to the modeling of the
membrane potential of nerve fibers damaged by a neuropathy is provided.
",mathematics
"  A convex optimization-based method is proposed to numerically solve dynamic
programs in continuous state and action spaces. This approach using a
discretization of the state space has the following salient features. First, by
introducing an auxiliary optimization variable that assigns the contribution of
each grid point, it does not require an interpolation in solving an associated
Bellman equation and constructing a control policy. Second, the proposed method
allows us to solve the Bellman equation with a desired level of precision via
convex programming in the case of linear systems and convex costs. We can also
construct a control policy of which performance converges to the optimum as the
grid resolution becomes finer in this case. Third, when a nonlinear
control-affine system is considered, the convex optimization approach provides
an approximate control policy with a provable suboptimality bound. Fourth, for
general cases, the proposed convex formulation of dynamic programming operators
can be simply modified as a nonconvex bi-level program, in which the inner
problem is a linear program, without losing convergence properties. From our
convex methods and analyses, we observe that convexity in dynamic programming
deserves attention as it can play a critical role in obtaining a tractable and
convergent numerical solution.
",computer-science
"  Economic evaluations from individual-level data are an important component of
the process of technology appraisal, with a view to informing resource
allocation decisions. A critical problem in these analyses is that both
effectiveness and cost data typically present some complexity (e.g. non
normality, spikes and missingness) that should be addressed using appropriate
methods. However, in routine analyses, simple standardised approaches are
typically used, possibly leading to biased inferences. We present a general
Bayesian framework that can handle the complexity. We show the benefits of
using our approach with a motivating example, the MenSS trial, for which there
are spikes at one in the effectiveness and missingness in both outcomes. We
contrast a set of increasingly complex models and perform sensitivity analysis
to assess the robustness of the conclusions to a range of plausible missingness
assumptions. This paper highlights the importance of adopting a comprehensive
modelling approach to economic evaluations and the strategic advantages of
building these complex models within a Bayesian framework.
",statistics
"  Cauchy and exponential transforms are characterized, and constructed, as
canonical holomorphic sections of certain line bundles on the Riemann sphere
defined in terms of the Schwarz function. A well known natural connection
between Schwarz reflection and line bundles defined on the Schottky double of a
planar domain is briefly discussed in the same context.
",mathematics
"  This paper studies improving solvers based on their past solving experiences,
and focuses on improving solvers by offline training. Specifically, the key
issues of offline training methods are discussed, and research belonging to
this category but from different areas are reviewed in a unified framework.
Existing training methods generally adopt a two-stage strategy in which
selecting the training instances and training instances are treated in two
independent phases. This paper proposes a new training method, dubbed LiangYi,
which addresses these two issues simultaneously. LiangYi includes a training
module for a population-based solver and an instance sampling module for
updating the training instances. The idea behind LiangYi is to promote the
population-based solver by training it (with the training module) to improve
its performance on those instances (discovered by the sampling module) on which
it performs badly, while keeping the good performances obtained by it on
previous instances. An instantiation of LiangYi on the Travelling Salesman
Problem is also proposed. Empirical results on a huge testing set containing
10000 instances showed LiangYi could train solvers that perform significantly
better than the solvers trained by other state-of-the-art training method.
Moreover, empirical investigation of the behaviours of LiangYi confirmed it was
able to continuously improve the solver through training.
",computer-science
"  For lattice Monte Carlo simulations parallelization is crucial to make
studies of large systems and long simulation time feasible, while sequential
simulations remain the gold-standard for correlation-free dynamics. Here,
various domain decomposition schemes are compared, concluding with one which
delivers virtually correlation-free simulations on GPU Extensive simulations of
the octahedron model for $2+1$ dimensional Karda--Parisi--Zhang surface growth,
which is very sensitive to correlation in the site-selection dynamics, were
performed to show self-consistency of the parallel runs and agreement with the
sequential algorithm. We present a GPU implementation providing a speedup of
about $30\times$ over a parallel CPU implementation on a single socket and at
least $180\times$ with respect to the sequential reference.
",physics
"  We prove that all eigenstates of many-body localized symmetry protected
topological systems with time reversal symmetry have four-fold degenerate
entanglement spectra in the thermodynamic limit. To that end, we employ unitary
quantum circuits where the number of sites the gates act on grows linearly with
the system size. We find that the corresponding matrix product operator
representation has similar local symmetries as matrix product ground states of
symmetry protected topological phases. Those local symmetries give rise to a
$\mathbb{Z}_2$ topological index, which is robust against arbitrary
perturbations so long as they do not break time reversal symmetry or drive the
system out of the fully many-body localized phase.
",physics
"  Recently, the vertical shear instability (VSI) has become an attractive
purely hydrodynamic candidate for the anomalous angular momentum transport
required for weakly ionized accretion disks. In direct three-dimensional
numerical simulations of VSI turbulence in disks, a meridional circulation
pattern was observed that is opposite to the usual viscous flow behavior. Here,
we investigate whether this feature can possibly be explained by an anisotropy
of the VSI turbulence. Using three-dimensional hydrodynamical simulations, we
calculate the turbulent Reynolds stresses relevant for angular momentum
transport for a representative section of a disk.
We find that the vertical stress is significantly stronger than the radial
stress. Using our results in viscous disk simulations with different viscosity
coefficients for the radial and vertical direction, we find good agreement with
the VSI turbulence for the stresses and meridional flow; this provides
additional evidence for the anisotropy. The results are important with respect
to the transport of small embedded particles in disks.
",physics
"  Illicit online pharmacies allow the purchase of prescription drugs online
without a prescription. Such pharmacies leverage social media platforms such as
Twit- ter as a promotion and marketing tool with the intent of reaching out to
a larger, potentially younger demographics of the population. Given the serious
negative health effects that arise from abusing such drugs, it is important to
identify the relevant content on social media and exterminate their presence as
quickly as pos- sible. In response, we collected all the tweets that contained
the names of certain preselected controlled substances over a period of 5
months. We found that an unsupervised topic modeling based methodology is able
to identify tweets that promote and market controlled substances with high
precision. We also study the meta-data characteristics of such tweets and the
users who post them and find that they have several distinguishing
characteristics that sets them apart. We were able to train supervised methods
and achieve high performance in detecting such content and the users who post
them.
",computer-science
"  Network models have been increasingly used in the past years to support
summarization and analysis of narratives, such as famous TV series, books and
news. Inspired by social network analysis, most of these models focus on the
characters at play. The network model well captures all characters
interactions, giving a broad picture of the narration's content. A few works
went beyond by introducing additional semantic elements, always captured in a
single layer network. In contrast, we introduce in this work a multilayer
network model to capture more elements of the narration of a movie from its
script: people, locations, and other semantic elements. This model enables new
measures and insights on movies. We demonstrate this model on two very popular
movies.
",computer-science
"  Given functional data samples from a survival process with time dependent
covariates, we propose a practical boosting procedure for estimating its hazard
function nonparametrically. The estimator is consistent if the model is
correctly specified; alternatively an oracle inequality can be demonstrated for
tree-based models. To avoid overfitting, boosting employs several
regularization devices. One of them is step-size restriction, but the rationale
for this is somewhat mysterious from the viewpoint of consistency. Our
convergence bounds bring some clarity to this issue by revealing that step-size
restriction is a mechanism for preventing the curvature of the risk from
derailing convergence. We use our boosting procedure to shed new light on a
question from the operations literature concerning the effect of workload on
service rates in an emergency department.
",statistics
"  Superconducting bulk (RE)Ba$_2$Cu$_3$O$_{7-x}$ materials (RE-rare earth
elements) have been successfully used to generate magnetic flux densities in
excess of 17 T. This work investigates an alternative approach by trapping flux
in stacks of second generation high temperature superconducting tape from
several manufacturers using field cooling and pulsed field magnetisation
techniques. Flux densities of up to 13.4 T were trapped by field cooling at ~5
K between two 12 mm square stacks, an improvement of 70% over previous value
achieved in an HTS tape stack. The trapped flux approaches the record values in
(RE)BCO bulks and reflects the rapid developments still being made in the HTS
tape performance.
",physics
"  Modern cities are growing ecosystems that face new challenges due to the
increasing population demands. One of the many problems they face nowadays is
waste management, which has become a pressing issue requiring new solutions.
Swarm robotics systems have been attracting an increasing amount of attention
in the past years and they are expected to become one of the main driving
factors for innovation in the field of robotics. The research presented in this
paper explores the feasibility of a swarm robotics system in an urban
environment. By using bio-inspired foraging methods such as multi-place
foraging and stigmergy-based navigation, a swarm of robots is able to improve
the efficiency and autonomy of the urban waste management system in a realistic
scenario. To achieve this, a diverse set of simulation experiments was
conducted using real-world GIS data and implementing different garbage
collection scenarios driven by robot swarms. Results presented in this research
show that the proposed system outperforms current approaches. Moreover, results
not only show the efficiency of our solution, but also give insights about how
to design and customize these systems.
",computer-science
"  Universal properties of entangled many-body states are controlled by their
symmetry and quantum fluctuations. By magnetic-field tuning of the spin-orbital
degeneracy in a Kondo-correlated quantum dot, we have modified quantum
fluctuations to directly measure their influence on the many-body properties
along the crossover from $SU(4)$ to $SU(2)$ symmetry of the ground state.
High-sensitive current noise measurements combined with the non-equilibrium
Fermi liquid theory clarify that the Kondo resonance and electron correlations
are enhanced as the fluctuations, measured by the Wilson ratio, increase along
the symmetry crossover. Our achievement demonstrates that non-linear noise
constitutes a measure of quantum fluctuations that can be used to tackle
quantum phase transitions.
",physics
"  The band structure of a Si inverse diamond structure whose lattice point
shape was vacant regular octahedrons was calculated using plane wave expansion
method and a complete photonic band gap was theoretically confirmed at around
0.4 THz. It is said that three-dimensional photonic crystals have no
polarization anisotropy in photonic band gap (stop gap, stop band) of high
symmetry points in normal incidence. However, it was experimentally confirmed
that the polarization orientation of a reflected light was different from that
of a incident light, {I(X,Y)}, where (X,Y) is the coordinate system fixed in
the photonic crystal. It was studied on a plane (001) at around X point's
photonic band gap (0.36 - 0.44 THz) for incident light direction [001]
($\Gamma$-X direction) by rotating a sample in the plane (001), relatively. The
polarization orientation of the reflected light was parallel to that of the
incident light for the incident polarization orientation I(1,1), I(1,-1). In
contrast, the former was perpendicular to the latter for the incident
polarization orientation I(1,0), I(0,-1) in the vicinity of 0.38 THz. As far as
the photonic crystal in this work is concerned, method of resolution and
synthesis of the incident polarization vector isn't apparently able to apply to
the analysis of experimental results.
",physics
"  Estimates of the Hubble constant, $H_0$, from the distance ladder and the
cosmic microwave background (CMB) differ at the $\sim$3-$\sigma$ level,
indicating a potential issue with the standard $\Lambda$CDM cosmology.
Interpreting this tension correctly requires a model comparison calculation
depending on not only the traditional `$n$-$\sigma$' mismatch but also the
tails of the likelihoods. Determining the form of the tails of the local $H_0$
likelihood is impossible with the standard Gaussian least-squares
approximation, as it requires using non-Gaussian distributions to faithfully
represent anchor likelihoods and model outliers in the Cepheid and supernova
(SN) populations, and simultaneous fitting of the full distance-ladder dataset
to correctly propagate uncertainties. We have developed a Bayesian hierarchical
model that describes the full distance ladder, from nearby geometric anchors
through Cepheids to Hubble-Flow SNe. This model does not rely on any
distributions being Gaussian, allowing outliers to be modeled and obviating the
need for arbitrary data cuts. Sampling from the $\sim$3000-parameter joint
posterior using Hamiltonian Monte Carlo, we find $H_0$ = (72.72 $\pm$ 1.67)
${\rm km\,s^{-1}\,Mpc^{-1}}$ when applied to the outlier-cleaned Riess et al.
(2016) data, and ($73.15 \pm 1.78$) ${\rm km\,s^{-1}\,Mpc^{-1}}$ with SN
outliers reintroduced. Our high-fidelity sampling of the low-$H_0$ tail of the
distance-ladder likelihood allows us to apply Bayesian model comparison to
assess the evidence for deviation from $\Lambda$CDM. We set up this comparison
to yield a lower limit on the odds of the underlying model being $\Lambda$CDM
given the distance-ladder and Planck XIII (2016) CMB data. The odds against
$\Lambda$CDM are at worst 10:1 or 7:1, depending on whether the SNe outliers
are cut or modeled, or 60:1 if an approximation to the Planck Int. XLVI (2016)
likelihood is used.
",physics
"  In cellular massive Machine-Type Communications (MTC), a device can transmit
directly to the base station (BS) or through an aggregator (intermediate node).
While direct device-BS communication has recently been in the focus of 5G/3GPP
research and standardization efforts, the use of aggregators remains a less
explored topic. In this paper we analyze the deployment scenarios in which
aggregators can perform cellular access on behalf of multiple MTC devices. We
study the effect of packet bundling at the aggregator, which alleviates
overhead and resource waste when sending small packets. The aggregators give
rise to a tradeoff between access congestion and resource starvation and we
show that packet bundling can minimize resource starvation, especially for
smaller numbers of aggregators. Under the limitations of the considered model,
we investigate the optimal settings of the network parameters, in terms of
number of aggregators and packet-bundle size. Our results show that, in
general, data aggregation can benefit the uplink massive MTC in LTE, by
reducing the signalling overhead.
",computer-science
"  We examine the 2008-2016 $\gamma$-ray and optical light curves of three
bright BL Lac objects, 0716+714, MRK 421, BL Lac, which exhibit large
structured variability. We searched for periodicities by using a fully Bayesian
approach. For two out of three sources investigated no significant periodic
variability was found. In the case of BL Lac we detected a periodicity of ~ 680
days. Although the signal related to this is modest, the coincidence of the
periods in both gamma and optical bands is indicative of a physical relevance.
Considering previous literature results, possibly related $\gamma$-ray and
optical periodicities of about one year time scale are proposed in 4 bright
$\gamma$-ray blazars out of the 10 examined in detail. Comparing with results
from periodicity search of optical archives of quasars, the presence of
quasi-periodicities in blazars might be more frequent by a large factor. This
suggests the intriguing possibility that the basic conditions for their
observability are related to the relativistic jet in the observer direction,
but the overall picture remains uncertain.
",physics
"  In this paper we develop a way of obtaining Green's functions for partial
differential equations with linear involutions by reducing the equation to a
higher-order PDE without involutions. The developed theory is applied to a
model of heat transfer in a conducting plate which is bent in half.
",mathematics
"  Estimating the causal effects of an intervention in the presence of
confounding is a frequently occurring problem in applications such as medicine.
The task is challenging since there may be multiple confounding factors, some
of which may be missing, and inferences must be made from high-dimensional,
noisy measurements. In this paper, we propose a decision-theoretic approach to
estimate the causal effects of interventions where a subset of the covariates
is unavailable for some patients during testing. Our approach uses the
information bottleneck principle to perform a discrete, low-dimensional
sufficient reduction of the covariate data to estimate a distribution over
confounders. In doing so, we can estimate the causal effect of an intervention
where only partial covariate information is available. Our results on a causal
inference benchmark and a real application for treating sepsis show that our
method achieves state-of-the-art performance, without sacrificing
interpretability.
",statistics
"  The effects of the spatial scale on the results of the optimisation of
transmission and generation capacity in Europe are quantified under a 95% CO2
reduction compared to 1990 levels, interpolating between one-node-per-country
solutions and many-nodes-per-country. The trade-offs that come with higher
spatial detail between better exposure of transmission bottlenecks,
exploitation of sites with good renewable resources (particularly wind power)
and computational limitations are discussed. It is shown that solutions with no
grid expansion beyond today's capacities are only around 20% more expensive
than with cost-optimal grid expansion.
",physics
"  We treat the emerging power systems with direct current (DC) MicroGrids,
characterized with high penetration of power electronic converters. We rely on
the power electronics to propose a decentralized solution for autonomous
learning of and adaptation to the operating conditions of the DC Mirogrids; the
goal is to eliminate the need to rely on an external communication system for
such purpose. The solution works within the primary droop control loops and
uses only local bus voltage measurements. Each controller is able to estimate
(i) the generation capacities of power sources, (ii) the load demands, and
(iii) the conductances of the distribution lines. To define a well-conditioned
estimation problem, we employ decentralized strategy where the primary droop
controllers temporarily switch between operating points in a coordinated
manner, following amplitude-modulated training sequences. We study the use of
the estimator in a decentralized solution of the Optimal Economic Dispatch
problem. The evaluations confirm the usefulness of the proposed solution for
autonomous MicroGrid operation.
",computer-science
"  The motility mechanism of certain rod-shaped bacteria has long been a
mystery, since no external appendages are involved in their motion which is
known as gliding. However, the physical principles behind gliding motility
still remain poorly understood. Using myxobacteria as a canonical example of
such organisms, we identify here the physical principles behind gliding
motility, and develop a theoretical model that predicts a two-regime behavior
of the gliding speed as a function of the substrate stiffness. Our theory
describes the elastic, viscous, and capillary interactions between the
bacterial membrane carrying a traveling wave, the secreted slime acting as a
lubricating film, and the substrate which we model as a soft solid. Defining
the myxobacterial gliding as the horizontal motion on the substrate under zero
net force, we find the two-regime behavior is due to two different mechanisms
of motility thrust. On stiff substrates, the thrust arises from the bacterial
shape deformations creating a flow of slime that exerts a pressure along the
bacterial length. This pressure in conjunction with the bacterial shape
provides the necessary thrust for propulsion. However, we show that such a
mechanism cannot lead to gliding on very soft substrates. Instead, we show that
capillary effects lead to the formation of a ridge at the slime-substrate-air
interface, which creates a thrust in the form of a localized pressure gradient
at the tip of the bacteria. To test our theory, we perform experiments with
isolated cells on agar substrates of varying stiffness and find the measured
gliding speeds to be in good agreement with the predictions from our
elasto-capillary-hydrodynamic model. The physical mechanisms reported here
serve as an important step towards an accurate theory of friction and
substrate-mediated interaction between bacteria in a swarm of cells
proliferating in soft media.
",quantitative-biology
"  Tor is a low-latency anonymity system intended to provide low-latency
anonymous and uncensored network access against a local or network adversary.
Because of the design choice to minimize traffic overhead (and increase the
pool of potential users) Tor allows some information about the client's
connections to leak in the form of packet timing. Attacks that use (features
extracted from) this information to infer the website a user visits are
referred to as Website Fingerprinting (WF) attacks. We develop a methodology
and tools to directly measure the amount of information about a website leaked
by a given set of features. We apply this tool to a comprehensive set of
features extracted from a large set of websites and WF defense mechanisms,
allowing us to make more fine-grained observations about WF attack and defense
mechanisms.
",computer-science
"  Earlier this decade, the so-called FEAST algorithm was released for computing
the eigenvalues of a matrix in a given interval. Previously, rational filter
functions have been examined as a parameter of FEAST. In this thesis, we expand
on existing work with the following contributions: (i) Obtaining
well-performing rational filter functions via standard minimisation algorithms,
(ii) Obtaining constrained rational filter functions efficiently, and (iii)
Improving existing rational filter functions algorithmically. Using our new
rational filter functions, FEAST requires up to one quarter fewer iterations on
average compared to state-of-art rational filter functions.
",computer-science
"  While single measurement vector (SMV) models have been widely studied in
signal processing, there is a surging interest in addressing the multiple
measurement vectors (MMV) problem. In the MMV setting, more than one
measurement vector is available and the multiple signals to be recovered share
some commonalities such as a common support. Applications in which MMV is a
naturally occurring phenomenon include online streaming, medical imaging, and
video recovery. This work presents a stochastic iterative algorithm for the
support recovery of jointly sparse corrupted MMV. We present a variant of the
Sparse Randomized Kaczmarz algorithm for corrupted MMV and compare our proposed
method with an existing Kaczmarz type algorithm for MMV problems. We also
showcase the usefulness of our approach in the online (streaming) setting and
provide empirical evidence that suggests the robustness of the proposed method
to the distribution of the corruption and the number of corruptions occurring.
",computer-science
"  The new era of the Web is known as the semantic Web or the Web of data. The
semantic Web depends on ontologies that are seen as one of its pillars. The
bigger these ontologies, the greater their exploitation. However, when these
ontologies become too big other problems may appear, such as the complexity to
charge big files in memory, the time it needs to download such files and
especially the time it needs to make reasoning on them. We discuss in this
paper approaches for segmenting such big Web ontologies as well as its
usefulness. The segmentation method extracts from an existing ontology a
segment that represents a layer or a generation in the existing ontology; i.e.
a horizontally extraction. The extracted segment should be itself an ontology.
",computer-science
"  We introduce an up-down coloring of a virtual-link diagram. The
colorabilities give a lower bound of the minimum number of Reidemeister moves
of type II which are needed between two 2-component virtual-link diagrams. By
using the notion of a quandle cocycle invariant, we determine the necessity of
Reidemeister moves of type II for a pair of diagrams of the trivial
virtual-knot. This implies that for any virtual-knot diagram $D$, there exists
a diagram $D'$ representing the same virtual-knot such that any sequence of
generalized Reidemeister moves between them includes at least one Reidemeister
move of type II.
",mathematics
"  Inspired by recent interests of developing machine learning and data mining
algorithms on hypergraphs, we investigate in this paper the semi-supervised
learning algorithm of propagating ""soft labels"" (e.g. probability
distributions, class membership scores) over hypergraphs, by means of optimal
transportation. Borrowing insights from Wasserstein propagation on graphs
[Solomon et al. 2014], we re-formulate the label propagation procedure as a
message-passing algorithm, which renders itself naturally to a generalization
applicable to hypergraphs through Wasserstein barycenters. Furthermore, in a
PAC learning framework, we provide generalization error bounds for propagating
one-dimensional distributions on graphs and hypergraphs using 2-Wasserstein
distance, by establishing the \textit{algorithmic stability} of the proposed
semi-supervised learning algorithm. These theoretical results also shed new
lights upon deeper understandings of the Wasserstein propagation on graphs.
",statistics
"  We study the stochastic homogenization for a Cauchy problem for a first-order
Hamilton-Jacobi equation whose operator is not coercive w.r.t. the gradient
variable. We look at Hamiltonians like $H(x,\sigma(x)p,\omega)$ where
$\sigma(x)$ is a matrix associated to a Carnot group. The rescaling considered
is consistent with the underlying Carnot group structure, thus anisotropic. We
will prove that under suitable assumptions for the Hamiltonian, the solutions
of the $\varepsilon$-problem converge to a deterministic function which can be
characterized as the unique (viscosity) solution of a suitable deterministic
Hamilton-Jacobi problem.
",mathematics
"  As the focus of applied research in topological insulators (TI) evolves, the
need to synthesize large-area TI films for practical device applications takes
center stage. However, constructing scalable and adaptable processes for
high-quality TI compounds remains a challenge. To this end, a versatile van der
Waals epitaxy (vdWE) process for custom-feature Bismuth Telluro-Sulfide TI
growth and fabrication is presented, achieved through selective-area
fluorination and modification of surface free-energy on mica. The TI features
grow epitaxially in large single-crystal trigonal domains, exhibiting armchair
or zigzag crystalline edges highly oriented with the underlying mica lattice
and only two preferred domain orientations mirrored at $180^\circ$. As-grown
feature thickness dependence on lateral dimensions and denuded zones at
boundaries are observed, as explained by a semi-empirical two-species surface
migration model with robust estimates of growth parameters and elucidating the
role of selective-area surface modification. Topological surface states
contribute up to 60% of device conductance at room-temperature, indicating
excellent electronic quality. High-yield microfabrication and the adaptable
vdWE growth mechanism with readily alterable precursor and substrate
combinations, lend the process versatility to realize crystalline TI synthesis
in arbitrary shapes and arrays suitable for facile integration with processes
ranging from rapid prototyping to scalable manufacturing.
",physics
"  Since the matrix formed by nonlocal similar patches in a natural image is of
low rank, the nuclear norm minimization (NNM) has been widely used in various
image processing studies. Nonetheless, nuclear norm based convex surrogate of
the rank function usually over-shrinks the rank components and makes different
components equally, and thus may produce a result far from the optimum. To
alleviate the above-mentioned limitations of the nuclear norm, in this paper we
propose a new method for image restoration via the non-convex weighted Lp
nuclear norm minimization (NCW-NNM), which is able to more accurately enforce
the image structural sparsity and self-similarity simultaneously. To make the
proposed model tractable and robust, the alternative direction multiplier
method (ADMM) is adopted to solve the associated non-convex minimization
problem. Experimental results on various types of image restoration problems,
including image deblurring, image inpainting and image compressive sensing (CS)
recovery, demonstrate that the proposed method outperforms many current
state-of-the-art methods in both the objective and the perceptual qualities.
",computer-science
"  The recent empirical success of cross-domain mapping algorithms, between two
domains that share common characteristics, is not well-supported by theoretical
justifications. This lacuna is especially troubling, given the clear ambiguity
in such mappings. We work with the adversarial training method called the
Wasserstein GAN. We derive a novel generalization bound, which limits the risk
between the learned mapping $h$ and the target mapping $y$, by a sum of two
terms: (i) the risk between $h$ and the most distant alternative mapping that
was learned by the same cross-domain mapping algorithm, and (ii) the minimal
Wasserstein GAN divergence between the target domain and the domain obtained by
applying a hypothesis $h^*$ on the samples of the source domain, where $h^*$ is
a hypothesis selected by the same algorithm. The bound is directly related to
Occam's razor and it encourages the selection of the minimal architecture that
supports a small Wasserstein GAN divergence. From the bound, we derive
algorithms for hyperparameter selection and early stopping in cross-domain
mapping GANs. We also demonstrate a novel capability of estimating confidence
in the mapping of every specific sample. Lastly, we show how non-minimal
architectures can be effectively trained by an inverted knowledge distillation
in which a minimal architecture is used to train a larger one, leading to
higher quality outputs.
",statistics
"  In this paper, we introduce a new form of amortized variational inference by
using the forward KL divergence in a joint-contrastive variational loss. The
resulting forward amortized variational inference is a likelihood-free method
as its gradient can be sampled without bias and without requiring any
evaluation of either the model joint distribution or its derivatives. We prove
that our new variational loss is optimized by the exact posterior marginals in
the fully factorized mean-field approximation, a property that is not shared
with the more conventional reverse KL inference. Furthermore, we show that
forward amortized inference can be easily marginalized over large families of
latent variables in order to obtain a marginalized variational posterior. We
consider two examples of variational marginalization. In our first example we
train a Bayesian forecaster for predicting a simplified chaotic model of
atmospheric convection. In the second example we train an amortized variational
approximation of a Bayesian optimal classifier by marginalizing over the model
space. The result is a powerful meta-classification network that can solve
arbitrary classification problems without further training.
",statistics
"  Uncertainty analysis in the form of probabilistic forecasting can
significantly improve decision making processes in the smart power grid for
better integrating renewable energy sources such as wind. Whereas point
forecasting provides a single expected value, probabilistic forecasts provide
more information in the form of quantiles, prediction intervals, or full
predictive densities. This paper analyzes the effectiveness of a novel approach
for nonparametric probabilistic forecasting of wind power that combines a
smooth approximation of the pinball loss function with a neural network
architecture and a weighting initialization scheme to prevent the quantile
cross over problem. A numerical case study is conducted using publicly
available wind data from the Global Energy Forecasting Competition 2014.
Multiple quantiles are estimated to form 10%, to 90% prediction intervals which
are evaluated using a quantile score and reliability measures. Benchmark models
such as the persistence and climatology distributions, multiple quantile
regression, and support vector quantile regression are used for comparison
where results demonstrate the proposed approach leads to improved performance
while preventing the problem of overlapping quantile estimates.
",statistics
"  In this paper, we address the problem of learning compact
similarity-preserving embeddings for massive high-dimensional streams of data
in order to perform efficient similarity search. We present a new online method
for computing binary compressed representations -sketches- of high-dimensional
real feature vectors. Given an expected code length $c$ and high-dimensional
input data points, our algorithm provides a $c$-bits binary code for preserving
the distance between the points from the original high-dimensional space. Our
algorithm does not require neither the storage of the whole dataset nor a
chunk, thus it is fully adaptable to the streaming setting. It also provides
low time complexity and convergence guarantees. We demonstrate the quality of
our binary sketches through experiments on real data for the nearest neighbors
search task in the online setting.
",computer-science
"  Analog-to-digital converters (ADCs) are a major contributor to the power
consumption of multiple-input multiple-output (MIMO) communication systems with
large number of antennas. Use of low resolution ADCs has been proposed as a
means to decrease power consumption in MIMO receivers. However, reducing the
ADC resolution leads to performance loss in terms of achievable transmission
rates. In order to mitigate the rate-loss, the receiver can perform analog
processing of the received signals before quantization. Prior works consider
one-shot analog processing where at each channel-use, analog linear
combinations of the received signals are fed to a set of one-bit threshold
ADCs. In this paper, a receiver architecture is proposed which uses a sequence
of delay elements to allow for blockwise linear combining of the received
analog signals. In the high signal to noise ratio regime, it is shown that the
proposed architecture achieves the maximum achievable transmission rate given a
fixed number of one-bit ADCs. Furthermore, a tradeoff between transmission rate
and the number of delay elements is identified which quantifies the increase in
maximum achievable rate as the number of delay elements is increased.
",computer-science
"  This report describes the development of an aptamer for sensing azole
antifungal drugs for therapeutic drug monitoring. Modified Synthetic Evolution
of Ligands through Exponential Enrichment (SELEX) was used to discover a DNA
aptamer recognizing azole class antifungal drugs. This aptamer undergoes a
secondary structural change upon binding to its target molecule as shown
through fluorescence anisotropy-based binding measurements. Experiments using
circular dichroism spectroscopy, revealed a unique double G-quadruplex
structure that was essential and specific for binding to the azole antifungal
target. Aptamer-functionalized Graphene Field Effect Transistor (GFET) devices
were created and used to measure the binding of strength of azole antifungals
to this surface. In total this aptamer and the supporting sensing platform
could provide a valuable tool for improving the treatment of patients with
invasive fungal infections.
",physics
"  We are concerned with the regularity of solutions of the Lighthill problem
for shock diffraction by a convex corned wedge, which can be formulated as a
free boundary problem. In this paper, we prove that there is no regular
solution that is subsonic up to the wedge corner for potential flow. This
indicates that, if the solution is subsonic at the wedge corner, at least a
characteristic discontinuity (vortex sheet or entropy wave) is expected to be
generated, which is consistent with the experimental and computational results.
In order to achieve the non-existence result, a weak maximum principle for the
solution is established, and several other mathematical techniques are
developed. The methods and techniques developed here are also useful to the
other problems with similar difficulties.
",mathematics
"  In this paper we introduce the concept of singular Finsler foliation, which
generalizes the concepts of Finsler actions, Finsler submersions and (regular)
Finsler foliations. We show that if $\mathcal{F}$ is a singular Finsler
foliation on a Randers manifold $(M,Z)$ with Zermelo data $(\mathtt{h},W),$
then $\mathcal{F}$ is a singular Riemannian foliation on the Riemannian
manifold $(M,\mathtt{h} )$. As a direct consequence we infer that the regular
leaves are equifocal submanifolds (a generalization of isoparametric
submanifolds) when the wind $W$ is an infinitesimal homothety of $\mathtt{h}$
(e.,g when $W$ is killing vector field or $M$ has constant Finsler curvature).
We also present a slice theorem that relates local singular Finsler
foliations on Finsler manifolds with singular Finsler foliations on Minkowski
spaces.
",mathematics
"  The idea of posing a command following or tracking control problem as an
input reconstruction problem is explored in the paper. For a class of square
MIMO systems with known dynamics, by pretending that reference commands are
actual outputs of the system, input reconstruction methods can be used to
determine control action that will result in a system following desired
reference commands. A feedback controller which is a combination of an unbiased
state estimator and an input reconstructor that ensures unbiased tracking of
reference commands is proposed. Simulations and real-time implementation are
presented to demonstrate utility of the proposed idea. Conditions under which
proposed controller may be used for non-square systems are also discussed.
",computer-science
"  This article presents a survey on automatic software repair. Automatic
software repair consists of automatically finding a solution to software bugs
without human intervention. This article considers all kinds of repairs. First,
it discusses behavioral repair where test suites, contracts, models, and
crashing inputs are taken as oracle. Second, it discusses state repair, also
known as runtime repair or runtime recovery, with techniques such as checkpoint
and restart, reconfiguration, and invariant restoration. The uniqueness of this
article is that it spans the research communities that contribute to this body
of knowledge: software engineering, dependability, operating systems,
programming languages, and security. It provides a novel and structured
overview of the diversity of bug oracles and repair operators used in the
literature.
",computer-science
"  The design of sparse spatially stretched tripole arrays is an important but
also challenging task and this paper proposes for the very first time efficient
solutions to this problem. Unlike for the design of traditional sparse antenna
arrays, the developed approaches optimise both the dipole locations and
orientations. The novelty of the paper consists in formulating these
optimisation problems into a form that can be solved by the proposed
compressive sensing and Bayesian compressive sensing based approaches. The
performance of the developed approaches is validated and it is shown that
accurate approximation of a reference response can be achieved with a 67%
reduction in the number of dipoles required as compared to an equivalent
uniform spatially stretched tripole array, leading to a significant reduction
in the cost associated with the resulting arrays.
",computer-science
"  The stochastic block model is widely used for detecting community structures
in network data. How to test the goodness-of-fit of the model is one of the
fundamental problems and has gained growing interests in recent years. In this
paper, we propose a novel goodness-of-fit test based on the maximum entry of
the centered and re-scaled adjacency matrix for the stochastic block model. One
noticeable advantage of the proposed test is that the number of communities can
be allowed to grow linearly with the number of nodes ignoring a logarithmic
factor. We prove that the null distribution of the test statistic converges in
distribution to a Gumbel distribution, and we show that both the number of
communities and the membership vector can be tested via the proposed method.
Further, we show that the proposed test has asymptotic power guarantee against
a class of alternatives. We also demonstrate that the proposed method can be
extended to the degree-corrected stochastic block model. Both simulation
studies and real-world data examples indicate that the proposed method works
well.
",statistics
"  We rework and generalize equivariant infinite loop space theory, which shows
how to construct G-spectra from G-spaces with suitable structure. There is a
naive version which gives naive G-spectra for any topological group G, but our
focus is on the construction of genuine G-spectra when G is finite.
We give new information about the Segal and operadic equivariant infinite
loop space machines, supplying many details that are missing from the
literature, and we prove by direct comparison that the two machines give
equivalent output when fed equivalent input. The proof of the corresponding
nonequivariant uniqueness theorem, due to May and Thomason, works for naive
G-spectra for general G but fails hopelessly for genuine G-spectra when G is
finite. Even in the nonequivariant case, our comparison theorem is considerably
more precise, giving a direct point-set level comparison.
We have taken the opportunity to update this general area, equivariant and
nonequivariant, giving many new proofs, filling in some gaps, and giving some
corrections to results in the literature.
",mathematics
"  Navigation in unknown, chaotic environments continues to present a
significant challenge for the robotics community. Lighting changes,
self-similar textures, motion blur, and moving objects are all considerable
stumbling blocks for state-of-the-art vision-based navigation algorithms. In
this paper we present a novel technique for improving localization accuracy
within a visual-inertial navigation system (VINS). We make use of training data
to learn a model for the quality of visual features with respect to
localization error in a given environment. This model maps each visual
observation from a predefined prediction space of visual-inertial predictors
onto a scalar weight, which is then used to scale the observation covariance
matrix. In this way, our model can adjust the influence of each observation
according to its quality. We discuss our choice of predictors and report
substantial reductions in localization error on 4 km of data from the KITTI
dataset, as well as on experimental datasets consisting of 700 m of indoor and
outdoor driving on a small ground rover equipped with a Skybotix VI-Sensor.
",computer-science
"  Most blind deconvolution methods usually pre-define a large kernel size to
guarantee the support domain. Blur kernel estimation error is likely to be
introduced, and is proportional to kernel size. In this paper, we
experimentally and theoretically show the reason of noises introduction in
oversized kernel by demonstrating that sizeable kernels lead to lower
optimization cost. To eliminate this adverse effect, we propose a low
rank-based regularization on blur kernel by analyzing the structural
information in degraded kernels. Compared with the sparsity prior, e.g.,
$\ell_\alpha$-norm, our regularization term can effectively suppress random
noises in oversized kernels. On benchmark test dataset, the proposed method is
compared with several state-of-the-art methods, and can achieve better
quantitative score. Especially, the improvement margin is much more significant
for oversized blur kernels. We also validate the proposed method on real-world
blurry images.
",computer-science
"  We give upper and lower bounds for the number of solutions of the equation
$p(z)\log|z|+q(z)=0$ with polynomials $p$ and $q$.
",mathematics
"  This paper replicates, extends, and refutes conclusions made in a study
published in PLoS ONE (""Even Good Bots Fight""), which claimed to identify
substantial levels of conflict between automated software agents (or bots) in
Wikipedia using purely quantitative methods. By applying an integrative
mixed-methods approach drawing on trace ethnography, we place these alleged
cases of bot-bot conflict into context and arrive at a better understanding of
these interactions. We found that overwhelmingly, the interactions previously
characterized as problematic instances of conflict are typically better
characterized as routine, productive, even collaborative work. These results
challenge past work and show the importance of qualitative/quantitative
collaboration. In our paper, we present quantitative metrics and qualitative
heuristics for operationalizing bot-bot conflict. We give thick descriptions of
kinds of events that present as bot-bot reverts, helping distinguish conflict
from non-conflict. We computationally classify these kinds of events through
patterns in edit summaries. By interpreting found/trace data in the
socio-technical contexts in which people give that data meaning, we gain more
from quantitative measurements, drawing deeper understandings about the
governance of algorithmic systems in Wikipedia. We have also released our data
collection, processing, and analysis pipeline, to facilitate computational
reproducibility of our findings and to help other researchers interested in
conducting similar mixed-method scholarship in other platforms and contexts.
",computer-science
"  Introductory and pedagogical treatmeant of the article : P. Broussous
""Distinction of the Steinberg representation"", with an appendix by François
Courtès, IMRN 2014, no 11, 3140-3157. To appear in Proceedings of Chaire Jean
Morlet, Dipendra Prasad, Volker Heiermann Ed. 2017. Contains modified and
simplified proofs of loc. cit. This article is written in memory of
François Courtès who passed away in september 2016.
",mathematics
"  Many technologies have been developed to help improve spatial resolution of
observational images for ground-based solar telescopes, such as adaptive optics
(AO) systems and post-processing reconstruction. As any AO system correction is
only partial, it is indispensable to use post-processing reconstruction
techniques. In the New Vacuum Solar Telescope (NVST), speckle masking method is
used to achieve the diffraction limited resolution of the telescope. Although
the method is very promising, the computation is quite intensive, and the
amount of data is tremendous, requiring several months to reconstruct
observational data of one day on a high-end computer. To accelerate image
reconstruction, we parallelize the program package on a high performance
cluster. We describe parallel implementation details for several reconstruction
procedures. The code is written in C language using Message Passing Interface
(MPI) and optimized for parallel processing in a multi-processor environment.
We show the excellent performance of parallel implementation, and the whole
data processing speed is about 71 times faster than before. Finally, we analyze
the scalability of the code to find possible bottlenecks, and propose several
ways to further improve the parallel performance. We conclude that the
presented program is capable of executing in real-time reconstruction
applications at NVST.
",physics
"  Homographs, words with different meanings but the same surface form, have
long caused difficulty for machine translation systems, as it is difficult to
select the correct translation based on the context. However, with the advent
of neural machine translation (NMT) systems, which can theoretically take into
account global sentential context, one may hypothesize that this problem has
been alleviated. In this paper, we first provide empirical evidence that
existing NMT systems in fact still have significant problems in properly
translating ambiguous words. We then proceed to describe methods, inspired by
the word sense disambiguation literature, that model the context of the input
word with context-aware word embeddings that help to differentiate the word
sense be- fore feeding it into the encoder. Experiments on three language pairs
demonstrate that such models improve the performance of NMT systems both in
terms of BLEU score and in the accuracy of translating homographs.
",computer-science
"  In this paper, we explore the connection between convergence in distribution
and Mallows distance in the context of positively associated random variables.
Our results extend some known invariance principles for sequences with FKG
property. Applications for processes with Gibbssian dependence structures are
included.
",mathematics
"  In this paper, we argue that the future of Artificial Intelligence research
resides in two keywords: integration and embodiment. We support this claim by
analyzing the recent advances of the field. Regarding integration, we note that
the most impactful recent contributions have been made possible through the
integration of recent Machine Learning methods (based in particular on Deep
Learning and Recurrent Neural Networks) with more traditional ones (e.g.
Monte-Carlo tree search, goal babbling exploration or addressable memory
systems). Regarding embodiment, we note that the traditional benchmark tasks
(e.g. visual classification or board games) are becoming obsolete as
state-of-the-art learning algorithms approach or even surpass human performance
in most of them, having recently encouraged the development of first-person 3D
game platforms embedding realistic physics. Building upon this analysis, we
first propose an embodied cognitive architecture integrating heterogenous
sub-fields of Artificial Intelligence into a unified framework. We demonstrate
the utility of our approach by showing how major contributions of the field can
be expressed within the proposed framework. We then claim that benchmarking
environments need to reproduce ecologically-valid conditions for bootstrapping
the acquisition of increasingly complex cognitive skills through the concept of
a cognitive arms race between embodied agents.
",computer-science
"  The effects of nitridation on the density of traps at SiO$_2$/SiC interfaces
near the conduction band edge were qualitatively examined by a simple, newly
developed characterization method that utilizes Hall effect measurements and
split capacitance-voltage measurements. The results showed a significant
reduction in the density of interface traps near the conduction band edge by
nitridation, as well as the high density of interface traps that was not
eliminated by nitridation.
",physics
"  This paper is concerned with a multi-asset mean-variance portfolio selection
problem under model uncertainty. We develop a continuous time framework for
taking into account ambiguity aversion about both expected return rates and
correlation matrix of the assets, and for studying the effects on portfolio
diversification. We prove a separation principle for the associated robust
control problem, which allows to reduce the determination of the optimal
dynamic strategy to the parametric computation of the minimal risk premium
function. Our results provide a justification for under-diversification, as
documented in empirical studies. We explicitly quantify the degree of
under-diversification in terms of correlation and Sharpe ratio ambiguity. In
particular, we show that an investor with a poor confidence in the expected
return estimation does not hold any risky asset, and on the other hand, trades
only one risky asset when the level of ambiguity on correlation matrix is
large. This extends to the continuous-time setting the results obtained by
Garlappi, Uppal and Wang [13], and Liu and Zeng [24] in a one-period model. JEL
Classification: G11, C61 MSC Classification: 91G10, 91G80, 60H30
",quantitative-finance
"  Solitary waves propagation of baryonic density perturbations, ruled by the
Korteweg--de Vries equation in a mean-field quark-gluon plasma model, are
investigated from the point of view of the theory of information. A recently
proposed continuous logarithmic measure of information, called configurational
entropy, is used to derive the soliton width, defining the pulse, for which the
informational content of the soliton spatial profile is more compressed, in the
Shannon's sense.
",physics
"  We initiate the study of the communication complexity of fair division with
indivisible goods. We focus on some of the most well-studied fairness notions
(envy-freeness, proportionality, and approximations thereof) and valuation
classes (submodular, subadditive and unrestricted). Within these parameters,
our results completely resolve whether the communication complexity of
computing a fair allocation (or determining that none exist) is polynomial or
exponential (in the number of goods), for every combination of fairness notion,
valuation class, and number of players, for both deterministic and randomized
protocols.
",computer-science
"  Optical and near-infrared photometry, optical spectroscopy, and soft X-ray
and UV monitoring of the changing look active galactic nucleus NGC 2617 show
that it continues to have the appearance of a type-1 Seyfert galaxy. An optical
light curve for 2010-2016 indicates that the change of type probably occurred
between 2010 October and 2012 February and was not related to the brightening
in 2013. In 2016 NGC 2617 brightened again to a level of activity close to that
in 2013 April. We find variations in all passbands and in both the intensities
and profiles of the broad Balmer lines. A new displaced emission peak has
appeared in H$\beta$. X-ray variations are well correlated with UV-optical
variability and possibly lead by $\sim$ 2-3 d. The $K$ band lags the $J$ band
by about 21.5 $\pm$ 2.5 d. and lags the combined $B+J$ filters by $\sim$ 25 d.
$J$ lags $B$ by about 3 d. This could be because $J$-band variability arises
from the outer part of the accretion disc, while $K$-band variability comes
from thermal re-emission by dust. We propose that spectral-type changes are a
result of increasing central luminosity causing sublimation of the innermost
dust in the hollow biconical outflow. We briefly discuss various other possible
reasons that might explain the dramatic changes in NGC 2617.
",physics
"  We study rewriting for equational theories in the context of symmetric
monoidal categories where there is a separable Frobenius monoid on each object.
These categories, also called hypergraph categories, are increasingly relevant:
Frobenius structures recently appeared in cross-disciplinary applications,
including the study of quantum processes, dynamical systems and natural
language processing. In this work we give a combinatorial characterisation of
arrows of a free hypergraph category as cospans of labelled hypergraphs and
establish a precise correspondence between rewriting modulo Frobenius structure
on the one hand and double-pushout rewriting of hypergraphs on the other. This
interpretation allows to use results on hypergraphs to ensure decidability of
confluence for rewriting in a free hypergraph category. Our results generalise
previous approaches where only categories generated by a single object (props)
were considered.
",computer-science
"  There are many different relatedness measures, based for instance on citation
relations or textual similarity, that can be used to cluster scientific
publications. We propose a principled methodology for evaluating the accuracy
of clustering solutions obtained using these relatedness measures. We formally
show that the proposed methodology has an important consistency property. The
empirical analyses that we present are based on publications in the fields of
cell biology, condensed matter physics, and economics. Using the BM25
text-based relatedness measure as evaluation criterion, we find that
bibliographic coupling relations yield more accurate clustering solutions than
direct citation relations and co-citation relations. The so-called extended
direct citation approach performs similarly to or slightly better than
bibliographic coupling in terms of the accuracy of the resulting clustering
solutions. The other way around, using a citation-based relatedness measure as
evaluation criterion, BM25 turns out to yield more accurate clustering
solutions than other text-based relatedness measures.
",computer-science
"  The non--commuting graph $\Gamma(G)$ of a non--abelian group $G$ is defined
as follows. The vertex set $V(\Gamma(G))$ of $\Gamma(G)$ is $G\setminus Z(G)$
where $Z(G)$ denotes the center of $G$ and two vertices $x$ and $y$ are
adjacent if and only if $xy\neq yx$. For non--abelian finite groups $G$ and $H$
it is conjectured that if $\Gamma(G) \cong \Gamma(H)$, then $|G|=|H|$. We prove
the conjecture.
",mathematics
"  We show that if $X$ is an abelian variety of dimension $g \geq 1$ and
${\mathcal E}$ is an M-regular coherent sheaf on $X$, the Castelnuovo-Mumford
regularity of ${\mathcal E}$ with respect to an ample and globally generated
line bundle ${\mathcal O}(1)$ on $X$ is at most $g$, and that equality is
obtained when ${\mathcal E}^{\vee}(1)$ is continuously globally generated. As
an application, we give a numerical characterization of ample semihomogeneous
vector bundles for which this bound is attained.
",mathematics
"  The Juno Orbiter has provided improved estimates of the even gravitational
harmonics J2 to J8 of Jupiter. To compute higher-order moments, new methods
such as the Concentric Maclaurin Spheroids (CMS) method have been developed
which surpass the so far commonly used Theory of Figures (ToF) method in
accuracy. This progress rises the question whether ToF can still provide a
useful service for deriving the internal structure of giant planets in the
Solar system. In this paper, I apply both the ToF and the CMS method to compare
results for polytropic Jupiter and for the physical equation of state
H/He-REOS.3 based models. An accuracy in the computed values of J2 and J4 of
0.1% is found to be sufficient in order to obtain the core mass safely within
0.5 Mearth numerical accuracy and the atmospheric metallicity within about
0.0004. ToF to 4th order provides that accuracy, while ToF to 3rd order does
not for J4. Furthermore, I find that the assumption of rigid rotation yields J6
and J8 values in agreement with the current Juno estimates, and that higher
order terms (J10 to J18) deviate by about 10% from predictions by polytropic
models. This work suggests that ToF4 can still be applied to infer the deep
internal structure, and that the zonal winds on Jupiter reach less deep than
0.9 RJup.
",physics
"  Being motivated by the problem of deducing $L^p$-bounds on the second
fundamental form of an isometric immersion from $L^p$-bounds on its mean
curvature vector field, we prove a (nonlinear) Calderón-Zygmund inequality
for maps between complete (possibly noncompact) Riemannian manifolds.
",mathematics
"  Single molecule magnets (SMMs) with single-ion anisotropies $\mathbf d$,
comparable to exchange interactions J, between spins have recently been
synthesized. In this paper, we provide theoretical insights into the magnetism
of such systems. We study spin chains with site spins, s=1, 3/2 and 2 and
on-site anisotropy $\mathbf d$ comparable to the exchange constants between the
spins. We find that large $\mathbf d$ leads to crossing of the states with
different $M_S$ values in the same spin manifold of the $\mathbf d = 0$ limit.
For very large $\mathbf d$'s we also find that the $M_S$ states of the higher
energy spin states descend below the $M_S$ states of the ground state spin
manifold. Total spin in this limit is no longer conserved and describing the
molecular anisotropy by the constants $D_M$ and $E_M$ is not possible. However,
the total spin of the low-lying large $M_S$ states is very nearly an integer
and using this spin value it is possible to construct an effective spin
Hamiltonian and compute the molecular magnetic anisotropy constants $D_M$ and
$E_M$. We report effect of finite sizes, rotations of site anisotropies and
chain dimerization on the effective anisotropy of the spin chains.
",physics
"  The space of n-point correlation functions, for all possible time-orderings
of operators, can be computed by a non-trivial path integral contour, which
depends on how many time-ordering violations are present in the correlator.
These contours, which have come to be known as timefolds, or out-of-time-order
(OTO) contours, are a natural generalization of the Schwinger-Keldysh contour
(which computes singly out-of-time-ordered correlation functions). We provide a
detailed discussion of such higher OTO functional integrals, explaining their
general structure, and the myriad ways in which a particular correlation
function may be encoded in such contours. Our discussion may be seen as a
natural generalization of the Schwinger-Keldysh formalism to higher OTO
correlation functions. We provide explicit illustration for low point
correlators (n=2,3,4) to exemplify the general statements.
",physics
"  We prove a convexity theorem for Hamiltonian torus actions on compact
cosymplectic manifolds. We show that compact toric cosymplectic manifolds are
mapping tori of equivariant symplectomorphisms of toric symplectic manifolds.
",mathematics
"  This paper derives two new optimization-driven Monte Carlo algorithms
inspired from variable splitting and data augmentation. In particular, the
formulation of one of the proposed approaches is closely related to the
alternating direction method of multipliers (ADMM) main steps. The proposed
framework enables to derive faster and more efficient sampling schemes than the
current state-of-the-art methods and can embed the latter. By sampling
efficiently the parameter to infer as well as the hyperparameters of the
problem, the generated samples can be used to approximate Bayesian estimators
of the parameters to infer. Additionally, the proposed approach brings
confidence intervals at a low cost contrary to optimization methods.
Simulations on two often-studied signal processing problems illustrate the
performance of the two proposed samplers. All results are compared to those
obtained by recent state-of-the-art optimization and MCMC algorithms used to
solve these problems.
",statistics
"  This paper provides a link between causal inference and machine learning
techniques - specifically, Classification and Regression Trees (CART) - in
observational studies where the receipt of the treatment is not randomized, but
the assignment to the treatment can be assumed to be randomized (irregular
assignment mechanism). The paper contributes to the growing applied machine
learning literature on causal inference, by proposing a modified version of the
Causal Tree (CT) algorithm to draw causal inference from an irregular
assignment mechanism. The proposed method is developed by merging the CT
approach with the instrumental variable framework to causal inference, hence
the name Causal Tree with Instrumental Variable (CT-IV). As compared to CT, the
main strength of CT-IV is that it can deal more efficiently with the
heterogeneity of causal effects, as demonstrated by a series of numerical
results obtained on synthetic data. Then, the proposed algorithm is used to
evaluate a public policy implemented by the Tuscan Regional Administration
(Italy), which aimed at easing the access to credit for small firms. In this
context, CT-IV breaks fresh ground for target-based policies, identifying
interesting heterogeneous causal effects.
",statistics
"  This work verifies the instrumental characteristics of the CCD detector which
is part of the UNI astronomical observatory. We measured the linearity of the
CCD detector of the SBIG STXL6303E camera, along with the associated gain and
readout noise. The linear response to the incident light of the detector is
extremely linear (R2 =99.99%), its effective gain is 1.65 +/- 0.01 e-/ADU and
its readout noise is 12.2 e-. These values are in agreement with the
manufacturer. We confirm that this detector is extremely precise to make
measurements for astronomical purposes.
",physics
"  Understanding planetary interiors is directly linked to our ability of
simulating exotic quantum mechanical systems such as hydrogen (H) and
hydrogen-helium (H-He) mixtures at high pressures and temperatures. Equations
of State (EOSs) tables based on Density Functional Theory (DFT), are commonly
used by planetary scientists, although this method allows only for a
qualitative description of the phase diagram, due to an incomplete treatment of
electronic interactions. Here we report Quantum Monte Carlo (QMC) molecular
dynamics simulations of pure H and H-He mixture. We calculate the first QMC EOS
at 6000 K for an H-He mixture of a proto-solar composition, and show the
crucial influence of He on the H metallization pressure. Our results can be
used to calibrate other EOS calculations and are very timely given the accurate
determination of Jupiter's gravitational field from the NASA Juno mission and
the effort to determine its structure.
",physics
"  Stellar evolution models are most uncertain for evolved massive stars.
Asteroseismology based on high-precision uninterrupted space photometry has
become a new way to test the outcome of stellar evolution theory and was
recently applied to a multitude of stars, but not yet to massive evolved
supergiants.Our aim is to detect, analyse and interpret the photospheric and
wind variability of the O9.5Iab star HD 188209 from Kepler space photometry and
long-term high-resolution spectroscopy. We used Kepler scattered-light
photometry obtained by the nominal mission during 1460d to deduce the
photometric variability of this O-type supergiant. In addition, we assembled
and analysed high-resolution high signal-to-noise spectroscopy taken with four
spectrographs during some 1800d to interpret the temporal spectroscopic
variability of the star. The variability of this blue supergiant derived from
the scattered-light space photometry is in full in agreement with the one found
in the ground-based spectroscopy. We find significant low-frequency variability
that is consistently detected in all spectral lines of HD 188209. The
photospheric variability propagates into the wind, where it has similar
frequencies but slightly higher amplitudes. The morphology of the frequency
spectra derived from the long-term photometry and spectroscopy points towards a
spectrum of travelling waves with frequency values in the range expected for an
evolved O-type star. Convectively-driven internal gravity waves excited in the
stellar interior offer the most plausible explanation of the detected
variability.
",physics
"  Policy-gradient approaches to reinforcement learning have two common and
undesirable overhead procedures, namely warm-start training and sample variance
reduction. In this paper, we describe a reinforcement learning method based on
a softmax value function that requires neither of these procedures. Our method
combines the advantages of policy-gradient methods with the efficiency and
simplicity of maximum-likelihood approaches. We apply this new cold-start
reinforcement learning method in training sequence generation models for
structured output prediction problems. Empirical evidence validates this method
on automatic summarization and image captioning tasks.
",computer-science
"  We construct a toy a model which demonstrates that large field single scalar
inflation can produce an arbitrarily small scalar to tensor ratio in the window
of e-foldings recoverable from CMB experiments. This is done by generalizing
the $\alpha$-attractor models to allow the potential to approach a constant as
rapidly as we desire for super-planckian field values. This implies that a
non-detection of r alone can never rule out entirely the theory of large field
inflation.
",physics
"  In this paper, we propose a new differentiable neural network alignment
mechanism for text-dependent speaker verification which uses alignment models
to produce a supervector representation of an utterance. Unlike previous works
with similar approaches, we do not extract the embedding of an utterance from
the mean reduction of the temporal dimension. Our system replaces the mean by a
phrase alignment model to keep the temporal structure of each phrase which is
relevant in this application since the phonetic information is part of the
identity in the verification task. Moreover, we can apply a convolutional
neural network as front-end, and thanks to the alignment process being
differentiable, we can train the whole network to produce a supervector for
each utterance which will be discriminative with respect to the speaker and the
phrase simultaneously. As we show, this choice has the advantage that the
supervector encodes the phrase and speaker information providing good
performance in text-dependent speaker verification tasks. In this work, the
process of verification is performed using a basic similarity metric, due to
simplicity, compared to other more elaborate models that are commonly used. The
new model using alignment to produce supervectors was tested on the
RSR2015-Part I database for text-dependent speaker verification, providing
competitive results compared to similar size networks using the mean to extract
embeddings.
",computer-science
"  We design and implement the first private and anonymous decentralized
crowdsourcing system ZebraLancer. It realizes the fair exchange (i.e. security
against malicious workers and dishonest requesters) without using any
third-party arbiter. More importantly, it overcomes two fundamental challenges
of decentralization, i.e. data leakage and identity breach.
First, our outsource-then-prove methodology resolves the critical tension
between blockchain transparency and data confidentiality without sacrificing
the fairness of exchange. ZebraLancer ensures: a requester will not pay more
than what data deserve, according to a policy announced when her task is
published through the blockchain; each worker indeed gets a payment based on
the policy, if submits data to the blockchain; the above properties are
realized not only without a central arbiter, but also without leaking the data
to blockchain network.
Furthermore, the blockchain transparency might allow one to infer private
information of workers/requesters through their participation history.
ZebraLancer solves the problem by allowing anonymous participations without
surrendering user accountability. Specifically, workers cannot misuse anonymity
to submit multiple times to reap rewards, and an anonymous requester cannot
maliciously submit colluded answers to herself to repudiate payments. The idea
behind is a subtle linkability: if one authenticates twice in a task, everybody
can tell, or else staying anonymous. To realize such delicate linkability, we
put forth a novel cryptographic notion, the common-prefix-linkable anonymous
authentication.
Finally, we implement our protocol for a common image annotation task and
deploy it in a test net of Ethereum. The experiment results show the
applicability of our protocol and highlight subtleties of tailoring the
protocol to be compatible with the existing real-world open blockchain.
",computer-science
"  Endowing a dialogue system with particular personality traits is essential to
deliver more human-like conversations. However, due to the challenge of
embodying personality via language expression and the lack of large-scale
persona-labeled dialogue data, this research problem is still far from
well-studied. In this paper, we investigate the problem of incorporating
explicit personality traits in dialogue generation to deliver personalized
dialogues.
To this end, firstly, we construct PersonalDialog, a large-scale multi-turn
dialogue dataset containing various traits from a large number of speakers. The
dataset consists of 20.83M sessions and 56.25M utterances from 8.47M speakers.
Each utterance is associated with a speaker who is marked with traits like Age,
Gender, Location, Interest Tags, etc. Several anonymization schemes are
designed to protect the privacy of each speaker. This large-scale dataset will
facilitate not only the study of personalized dialogue generation, but also
other researches on sociolinguistics or social science.
Secondly, to study how personality traits can be captured and addressed in
dialogue generation, we propose persona-aware dialogue generation models within
the sequence to sequence learning framework. Explicit personality traits
(structured by key-value pairs) are embedded using a trait fusion module.
During the decoding process, two techniques, namely persona-aware attention and
persona-aware bias, are devised to capture and address trait-related
information. Experiments demonstrate that our model is able to address proper
traits in different contexts. Case studies also show interesting results for
this challenging research problem.
",computer-science
"  The complexity and size of state-of-the-art cell models have significantly
increased in part due to the requirement that these models possess complex
cellular functions which are thought--but not necessarily proven--to be
important. Modern cell models often involve hundreds of parameters; the values
of these parameters come, more often than not, from animal experiments whose
relationship to the human physiology is weak with very little information on
the errors in these measurements. The concomitant uncertainties in parameter
values result in uncertainties in the model outputs or Quantities of Interest
(QoIs). Global Sensitivity Analysis (GSA) aims at apportioning to individual
parameters (or sets of parameters) their relative contribution to output
uncertainty thereby introducing a measure of influence or importance of said
parameters. New GSA approaches are required to deal with increased model size
and complexity; a three stage methodology consisting of screening (dimension
reduction), surrogate modeling, and computing Sobol' indices, is presented. The
methodology is used to analyze a physiologically validated numerical model of
neurovascular coupling which possess 160 uncertain parameters. The sensitivity
analysis investigates three quantities of interest (QoIs), the average value of
$K^+$ in the extracellular space, the average volumetric flow rate through the
perfusing vessel, and the minimum value of the actin/myosin complex in the
smooth muscle cell. GSA provides a measure of the influence of each parameter,
for each of the three QoIs, giving insight into areas of possible physiological
dysfunction and areas of further investigation.
",quantitative-biology
"  The aim of our study is to investigate the dynamics of possible comets in the
HD 10180 system. This investigation is motivated by the discovery of exocomets
in various systems, especially $\beta$ Pictoris, as well as in at least ten
other systems. Detailed theoretical studies about the formation and evolution
of star--planet systems indicate that exocomets should be quite common. Further
observational results are expected in the foreseeable future, in part due to
the availability of the Large Synoptic Survey Telescope. Nonetheless, the Solar
System represents the best studied example for comets, thus serving as a prime
motivation for investigating comets in HD 10180 as well. HD 10180 is strikingly
similar to the Sun. This system contains six confirmed planets and (at least)
two additional planets subject to final verification. In our studies, we
consider comets of different inclinations and eccentricities and find an array
of different outcomes such as encounters with planets, captures, and escapes.
Comets with relatively large eccentricities are able to enter the inner region
of the system facing early planetary encounters. Stable comets experience
long-term evolution of orbital elements, as expected. We also tried to
distinguish cometary families akin to our Solar System but no clear distinction
between possible families was found. Generally, theoretical and observational
studies of exoplanets have a large range of ramifications, involving the
origin, structure and evolution of systems as well as the proliferation of
water and prebiotic compounds to terrestrial planets, which will increase their
chances of being habitable.
",physics
"  We propose an alternative evaluation of quantum entanglement by measuring the
maximum violation of the Bell's inequality without performing a partial trace
operation. This proposal is demonstrated by bridging the maximum violation of
the Bell's inequality and the concurrence of a pure state in an $n$-qubit
system, in which one subsystem only contains one qubit and the state is a
linear combination of two product states. We apply this relation to the ground
states of four qubits in the Wen-Plaquette model and show that they are
maximally entangled. A topological entanglement entropy of the Wen-Plaquette
model could be obtained by relating the upper bound of the maximum violation of
the Bell's inequality to the concurrences of a pure state with respect to
different bipartitions.
",physics
"  The composition of natural liquidity has been changing over time. An analysis
of intraday volumes for the S&P500 constituent stocks illustrates that (i)
volume surprises, i.e., deviations from their respective forecasts, are
correlated across stocks, and (ii) this correlation increases during the last
few hours of the trading session. These observations could be attributed, in
part, to the prevalence of portfolio trading activity that is implicit in the
growth of ETF, passive and systematic investment strategies; and, to the
increased trading intensity of such strategies towards the end of the trading
session, e.g., due to execution of mutual fund inflows/outflows that are
benchmarked to the closing price on each day. In this paper, we investigate the
consequences of such portfolio liquidity on price impact and portfolio
execution. We derive a linear cross-asset market impact from a stylized model
that explicitly captures the fact that a certain fraction of natural liquidity
providers only trade portfolios of stocks whenever they choose to execute. We
find that due to cross-impact and its intraday variation, it is optimal for a
risk-neutral, cost minimizing liquidator to execute a portfolio of orders in a
coupled manner, as opposed to a separable VWAP-like execution that is often
assumed. The optimal schedule couples the execution of the various orders so as
to be able to take advantage of increased portfolio liquidity towards the end
of the day. A worst case analysis shows that the potential cost reduction from
this optimized execution schedule over the separable approach can be as high as
6% for plausible model parameters. Finally, we discuss how to estimate
cross-sectional price impact if one had a dataset of realized portfolio
transaction records that exploits the low-rank structure of its coefficient
matrix suggested by our analysis.
",quantitative-finance
"  We describe the category of integrable sl(1|n)^ -modules with the positive
central charge and show that the irreducible modules provide the full set of
irreducible representations for the corresponding simple vertex algebra.
",mathematics
"  The search for flat-band solid-state realizations is a crucial issue to
verify or to challenge theoretical predictions for quantum many-body flat-band
systems. For frustrated quantum magnets flat bands lead to various
unconventional properties related to the existence of localized many-magnon
states. The recently synthesized magnetic compound Ba$_2$CoSi$_2$O$_6$Cl$_2$
seems to be an almost perfect candidate to observe these features in
experiments. We develop a theory for Ba$_2$CoSi$_2$O$_6$Cl$_2$ by adapting the
localized-magnon concept to this compound. We first show that our theory
describes the known experimental facts and then we propose new experimental
studies to detect a field-driven phase transition related to a
Wigner-crystal-like ordering of localized magnons at low temperatures.
",physics
"  The natural uranium assembly, ""QUINTA"", was irradiated with 2, 4, and 8 GeV
deuterons. The $^{232}$Th, $^{127}$I, and $^{129}$I samples have been exposed
to secondary neutrons produced in the assembly at a 20-cm radial distance from
the deuteron beam axis. The spectra of gamma rays emitted by the activated
$^{232}$Th, $^{127}$I, and $^{129}$I samples have been analyzed and several
tens of product nuclei have been identified. For each of those products,
neutron-induced reaction rates have been determined. The transmutation power
for the $^{129}$I samples is estimated. Experimental results were compared to
those calculated with well-known stochastic and deterministic codes.
",physics
"  Acoustical radiation force (ARF) induced by a single Bessel beam with
arbitrary order and location on a nonspherical shape is studied with the
emphasis on the physical mechanism and parameter conditions of negative
(pulling) forces. Numerical experiments are conducted to verify the T-matrix
method (TMM) for axial ARFs. This study may guide the experimental set-up to
find negative axial ARF quickly and effectively based on the predicted
parameters with TMM, and could be extended for lateral forces. The present work
could help to design acoustic tweezers numerical toolbox, which provides an
alternate to the optic tweezers.
",physics
"  When analyzing empirical data, we often find that global linear models
overestimate the number of parameters required. In such cases, we may ask
whether the data lies on or near a manifold or a set of manifolds (a so-called
multi-manifold) of lower dimension than the ambient space. This question can be
phrased as a (multi-) manifold hypothesis. The identification of such intrinsic
multiscale features is a cornerstone of data analysis and representation and
has given rise to a large body of work on manifold learning. In this work, we
review key results on multi-scale data analysis and intrinsic dimension
followed by the introduction of a heuristic, multiscale framework for testing
the multi-manifold hypothesis. Our method implements a hypothesis test on a set
of spline-interpolated manifolds constructed from variance-based intrinsic
dimensions. The workflow is suitable for empirical data analysis as we
demonstrate on two use cases.
",statistics
"  Place recognition is a challenging problem in mobile robotics, especially in
unstructured environments or under viewpoint and illumination changes. Most
LiDAR-based methods rely on geometrical features to overcome such challenges,
as generally scene geometry is invariant to these changes, but tend to affect
camera-based solutions significantly. Compared to cameras, however, LiDARs lack
the strong and descriptive appearance information that imaging can provide.
To combine the benefits of geometry and appearance, we propose coupling the
conventional geometric information from the LiDAR with its calibrated intensity
return. This strategy extracts extremely useful information in the form of a
new descriptor design, coined ISHOT, outperforming popular state-of-art
geometric-only descriptors by significant margin in our local descriptor
evaluation. To complete the framework, we furthermore develop a probabilistic
keypoint voting place recognition algorithm, leveraging the new descriptor and
yielding sublinear place recognition performance. The efficacy of our approach
is validated in challenging global localization experiments in large-scale
built-up and unstructured environments.
",computer-science
"  The goal of this paper is to extend the classical and multiplicative
fractional derivatives. For this purpose, it is introduced the new extended
modified Bessel function and also given an important relation between this new
function I(v,q;x) and the confluent hypergeometric function. Besides, it is
used to generalize the hypergeometric, the confluent hypergeometric and the
extended beta functions by using the new extended modified Bessel function.
Also, the asymptotic formulae and the generating function of the extended
modified Bessel function are obtained. The extensions of classical and
multiplicative fractional derivatives are defined via extended modified Bessel
function and, first time the fractional derivative of rational functions is
explicitly given via complex partial fraction decomposition.
",mathematics
"  We construct and analyze a strongly consistent second-order finite difference
scheme for the steady two-dimensional Stokes flow. The pressure Poisson
equation is explicitly incorporated into the scheme. Our approach suggested by
the first two authors is based on a combination of the finite volume method,
difference elimination, and numerical integration. We make use of the
techniques of the differential and difference Janet/Groebner bases. In order to
prove strong consistency of the generated scheme we correlate the differential
ideal generated by the polynomials in the Stokes equations with the difference
ideal generated by the polynomials in the constructed difference scheme.
Additionally, we compute the modified differential system of the obtained
scheme and analyze the scheme's accuracy and strong consistency by considering
this system. An evaluation of our scheme against the established
marker-and-cell method is carried out.
",computer-science
"  As robots become increasingly prevalent in almost all areas of society, the
factors affecting humans trust in those robots becomes increasingly important.
This paper is intended to investigate the factor of robot attributes, looking
specifically at the relationship between anthropomorphism and human development
of trust. To achieve this, an interaction game, Matching the Pairs, was
designed and implemented on two robots of varying levels of anthropomorphism,
Pepper and Husky. Participants completed both pre- and post-test questionnaires
that were compared and analyzed predominantly with the use of quantitative
methods, such as paired sample t-tests. Post-test analyses suggested a positive
relationship between trust and anthropomorphism with $80\%$ of participants
confirming that the robots' adoption of facial features assisted in
establishing trust. The results also indicated a positive relationship between
interaction and trust with $90\%$ of participants confirming this for both
robots post-test
",computer-science
"  In this paper, we investigate the complexity of one-dimensional dynamic
programming, or more specifically, of the Least-Weight Subsequence (LWS)
problem: Given a sequence of $n$ data items together with weights for every
pair of the items, the task is to determine a subsequence $S$ minimizing the
total weight of the pairs adjacent in $S$. A large number of natural problems
can be formulated as LWS problems, yielding obvious $O(n^2)$-time solutions.
In many interesting instances, the $O(n^2)$-many weights can be succinctly
represented. Yet except for near-linear time algorithms for some specific
special cases, little is known about when an LWS instantiation admits a
subquadratic-time algorithm and when it does not. In particular, no lower
bounds for LWS instantiations have been known before. In an attempt to remedy
this situation, we provide a general approach to study the fine-grained
complexity of succinct instantiations of the LWS problem. In particular, given
an LWS instantiation we identify a highly parallel core problem that is
subquadratically equivalent. This provides either an explanation for the
apparent hardness of the problem or an avenue to find improved algorithms as
the case may be.
More specifically, we prove subquadratic equivalences between the following
pairs (an LWS instantiation and the corresponding core problem) of problems: a
low-rank version of LWS and minimum inner product, finding the longest chain of
nested boxes and vector domination, and a coin change problem which is closely
related to the knapsack problem and (min,+)-convolution. Using these
equivalences and known SETH-hardness results for some of the core problems, we
deduce tight conditional lower bounds for the corresponding LWS instantiations.
We also establish the (min,+)-convolution-hardness of the knapsack problem.
",computer-science
"  Recently, dinitriles (NC(CH2)nCN) and especially adiponitrile (ADN, n=4) have
attracted the attention as secure electrolyte solvents due to their chemical
stability, high boiling points, high flash points and low vapor pressure. The
good solvating properties of ADN toward lithium salts and its high
electrochemical stability (~ 6V vs. Li/Li+) make it suitable for safer Li-ions
cells without performances loss. In this study, ADN is used as a single
electrolyte solvent with lithium bis(trimethylsulfonyl)imide (LiTFSI). This
electrolyte allows the use of aluminum collectors as almost no corrosion occurs
at voltages up to 4.2 V. Physico-chemical properties of ADN-LiTFSI electrolyte
such as salt dissolution, conductivity and viscosity were determined. The
cycling performances of batteries using Li4Ti5O12 (LTO) as anode and
LiNi1/3Co1/3Mn1/3O2 (NMC) as cathode were determined. The results indicate that
LTO/NMC batteries exhibit excellent rate capabilities with a columbic
efficiency close to 100%. As an example, cells were able to reach a capacity of
165 mAh.g-1 at 0.1C and a capacity retention of more than 98% after 200 cycles
at 0.5C. In addition, electrodes analyses by SEM, XPS and electrochemical
impedance spectroscopy after cycling confirming minimal surface changes of the
electrodes in the studied battery system
",physics
"  Predictive models allow subject-specific inference when analyzing disease
related alterations in neuroimaging data. Given a subject's data, inference can
be made at two levels: global, i.e. identifiying condition presence for the
subject, and local, i.e. detecting condition effect on each individual
measurement extracted from the subject's data. While global inference is widely
used, local inference, which can be used to form subject-specific effect maps,
is rarely used because existing models often yield noisy detections composed of
dispersed isolated islands. In this article, we propose a reconstruction
method, named RSM, to improve subject-specific detections of predictive
modeling approaches and in particular, binary classifiers. RSM specifically
aims to reduce noise due to sampling error associated with using a finite
sample of examples to train classifiers. The proposed method is a wrapper-type
algorithm that can be used with different binary classifiers in a diagnostic
manner, i.e. without information on condition presence. Reconstruction is posed
as a Maximum-A-Posteriori problem with a prior model whose parameters are
estimated from training data in a classifier-specific fashion. Experimental
evaluation is performed on synthetically generated data and data from the
Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Results on
synthetic data demonstrate that using RSM yields higher detection accuracy
compared to using models directly or with bootstrap averaging. Analyses on the
ADNI dataset show that RSM can also improve correlation between
subject-specific detections in cortical thickness data and non-imaging markers
of Alzheimer's Disease (AD), such as the Mini Mental State Examination Score
and Cerebrospinal Fluid amyloid-$\beta$ levels. Further reliability studies on
the longitudinal ADNI dataset show improvement on detection reliability when
RSM is used.
",computer-science
"  In this paper, we give some counting results on integer polynomials of fixed
degree and bounded height whose distinct non-zero roots are multiplicatively
dependent. These include sharp lower bounds, upper bounds and asymptotic
formulas for various cases, although in general there is a logarithmic gap
between lower and upper bounds.
",mathematics
"  We construct an extended oriented $(2+\epsilon)$-dimensional topological
field theory, the character field theory $X_G$ attached to a affine algebraic
group in characteristic zero, which calculates the homology of character
varieties of surfaces. It is a model for a dimensional reduction of
Kapustin-Witten theory ($N=4$ $d=4$ super-Yang-Mills in the GL twist), and a
universal version of the unipotent character field theory introduced in
arXiv:0904.1247. Boundary conditions in $X_G$ are given by quantum Hamiltonian
$G$-spaces, as captured by de Rham (or strong) $G$-categories, i.e., module
categories for the monoidal dg category $D(G)$ of $D$-modules on $G$. We show
that the circle integral $X_G(S^1)$ (the center and trace of $D(G)$) is
identified with the category $D(G/G)$ of ""class $D$-modules"", while for an
oriented surface $S$ (with arbitrary decorations at punctures) we show that
$X_G(S)\simeq{\rm H}_*^{BM}(Loc_G(S))$ is the Borel-Moore homology of the
corresponding character stack. We also describe the ""Hodge filtration"" on the
character theory, a one parameter degeneration to a TFT whose boundary
conditions are given by classical Hamiltonian $G$-spaces, and which encodes a
variant of the Hodge filtration on character varieties.
",mathematics
"  Paraphrase generation is an important problem in NLP, especially in question
answering, information retrieval, information extraction, conversation systems,
to name a few. In this paper, we address the problem of generating paraphrases
automatically. Our proposed method is based on a combination of deep generative
models (VAE) with sequence-to-sequence models (LSTM) to generate paraphrases,
given an input sentence. Traditional VAEs when combined with recurrent neural
networks can generate free text but they are not suitable for paraphrase
generation for a given sentence. We address this problem by conditioning the
both, encoder and decoder sides of VAE, on the original sentence, so that it
can generate the given sentence's paraphrases. Unlike most existing models, our
model is simple, modular and can generate multiple paraphrases, for a given
sentence. Quantitative evaluation of the proposed method on a benchmark
paraphrase dataset demonstrates its efficacy, and its performance improvement
over the state-of-the-art methods by a significant margin, whereas qualitative
human evaluation indicate that the generated paraphrases are well-formed,
grammatically correct, and are relevant to the input sentence. Furthermore, we
evaluate our method on a newly released question paraphrase dataset, and
establish a new baseline for future research.
",computer-science
"  A famous theorem of Weyl states that if $M$ is a compact submanifold of
euclidean space, then the volumes of small tubes about $M$ are given by a
polynomial in the radius $r$, with coefficients that are expressible as
integrals of certain scalar invariants of the curvature tensor of $M$ with
respect to the induced metric. It is natural to interpret this phenomenon in
terms of curvature measures and smooth valuations, in the sense of Alesker,
canonically associated to the Riemannian structure of $M$. This perspective
yields a fundamental new structure in Riemannian geometry, in the form of a
certain abstract module over the polynomial algebra $\mathbb R[t]$ that
reflects the behavior of Alesker multiplication. This module encodes a key
piece of the array of kinematic formulas of any Riemannian manifold on which a
group of isometries acts transitively on the sphere bundle. We illustrate this
principle in precise terms in the case where $M$ is a complex space form.
",mathematics
"  In distributed systems based on the Quantum Recursive Network Architecture,
quantum channels and quantum memories are used to establish entangled quantum
states between node pairs. Such systems are robust against attackers that
interact with the quantum channels. Conversely, weaknesses emerge when an
attacker takes full control of a node and alters the configuration of the local
quantum memory, either to make a denial-of-service attack or to reprogram the
node. In such a scenario, entanglement verification over quantum memories is a
means for detecting the intruder. Usually, entanglement verification approaches
focus either on untrusted sources of entangled qubits (photons, in most cases)
or on eavesdroppers that interfere with the quantum channel while entangled
qubits are transmitted. Instead, in this work we assume that the source of
entanglement is trusted, but parties may be dishonest. Looking for efficient
entanglement verification protocols that only require classical channels and
local quantum operations to work, we thoroughly analyze the one proposed by
Nagy and Akl, that we denote as NA2010 for simplicity, and we define and
analyze two entanglement verification protocols based on teleportation (denoted
as AC1 and AC2), characterized by increasing efficiency in terms of intrusion
detection probability versus sacrificed quantum resources.
",computer-science
"  This paper introduces the variational implicit processes (VIPs), a Bayesian
nonparametric method based on a class of highly flexible priors over functions.
Similar to Gaussian processes (GPs), in implicit processes (IPs), an implicit
multivariate prior (data simulators, Bayesian neural networks, etc.) is placed
over any finite collections of random variables. A novel and efficient
variational inference algorithm for IPs is derived using wake-sleep updates,
which gives analytic solutions and allows scalable hyper-parameter learning
with stochastic optimization. Experiments on real-world regression datasets
demonstrate that VIPs return better uncertainty estimates and superior
performance over existing inference methods for GPs and Bayesian neural
networks. With a Bayesian LSTM as the implicit prior, the proposed approach
achieves state-of-the-art results on predicting power conversion efficiency of
molecules based on raw chemical formulas.
",statistics
"  Nearly all previous work on small-footprint keyword spotting with neural
networks quantify model footprint in terms of the number of parameters and
multiply operations for a feedforward inference pass. These values are,
however, proxy measures since empirical performance in actual deployments is
determined by many factors. In this paper, we study the power consumption of a
family of convolutional neural networks for keyword spotting on a Raspberry Pi.
We find that both proxies are good predictors of energy usage, although the
number of multiplies is more predictive than the number of model parameters. We
also confirm that models with the highest accuracies are, unsurprisingly, the
most power hungry.
",computer-science
"  J. Willard Gibbs' Elementary Principles in Statistical Mechanics was the
definitive work of one of America's greatest physicists. Gibbs' book on
statistical mechanics establishes the basic principles and fundamental results
that have flowered into the modern field of statistical mechanics. However, at
a number of points, Gibbs' teachings on statistical mechanics diverge from
positions on the canonical ensemble found in more recent works, at points where
seemingly there should be agreement. The objective of this paper is to note
some of these points, so that Gibbs' actual positions are not misrepresented to
future generations of students.
",physics
"  In this paper, we show that the category of module spectra over
$C^*(B\mathcal{G},\mathbb{F}_p)$ is stratified for any $p$-local compact group
$\mathcal{G}$, thereby giving a support-theoretic classification of all
localizing subcategories of this category. To this end, we generalize Quillen's
$F$-isomorphism theorem, Quillen's stratification theorem, Chouinard's theorem,
and the finite generation of cohomology rings from finite groups to homotopical
groups. Moreover, we show that $p$-compact groups admit a homotopical form of
Gorenstein duality.
",mathematics
"  Microcolonies are aggregates of a few dozen to a few thousand cells exhibited
by many bacteria. The formation of microcolonies is a crucial step towards the
formation of more mature bacterial communities known as biofilms, but also
marks a significant change in bacterial physiology. Within a microcolony,
bacteria forgo a single cell lifestyle for a communal lifestyle hallmarked by
high cell density and physical interactions between cells potentially altering
their behaviour. It is thus crucial to understand how initially identical
single cells start to behave differently while assembling in these tight
communities. Here we show that cells in the microcolonies formed by the human
pathogen Neisseria gonorrhoeae (Ng) present differential motility behaviors
within an hour upon colony formation. Observation of merging microcolonies and
tracking of single cells within microcolonies reveal a heterogeneous motility
behavior: cells close to the surface of the microcolony exhibit a much higher
motility compared to cells towards the center. Numerical simulations of a
biophysical model for the microcolonies at the single cell level suggest that
the emergence of differential behavior within a multicellular microcolony of
otherwise identical cells is of mechanical origin. It could suggest a route
toward further bacterial differentiation and ultimately mature biofilms.
",physics
"  The study of neuronal interactions is currently at the center of several
neuroscience big collaborative projects (including the Human Connectome, the
Blue Brain, the Brainome, etc.) which attempt to obtain a detailed map of the
entire brain matrix. Under certain constraints, mathematical theory can advance
predictions of the expected neural dynamics based solely on the statistical
properties of such synaptic interaction matrix. This work explores the
application of free random variables (FRV) to the study of large synaptic
interaction matrices. Besides recovering in a straightforward way known results
on eigenspectra of neural networks, we extend them to heavy-tailed
distributions of interactions. More importantly, we derive analytically the
behavior of eigenvector overlaps, which determine stability of the spectra. We
observe that upon imposing the neuronal excitation/inhibition balance, although
the eigenvalues remain unchanged, their stability dramatically decreases due to
strong non-orthogonality of associated eigenvectors. It leads us to the
conclusion that the understanding of the temporal evolution of asymmetric
neural networks requires considering the entangled dynamics of both
eigenvectors and eigenvalues, which might bear consequences for learning and
memory processes in these models. Considering the success of FRV analysis in a
wide variety of branches disciplines, we hope that the results presented here
foster additional application of these ideas in the area of brain sciences.
",quantitative-biology
"  In this paper we study an anisotropic variant of the Rudin-Osher-Fatemi
functional with $L^1$ fidelity term of the form \[ E(u) = \int_{\mathbb{R}^n}
\phi(\nabla u) + \lambda \| u -f \|_{L^1(\mathbb{R}^n)}. \] We will
characterize the minimizers of $E$ in terms of the Wulff shape of $\phi$ and
the dual anisotropy. In particular we will calculate the subdifferential of
$E$. We will apply this characterization to the special case $\phi = |\cdot|_1$
and $n=2$, which has been used in the denoising of 2D bar codes. In this case,
we determine the shape of a minimizer $u$ when $f$ is the characteristic
function of a circle.
",mathematics
"  Although for a number of semilinear stochastic wave equations existence and
uniqueness results for corresponding solution processes are known from the
literature, these solution processes are typically not explicitly known and
numerical approximation methods are needed in order for mathematical modelling
with stochastic wave equations to become relevant for real world applications.
This, in turn, requires the numerical analysis of convergence rates for such
numerical approximation processes. A recent article by the authors proves upper
bounds for weak errors for spatial spectral Galerkin approximations of a class
of semilinear stochastic wave equations. The findings there are complemented by
the main result of this work, that provides lower bounds for weak errors which
show that in the general framework considered the established upper bounds can
essentially not be improved.
",mathematics
"  In this work, we examine two approaches to interprocedural data-flow analysis
of Sharir and Pnueli in terms of precision: the functional and the call-string
approach. In doing so, not only the theoretical best, but all solutions are
regarded which occur when using abstract interpretation or widening
additionally. It turns out that the solutions of both approaches coincide. This
property is preserved when using abstract interpretation; in the case of
widening, a comparison of the results is not always possible.
",computer-science
"  Many barred galaxies, possibly including the Milky Way, have cusps in the
centres. There is a widespread belief, however, that usual bar instability
taking place in bulgeless galaxy models is impossible for the cuspy models,
because of the presence of the inner Lindblad resonance for any pattern speed.
At the same time there are numerical evidences that the bar instability can
form a bar. We analyse this discrepancy, by accurate and diverse N-body
simulations and using the calculation of normal modes. We show that bar
formation in cuspy galaxies can be explained by taking into account the disc
thickness. The exponential growth time is moderate for typical current disc
masses (about 250 Myr), but considerably increases (factor 2 or more) upon
substitution of the live halo and bulge with a rigid halo/bulge potential;
meanwhile pattern speeds remain almost the same. Normal mode analysis with
different disc mass favours a young bar hypothesis, according to which the bar
instability saturated only recently.
",physics
"  Foreshock transients upstream of Earth's bow shock have been recently
observed to accelerate electrons to many times their thermal energy. How such
acceleration occurs is unknown, however. Using THEMIS case studies, we examine
a subset of acceleration events (31 of 247 events) in foreshock transients with
cores that exhibit gradual electron energy increases accompanied by low
background magnetic field strength and large-amplitude magnetic fluctuations.
Using the evolution of electron distributions and the energy increase rates at
multiple spacecraft, we suggest that Fermi acceleration between a converging
foreshock transient's compressional boundary and the bow shock is responsible
for the observed electron acceleration. We then show that a one-dimensional
test particle simulation of an ideal Fermi acceleration model in fluctuating
fields prescribed by the observations can reproduce the observed evolution of
electron distributions, energy increase rate, and pitch-angle isotropy,
providing further support for our hypothesis. Thus, Fermi acceleration is
likely the principal electron acceleration mechanism in at least this subset of
foreshock transient cores.
",physics
"  Face recognition has made great progress with the development of deep
learning. However, video face recognition (VFR) is still an ongoing task due to
various illumination, low-resolution, pose variations and motion blur. Most
existing CNN-based VFR methods only obtain a feature vector from a single image
and simply aggregate the features in a video, which less consider the
correlations of face images in one video. In this paper, we propose a novel
Attention-Set based Metric Learning (ASML) method to measure the statistical
characteristics of image sets. It is a promising and generalized extension of
Maximum Mean Discrepancy with memory attention weighting. First, we define an
effective distance metric on image sets, which explicitly minimizes the
intra-set distance and maximizes the inter-set distance simultaneously. Second,
inspired by Neural Turing Machine, a Memory Attention Weighting is proposed to
adapt set-aware global contents. Then ASML is naturally integrated into CNNs,
resulting in an end-to-end learning scheme. Our method achieves
state-of-the-art performance for the task of video face recognition on the
three widely used benchmarks including YouTubeFace, YouTube Celebrities and
Celebrity-1000.
",computer-science
"  Global sensitivity analysis aims at determining which uncertain input
parameters of a computational model primarily drives the variance of the output
quantities of interest. Sobol' indices are now routinely applied in this
context when the input parameters are modelled by classical probability theory
using random variables. In many practical applications however, input
parameters are affected by both aleatory and epistemic (so-called polymorphic)
uncertainty, for which imprecise probability representations have become
popular in the last decade. In this paper, we consider that the uncertain input
parameters are modelled by parametric probability boxes (p-boxes). We propose
interval-valued (so-called imprecise) Sobol' indices as an extension of their
classical definition. An original algorithm based on the concepts of augmented
space, isoprobabilistic transforms and sparse polynomial chaos expansions is
devised to allow for the computation of these imprecise Sobol' indices at
extremely low cost. In particular, phantoms points are introduced to build an
experimental design in the augmented space (necessary for the calibration of
the sparse PCE) which leads to a smart reuse of runs of the original
computational model. The approach is illustrated on three analytical and
engineering examples which allows one to validate the proposed algorithms
against brute-force double-loop Monte Carlo simulation.
",statistics
"  Cameras are the most widely exploited sensor in both robotics and computer
vision communities. Despite their popularity, two dominant attributes (i.e.,
gain and exposure time) have been determined empirically and images are
captured in very passive manner. In this paper, we present an active and
generic camera attribute control scheme using Bayesian optimization. We extend
from our previous work [1] in two aspects. First, we propose a method that
jointly controls camera gain and exposure time. Secondly, to speed up the
Bayesian optimization process, we introduce image synthesis using the camera
response function (CRF). These synthesized images allowed us to diminish the
image acquisition time during the Bayesian optimization phase, substantially
improving overall control performance. The proposed method is validated both in
an indoor and an outdoor environment where light condition rapidly changes.
Supplementary material is available at this https URL .
",computer-science
"  In this paper we address the convergence of stochastic approximation when the
functions to be minimized are not convex and nonsmooth. We show that the
""mean-limit"" approach to the convergence which leads, for smooth problems, to
the ODE approach can be adapted to the non-smooth case. The limiting dynamical
system may be shown to be, under appropriate assumption, a differential
inclusion. Our results expand earlier works in this direction by Benaim et al.
(2005) and provide a general framework for proving convergence for
unconstrained and constrained stochastic approximation problems, with either
explicit or implicit updates. In particular, our results allow us to establish
the convergence of stochastic subgradient and proximal stochastic gradient
descent algorithms arising in a large class of deep learning and
high-dimensional statistical inference with sparsity inducing penalties.
",statistics
"  High-resolution imaging has delivered new prospects for detecting the
material composition and structure of cultural treasures. Despite the various
techniques for analysis, a significant diagnostic gap remained in the range of
available research capabilities for works on paper. Old master drawings were
mostly composed in a multi-step manner with various materials. This resulted in
the overlapping of different layers which made the subjacent strata difficult
to differentiate. The separation of stratified layers using imaging methods
could provide insights into the artistic work processes and help answer
questions about the object, its attribution, or in identifying forgeries. The
pattern recognition procedure was tested with mock replicas to achieve the
separation and the capability of displaying concealed red chalk under ink. In
contrast to RGB-sensor based imaging, the multi- or hyperspectral technology
allows accurate layer separation by recording the characteristic signatures of
the material's reflectance. The risk of damage to the artworks as a result of
the examination can be reduced by using combinations of defined spectra for
lightning and image capturing. By guaranteeing the maximum level of
readability, our results suggest that the technique can be applied to a broader
range of objects and assist in diagnostic research into cultural treasures in
the future.
",computer-science
"  Ontology alignment is widely-used to find the correspondences between
different ontologies in diverse fields.After discovering the alignments,several
performance scores are available to evaluate them.The scores typically require
the identified alignment and a reference containing the underlying actual
correspondences of the given ontologies.The current trend in the alignment
evaluation is to put forward a new score(e.g., precision, weighted precision,
etc.)and to compare various alignments by juxtaposing the obtained scores.
However,it is substantially provocative to select one measure among others for
comparison.On top of that, claiming if one system has a better performance than
one another cannot be substantiated solely by comparing two scalars.In this
paper,we propose the statistical procedures which enable us to theoretically
favor one system over one another.The McNemar's test is the statistical means
by which the comparison of two ontology alignment systems over one matching
task is drawn.The test applies to a 2x2 contingency table which can be
constructed in two different ways based on the alignments,each of which has
their own merits/pitfalls.The ways of the contingency table construction and
various apposite statistics from the McNemar's test are elaborated in minute
detail.In the case of having more than two alignment systems for comparison,
the family-wise error rate is expected to happen. Thus, the ways of preventing
such an error are also discussed.A directed graph visualizes the outcome of the
McNemar's test in the presence of multiple alignment systems.From this graph,
it is readily understood if one system is better than one another or if their
differences are imperceptible.The proposed statistical methodologies are
applied to the systems participated in the OAEI 2016 anatomy track, and also
compares several well-known similarity metrics for the same matching problem.
",computer-science
"  We present Generative Adversarial Capsule Network (CapsuleGAN), a framework
that uses capsule networks (CapsNets) instead of the standard convolutional
neural networks (CNNs) as discriminators within the generative adversarial
network (GAN) setting, while modeling image data. We provide guidelines for
designing CapsNet discriminators and the updated GAN objective function, which
incorporates the CapsNet margin loss, for training CapsuleGAN models. We show
that CapsuleGAN outperforms convolutional-GAN at modeling image data
distribution on MNIST and CIFAR-10 datasets, evaluated on the generative
adversarial metric and at semi-supervised image classification.
",statistics
"  From just a glance, humans can make rich predictions about the future state
of a wide range of physical systems. On the other hand, modern approaches from
engineering, robotics, and graphics are often restricted to narrow domains and
require direct measurements of the underlying states. We introduce the Visual
Interaction Network, a general-purpose model for learning the dynamics of a
physical system from raw visual observations. Our model consists of a
perceptual front-end based on convolutional neural networks and a dynamics
predictor based on interaction networks. Through joint training, the perceptual
front-end learns to parse a dynamic visual scene into a set of factored latent
object representations. The dynamics predictor learns to roll these states
forward in time by computing their interactions and dynamics, producing a
predicted physical trajectory of arbitrary length. We found that from just six
input video frames the Visual Interaction Network can generate accurate future
trajectories of hundreds of time steps on a wide range of physical systems. Our
model can also be applied to scenes with invisible objects, inferring their
future states from their effects on the visible objects, and can implicitly
infer the unknown mass of objects. Our results demonstrate that the perceptual
module and the object-based dynamics predictor module can induce factored
latent representations that support accurate dynamical predictions. This work
opens new opportunities for model-based decision-making and planning from raw
sensory observations in complex physical environments.
",computer-science
"  Large-scale vortices in protoplanetary disks are thought to form and survive
for long periods of time. Hence, they can significantly change the global disk
evolution and particularly the distribution of the solid particles embedded in
the gas, possibly explaining asymmetries and dust concentrations recently
observed at sub-millimeter and millimeter wavelengths. We investigate the
spatial distribution of dust grains using a simple model of protoplanetary disk
hosted by a giant gaseous vortex. We explore the dependence of the results on
grain size and deduce possible consequences and predictions for observations of
the dust thermal emission at sub-millimeter and millimeter wavelengths. Global
2D simulations with a bi-fluid code are used to follow the evolution of a
single population of solid particles aerodynamically coupled to the gas.
Possible observational signatures of the dust thermal emission are obtained
using simulators of ALMA and ngVLA observations. We find that a giant vortex
not only captures dust grains with Stokes number St < 1 but can also affect the
distribution of larger grains (with St '~' 1) carving a gap associated to a
ring composed of incompletely trapped particles. The results are presented for
different particle size and associated to their possible signatures in disk
observations. Gap clearing in the dust spatial distribution could be due to the
interaction with a giant gaseous vortex and their associated spiral waves,
without the gravitational assistance of a planet. Hence, strong dust
concentrations at short sub-mm wavelengths associated with a gap and an
irregular ring at longer mm and cm wavelengths could indicate the presence of
an unseen gaseous vortex.
",physics
"  To reduce data collection time for deep learning of robust robotic grasp
plans, we explore training from a synthetic dataset of 6.7 million point
clouds, grasps, and analytic grasp metrics generated from thousands of 3D
models from Dex-Net 1.0 in randomized poses on a table. We use the resulting
dataset, Dex-Net 2.0, to train a Grasp Quality Convolutional Neural Network
(GQ-CNN) model that rapidly predicts the probability of success of grasps from
depth images, where grasps are specified as the planar position, angle, and
depth of a gripper relative to an RGB-D sensor. Experiments with over 1,000
trials on an ABB YuMi comparing grasp planning methods on singulated objects
suggest that a GQ-CNN trained with only synthetic data from Dex-Net 2.0 can be
used to plan grasps in 0.8sec with a success rate of 93% on eight known objects
with adversarial geometry and is 3x faster than registering point clouds to a
precomputed dataset of objects and indexing grasps. The Dex-Net 2.0 grasp
planner also has the highest success rate on a dataset of 10 novel rigid
objects and achieves 99% precision (one false positive out of 69 grasps
classified as robust) on a dataset of 40 novel household objects, some of which
are articulated or deformable. Code, datasets, videos, and supplementary
material are available at this http URL .
",computer-science
"  We show that two Hamiltonian isotopic Lagrangians in
(CP^2,\omega_\textup{FS}) induce two Lagrangian submanifolds in the one-point
blow-up (\widetilde{CP}^2,\widetilde{\omega}_\rho) that are not Hamiltonian
isotopic. Furthermore, we show that for any integer k>1 there are k Hamiltonian
isotopic Lagrangians in (CP^2,\omega_\textup{FS}) that induce k Lagrangian
submanifolds in the one-point blow-up such that no two of them are Hamiltonian
isotopic.
",mathematics
"  Despite their significant functional roles, beta-band oscillations are least
understood. Synchronization in neuronal networks have attracted much attention
in recent years with the main focus on transition type. Whether one obtains
explosive transition or a continuous transition is an important feature of the
neuronal network which can depend on network structure as well as synaptic
types. In this study we consider the effect of synaptic interaction (electrical
and chemical) as well as structural connectivity on synchronization transition
in network models of Izhikevich neurons which spike regularly with beta
rhythms. We find a wide range of behavior including continuous transition,
explosive transition, as well as lack of global order. The stronger electrical
synapses are more conducive to synchronization and can even lead to explosive
synchronization. The key network element which determines the order of
transition is found to be the clustering coefficient and not the small world
effect, or the existence of hubs in a network. These results are in contrast to
previous results which use phase oscillator models such as the Kuramoto model.
Furthermore, we show that the patterns of synchronization changes when one goes
to the gamma band. We attribute such a change to the change in the refractory
period of Izhikevich neurons which changes significantly with frequency.
",quantitative-biology
"  We investigate the Goos-Hanchen (G-H) shifts reflected and transmitted by a
yttrium-iron-garnet (YIG) film for both normal and oblique incidence. It is
found that the nonreciprocity effect of the MO material does not only result in
a nonvanishing reflected shift at normal incidence, but also leads to a
slab-thickness-independent term which breaks the symmetry between the reflected
and transmitted shifts at oblique incidence. The asymptotic behaviors of the
normal-incidence reflected shift are obtained in the vicinity of two
characteristic frequencies corresponding to a minimum reflectivity and a total
reflection, respectively. Moreover, the coexistence of two types of
negative-reflected-shift (NRS) at oblique incidence is discussed. We show that
the reversal of the shifts from positive to negative values can be realized by
tuning the magnitude of applied magnetic field, the frequency of incident wave
and the slab thickness as well as the incident angle. In addition, we further
investigate two special cases for practical purposes: the reflected shift with
a total reflection and the transmitted shift with a total transmission.
Numerical simulations are also performed to verify our analytical results.
",physics
"  The main objective of this paper is to study the global strong solution of
the parabolic-hyperbolic incompressible magnetohydrodynamic (MHD) model in two
dimensional space. Based on Agmon, Douglis and Nirenberg's estimates for the
stationary Stokes equation and the Solonnikov's theorem of
$L^p$-$L^q$-estimates for the evolution Stokes equation, it is shown that the
mixed-type MHD equations exist a global strong solution.
",mathematics
"  Political polarization in public space can seriously hamper the function and
the integrity of contemporary democratic societies. In this paper, we propose a
novel measure of such polarization, which, by way of simple topic modelling,
quantifies differences in collective articulation of public agendas among
relevant political actors. Unlike most other polarization measures, our measure
allows cross-national comparison. Analyzing a large amount of speech records of
legislative debate in the United States Congress and the Japanese Diet over a
long period of time, we have reached two intriguing findings. First, on
average, Japanese political actors are far more polarized in their issue
articulation than their counterparts in the U.S., which is somewhat surprising
given the recent notion of U.S. politics as highly polarized. Second, the
polarization in each country shows its own temporal dynamics in response to a
different set of factors. In Japan, structural factors such as the roles of the
ruling party and the opposition often dominate such dynamics, whereas the U.S.
legislature suffers from persistent ideological differences over particular
issues between major political parties. The analysis confirms a strong
influence of institutional differences on legislative debate in parliamentary
democracies.
",computer-science
"  This paper proposes a new convex model predictive control strategy for
dynamic optimal power flow between battery energy storage systems distributed
in an AC microgrid. The proposed control strategy uses a new problem
formulation, based on a linear d-q reference frame voltage-current model and
linearised power flow approximations. This allows the optimal power flows to be
solved as a convex optimisation problem, for which fast and robust solvers
exist. The proposed method does not assume real and reactive power flows are
decoupled, allowing line losses, voltage constraints and converter current
constraints to be addressed. In addition, non-linear variations in the charge
and discharge efficiencies of lithium ion batteries are analysed and included
in the control strategy. Real-time digital simulations were carried out for an
islanded microgrid based on the IEEE 13 bus prototypical feeder, with
distributed battery energy storage systems and intermittent photovoltaic
generation. It is shown that the proposed control strategy approaches the
performance of a strategy based on non-convex optimisation, while reducing the
required computation time by a factor of 1000, making it suitable for a
real-time model predictive control implementation.
",computer-science
"  A Floquet systems is a periodically driven quantum system. It can be
described by a Floquet operator. If this unitary operator has a gap in the
spectrum, then one can define associated topological bulk invariants which can
either only depend on the bands of the Floquet operator or also on the time as
a variable. It is shown how a K-theoretic result combined with the
bulk-boundary correspondence leads to edge invariants for the half-space
Floquet operators. These results also apply to topological quantum walks.
",physics
"  Commented translation of the paper ""Universelle Bedeutung des
Wirkungsquantums"", published by Jun Ishiwara in German in the Proceedings of
Tokyo Mathematico-Physical Society 8 106-116 (1915). In his work, Ishiwara,
tenured at Sendai University, Japan, proposed - simultaneously with Arnold
Sommerfeld, William Wilson and Niels Bohr in Europe - the pase-space-integral
quantization, a rule that would be incorporated into the old-quantum-mechanics
formalism.
",physics
"  In this paper, we consider a general twisted-curved space-time hosting Dirac
spinors and we take into account the Lorentz covariant polar decomposition of
the Dirac spinor field: the corresponding decomposition of the Dirac spinor
field equation leads to a set of field equations that are real and where
spinorial components have disappeared while still maintaining Lorentz
covariance. We will see that the Dirac spinor will contain two real scalar
degrees of freedom, the module and the so-called Yvon-Takabayashi angle, and we
will display their field equations. This will permit us to study the coupling
of curvature and torsion respectively to the module and the YT angle.
",physics
"  We study the size and the complexity of computing finite state automata (FSA)
representing and approximating the downward and the upward closure of Petri net
languages with coverability as the acceptance condition. We show how to
construct an FSA recognizing the upward closure of a Petri net language in
doubly-exponential time, and therefore the size is at most doubly exponential.
For downward closures, we prove that the size of the minimal automata can be
non-primitive recursive. In the case of BPP nets, a well-known subclass of
Petri nets, we show that an FSA accepting the downward/upward closure can be
constructed in exponential time. Furthermore, we consider the problem of
checking whether a simple regular language is included in the downward/upward
closure of a Petri net/BPP net language. We show that this problem is
EXPSPACE-complete (resp. NP-complete) in the case of Petri nets (resp. BPP
nets). Finally, we show that it is decidable whether a Petri net language is
upward/downward closed. To this end, we prove that one can decide whether a
given regular language is a subset of a Petri net coverability language.
",computer-science
"  These notes were written as supplementary material for a five-hour lecture
series presented at the Centre de Recerca Mathemàtica at the Universitat
Autònoma de Barcelona from the 13th to the 17th of March 2017. The intention
of these notes is to give a brief overview of some key topics in the area of
$C^*$-algebras associated to étale groupoids. The scope has been deliberately
contained to the case of étale groupoids with the intention that much of the
representation-theoretic technology and measure-theoretic analysis required to
handle general groupoids can be suppressed in this simpler setting.
A published version of these notes will appear in the volume tentatively
titled ""Operator algebras and dynamics: groupoids, crossed products and Rokhlin
dimension"" by Gabor Szabo, Dana P. Williams and myself, and edited by Francesc
Perera, in the series ""Advanced Courses in Mathematics. CRM Barcelona."" The
pagination of this arXiv version is not identical to Birkhäuser's style, but
I have tried to make it close. The theorem numbering should be correct. I'm
grateful to the CRM and Birkhäuser for allowing me to post a version on
arXiv.
",mathematics
"  In disordered elastic systems, driven by displacing a parabolic confining
potential adiabatically slowly, all advance of the system is in bursts, termed
avalanches. Avalanches have a finite extension in time, which is much smaller
than the waiting-time between them. Avalanches also have a finite extension
$\ell$ in space, i.e. only a part of the interface of size $\ell$ moves during
an avalanche. Here we study their spatial shape $\left< S(x)\right>_{\ell}$
given $\ell$, as well as its fluctuations encoded in the second cumulant
$\left< S^{2}(x)\right>_{\ell}^{\rm c}$. We establish scaling relations
governing the behavior close to the boundary. We then give analytic results for
the Brownian force model, in which the microscopic disorder for each degree of
freedom is a random walk. Finally, we confirm these results with numerical
simulations. To do this properly we elucidate the influence of discretization
effects, which also confirms the assumptions entering into the scaling ansatz.
This allows us to reach the scaling limit already for avalanches of moderate
size. We find excellent agreement for the universal shape, its fluctuations,
including all amplitudes.
",physics
"  Sleep stage classification constitutes an important preliminary exam in the
diagnosis of sleep disorders. It is traditionally performed by a sleep expert
who assigns to each 30s of signal a sleep stage, based on the visual inspection
of signals such as electroencephalograms (EEG), electrooculograms (EOG),
electrocardiograms (ECG) and electromyograms (EMG). We introduce here the first
deep learning approach for sleep stage classification that learns end-to-end
without computing spectrograms or extracting hand-crafted features, that
exploits all multivariate and multimodal Polysomnography (PSG) signals (EEG,
EMG and EOG), and that can exploit the temporal context of each 30s window of
data. For each modality the first layer learns linear spatial filters that
exploit the array of sensors to increase the signal-to-noise ratio, and the
last layer feeds the learnt representation to a softmax classifier. Our model
is compared to alternative automatic approaches based on convolutional networks
or decisions trees. Results obtained on 61 publicly available PSG records with
up to 20 EEG channels demonstrate that our network architecture yields
state-of-the-art performance. Our study reveals a number of insights on the
spatio-temporal distribution of the signal of interest: a good trade-off for
optimal classification performance measured with balanced accuracy is to use 6
EEG with 2 EOG (left and right) and 3 EMG chin channels. Also exploiting one
minute of data before and after each data segment offers the strongest
improvement when a limited number of channels is available. As sleep experts,
our system exploits the multivariate and multimodal nature of PSG signals in
order to deliver state-of-the-art classification performance with a small
computational cost.
",statistics
"  This paper reproduces the text of a part of the Author's DPhil thesis. It
gives a proof of the classification of non-trivial, finite homogeneous
geometries of sufficiently high dimension which does not depend on the
classification of the finite simple groups.
",mathematics
"  The growing importance and utilization of measuring brain waves (e.g. EEG
signals of eye state) in brain-computer interface (BCI) applications
highlighted the need for suitable classification methods. In this paper, a
comparison between three of well-known classification methods (i.e. support
vector machine (SVM), hidden Markov map (HMM), and radial basis function (RBF))
for EEG based eye state classification was achieved. Furthermore, a suggested
method that is based on ensemble model was tested. The suggested (ensemble
system) method based on a voting algorithm with two kernels: random forest (RF)
and Kstar classification methods. The performance was tested using three
measurement parameters: accuracy, mean absolute error (MAE), and confusion
matrix. Results showed that the proposed method outperforms the other tested
methods. For instance, the suggested method's performance was 97.27% accuracy
and 0.13 MAE.
",computer-science
"  Extreme mass ratio inspiral (EMRI) events are vulnerable to perturbations by
the stellar background, which can abort them prematurely by deflecting EMRI
orbits to plunging ones that fall directly into the massive black hole (MBH),
or to less eccentric ones that no longer interact strongly with the MBH. A
coincidental hierarchy between the collective resonant Newtonian torques due to
the stellar background, and the relative magnitudes of the leading-order
post-Newtonian precessional and radiative terms of the general relativistic
2-body problem, allows EMRIs to decouple from the background and produce
semi-periodic gravitational wave signals. I review the recent theoretical
developments that confirm this conjectured fortunate coincidence, and briefly
discuss the implications for EMRI rates, and show how these dynamical effects
can be probed locally by stars near the Galactic MBH.
",physics
"  Online social network (OSN) discussion groups are exerting significant
effects on political dialogue. In the absence of access control mechanisms, any
user can contribute to any OSN thread. Individuals can exploit this
characteristic to execute targeted attacks, which increases the potential for
subsequent malicious behaviors such as phishing and malware distribution. These
kinds of actions will also disrupt bridges among the media, politicians, and
their constituencies.
For the concern of Security Management, blending malicious cyberattacks with
online social interactions has introduced a brand new challenge. In this paper
we describe our proposal for a novel approach to studying and understanding the
strategies that attackers use to spread malicious URLs across Facebook
discussion groups. We define and analyze problems tied to predicting the
potential for attacks focused on threads created by news media organizations.
We use a mix of macro static features and the micro dynamic evolution of posts
and threads to identify likely targets with greater than 90% accuracy. One of
our secondary goals is to make such predictions within a short (10 minute) time
frame. It is our hope that the data and analyses presented in this paper will
support a better understanding of attacker strategies and footprints, thereby
developing new system management methodologies in handing cyber attacks on
social networks.
",computer-science
"  The domain name system translates human friendly web addresses to a computer
readable internet protocol address. This basic infrastructure is insecure and
can be manipulated. Deployment of technology to secure the DNS system has been
slow, reaching about 20% of all web sites based in the USA. Little is known
about the efforts hospitals and health systems make to secure the domain name
system for their websites. To investigate the prevalence of implementing Domain
Name System Security Extensions (DNSSEC), we analyzed the websites of the 210
public hospitals in the state of Illinois, USA. Only one Illinois hospital
website was found to have implemented DNSSEC by December, 2017.
",computer-science
"  We present a novel approach to achieve adaptable band structures and
non-reciprocal wave propagation by exploring and exploiting the concept of
metastable modular metastructures. Through studying the dynamics of wave
propagation in a chain composed of finite metastable modules, we provide
experimental and analysis results on non-reciprocal wave propagation and unveil
the underlying mechanisms in accomplishing such unidirectional energy
transmission. Utilizing the property adaptation feature afforded via
transitioning amongst metastable states, we uncovered an unprecedented bandgap
reconfiguration characteristic, which enables the adaptivity of wave
propagation within the metastructure. Overall, this investigation elucidates
the rich dynamics attainable by periodicity, nonlinearity, asymmetry, and
metastability, and creates a new class of adaptive structural and material
systems capable of realizing tunable bandgaps and non-reciprocal wave
transmissions.
",physics
"  We demonstrated sympathetic cooling of a single ion in a buffer gas of
ultracold atoms with small mass. Efficient collisional cooling was realized by
suppressing collision-induced heating. We attempt to explain the experimental
results with a simple rate equation model and provide a quantitative discussion
of the cooling efficiency per collision. The knowledge we obtained in this work
is an important ingredient for advancing the technique of sympathetic cooling
of ions with neutral atoms.
",physics
"  The Klein-Kramers equation, governing the Brownian motion of a classical
particle in quantum environment under the action of an arbitrary external
potential, is derived. Quantum temperature and friction operators are
introduced and at large friction the corresponding Smoluchowski equation is
obtained. Introducing the Bohm quantum potential, this Smoluchowski equation is
extended to describe the Brownian motion of a quantum particle in quantum
environment.
",physics
"  Suppose $F:=(f_1,\ldots,f_n)$ is a system of random $n$-variate polynomials
with $f_i$ having degree $\leq\!d_i$ and the coefficient of $x^{a_1}_1\cdots
x^{a_n}_n$ in $f_i$ being an independent complex Gaussian of mean $0$ and
variance $\frac{d_i!}{a_1!\cdots a_n!\left(d_i-\sum^n_{j=1}a_j \right)!}$.
Recent progress on Smale's 17th Problem by Lairez --- building upon seminal
work of Shub, Beltran, Pardo, Bürgisser, and Cucker --- has resulted in a
deterministic algorithm that finds a single (complex) approximate root of $F$
using just $N^{O(1)}$ arithmetic operations on average, where
$N\!:=\!\sum^n_{i=1}\frac{(n+d_i)!}{n!d_i!}$ ($=n(n+\max_i
d_i)^{O(\min\{n,\max_i d_i)\}}$) is the maximum possible total number of
monomial terms for such an $F$. However, can one go faster when the number of
terms is smaller, and we restrict to real coefficient and real roots? And can
one still maintain average-case polynomial-time with more general probability
measures?
We show the answer is yes when $F$ is instead a binomial system --- a case
whose numerical solution is a key step in polyhedral homotopy algorithms for
solving arbitrary polynomial systems. We give a deterministic algorithm that
finds a real approximate root (or correctly decides there are none) using just
$O(n^2(\log(n)+\log\max_i d_i))$ arithmetic operations on average. Furthermore,
our approach allows Gaussians with arbitrary variance. We also discuss briefly
the obstructions to maintaining average-case time polynomial in $n\log \max_i
d_i$ when $F$ has more terms.
",computer-science
"  We employ the exponentially improved asymptotic expansions of the confluent
hypergeometric functions on the Stokes lines discussed by the author [Appl.
Math. Sci. {\bf 7} (2013) 6601--6609] to give the analogous expansions of the
modified Bessel functions $I_\nu(z)$ and $K_\nu(z)$ for large $z$ and finite
$\nu$ on $\arg\,z=\pm\pi$ (and, in the case of $I_\nu(z)$, also on
$\arg\,z=0$). Numerical results are presented to illustrate the accuracy of
these expansions.
",mathematics
"  Program slicing provides explanations that illustrate how program outputs
were produced from inputs. We build on an approach introduced in prior work by
Perera et al., where dynamic slicing was defined for pure higher-order
functional programs as a Galois connection between lattices of partial inputs
and partial outputs. We extend this approach to imperative functional programs
that combine higher-order programming with references and exceptions. We
present proofs of correctness and optimality of our approach and a
proof-of-concept implementation and experimental evaluation.
",computer-science
"  An experimental setup for consecutive measurement of ion and x-ray absorption
in tissue or other materials is introduced. With this setup using a 3D-printed
sample container, the reference stopping-power ratio (SPR) of materials can be
measured with an uncertainty of below 0.1%. A total of 65 porcine and bovine
tissue samples were prepared for measurement, comprising five samples each of
13 tissue types representing about 80% of the total body mass (three different
muscle and fatty tissues, liver, kidney, brain, heart, blood, lung and bone).
Using a standard stoichiometric calibration for single-energy CT (SECT) as well
as a state-of-the-art dual-energy CT (DECT) approach, SPR was predicted for all
tissues and then compared to the measured reference. With the SECT approach,
the SPRs of all tissues were predicted with a mean error of (-0.84 $\pm$ 0.12)%
and a mean absolute error of (1.27 $\pm$ 0.12)%. In contrast, the DECT-based
SPR predictions were overall consistent with the measured reference with a mean
error of (-0.02 $\pm$ 0.15)% and a mean absolute error of (0.10 $\pm$ 0.15)%.
Thus, in this study, the potential of DECT to decrease range uncertainty could
be confirmed in biological tissue.
",physics
"  We present a micro aerial vehicle (MAV) system, built with inexpensive
off-the-shelf hardware, for autonomously following trails in unstructured,
outdoor environments such as forests. The system introduces a deep neural
network (DNN) called TrailNet for estimating the view orientation and lateral
offset of the MAV with respect to the trail center. The DNN-based controller
achieves stable flight without oscillations by avoiding overconfident behavior
through a loss function that includes both label smoothing and entropy reward.
In addition to the TrailNet DNN, the system also utilizes vision modules for
environmental awareness, including another DNN for object detection and a
visual odometry component for estimating depth for the purpose of low-level
obstacle detection. All vision systems run in real time on board the MAV via a
Jetson TX1. We provide details on the hardware and software used, as well as
implementation details. We present experiments showing the ability of our
system to navigate forest trails more robustly than previous techniques,
including autonomous flights of 1 km.
",computer-science
"  The regular icosahedron is connected to many exceptional objects in
mathematics. Here we describe two constructions of the $\mathrm{E}_8$ lattice
from the icosahedron. One uses a subring of the quaternions called the
""icosians"", while the other uses du Val's work on the resolution of Kleinian
singularities. Together they link the golden ratio, the quaternions, the
quintic equation, the 600-cell, and the Poincare homology 3-sphere. We leave it
as a challenge to the reader to find the connection between these two
constructions.
",mathematics
"  Traceroute is the main tools to explore Internet path. It provides limited
information about each node along the path. However, Traceroute cannot go
further in statistics analysis, or \emph{Man-Machine Interface (MMI)}.
Indeed, there are no graphical tool that is able to draw all paths used by IP
routes. We present a new tool that can handle more than 1,000 Traceroute
results, map them, identify graphically MPLS links, get information of usage of
all routes (in percent) to improve the knowledge between countries' links.
rTraceroute want to go deeper in usage of atomic traces. In this paper, we will
discuss the concept of rTraceroute and present some example of usage.
",computer-science
"  An immense class of physical counterexamples to the four dimensional strong
cosmic censor conjecture---in its usual broad formulation---is exhibited. More
precisely, out of any closed and simply connected 4-manifold an open Ricci-flat
Lorentzian 4-manifold is constructed which is not globally hyperbolic and no
perturbation of it, in any sense, can be globally hyperbolic. This very stable
non-global-hyperbolicity is the consequence of our open spaces having a
""creased end"" i.e., an end diffeomorphic to an exotic ${\mathbb R}^4$. Open
manifolds having an end like this is a typical phenomenon in four dimensions.
The construction is based on a collection of results of Gompf and Taubes on
exotic and self-dual spaces, respectively, as well as applying Penrose'
non-linear graviton construction (i.e., twistor theory) to solve the Riemannian
Einstein's equation. These solutions then are converted into stably
non-globally-hyperbolic Lorentzian vacuum solutions. It follows that the
plethora of vacuum solutions we found cannot be obtained via the initial value
formulation of the Einstein's equation because they are ""too long"" in a certain
sense (explained in the text). This different (i.e., not based on the initial
value formulation but twistorial) technical background might partially explain
why the existence of vacuum solutions of this kind have not been realized so
far in spite of the fact that, apparently, their superabundance compared to the
well-known globally hyperbolic vacuum solutions is overwhelming.
",mathematics
"  The heavyweight stellar initial mass function (IMF) observed in the cores of
massive early-type galaxies (ETGs) has been linked to formation of their cores
in an initial swiftly-quenched rapid starburst. However, the outskirts of ETGs
are thought to be assembled via the slow accumulation of smaller systems in
which the star formation is less extreme; this suggests the form of the IMF
should exhibit a radial trend in ETGs. Here we report radial stellar population
gradients out to the half-light radii of a sample of eight nearby ETGs.
Spatially resolved spectroscopy at 0.8-1.35{\mu}m from the VLT's KMOS
instrument was used to measure radial trends in the strengths of a variety of
IMF-sensitive absorption features (including some which are previously
unexplored). We find weak or no radial variation in some of these which, given
a radial IMF trend, ought to vary measurably, e.g. for the Wing-Ford band we
measure a gradient of +0.06$\pm$0.04 per decade in radius.
Using stellar population models to fit stacked and individual spectra, we
infer that the measured radial changes in absorption feature strengths are
primarily accounted for by abundance gradients which are fairly consistent
across our sample (e.g. we derive an average [Na/H] gradient of
-0.53$\pm$0.07). The inferred contribution of dwarf stars to the total light
typically corresponds to a bottom heavy IMF, but we find no evidence for radial
IMF variations in the majority of our sample galaxies.
",physics
"  An infinite chain of driven-dissipative condensate spins with uniform
nearest-neighbor coherent coupling is solved analytically and investigated
numerically. Above a critical occupation threshold the condensates undergo
spontaneous spin bifurcation (becoming magnetized) forming a binary chain of
spin-up or spin-down states. Minimization of the bifurcation threshold
determines the magnetic order as a function of the coupling strength. This
allows control of multiple magnetic orders via adiabatic (slow ramping of)
pumping. In addition to ferromagnetic and anti-ferromagnetic ordered states we
show the formation of a paired-spin ordered state $\left|\dots \uparrow
\uparrow \downarrow \downarrow \dots \right. \rangle$ as a consequence of the
phase degree of freedom between condensates.
",physics
"  The present study proposes LitStoryTeller, an interactive system for visually
exploring the semantic structure of a scientific article. We demonstrate how
LitStoryTeller could be used to answer some of the most fundamental research
questions, such as how a new method was built on top of existing methods, based
on what theoretical proof and experimental evidences. More importantly,
LitStoryTeller can assist users to understand the full and interesting story a
scientific paper, with a concise outline and important details. The proposed
system borrows a metaphor from screen play, and visualizes the storyline of a
scientific paper by arranging its characters (scientific concepts or
terminologies) and scenes (paragraphs/sentences) into a progressive and
interactive storyline. Such storylines help to preserve the semantic structure
and logical thinking process of a scientific paper. Semantic structures, such
as scientific concepts and comparative sentences, are extracted using existing
named entity recognition APIs and supervised classifiers, from a scientific
paper automatically. Two supplementary views, ranked entity frequency view and
entity co-occurrence network view, are provided to help users identify the
""main plot"" of such scientific storylines. When collective documents are ready,
LitStoryTeller also provides a temporal entity evolution view and entity
community view for collection digestion.
",computer-science
"  Public speaking is an important aspect of human communication and
interaction. The majority of computational work on public speaking concentrates
on analyzing the spoken content, and the verbal behavior of the speakers. While
the success of public speaking largely depends on the content of the talk, and
the verbal behavior, non-verbal (visual) cues, such as gestures and physical
appearance also play a significant role. This paper investigates the importance
of visual cues by estimating their contribution towards predicting the
popularity of a public lecture. For this purpose, we constructed a large
database of more than $1800$ TED talk videos. As a measure of popularity of the
TED talks, we leverage the corresponding (online) viewers' ratings from
YouTube. Visual cues related to facial and physical appearance, facial
expressions, and pose variations are extracted from the video frames using
convolutional neural network (CNN) models. Thereafter, an attention-based long
short-term memory (LSTM) network is proposed to predict the video popularity
from the sequence of visual features. The proposed network achieves
state-of-the-art prediction accuracy indicating that visual cues alone contain
highly predictive information about the popularity of a talk. Furthermore, our
network learns a human-like attention mechanism, which is particularly useful
for interpretability, i.e. how attention varies with time, and across different
visual cues by indicating their relative importance.
",computer-science
"  Modern surveys have provided the astronomical community with a flood of
high-dimensional data, but analyses of these data often occur after their
projection to lower-dimensional spaces. In this work, we introduce a local
two-sample hypothesis test framework that an analyst may directly apply to data
in their native space. In this framework, the analyst defines two classes based
on a response variable of interest (e.g. higher-mass galaxies versus lower-mass
galaxies) and determines at arbitrary points in predictor space whether the
local proportions of objects that belong to the two classes significantly
differs from the global proportion.
Our framework has a potential myriad of uses throughout astronomy; here, we
demonstrate its efficacy by applying it to a sample of 2487 i-band-selected
galaxies observed by the HST ACS in four of the CANDELS program fields. For
each galaxy, we have seven morphological summary statistics along with an
estimated stellar mass and star-formation rate. We perform two studies: one in
which we determine regions of the seven-dimensional space of morphological
statistics where high-mass galaxies are significantly more numerous than
low-mass galaxies, and vice-versa, and another study where we use SFR in place
of mass. We find that we are able to identify such regions, and show how
high-mass/low-SFR regions are associated with concentrated and undisturbed
galaxies while galaxies in low-mass/high-SFR regions appear more extended
and/or disturbed than their high-mass/low-SFR counterparts.
",physics
"  In this paper, we measure systematic risk with a new nonparametric factor
model, the neural network factor model. The suitable factors for systematic
risk can be naturally found by inserting daily returns on a wide range of
assets into the bottleneck network. The network-based model does not stick to a
probabilistic structure unlike parametric factor models, and it does not need
feature engineering because it selects notable features by itself. In addition,
we compare performance between our model and the existing models using 20-year
data of S&P 100 components. Although the new model can not outperform the best
ones among the parametric factor models due to limitations of the variational
inference, the estimation method used for this study, it is still noteworthy in
that it achieves the performance as best the comparable models could without
any prior knowledge.
",quantitative-finance
"  Given a traveling salesman problem (TSP) tour $H$ in graph $G$ a $k$-move is
an operation which removes $k$ edges from $H$, and adds $k$ edges of $G$ so
that a new tour $H'$ is formed. The popular $k$-OPT heuristics for TSP finds a
local optimum by starting from an arbitrary tour $H$ and then improving it by a
sequence of $k$-moves.
Until 2016, the only known algorithm to find an improving $k$-move for a
given tour was the naive solution in time $O(n^k)$. At ICALP'16 de Berg,
Buchin, Jansen and Woeginger showed an $O(n^{\lfloor 2/3k \rfloor+1})$-time
algorithm.
We show an algorithm which runs in $O(n^{(1/4+\epsilon_k)k})$ time, where
$\lim \epsilon_k = 0$. We are able to show that it improves over the state of
the art for every $k=5,\ldots,10$. For the most practically relevant case $k=5$
we provide a slightly refined algorithm running in $O(n^{3.4})$ time. We also
show that for the $k=4$ case, improving over the $O(n^3)$-time algorithm of de
Berg et al. would be a major breakthrough: an $O(n^{3-\epsilon})$-time
algorithm for any $\epsilon>0$ would imply an $O(n^{3-\delta})$-time algorithm
for the ALL PAIRS SHORTEST PATHS problem, for some $\delta>0$.
",computer-science
"  We prove the nonvanishing of the twisted central critical values of a class
of automorphic L-functions for twists by all but finitely many unitary
characters in particular infinite families. The methods build on a
non-archimedean approach introduced by Greenberg in the context of the Birch
and Swinnerton-Dyer Conjecture. While this paper focuses on L-functions
associated to certain automorphic representations of unitary groups, it
illustrates how decades-old methods from Iwasawa theory can be combined with
the output of new machinery to achieve broader nonvanishing results.
",mathematics
"  Many asteroid databases with lightcurve brightness measurements (e.g. WISE,
Pan-STARRS1) contain enormous amounts of data for asteroid shape and spin
modelling. While lightcurve inversion is not plausible for individual targets
with scarce data, it is possible for large populations with thousands of
asteroids, where the distributions of the shape and spin characteristics of the
populations are obtainable.
We aim to introduce a software implementation of a method that computes the
joint shape elongation p and spin latitude beta distributions for a population,
with the brightness observations given in an asteroid database. Other main
goals are to include a method for performing validity checks of the algorithm,
and a tool for a statistical comparison of populations.
The LEADER software package read the brightness measurement data for a
user-defined subpopulation from a given database. The observations were used to
compute estimates of the brightness variations of the population members. A
cumulative distribution function (CDF) was constructed of these estimates. A
superposition of known analytical basis functions yielded this CDF as a
function of the (shape, spin) distribution. The joint distribution can be
reconstructed by solving a linear constrained inverse problem. To test the
validity of the method, the algorithm can be run with synthetic asteroid
models, where the shape and spin characteristics are known, and by using the
geometries taken from the examined database.
LEADER is a fast and robust software package for solving shape and spin
distributions for large populations. There are major differences in the quality
and coverage of measurements depending on the database used, so synthetic
simulations are always necessary before a database can be reliably used. We
show examples of differences in the results when switching to another database.
",physics
"  Automatic segmentation in MR brain images is important for quantitative
analysis in large-scale studies with images acquired at all ages.
This paper presents a method for the automatic segmentation of MR brain
images into a number of tissue classes using a convolutional neural network. To
ensure that the method obtains accurate segmentation details as well as spatial
consistency, the network uses multiple patch sizes and multiple convolution
kernel sizes to acquire multi-scale information about each voxel. The method is
not dependent on explicit features, but learns to recognise the information
that is important for the classification based on training data. The method
requires a single anatomical MR image only.
The segmentation method is applied to five different data sets: coronal
T2-weighted images of preterm infants acquired at 30 weeks postmenstrual age
(PMA) and 40 weeks PMA, axial T2- weighted images of preterm infants acquired
at 40 weeks PMA, axial T1-weighted images of ageing adults acquired at an
average age of 70 years, and T1-weighted images of young adults acquired at an
average age of 23 years. The method obtained the following average Dice
coefficients over all segmented tissue classes for each data set, respectively:
0.87, 0.82, 0.84, 0.86 and 0.91.
The results demonstrate that the method obtains accurate segmentations in all
five sets, and hence demonstrates its robustness to differences in age and
acquisition protocol.
",computer-science
"  Light carrying orbital angular momentum (OAM) has been shown to be of use in
a disparate range of fields ranging from astronomy to optical trapping, and as
a promising new dimension for multiplexing signals in optical communications
and data storage. A challenge to many of these applications is a reliable and
dynamic method that sorts incident OAM states without altering them. Here we
report a wavelength-independent technique capable of dynamically filtering
individual OAM states based on the resonant transmission of a tunable optical
cavity. The cavity length is piezo-controlled to facilitate dynamic
reconfiguration, and the sorting process leaves both the transmitted and
reflected signals in their original states for subsequent processing. As a
result, we also show that a reconfigurable sorting network can be constructed
by cascading such optical resonators to handle multiple OAM states
simultaneously. This approach to sorting OAM states is amenable to integration
into optical communication networks and has implications in quantum optics,
astronomy, optical data storage and optical trapping.
",physics
"  We present the first real-world application of methods for improving neural
machine translation (NMT) with human reinforcement, based on explicit and
implicit user feedback collected on the eBay e-commerce platform. Previous work
has been confined to simulation experiments, whereas in this paper we work with
real logged feedback for offline bandit learning of NMT parameters. We conduct
a thorough analysis of the available explicit user judgments---five-star
ratings of translation quality---and show that they are not reliable enough to
yield significant improvements in bandit learning. In contrast, we successfully
utilize implicit task-based feedback collected in a cross-lingual search task
to improve task-specific and machine translation quality metrics.
",statistics
"  We test the Coulomb exchange and correlation energy density functionals of
electron systems for atomic nuclei in the local density approximation (LDA) and
the generalized gradient approximation (GGA). For the exchange Coulomb
energies, it is found that the deviation between the LDA and GGA ranges from
around $ 11 \, \% $ in $ {}^{4} \mathrm{He} $ to around $ 2.2 \, \% $ in $
{}^{208} \mathrm{Pb} $, by taking the Perdew-Burke-Ernzerhof (PBE) functional
as an example of the GGA\@. For the correlation Coulomb energies, it is shown
that those functionals of electron systems are not suitable for atomic nuclei.
",physics
"  Since the 1940s, population projections have in most cases been produced
using the deterministic cohort component method. However, in 2015, for the
first time, in a major advance, the United Nations issued official
probabilistic population projections for all countries based on Bayesian
hierarchical models for total fertility and life expectancy. The estimates of
these models and the resulting projections are conditional on the UN's official
estimates of past values. However, these past values are themselves uncertain,
particularly for the majority of the world's countries that do not have
longstanding high-quality vital registration systems, when they rely on surveys
and censuses with their own biases and measurement errors. This paper is a
first attempt to remedy this for total fertility rates, by extending the UN
model for the future to take account of uncertainty about past values. This is
done by adding an additional level to the hierarchical model to represent the
multiple data sources, in each case estimating their bias and measurement error
variance. We assess the method by out-of-sample predictive validation. While
the prediction intervals produced by the current method have somewhat less than
nominal coverage, we find that our proposed method achieves close to nominal
coverage. The prediction intervals become wider for countries for which the
estimates of past total fertility rates rely heavily on surveys rather than on
vital registration data.
",statistics
"  We study the behavior of the spectrum of the Dirac operator together with a
symmetric $W^{1, \infty}$-potential on spin manifolds under a collapse of
codimension one with bounded sectional curvature and diameter. If there is an
induced spin structure on the limit space $N$ then there are convergent
eigenvalues which converge to the spectrum of a first order differential
operator $D$ on $N$ together with a symmetric $W^{1,\infty}$-potential. If $N$
is orientable and the dimension of the limit space is even then $D$ is the
Dirac operator $D^N$ on $N$ and if the dimension of the limit space is odd,
then $D = D^N \oplus -D^N$.
",mathematics
"  Finding patterns in data and being able to retrieve information from those
patterns is an important task in Information retrieval. Complex search
requirements which are not fulfilled by simple string matching and require
exploring certain patterns in data demand a better query engine that can
support searching via structured queries. In this article, we built a
structured query engine which supports searching data through structured
queries on the lines of ElasticSearch. We will show how we achieved real time
indexing and retrieving of data through a RESTful API and how complex queries
can be created and processed using efficient data structures we created for
storing the data in structured way. Finally, we will conclude with an example
of movie recommendation system built on top of this query engine.
",computer-science
"  This volume contains a selection of the papers presented at the XVI Jornadas
sobre Programación y Lenguajes (PROLE 2016), held at Salamanca, Spain, during
September 14th-15th, 2016. Previous editions of the workshop were held in
Santander (2015), Cádiz (2014), Madrid (2013), Almería (2012), A Coruña
(2011), València (2010), San Sebastián (2009), Gijón (2008), Zaragoza
(2007), Sitges (2006), Granada (2005), Málaga (2004), Alicante (2003), El
Escorial (2002), and Almagro (2001). Programming languages provide a conceptual
framework which is necessary for the development, analysis, optimization and
understanding of programs and programming tasks. The aim of the PROLE series of
conferences (PROLE stems from PROgramación y LEnguajes) is to serve as a
meeting point for Spanish research groups which develop their work in the area
of programming and programming languages. The organization of this series of
events aims at fostering the exchange of ideas, experiences and results among
these groups. Promoting further collaboration is also one of its main goals.
",computer-science
"  We have extended the biquaternionic Dirac's equation to include interactions
with photons. The electric field is found to be perpendicular to the matter
magnetic field, and the magnetic field is perpendicular to the matter inertial
field. Inertial and magnetic masses are found to be conserved separately. The
magnetic mass density is a consequence of the coupling between the vector
potential and the matter inertial field. The presence of the vector and scalar
potentials, and the matter inertial and magnetic fields are found to modify the
standard form of the derived Maxwell's equations. The resulting interacting
electrodynamics equations are found to generalize those of axion-like fields of
Frank Wilczek or Chern-Simons equations. The axion field satisfies massive
Klein-Gordon equation if Lorenz gauge condition is violated. Therefore, axion
could be our massive photon. The electromagnetic field vector,
$\vec{F}=\vec{E}+ic\vec{B}$, is found to satisfy massive Dirac's equation in
addition to the fact that $\vec{\nabla}\cdot\vec{F}=0$, where $\vec{E}$ and
$\vec{B}$ are the electric and magnetic fields, respectively.
",physics
"  During visuomotor tasks, robots must compensate for temporal delays inherent
in their sensorimotor processing systems. Delay compensation becomes crucial in
a dynamic environment where the visual input is constantly changing, e.g.,
during the interacting with a human demonstrator. For this purpose, the robot
must be equipped with a prediction mechanism for using the acquired perceptual
experience to estimate possible future motor commands. In this paper, we
present a novel neural network architecture that learns prototypical visuomotor
representations and provides reliable predictions on the basis of the visual
input. These predictions are used to compensate for the delayed motor behavior
in an online manner. We investigate the performance of our method with a set of
experiments comprising a humanoid robot that has to learn and generate visually
perceived arm motion trajectories. We evaluate the accuracy in terms of mean
prediction error and analyze the response of the network to novel movement
demonstrations. Additionally, we report experiments with incomplete data
sequences, showing the robustness of the proposed architecture in the case of a
noisy and faulty visual sensor.
",computer-science
"  We obtain the non-linear generalization of the Sachs-Wolfe + integrated
Sachs-Wolfe (ISW) formula describing the CMB temperature anisotropies. Our
formula is valid at all orders in perturbation theory, is also valid in all
gauges and includes scalar, vector and tensor modes. A direct consequence of
our results is that the maps of the logarithmic temperature anisotropies are
much cleaner than the usual CMB maps, because they automatically remove many
secondary anisotropies. This can for instance, facilitate the search for
primordial non-Gaussianity in future works. It also disentangles the non-linear
ISW from other effects. Finally, we provide a method which can iteratively be
used to obtain the lensing solution at the desired order.
",physics
"  We estimate the average flux density of minimally-coupled axion-like
particles generated by a laser-driven plasma wakefield propagating along a
constant strong magnetic field. Our calculations suggest that a terrestrial
source based on this approach could generate a pulse of axion-like particles
whose flux density is comparable to that of solar axion-like particles at
Earth. This mechanism is optimal for axion-like particles with mass in the
range of interest of contemporary experiments designed to detect dark matter
using microwave cavities.
",physics
"  This paper addresses the problem of depth estimation from a single still
image. Inspired by recent works on multi- scale convolutional neural networks
(CNN), we propose a deep model which fuses complementary information derived
from multiple CNN side outputs. Different from previous methods, the
integration is obtained by means of continuous Conditional Random Fields
(CRFs). In particular, we propose two different variations, one based on a
cascade of multiple CRFs, the other on a unified graphical model. By designing
a novel CNN implementation of mean-field updates for continuous CRFs, we show
that both proposed models can be regarded as sequential deep networks and that
training can be performed end-to-end. Through extensive experimental evaluation
we demonstrate the effective- ness of the proposed approach and establish new
state of the art results on publicly available datasets.
",computer-science
"  We consider the quantum complexity of computing Schatten $p$-norms and
related quantities, and find that the problem of estimating these quantities is
closely related to the one clean qubit model of computation. We show that the
problem of approximating $\text{Tr}\, (|A|^p)$ for a log-local $n$-qubit
Hamiltonian $A$ and $p=\text{poly}(n)$, up to a suitable level of accuracy, is
contained in DQC1; and that approximating this quantity up to a somewhat higher
level of accuracy is DQC1-hard. In some cases the level of accuracy achieved by
the quantum algorithm is substantially better than a natural classical
algorithm for the problem. The same problem can be solved for arbitrary sparse
matrices in BQP. One application of the algorithm is the approximate
computation of the energy of a graph.
",computer-science
"  The approximation power of general feedforward neural networks with piecewise
linear activation functions is investigated. First, lower bounds on the size of
a network are established in terms of the approximation error and network depth
and width. These bounds improve upon state-of-the-art bounds for certain
classes of functions, such as strongly convex functions. Second, an upper bound
is established on the difference of two neural networks with identical weights
but different activation functions.
",statistics
"  Heterogeneity has been studied as one of the most common explanations of the
puzzle of cooperation in social dilemmas. A large number of papers have been
published discussing the effects of increasing heterogeneity in structured
populations of agents, where it has been established that heterogeneity may
favour cooperative behaviour if it supports agents to locally coordinate their
strategies. In this paper, assuming an existing model of a heterogeneous
weighted network, we aim to further this analysis by exploring the relationship
(if any) between heterogeneity and cooperation. We adopt a weighted network
which is fully populated by agents playing both the Prisoner's Dilemma or the
Optional Prisoner's Dilemma games with coevolutionary rules, i.e., not only the
strategies but also the link weights evolve over time. Surprisingly, results
show that the heterogeneity of link weights (states) on their own does not
always promote cooperation; rather cooperation is actually favoured by the
increase in the number of overlapping states and not by the heterogeneity
itself. We believe that these results can guide further research towards a more
accurate analysis of the role of heterogeneity in social dilemmas.
",computer-science
"  In this paper, we consider zero-sum repeated games in which the maximizer is
restricted to strategies requiring no more than a limited amount of randomness.
Particularly, we analyze the maxmin payoff of the maximizer in two models: the
first model forces the maximizer to randomize her action in each stage just by
conditioning her decision to outcomes of a given sequence of random source,
whereas, in the second model, the maximizer is a team of players who are free
to privately randomize their corresponding actions but do not have access to
any explicit source of shared randomness needed for cooperation. The works of
Gossner and Vieille, and Gossner and Tomala adopted the method of types to
establish their results; however, we utilize the idea of random hashing which
is the core of randomness extractors in the information theory literature. In
addition, we adopt the well-studied tool of simulation of a source from another
source. By utilizing these tools, we are able to simplify the prior results and
generalize them as well. We characterize the maxmin payoff of the maximizer in
the repeated games under study. Particularly, the maxmin payoff of the first
model is fully described by the function $J(h)$ which is the maximum payoff
that the maximizer can secure in a one-shot game by choosing mixed strategies
of entropy at most $h$. In the second part of the paper, we study the
computational aspects of $J(h)$. We offer three explicit lower bounds on the
entropy-payoff trade-off curve. To do this, we provide and utilize new results
for the set of distributions that guarantee a certain payoff for Alice. In
particular, we study how this set of distributions shrinks as we increase the
security level. While the use of total variation distance is common in game
theory, our derivation indicates the suitability of utilizing the
Renyi-divergence of order two.
",computer-science
"  Scanning Microwave Impedance Microscopy (MIM) measurement of
photoconductivity with 50 nm resolution is demonstrated using a modulated
optical source. The use of a modulated source allows for measurement of
photoconductivity in a single scan without a reference region on the sample, as
well as removing most topographical artifacts and enhancing signal to noise as
compared with unmodulated measurement. A broadband light source with tunable
monochrometer is then used to measure energy resolved photoconductivity with
the same methodology. Finally, a pulsed optical source is used to measure local
photo-carrier lifetimes via MIM, using the same 50 nm resolution tip.
",physics
"  This paper argues that the judicial use of formal language theory and
grammatical inference are invaluable tools in understanding how deep neural
networks can and cannot represent and learn long-term dependencies in temporal
sequences. Learning experiments were conducted with two types of Recurrent
Neural Networks (RNNs) on six formal languages drawn from the Strictly Local
(SL) and Strictly Piecewise (SP) classes. The networks were Simple RNNs
(s-RNNs) and Long Short-Term Memory RNNs (LSTMs) of varying sizes. The SL and
SP classes are among the simplest in a mathematically well-understood hierarchy
of subregular classes. They encode local and long-term dependencies,
respectively. The grammatical inference algorithm Regular Positive and Negative
Inference (RPNI) provided a baseline. According to earlier research, the LSTM
architecture should be capable of learning long-term dependencies and should
outperform s-RNNs. The results of these experiments challenge this narrative.
First, the LSTMs' performance was generally worse in the SP experiments than in
the SL ones. Second, the s-RNNs out-performed the LSTMs on the most complex SP
experiment and performed comparably to them on the others.
",computer-science
"  We study the ultimate bounds on the estimation of temperature for an
interacting quantum system. We consider two coupled bosonic modes that are
assumed to be thermal and using quantum estimation theory establish the role
the Hamiltonian parameters play in thermometry. We show that in the case of a
conserved particle number the interaction between the modes leads to a decrease
in the overall sensitivity to temperature, while interestingly, if particle
exchange is allowed with the thermal bath the converse is true. We explain this
dichotomy by examining the energy spectra. Finally, we devise experimentally
implementable thermometry schemes that rely only on locally accessible
information from the total system, showing that almost Heisenberg limited
precision can still be achieved, and we address the (im)possibility for
multiparameter estimation in the system.
",physics
"  Building on insights of Jovanovic (1982) and subsequent authors, we develop a
comprehensive theory of optimal timing of decisions based around continuation
value functions and operators that act on them. Optimality results are provided
under general settings, with bounded or unbounded reward functions. This
approach has several intrinsic advantages that we exploit in developing the
theory. One is that continuation value functions are smoother than value
functions, allowing for sharper analysis of optimal policies and more efficient
computation. Another is that, for a range of problems, the continuation value
function exists in a lower dimensional space than the value function,
mitigating the curse of dimensionality. In one typical experiment, this reduces
the computation time from over a week to less than three minutes.
",mathematics
"  Refractory organic compounds formed in molecular clouds are among the
building blocks of the solar system objects and could be the precursors of
organic matter found in primitive meteorites and cometary materials. However,
little is known about the evolutionary pathways of molecular cloud organics
from dense molecular clouds to planetary systems. In this study, we focus on
the evolution of the morphological and viscoelastic properties of molecular
cloud refractory organic matter. We found that the organic residue,
experimentally synthesized at about 10 K from UV-irradiated H2O-CH3OH-NH3 ice,
changed significantly in terms of its nanometer- to micrometer-scale morphology
and viscoelastic properties after UV irradiation at room temperature. The dose
of this irradiation was equivalent to that experienced after short residence in
diffuse clouds (equal or less than 10,000 years) or irradiation in outer
protoplanetary disks. The irradiated organic residues became highly porous and
more rigid and formed amorphous nanospherules. These nanospherules are
morphologically similar to organic nanoglobules observed in the least-altered
chondrites, chondritic porous interplanetary dust particles, and cometary
samples, suggesting that irradiation of refractory organics could be a possible
formation pathway for such nanoglobules. The storage modulus (elasticity) of
photo-irradiated organic residues is about 100 MPa irrespective of vibrational
frequency, a value that is lower than the storage moduli of minerals and ice.
Dust grains coated with such irradiated organics would therefore stick together
efficiently, but growth to larger grains might be suppressed due to an increase
in aggregate brittleness caused by the strong connections between grains.
",physics
"  We prove, combinatorially, that the product of a Schubert polynomial by a
Stanley symmetric polynomial is a truncated Schubert polynomial. Using Monk's
rule, we derive a nonnegative combinatorial formula for the Schubert polynomial
expansion of a truncated Schubert polynomial. Combining these results, we give
a nonnegative combinatorial rule for the product of a Schubert and a Schur
polynomial in the Schubert basis.
",mathematics
"  Ultra-faint dwarf galaxies (UFDs) are the faintest known galaxies and due to
their incredibly low surface brightness, it is difficult to find them beyond
the Local Group. We report a serendipitous discovery of an UFD, Fornax UFD1, in
the outskirts of NGC 1316, a giant galaxy in the Fornax cluster. The new galaxy
is located at a projected radius of 55 kpc in the south-east of NGC 1316. This
UFD is found as a small group of resolved stars in the Hubble Space Telescope
images of a halo field of NGC 1316, obtained as part of the Carnegie-Chicago
Hubble Program. Resolved stars in this galaxy are consistent with being mostly
metal-poor red giant branch (RGB) stars. Applying the tip of the RGB method to
the mean magnitude of the two brightest RGB stars, we estimate the distance to
this galaxy, 19.0 +- 1.3 Mpc. Fornax UFD1 is probably a member of the Fornax
cluster. The color-magnitude diagram of these stars is matched by a 12 Gyr
isochrone with low metallicity ([Fe/H] ~ -2.4). Total magnitude and effective
radius of Fornax UFD1 are Mv ~ -7.6 +- 0.2 mag and r_eff = 146 +- 9 pc, which
are similar to those of Virgo UFD1 that was discovered recently in the
intracluster field of Virgo by Jang & Lee (2014).Fornax UFD1 is the most
distant known UFD that is confirmed by resolved stars. This indicates that UFDs
are ubiquitous and that more UFDs remain to be discovered in the Fornax
cluster.
",physics
"  We study the fundamental group of the complement of the singular locus of
Lauricella's hypergeometric function $F_C$ of $n$ variables. The singular locus
consists of $n$ hyperplanes and a hypersurface of degree $2^{n-1}$ in the
complex $n$-space. We derive some relations that holds for general $n\geq 3$.
We give an explicit presentation of the fundamental groupin the
three-dimensional case. We also consider a presentation of the fundamental
group of $2^3$-covering of this space.
In the version 2, we omit some of the calculations. For all the calculations,
refer to the version 1 (arXiv:1710.09594v1) of this article.
",mathematics
"  We construct firstly the complete list of five quantum deformations of $D=4$
complex homogeneous orthogonal Lie algebra $\mathfrak{o}(4;\mathbb{C})\cong
\mathfrak{o}(3;\mathbb{C})\oplus \mathfrak{o}(3;\mathbb{C})$, describing
quantum rotational symmetry of four-dimensional complex space-time, in
particular we provide the corresponding universal quantum $R$-matrices. Further
applying four possible reality conditions we obtain all sixteen Hopf-algebraic
quantum deformations for the real forms of $\mathfrak{o}(4;\mathbb{C})$:
Euclidean $\mathfrak{o}(4)$, Lorentz $\mathfrak{o}(3,1)$, Kleinian
$\mathfrak{o}(2,2)$ and quaternionic $\mathfrak{o}^{\star}(4)$. For
$\mathfrak{o}(3,1)$ we only recall well-known results obtained previously by
the authors, but for other real Lie algebras (Euclidean, Kleinian,
quaternionic) as well as for the complex Lie algebra
$\mathfrak{o}(4;\mathbb{C})$ we present new results.
",mathematics
"  Many practical problems are characterized by a preference relation over
admissible solutions, where preferred solutions are minimal in some sense. For
example, a preferred diagnosis usually comprises a minimal set of reasons that
is sufficient to cause the observed anomaly. Alternatively, a minimal
correction subset comprises a minimal set of reasons whose deletion is
sufficient to eliminate the observed anomaly. Circumscription formalizes such
preference relations by associating propositional theories with minimal models.
The resulting enumeration problem is addressed here by means of a new algorithm
taking advantage of unsatisfiable core analysis. Empirical evidence of the
efficiency of the algorithm is given by comparing the performance of the
resulting solver, CIRCUMSCRIPTINO, with HCLASP, CAMUS MCS, LBX and MCSLS on the
enumeration of minimal models for problems originating from practical
applications.
This paper is under consideration for acceptance in TPLP.
",computer-science
"  Superconductivity in noncentrosymmetric compounds has attracted sustained
interest in the last decades. Here we present a detailed study on the
transport, thermodynamic properties and the band structure of the
noncentrosymmetric superconductor La$_7$Ir$_3$ ($T_c$ $\sim$2.3 K) that was
recently proposed to break the time-reversal symmetry. It is found that
La$_7$Ir$_3$ displays a moderately large electronic heat capacity (Sommerfeld
coefficient $\gamma_n$ $\sim$ 53.1 mJ/mol $\text{K}^2$) and a significantly
enhanced Kadowaki-Woods ratio (KWR $\sim$ 32 $\mu\Omega$ cm mol$^2$ K$^2$
J$^{-2}$) that is greater than the typical value ($\sim$ 10 $\mu\Omega$ cm
mol$^2$ K$^2$ J$^{-2}$) for strongly correlated electron systems. The upper
critical field $H_{c2}$ was seen to be nicely described by the single-band
Werthamer-Helfand-Hohenberg model down to very low temperatures. The
hydrostatic pressure effects on the superconductivity were also investigated.
The heat capacity below $T_c$ reveals a dominant s-wave gap with the magnitude
close to the BCS value. The first-principles calculations yield the
electron-phonon coupling constant $\lambda$ = 0.81 and the logarithmically
averaged frequency $\omega_{ln}$ = 78.5 K, resulting in a theoretical $T_c$ =
2.5 K, close to the experimental value. Our calculations suggest that the
enhanced electronic heat capacity is more likely due to electron-phonon
coupling, rather than the electron-electron correlation effects. Collectively,
these results place severe constraints on any theory of exotic
superconductivity in this system.
",physics
"  Highly robust and efficient estimators for the generalized linear model with
a dispersion parameter are proposed. The estimators are based on three steps.
In the first step the maximum rank correlation estimator is used to
consistently estimate the slopes up to a scale factor. In the second step, the
scale factor, the intercept, and the dispersion parameter are consistently
estimated using a MT-estimator of a simple regression model. The combined
estimator is highly robust but inefficient. Then, randomized quantile residuals
based on the initial estimators are used to detect outliers to be rejected and
to define a set S of observations to be retained. Finally, a conditional
maximum likelihood (CML) estimator given the observations in S is computed. We
show that, under the model, S tends to the complete sample for increasing
sample size. Therefore, the CML tends to the unconditional maximum likelihood
estimator. It is therefore highly efficient, while maintaining the high degree
of robustness of the initial estimator. The case of the negative binomial
regression model is studied in detail.
",statistics
"  Our goal is to find classes of convolution semigroups on Lie groups $G$ that
give rise to interesting processes in symmetric spaces $G/K$. The
$K$-bi-invariant convolution semigroups are a well-studied example. An
appealing direction for the next step is to generalise to right $K$-invariant
convolution semigroups, but recent work of Liao has shown that these are in
one-to-one correspondence with $K$-bi-invariant convolution semigroups. We
investigate a weaker notion of right $K$-invariance, but show that this is, in
fact, the same as the usual notion. Another possible approach is to use
generalised notions of negative definite functions, but this also leads to
nothing new. We finally find an interesting class of convolution semigroups
that are obtained by making use of the Cartan decomposition of a semisimple Lie
group, and the solution of certain stochastic differential equations. Examples
suggest that these are well-suited for generating random motion along geodesics
in symmetric spaces.
",mathematics
"  We present a quantitative analysis on the response of a dilute active
suspension of self-propelled rods (swimmers) in a planar channel subjected to
an imposed shear flow. To best capture the salient features of shear-induced
effects, we consider the case of an imposed Couette flow, providing a constant
shear rate across the channel. We argue that the steady-state behavior of
swimmers can be understood in the light of a population splitting phenomenon,
occurring as the shear rate exceeds a certain threshold, initiating the
reversal of swimming direction for a finite fraction of swimmers from down- to
upstream or vice versa, depending on swimmer position within the channel.
Swimmers thus split into two distinct, statistically significant and oppositely
swimming majority and minority populations. The onset of population splitting
translates into a transition from a self-propulsion-dominated regime to a
shear-dominated regime, corresponding to a unimodal-to-bimodal change in the
probability distribution function of the swimmer orientation. We present a
phase diagram in terms of the swim and flow Peclet numbers showing the
separation of these two regimes by a discontinuous transition line. Our results
shed further light on the behavior of swimmers in a shear flow and provide an
explanation for the previously reported non-monotonic behavior of the mean,
near-wall, parallel-to-flow orientation of swimmers with increasing shear
strength.
",physics
"  We study the statistical and computational aspects of kernel principal
component analysis using random Fourier features and show that under mild
assumptions, $O(\sqrt{n} \log n)$ features suffices to achieve
$O(1/\epsilon^2)$ sample complexity. Furthermore, we give a memory efficient
streaming algorithm based on classical Oja's algorithm that achieves this rate.
",statistics
"  In this paper, we aim to establish a new shape theory, compact Hausdorff
shape (CH-shape) for general Hausdorff spaces. We use the ""internal"" method and
direct system approach on the homotopy category of compact Hausdorff spaces.
Such a construction can preserve most good properties of H-shape given by Rubin
and Sanders. Most importantly, we can moreover develop the entire homology
theory for CH-shape, including the exactness, dual to the consequence of
Mardešić and Segal.
",mathematics
"  We propose PowerAlert, an efficient external integrity checker for untrusted
hosts. Current attestation systems suffer from shortcomings in requiring
complete checksum of the code segment, being static, use of timing information
sourced from the untrusted machine, or use of timing information with high
error (network round trip time). We address those shortcomings by (1) using
power measurements from the host to ensure that the checking code is executed
and (2) checking a subset of the kernel space over a long period of time. We
compare the power measurement against a learned power model of the execution of
the machine and validate that the execution was not tampered. Finally, power
diversifies the integrity checking program to prevent the attacker from
adapting. We implement a prototype of PowerAlert using Raspberry pi and
evaluate the performance of the integrity checking program generation. We model
the interaction between PowerAlert and an attacker as a game. We study the
effectiveness of the random initiation strategy in deterring the attacker. The
study shows that \power forces the attacker to trade-off stealthiness for the
risk of detection, while still maintaining an acceptable probability of
detection given the long lifespan of stealthy attacks.
",computer-science
"  Our view of the universe of genomic regions harboring various types of
candidate human-specific regulatory sequences (HSRS) has been markedly expanded
in recent years. To infer the evolutionary origins of loci harboring HSRS,
analyses of conservations patterns of 59,732 loci in Modern Humans, Chimpanzee,
Bonobo, Gorilla, Orangutan, Gibbon, and Rhesus genomes have been performed. Two
major evolutionary pathways have been identified comprising thousands of
sequences that were either inherited from extinct common ancestors (ECAs) or
created de novo in humans after human/chimpanzee split. Thousands of HSRS
appear inherited from ECAs yet bypassed genomes of our closest evolutionary
relatives, presumably due to the incomplete lineage sorting and/or
species-specific loss or regulatory DNA. The bypassing pattern is prominent for
HSRS associated with development and functions of human brain. Common genomic
loci that may contributed to speciation during evolution of Great Apes comprise
248 insertions sites of African Great Ape-specific retrovirus PtERV1 (45.9%; p
= 1.03E-44) intersecting regions harboring 442 HSRS, which are enriched for
HSRS associated with human-specific (HS) changes of gene expression in cerebral
organoids. Among non-human primates (NHP), most significant fractions of
candidate HSRS associated with HS expression changes in both excitatory neurons
(347 loci; 67%) and radial glia (683 loci; 72%) are highly conserved in Gorilla
genome. Modern Humans acquired unique combinations of regulatory sequences
highly conserved in distinct species of six NHP separated by 30 million years
of evolution. Concurrently, this unique mosaic of regulatory sequences
inherited from ECAs was supplemented with 12,486 created de novo HSRS. These
observations support the model of complex continuous speciation process during
evolution of Great Apes that is not likely to occur as an instantaneous event.
",quantitative-biology
"  An optimization-based approach for the Tucker tensor approximation of
parameter-dependent data tensors and solutions of tensor differential equations
with low Tucker rank is presented. The problem of updating the tensor
decomposition is reformulated as fitting problem subject to the tangent space
without relying on an orthogonality gauge condition. A discrete Euler scheme is
established in an alternating least squares framework, where the quadratic
subproblems reduce to trace optimization problems, that are shown to be
explicitly solvable and accessible using SVD of small size. In the presence of
small singular values, instability for larger ranks is reduced, since the
method does not need the (pseudo) inverse of matricizations of the core tensor.
Regularization of Tikhonov type can be used to compensate for the lack of
uniqueness in the tangent space. The method is validated numerically and shown
to be stable also for larger ranks in the case of small singular values of the
core unfoldings. Higher order explicit integrators of Runge-Kutta type can be
composed.
",physics
"  This work introduces our approach to the flat and textureless object grasping
problem. In particular, we address the tableware and cutlery manipulation
problem where a service robot has to clean up a table. Our solution integrates
colour and 2D and 3D geometry information to describe objects, and this
information is given to the robot action planner to find the best grasping
trajectory depending on the object class. Furthermore, we use visual feedback
as a verification step to determine if the grasping process has successfully
occurred. We evaluate our approach in both an open and a standard service robot
platform following the RoboCup@Home international tournament regulations.
",computer-science
"  Exploiting the powerful tool of strong gravitational lensing by galaxy
clusters to study the highest-redshift Universe and cluster mass distributions
relies on precise lens mass modelling. In this work, we present the first
attempt at modelling line-of-sight mass distribution in addition to that of the
cluster, extending previous modelling techniques that assume mass distributions
to be on a single lens plane. We focus on the Hubble Frontier Field cluster
MACS J0416.1-2403, and our multi-plane model reproduces the observed image
positions with a rms offset of ~0.53"". Starting from this best-fitting model,
we simulate a mock cluster that resembles MACS J0416.1-2403 in order to explore
the effects of line-of-sight structures on cluster mass modelling. By
systematically analysing the mock cluster under different model assumptions, we
find that neglecting the lensing environment has a significant impact on the
reconstruction of image positions (rms ~0.3""); accounting for line-of-sight
galaxies as if they were at the cluster redshift can partially reduce this
offset. Moreover, foreground galaxies are more important to include into the
model than the background ones. While the magnification factors of the lensed
multiple images are recovered within ~10% for ~95% of them, those ~5% that lie
near critical curves can be significantly affected by the exclusion of the
lensing environment in the models (up to a factor of ~200). In addition,
line-of-sight galaxies cannot explain the apparent discrepancy in the
properties of massive subhalos between MACS J0416.1-2403 and N-body simulated
clusters. Since our model of MACS J0416.1-2403 with line-of-sight galaxies only
reduced modestly the rms offset in the image positions, we conclude that
additional complexities, such as more flexible halo shapes, would be needed in
future models of MACS J0416.1-2403.
",physics
"  The paper explores various special functions which generalize the
two-parametric Mittag-Leffler type function of two variables. Integral
representations for these functions in different domains of variation of
arguments for certain values of the parameters are obtained. The asymptotic
expansions formulas and asymptotic properties of such functions are also
established for large values of the variables. This provides statements of
theorems for these formulas and their corresponding properties.
",mathematics
"  In the real world, many complex systems interact with other systems. In
addition, the intra- or inter-systems for the spread of information about
infectious diseases and the transmission of infectious diseases are often not
random, but with direction. Hence, in this paper, we build epidemic model based
on an interconnected directed network, which can be considered as the
generalization of undirected networks and bipartite networks. By using the
mean-field approach, we establish the Susceptible-Infectious-Susceptible model
on this network. We theoretically analyze the model, and obtain the basic
reproduction number, which is also the generalization of the critical number
corresponding to undirected or bipartite networks. And we prove the global
stability of disease-free and endemic equilibria via the basic reproduction
number as a forward bifurcation parameter. We also give a condition for
epidemic prevalence only on a single subnetwork. Furthermore, we carry out
numerical simulations, and find that the independence between each node's in-
and out-degrees greatly reduce the impact of the network's topological
structure on disease spread.
",physics
"  In the article, proposed is a new e-learning information technology based on
an ontology driven learning engine, which is matched with modern pedagogical
technologies. With the help of proposed engine and developed question database
we have conducted an experiment, where students were tested. The developed
ontology driven system of e-learning facilitates the creation of favorable
conditions for the development of personal qualities and creation of a holistic
understanding of the subject area among students throughout the educational
process.
",computer-science
"  DNA-mediated computing is a novel technology that seeks to capitalize on the
enormous informational capacity of DNA and has tremendous computational ability
to compete with the current silicon-mediated computing, due to massive
parallelism and unique characteristics inherent in DNA interaction. In this
paper, the methodology of DNA-mediated computing is utilized to enrich decision
theory, by demonstrating how a novel programmable DNA-mediated normative
decision-making apparatus is able to capture rational choice under uncertainty.
",computer-science
"  Cooling the rotation and the vibration of molecules by broadband light
sources was possible for trapped molecular ions or ultracold molecules. Because
of a low power spectral density, the cooling timescale has never fell below
than a few milliseconds. Here we report on rotational and vibrational cooling
of a supersonic beam of barium monofluoride molecules in less than 440 $\mu$s.
Vibrational cooling was optimized by enhancing the spectral power density of a
semiconductor light source at the underlying molecular transitions allowing us
to transfer all the populations of $v''=1-3$ into the vibrational ground state
($v''=0$). Rotational cooling, that requires an efficient vibrational pumping,
was then achieved. According to a Boltzmann fit, the rotation temperature was
reduced by almost a factor of 10. In this fashion, the population of the lowest
rotational levels increased by more than one order of magnitude.
",physics
"  The ability of physical layer relay caching to increase the degrees of
freedom (DoF) of a single cell was recently illustrated. In this paper, we
extend this result to the case of multiple cells in which a caching relay is
shared among multiple non-cooperative base stations (BSs). In particular, we
show that a large DoF gain can be achieved by exploiting the benefits of having
a shared relay that cooperates with the BSs. We first propose a cache-assisted
relaying protocol that improves the cooperation opportunity between the BSs and
the relay. Next, we consider the cache content placement problem that aims to
design the cache content at the relay such that the DoF gain is maximized. We
propose an optimal algorithm and a near-optimal low-complexity algorithm for
the cache content placement problem. Simulation results show significant
improvement in the DoF gain using the proposed relay-caching protocol.
",computer-science
"  We consider Jacobi matrices with eventually increasing sequences of diagonal
and off-diagonal Jacobi parameters. We describe the asymptotic behavior of the
subordinate solution at the top of the essential spectrum, and the asymptotic
behavior of the spectral density at the top of the essential spectrum.
In particular, allowing on both diagonal and off-diagonal Jacobi parameters
perturbations of the free case of the form $- \sum_{j=1}^J c_j n^{-\tau_j} +
o(n^{-\tau_1-1})$ with $0 < \tau_1 < \tau_2 < \dots < \tau_J$ and $c_1>0$, we
find the asymptotic behavior of the $\log$ of spectral density to order
$O(\log(2-x))$ as $x$ approaches $2$.
Apart from its intrinsic interest, the above results also allow us to
describe the asymptotics of the spectral density for orthogonal polynomials on
the unit circle with real-valued Verblunsky coefficients of the same form.
",mathematics
"  Graph matching in two correlated random graphs refers to the task of
identifying the correspondence between vertex sets of the graphs. Recent
results have characterized the exact information-theoretic threshold for graph
matching in correlated Erdős-Rényi graphs. However, very little is known
about the existence of efficient algorithms to achieve graph matching without
seeds. In this work we identify a region in which a straightforward $O(n^2\log
n)$-time canonical labeling algorithm, initially introduced in the context of
graph isomorphism, succeeds in matching correlated Erdős-Rényi graphs.
The algorithm has two steps. In the first step, all vertices are labeled by
their degrees and a trivial minimum distance matching (i.e., simply sorting
vertices according to their degrees) matches a fixed number of highest degree
vertices in the two graphs. Having identified this subset of vertices, the
remaining vertices are matched using a matching algorithm for bipartite graphs.
",statistics
"  The early time regime of the Kardar-Parisi-Zhang (KPZ) equation in $1+1$
dimension, starting from a Brownian initial condition with a drift $w$, is
studied using the exact Fredholm determinant representation. For large drift we
recover the exact results for the droplet initial condition, whereas a
vanishingly small drift describes the stationary KPZ case, recently studied by
weak noise theory (WNT). We show that for short time $t$, the probability
distribution $P(H,t)$ of the height $H$ at a given point takes the large
deviation form $P(H,t) \sim \exp{\left(-\Phi(H)/\sqrt{t} \right)}$. We obtain
the exact expressions for the rate function $\Phi(H)$ for $H<H_{c2}$. Our exact
expression for $H_{c2}$ numerically coincides with the value at which WNT was
found to exhibit a spontaneous reflection symmetry breaking. We propose two
continuations for $H>H_{c2}$, which apparently correspond to the symmetric and
asymmetric WNT solutions. The rate function $\Phi(H)$ is Gaussian in the
center, while it has asymmetric tails, $|H|^{5/2}$ on the negative $H$ side and
$H^{3/2}$ on the positive $H$ side.
",physics
"  This paper proves that every finite volume hyperbolic 3-manifold M contains a
ubiquitous collection of closed, immersed, quasi-Fuchsian surfaces. These
surfaces are ubiquitous in the sense that their preimages in the universal
cover separate any pair of disjoint, non-asymptotic geodesic planes. The proof
relies in a crucial way on the corresponding theorem of Kahn and Markovic for
closed 3-manifolds. As a corollary of this result and a companion statement
about surfaces with cusps, we recover Wise's theorem that the fundamental group
of M acts freely and cocompactly on a CAT(0) cube complex.
",mathematics
"  Multi-label classification is a practical yet challenging task in machine
learning related fields, since it requires the prediction of more than one
label category for each input instance. We propose a novel deep neural networks
(DNN) based model, Canonical Correlated AutoEncoder (C2AE), for solving this
task. Aiming at better relating feature and label domain data for improved
classification, we uniquely perform joint feature and label embedding by
deriving a deep latent space, followed by the introduction of label-correlation
sensitive loss function for recovering the predicted label outputs. Our C2AE is
achieved by integrating the DNN architectures of canonical correlation analysis
and autoencoder, which allows end-to-end learning and prediction with the
ability to exploit label dependency. Moreover, our C2AE can be easily extended
to address the learning problem with missing labels. Our experiments on
multiple datasets with different scales confirm the effectiveness and
robustness of our proposed method, which is shown to perform favorably against
state-of-the-art methods for multi-label classification.
",computer-science
"  Algorithms for equilibrium computation generally make no attempt to ensure
that the computed strategies are understandable by humans. For instance the
strategies for the strongest poker agents are represented as massive binary
files. In many situations, we would like to compute strategies that can
actually be implemented by humans, who may have computational limitations and
may only be able to remember a small number of features or components of the
strategies that have been computed. We study poker games where private
information distributions can be arbitrary. We create a large training set of
game instances and solutions, by randomly selecting the information
probabilities, and present algorithms that learn from the training instances in
order to perform well in games with unseen information distributions. We are
able to conclude several new fundamental rules about poker strategy that can be
easily implemented by humans.
",statistics
"  Beam search is a desirable choice of test-time decoding algorithm for neural
sequence models because it potentially avoids search errors made by simpler
greedy methods. However, typical cross entropy training procedures for these
models do not directly consider the behaviour of the final decoding method. As
a result, for cross-entropy trained models, beam decoding can sometimes yield
reduced test performance when compared with greedy decoding. In order to train
models that can more effectively make use of beam search, we propose a new
training procedure that focuses on the final loss metric (e.g. Hamming loss)
evaluated on the output of beam search. While well-defined, this ""direct loss""
objective is itself discontinuous and thus difficult to optimize. Hence, in our
approach, we form a sub-differentiable surrogate objective by introducing a
novel continuous approximation of the beam search decoding procedure. In
experiments, we show that optimizing this new training objective yields
substantially better results on two sequence tasks (Named Entity Recognition
and CCG Supertagging) when compared with both cross entropy trained greedy
decoding and cross entropy trained beam decoding baselines.
",computer-science
"  While the enhancement of the spin-space symmetry from the usual
$\mathrm{SU}(2)$ to $\mathrm{SU}(N)$ is promising for finding nontrivial
quantum spin liquids, its realization in magnetic materials remains
challenging. Here we propose a new mechanism by which the $\mathrm{SU}(4)$
symmetry emerges in the strong spin-orbit coupling limit. In $d^1$ transition
metal compounds with edge-sharing anion octahedra, the spin-orbit coupling
gives rise to strongly bond-dependent and apparently $\mathrm{SU}(4)$-breaking
hopping between the $J_\textrm{eff}=3/2$ quartets. However, in the honeycomb
structure, a gauge transformation maps the system to an
$\mathrm{SU}(4)$-symmetric Hubbard model. In the strong repulsion limit at
quarter filling, as realized in $\alpha$-ZrCl$_3,$ the low-energy effective
model is the $\mathrm{SU}(4)$ Heisenberg model on the honeycomb lattice, which
cannot have a trivial gapped ground state and is expected to host a gapless
spin-orbital liquid. By generalizing this model to other three-dimensional
lattices, we also propose crystalline spin-orbital liquids protected by this
emergent $\mathrm{SU}(4)$ symmetry and space group symmetries.
",physics
"  Analysis of conjugate natural convection with surface radiation in a
two-dimensional enclosure is carried out in order to search the optimal
location of the heat source with entropy generation minimization (EGM) approach
and conventional heat transfer parameters. The air as an incompressible fluid
and transparent media is considered the fluid filling the enclosure with the
steady and laminar regime. The enclosure internal surfaces are also gray,
opaque and diffuse. The governing equations with stream function and vorticity
formulation are solved using finite difference approach. Results include the
effect of Rayleigh number and emissivity on the dimensionless average rate of
entropy generation and its optimum location. The optimum location search with
conventional heat transfer parameters including maximum temperature and Nusselt
numbers are also examined.
",physics
"  Multilevel converters have found many applications within renewable energy
systems thanks to their unique capability of generating multiple voltage
levels. However, these converters need multiple DC sources and the voltage
balancing over capacitors for these systems is cumbersome. In this work, a new
grid-tie multicell inverter with high level of safety has been designed,
engineered and optimized for integrating energy storage devices to the electric
grid. The multilevel converter proposed in this work is capable of maintaining
the flying capacitors voltage in the desired value. The solar cells are the
primary energy sources for proposed inverter where the maximum power density is
obtained. Finally, the performance of the inverter and its control method
simulated using PSCAD/EMTDC software package and good agreement achieved with
experimental data.
",physics
"  In this paper we propose a method to model speaker and session variability
and able to generate likelihood ratios using neural networks in an end-to-end
phrase dependent speaker verification system. As in Joint Factor Analysis, the
model uses tied hidden variables to model speaker and session variability and a
MAP adaptation of some of the parameters of the model. In the training
procedure our method jointly estimates the network parameters and the values of
the speaker and channel hidden variables. This is done in a two-step
backpropagation algorithm, first the network weights and factor loading
matrices are updated and then the hidden variables, whose gradients are
calculated by aggregating the corresponding speaker or session frames, since
these hidden variables are tied. The last layer of the network is defined as a
linear regression probabilistic model whose inputs are the previous layer
outputs. This choice has the advantage that it produces likelihoods and
additionally it can be adapted during the enrolment using MAP without the need
of a gradient optimization. The decisions are made based on the ratio of the
output likelihoods of two neural network models, speaker adapted and universal
background model. The method was evaluated on the RSR2015 database.
",computer-science
"  In this paper we construct explicit smooth solutions to the Strominger system
on generalized Calabi-Gray manifolds, which are compact non-Kähler Calabi-Yau
3-folds with infinitely many distinct topological types and sets of Hodge
numbers.
",mathematics
"  We consider three notions of connectivity and their interactions in partially
ordered sets coming from reduced factorizations of an element in a generated
group. While one form of connectivity essentially reflects the connectivity of
the poset diagram, the other two are a bit more involved: Hurwitz-connectivity
has its origins in algebraic geometry, and shellability in topology. We propose
a framework to study these connectivity properties in a uniform way. Our main
tool is a certain total order of the generators that is compatible with the
chosen element.
",mathematics
"  We answer Mark Kac's famous question, ""can one hear the shape of a drum?"" in
the positive for orbifolds that are 3-dimensional and 4-dimensional lens
spaces; we thus complete the answer to this question for orbifold lens spaces
in all dimensions. We also show that the coefficients of the asymptotic
expansion of the trace of the heat kernel are not sufficient to determine the
above results.
",mathematics
"  In the inverse problem of the calculus of variations one is asked to find a
Lagrangian and a multiplier so that a given differential equation, after
multiplying with the multiplier, becomes the Euler--Lagrange equation for the
Lagrangian. An answer to this problem for the case of a scalar ordinary
differential equation of order $2n, n\geq 2,$ is proposed.
",mathematics
"  We show that in any $\mathbb{Q}$-Gorenstein flat family of klt singularities,
normalized volumes can only jump down at countably many subvarieties. A quick
consequence is that smooth points have the largest normalized volume among all
klt singularities. Using an alternative characterization of K-semistability
developed by Li, Xu and the author, we show that K-semistability is a very
generic or empty condition in any $\mathbb{Q}$-Gorenstein flat family of log
Fano pairs.
",mathematics
"  Inferring walls configuration of indoor environment could help robot
""understand"" the environment better. This allows the robot to execute a task
that involves inter-room navigation, such as picking an object in the kitchen.
In this paper, we present a method to inferring walls configuration from a
moving RGB-D sensor. Our goal is to combine a simple wall configuration model
and fast wall detection method in order to get a system that works online, is
real-time, and does not need a Manhattan World assumption. We tested our
preliminary work, i.e. wall detection and measurement from moving RGB-D sensor,
with MIT Stata Center Dataset. The performance of our method is reported in
terms of accuracy and speed of execution.
",computer-science
"  This volume contains the proceedings of the Eighth International Symposium on
Games, Automata, Logic and Formal Verification (GandALF 2017). The symposium
took place in Roma, Italy, from the 20th to the 22nd of September 2017. The
GandALF symposium was established by a group of Italian computer scientists
interested in mathematical logic, automata theory, game theory, and their
applications to the specification, design, and verification of complex systems.
Its aim is to provide a forum where people from different areas, and possibly
with different backgrounds, can fruitfully interact. GandALF has a truly
international spirit, as witnessed by the composition of the program and
steering committee and by the country distribution of the submitted papers.
",computer-science
"  The analysis of telemetry data is common in animal ecological studies. While
the collection of telemetry data for individual animals has improved
dramatically, the methods to properly account for inherent uncertainties (e.g.,
measurement error, dependence, barriers to movement) have lagged behind. Still,
many new statistical approaches have been developed to infer unknown quantities
affecting animal movement or predict movement based on telemetry data.
Hierarchical statistical models are useful to account for some of the
aforementioned uncertainties, as well as provide population-level inference,
but they often come with an increased computational burden. For certain types
of statistical models, it is straightforward to provide inference if the latent
true animal trajectory is known, but challenging otherwise. In these cases,
approaches related to multiple imputation have been employed to account for the
uncertainty associated with our knowledge of the latent trajectory. Despite the
increasing use of imputation approaches for modeling animal movement, the
general sensitivity and accuracy of these methods have not been explored in
detail. We provide an introduction to animal movement modeling and describe how
imputation approaches may be helpful for certain types of models. We also
assess the performance of imputation approaches in a simulation study. Our
simulation study suggests that inference for model parameters directly related
to the location of an individual may be more accurate than inference for
parameters associated with higher-order processes such as velocity or
acceleration. Finally, we apply these methods to analyze a telemetry data set
involving northern fur seals (Callorhinus ursinus) in the Bering Sea.
",statistics
"  Autonomous Surface Vehicles (ASVs) provide an effective way to actualize
applications such as environment monitoring, search and rescue, and scientific
researches. However, the conventional ASVs depends overly on the stored energy.
Hybrid Sailboat, mainly powered by the wind, can solve this problem by using an
auxiliary propulsion system. The electric energy cost of Hybrid Sailboat needs
to be optimized to achieve the ocean automatic cruise mission. Based on
adjusted setting on sails and rudders, this paper seeks the optimal trajectory
for autonomic cruising to reduce the energy cost by changing the heading angle
of sailing upwind. The experiment results validate the heading angle accounts
for energy cost and the trajectory with the best heading angle saves up to
23.7% than other conditions. Furthermore, the energy-time line can be used to
predict the energy cost for long-time sailing.
",computer-science
"  An algorithm for constructing a control function that transfers a wide class
of stationary nonlinear systems of ordinary differential equations from an
initial state to a final state under certain control restrictions is proposed.
The algorithm is designed to be convenient for numerical implementation. A
constructive criterion of the desired transfer possibility is presented. The
problem of an interorbital flight is considered as a test example and it is
simulated numerically with the presented method.
",mathematics
"  White-box test generator tools rely only on the code under test to select
test inputs, and capture the implementation's output as assertions. If there is
a fault in the implementation, it could get encoded in the generated tests.
Tool evaluations usually measure fault-detection capability using the number of
such fault-encoding tests. However, these faults are only detected, if the
developer can recognize that the encoded behavior is faulty. We designed an
exploratory study to investigate how developers perform in classifying
generated white-box test as faulty or correct. We carried out the study in a
laboratory setting with 54 graduate students. The tests were generated for two
open-source projects with the help of the IntelliTest tool. The performance of
the participants were analyzed using binary classification metrics and by
coding their observed activities. The results showed that participants
incorrectly classified a large number of both fault-encoding and correct tests
(with median misclassification rate 33% and 25% respectively). Thus the real
fault-detection capability of test generators could be much lower than
typically reported, and we suggest to take this human factor into account when
evaluating generated white-box tests.
",computer-science
"  In supervised machine learning, an agent is typically trained once and then
deployed. While this works well for static settings, robots often operate in
changing environments and must quickly learn new things from data streams. In
this paradigm, known as streaming learning, a learner is trained online, in a
single pass, from a data stream that cannot be assumed to be independent and
identically distributed (iid). Streaming learning will cause conventional deep
neural networks (DNNs) to fail for two reasons: 1) they need multiple passes
through the entire dataset; and 2) non-iid data will cause catastrophic
forgetting. An old fix to both of these issues is rehearsal. To learn a new
example, rehearsal mixes it with previous examples, and then this mixture is
used to update the DNN. Full rehearsal is slow and memory intensive because it
stores all previously observed examples, and its effectiveness for preventing
catastrophic forgetting has not been studied in modern DNNs. Here, we describe
the ExStream algorithm for memory efficient rehearsal and compare it to
alternatives. We find that full rehearsal can eliminate catastrophic forgetting
in a variety of streaming learning settings, with ExStream performing well
using far less memory and computation.
",statistics
"  Since a tweet is limited to 140 characters, it is ambiguous and difficult for
traditional Natural Language Processing (NLP) tools to analyse. This research
presents KeyXtract which enhances the machine learning based Stanford CoreNLP
Part-of-Speech (POS) tagger with the Twitter model to extract essential
keywords from a tweet. The system was developed using rule-based parsers and
two corpora. The data for the research was obtained from a Twitter profile of a
telecommunication company. The system development consisted of two stages. At
the initial stage, a domain specific corpus was compiled after analysing the
tweets. The POS tagger extracted the Noun Phrases and Verb Phrases while the
parsers removed noise and extracted any other keywords missed by the POS
tagger. The system was evaluated using the Turing Test. After it was tested and
compared against Stanford CoreNLP, the second stage of the system was developed
addressing the shortcomings of the first stage. It was enhanced using Named
Entity Recognition and Lemmatization. The second stage was also tested using
the Turing test and its pass rate increased from 50.00% to 83.33%. The
performance of the final system output was measured using the F1 score.
Stanford CoreNLP with the Twitter model had an average F1 of 0.69 while the
improved system had a F1 of 0.77. The accuracy of the system could be improved
by using a complete domain specific corpus. Since the system used linguistic
features of a sentence, it could be applied to other NLP tools.
",computer-science
"  In Mexico, 25 per cent of the urban population now lives in informal
settlements with varying degree of depravity. Although some informal
neighbourhoods have contributed to the upward mobility of the inhabitants, the
majority still lack basic services. Mexico City and the conurbation around it,
form a mega city of 21 million people that has been growing in a manner
qualified as ""highly unproductive, (that) deepens inequality, raises pollution
levels"" and contains the largest slum in the world, Neza-Chalco-Izta. Urban
reforms are now aiming to better the conditions in these slums and therefore it
is very important to have reliable measurement tools to assess the changes that
are undergoing. In this paper, we use exploratory factor analysis to define an
index of depravity in Mexico City, namely the Slum Severity Index (SSI), based
on the UN-HABITATs definition of a slum. We apply this novel approach to the
Census survey of Mexico and measure the housing deprivation levels types from
1990 - 2010. The analysis highlights high variability in housing conditions
within Mexico City. We find that the SSI decreased significantly between 1990 -
2000 due to several policy reforms, but increased between 2000 - 2010. We also
show correlations of the SSI with other social factors such as education,
health and migration. We present a validation of the SSI using Grey Level
Co-occurrence Matrix (GLCM) features extracted from Very-High Resolution (VHR)
remote-sensed satellite images. Finally, we show that the SSI can present a
cardinally meaningful assessment of the extent of the difference in depravity
as compared to a similar index defined by CONEVAL, a government institution
that studies poverty in Mexico.
",statistics
"  Ranking is used for a wide array of problems, most notably information
retrieval (search). There are a number of popular approaches to the evaluation
of ranking such as Kendall's $\tau$, Average Precision, and nDCG. When dealing
with problems such as user ranking or recommendation systems, all these
measures suffer from various problems, including an inability to deal with
elements of the same rank, inconsistent and ambiguous lower bound scores, and
an inappropriate cost function. We propose a new measure, rankDCG, that
addresses these problems. This is a modification of the popular nDCG algorithm.
We provide a number of criteria for any effective ranking algorithm and show
that only rankDCG satisfies all of them. Results are presented on constructed
and real data sets. We release a publicly available rankDCG evaluation package.
",computer-science
"  We study the effects of quantum fluctuations on the dynamical generation of a
gap and on the evolution of the spin-wave spectra of a frustrated magnet on a
triangular lattice with bond-dependent Ising couplings, analog of the Kitaev
honeycomb model. The quantum fluctuations lift the subextensive degeneracy of
the classical ground-state manifold by a quantum order-by-disorder mechanism.
Nearest-neighbor chains remain decoupled and the surviving discrete degeneracy
of the ground state is protected by a hidden model symmetry. We show how the
four-spin interaction, emergent from the fluctuations, generates a spin gap
shifting the nodal lines of the linear spin-wave spectrum to finite energies.
",physics
"  The Complex Kohn variational method for electron-polyatomic molecule
scattering is formulated using an overset grid representation of the scattering
wave function. The overset grid consists of a central grid and multiple dense,
atom-centered subgrids that allow the simultaneous spherical expansions of the
wave function about multiple centers. Scattering boundary conditions are
enforced by using a basis formed by the repeated application of the free
particle Green's function and potential, $\hat{G}^+_0\hat{V}$ on the overset
grid in a ""Born-Arnoldi"" solution of the working equations. The theory is shown
to be equivalent to a specific Padé approximant to the $T$-matrix, and has
rapid convergence properties, both in the number of numerical basis functions
employed and the number of partial waves employed in the spherical expansions.
The method is demonstrated in calculations on methane and CF$_4$ in the
static-exchange approximation, and compared in detail with calculations
performed with the numerical Schwinger variational approach based on single
center expansions. An efficient procedure for operating with the free-particle
Green's function and exchange operators (to which no approximation is made) is
also described.
",physics
"  We prove a universal limit theorem for the halting time, or iteration count,
of the power/inverse power methods and the QR eigenvalue algorithm.
Specifically, we analyze the required number of iterations to compute extreme
eigenvalues of random, positive-definite sample covariance matrices to within a
prescribed tolerance. The universality theorem provides a complexity estimate
for the algorithms which, in this random setting, holds with high probability.
The method of proof relies on recent results on the statistics of the
eigenvalues and eigenvectors of random sample covariance matrices (i.e.,
delocalization, rigidity and edge universality).
",mathematics
"  We show that the l-adic realization functor is conservative when restricted
to the Chow motives of abelian type over a finite field. A weak version of this
conservativity result extends to mixed motives of abelian type.
",mathematics
"  The registration of tremor was performed in two groups of subjects (15 people
in each group) with different physical fitness at rest and at a static loads of
3N. Each subject has been tested 15 series (number of series N=15) in both
states (with and without physical loads) and each series contained 15 samples
(n=15) of tremorogramm measurements (500 elements in each sample, registered
coordinates x1(t) of the finger position relative to eddy current sensor) of
the finger. Using non-parametric Wilcoxon test of each series of experiment a
pairwise comparison was made forming 15 tables in which the results of
calculation of pairwise comparison was presented as a matrix (15x15) for
tremorogramms are presented. The average number of hits random pairs of samples
(<k>) and standard deviation {\sigma} were calculated for all 15 matrices
without load and under the impact of physical load (3N), which showed an
increase almost in twice in the number k of pairs of matching samples of
tremorogramms at conditions of a static load. For all these samples it was
calculated special quasi-attractor (this square was presented the distinguishes
between physical load and without it. All samples present the stochastic
unstable state.
",quantitative-biology
"  The article investigates an evidence-based semantics for epistemic logics in
which pieces of evidence are interpreted as equivalence relations on the
epistemic worlds. It is shown that the properties of knowledge obtained from
potentially infinitely many pieces of evidence are described by modal logic S5.
At the same time, the properties of knowledge obtained from only a finite
number of pieces of evidence are described by modal logic S4. The main
technical result is a sound and complete bi-modal logical system that describes
properties of these two modalities and their interplay.
",computer-science
"  Real time evolution of classical gauge fields is relevant for a number of
applications in particle physics and cosmology, ranging from the early Universe
to dynamics of quark-gluon plasma. We present a lattice formulation of the
interaction between a $shift$-symmetric field and some $U(1)$ gauge sector,
$a(x)\tilde{F}_{\mu\nu}F^{\mu\nu}$, reproducing the continuum limit to order
$\mathcal{O}(dx_\mu^2)$ and obeying the following properties: (i) the system is
gauge invariant and (ii) shift symmetry is exact on the lattice. For this end
we construct a definition of the {\it topological number density} $Q =
\tilde{F}_{\mu\nu}F^{\mu\nu}$ that admits a lattice total derivative
representation $Q = \Delta_\mu^+ K^\mu$, reproducing to order
$\mathcal{O}(dx_\mu^2)$ the continuum expression $Q = \partial_\mu K^\mu
\propto \vec E \cdot \vec B$. If we consider a homogeneous field $a(x) = a(t)$,
the system can be mapped into an Abelian gauge theory with Hamiltonian
containing a Chern-Simons term for the gauge fields. This allow us to study in
an accompanying paper the real time dynamics of fermion number non-conservation
(or chirality breaking) in Abelian gauge theories at finite temperature. When
$a(x) = a(\vec x,t)$ is inhomogeneous, the set of lattice equations of motion
do not admit however a simple explicit local solution (while preserving an
$\mathcal{O}(dx_\mu^2)$ accuracy). We discuss an iterative scheme allowing to
overcome this difficulty.
",physics
"  The classical-input quantum-output (cq) wiretap channel is a communication
model involving a classical sender $X$, a legitimate quantum receiver $B$, and
a quantum eavesdropper $E$. The goal of a private communication protocol that
uses such a channel is for the sender $X$ to transmit a message in such a way
that the legitimate receiver $B$ can decode it reliably, while the eavesdropper
$E$ learns essentially nothing about which message was transmitted. The
$\varepsilon $-one-shot private capacity of a cq wiretap channel is equal to
the maximum number of bits that can be transmitted over the channel, such that
the privacy error is no larger than $\varepsilon\in(0,1)$. The present paper
provides a lower bound on the $\varepsilon$-one-shot private classical
capacity, by exploiting the recently developed techniques of Anshu,
Devabathini, Jain, and Warsi, called position-based coding and convex
splitting. The lower bound is equal to a difference of the hypothesis testing
mutual information between $X$ and $B$ and the ""alternate"" smooth
max-information between $X$ and $E$. The one-shot lower bound then leads to a
non-trivial lower bound on the second-order coding rate for private classical
communication over a memoryless cq wiretap channel.
",computer-science
"  Online programming discussion platforms such as Stack Overflow serve as a
rich source of information for software developers. Available information
include vibrant discussions and oftentimes ready-to-use code snippets.
Anecdotes report that software developers copy and paste code snippets from
those information sources for convenience reasons. Such behavior results in a
constant flow of community-provided code snippets into production software. To
date, the impact of this behaviour on code security is unknown. We answer this
highly important question by quantifying the proliferation of security-related
code snippets from Stack Overflow in Android applications available on Google
Play. Access to the rich source of information available on Stack Overflow
including ready-to-use code snippets provides huge benefits for software
developers. However, when it comes to code security there are some caveats to
bear in mind: Due to the complex nature of code security, it is very difficult
to provide ready-to-use and secure solutions for every problem. Hence,
integrating a security-related code snippet from Stack Overflow into production
software requires caution and expertise. Unsurprisingly, we observed insecure
code snippets being copied into Android applications millions of users install
from Google Play every day. To quantitatively evaluate the extent of this
observation, we scanned Stack Overflow for code snippets and evaluated their
security score using a stochastic gradient descent classifier. In order to
identify code reuse in Android applications, we applied state-of-the-art static
analysis. Our results are alarming: 15.4% of the 1.3 million Android
applications we analyzed, contained security-related code snippets from Stack
Overflow. Out of these 97.9% contain at least one insecure code snippet.
",computer-science
"  In warm dark matter scenarios structure formation is suppressed on small
scales with respect to the cold dark matter case, reducing the number of
low-mass halos and the fraction of ionized gas at high redshifts and thus,
delaying reionization. This has an impact on the ionization history of the
Universe and measurements of the optical depth to reionization, of the
evolution of the global fraction of ionized gas and of the thermal history of
the intergalactic medium, can be used to set constraints on the mass of the
dark matter particle. However, the suppression of the fraction of ionized
medium in these scenarios can be partly compensated by varying other
parameters, as the ionization efficiency or the minimum mass for which halos
can host star-forming galaxies. Here we use different data sets regarding the
ionization and thermal histories of the Universe and, taking into account the
degeneracies from several astrophysical parameters, we obtain a lower bound on
the mass of thermal warm dark matter candidates of $m_X > 1.3$ keV, or $m_s >
5.5$ keV for the case of sterile neutrinos non-resonantly produced in the early
Universe, both at 90\% confidence level.
",physics
"  Self-paced learning (SPL) is a new methodology that simulates the learning
principle of humans/animals to start learning easier aspects of a learning
task, and then gradually take more complex examples into training. This
new-coming learning regime has been empirically substantiated to be effective
in various computer vision and pattern recognition tasks. Recently, it has been
proved that the SPL regime has a close relationship to a implicit self-paced
objective function. While this implicit objective could provide helpful
interpretations to the effectiveness, especially the robustness, insights under
the SPL paradigms, there are still no theoretical results strictly proved to
verify such relationship. To this issue, in this paper, we provide some
convergence results on this implicit objective of SPL. Specifically, we prove
that the learning process of SPL always converges to critical points of this
implicit objective under some mild conditions. This result verifies the
intrinsic relationship between SPL and this implicit objective, and makes the
previous robustness analysis on SPL complete and theoretically rational.
",computer-science
"  Person re-identification (Re-ID) usually suffers from noisy samples with
background clutter and mutual occlusion, which makes it extremely difficult to
distinguish different individuals across the disjoint camera views. In this
paper, we propose a novel deep self-paced learning (DSPL) algorithm to
alleviate this problem, in which we apply a self-paced constraint and symmetric
regularization to help the relative distance metric training the deep neural
network, so as to learn the stable and discriminative features for person
Re-ID. Firstly, we propose a soft polynomial regularizer term which can derive
the adaptive weights to samples based on both the training loss and model age.
As a result, the high-confidence fidelity samples will be emphasized and the
low-confidence noisy samples will be suppressed at early stage of the whole
training process. Such a learning regime is naturally implemented under a
self-paced learning (SPL) framework, in which samples weights are adaptively
updated based on both model age and sample loss using an alternative
optimization method. Secondly, we introduce a symmetric regularizer term to
revise the asymmetric gradient back-propagation derived by the relative
distance metric, so as to simultaneously minimize the intra-class distance and
maximize the inter-class distance in each triplet unit. Finally, we build a
part-based deep neural network, in which the features of different body parts
are first discriminately learned in the lower convolutional layers and then
fused in the higher fully connected layers. Experiments on several benchmark
datasets have demonstrated the superior performance of our method as compared
with the state-of-the-art approaches.
",computer-science
"  Isotonic regression is a standard problem in shape-constrained estimation
where the goal is to estimate an unknown nondecreasing regression function $f$
from independent pairs $(x_i, y_i)$ where $\mathbb{E}[y_i]=f(x_i), i=1, \ldots
n$. While this problem is well understood both statistically and
computationally, much less is known about its uncoupled counterpart where one
is given only the unordered sets $\{x_1, \ldots, x_n\}$ and $\{y_1, \ldots,
y_n\}$. In this work, we leverage tools from optimal transport theory to derive
minimax rates under weak moments conditions on $y_i$ and to give an efficient
algorithm achieving optimal rates. Both upper and lower bounds employ
moment-matching arguments that are also pertinent to learning mixtures of
distributions and deconvolution.
",statistics
"  Users organize themselves into communities on web platforms. These
communities can interact with one another, often leading to conflicts and toxic
interactions. However, little is known about the mechanisms of interactions
between communities and how they impact users.
Here we study intercommunity interactions across 36,000 communities on
Reddit, examining cases where users of one community are mobilized by negative
sentiment to comment in another community. We show that such conflicts tend to
be initiated by a handful of communities---less than 1% of communities start
74% of conflicts. While conflicts tend to be initiated by highly active
community members, they are carried out by significantly less active members.
We find that conflicts are marked by formation of echo chambers, where users
primarily talk to other users from their own community. In the long-term,
conflicts have adverse effects and reduce the overall activity of users in the
targeted communities.
Our analysis of user interactions also suggests strategies for mitigating the
negative impact of conflicts---such as increasing direct engagement between
attackers and defenders. Further, we accurately predict whether a conflict will
occur by creating a novel LSTM model that combines graph embeddings, user,
community, and text features. This model can be used toreate early-warning
systems for community moderators to prevent conflicts. Altogether, this work
presents a data-driven view of community interactions and conflict, and paves
the way towards healthier online communities.
",computer-science
"  This paper studies the role of dg-Lie algebroids in derived deformation
theory. More precisely, we provide an equivalence between the homotopy theories
of formal moduli problems and dg-Lie algebroids over a commutative dg-algebra
of characteristic zero. At the level of linear objects, we show that the
category of representations of a dg-Lie algebroid is an extension of the
category of quasi-coherent sheaves on the corresponding formal moduli problem.
We describe this extension geometrically in terms of pro-coherent sheaves.
",mathematics
"  We consider the reproducing kernel function of the theta Bargmann-Fock
Hilbert space associated to given full-rank lattice and pseudo-character, and
we deal with some of its analytical and arithmetical properties. Specially, the
distribution and discreteness of its zeros are examined and analytic sets
inside a product of fundamental cells is characterized and shown to be finite
and of cardinal less or equal to the dimension of the theta Bargmann-Fock
Hilbert space. Moreover, we obtain some remarkable lattice sums by evaluating
the so-called complex Hermite-Taylor coefficients. Some of them generalize some
of the arithmetic identities established by Perelomov in the framework of
coherent states for the specific case of von Neumann lattice. Such complex
Hermite-Taylor coefficients are nontrivial examples of the so-called lattice's
functions according the Serre terminology. The perfect use of the basic
properties of the complex Hermite polynomials is crucial in this framework.
",mathematics
"  Over a dozen ultracool dwarfs (UCDs), low-mass objects of spectral types
$\geq$M7, are known to be sources of radio flares. These typically
several-minutes-long radio bursts can be up to 100\% circularly polarized and
have high brightness temperatures, consistent with coherent emission via the
electron cyclotron maser operating in $\sim$kG magnetic fields. Recently, the
statistical properties of the bulk physical parameters that describe these UCDs
have become adequately described to permit synthesis of the population of
radio-flaring objects. For the first time, I construct a Monte Carlo simulator
to model the population of these radio-flaring UCDs. This simulator is powered
by Intel Secure Key (ISK)- a new processor technology that uses a local entropy
source to improve random number generation that has heretofore been used to
improve cryptography. The results from this simulator indicate that only
$\sim$5% of radio-flaring UCDs within the local interstellar neighborhood
($<$25 pc away) have been discovered. I discuss a number of scenarios which may
explain this radio-flaring fraction, and suggest that the observed behavior is
likely a result of several factors. The performance of ISK as compared to other
pseudorandom number generators is also evaluated, and its potential utility for
other astrophysical codes briefly described.
",physics
"  We investigate the similarities of pairs of articles which are co-cited at
the different co-citation levels of the journal, article, section, paragraph,
sentence and bracket. Our results indicate that textual similarity,
intellectual overlap (shared references), author overlap (shared authors),
proximity in publication time all rise monotonically as the co-citation level
gets lower (from journal to bracket). While the main gain in similarity happens
when moving from journal to article co-citation, all level changes entail an
increase in similarity, especially section to paragraph and paragraph to
sentence/bracket levels. We compare results from four journals over the years
2010-2015: Cell, the European Journal of Operational Research, Physics Letters
B and Research Policy, with consistent general outcomes and some interesting
differences. Our findings motivate the use of granular co-citation information
as defined by meaningful units of text, with implications for, among others,
the elaboration of maps of science and the retrieval of scholarly literature.
",computer-science
"  We study the physical and dynamical properties of the ionized gas in the
prototypical HII galaxy Henize 2-10 using MUSE integral field spectroscopy. The
large scale dynamics is dominated by extended outflowing bubbles, probably the
results of massive gas ejection from the central star forming regions. We
derive a mass outflow rate dMout/dt~0.30 Msun/yr, corresponding to mass loading
factor eta~0.4, in range with similar measurements in local LIRGs. Such a
massive outflow has a total kinetic energy that is sustainable by the stellar
winds and Supernova Remnants expected in the galaxy. We use classical emission
line diagnostic to study the dust extinction, electron density and ionization
conditions all across the galaxy, confirming the extreme nature of the highly
star forming knots in the core of the galaxy, which show high density and high
ionization parameter. We measure the gas phase metallicity in the galaxy taking
into account the strong variation of the ionization parameter, finding that the
external parts of the galaxy have abundances as low as 12 + log(O/H)~8.3, while
the central star forming knots are highly enriched with super solar
metallicity. We find no sign of AGN ionization in the galaxy, despite the
recent claim of the presence of a super massive active Black Hole in the core
of He~2-10. We therefore reanalyze the X-ray data that were used to propose the
presence of the AGN, but we conclude that the observed X-ray emission can be
better explained with sources of a different nature, such as a Supernova
Remnant.
",physics
"  Process mining allows analysts to exploit logs of historical executions of
business processes to extract insights regarding the actual performance of
these processes. One of the most widely studied process mining operations is
automated process discovery. An automated process discovery method takes as
input an event log, and produces as output a business process model that
captures the control-flow relations between tasks that are observed in or
implied by the event log. Various automated process discovery methods have been
proposed in the past two decades, striking different tradeoffs between
scalability, accuracy and complexity of the resulting models. However, these
methods have been evaluated in an ad-hoc manner, employing different datasets,
experimental setups, evaluation measures and baselines, often leading to
incomparable conclusions and sometimes unreproducible results due to the use of
closed datasets. This article provides a systematic review and comparative
evaluation of automated process discovery methods, using an open-source
benchmark and covering twelve publicly-available real-life event logs, twelve
proprietary real-life event logs, and nine quality metrics. The results
highlight gaps and unexplored tradeoffs in the field, including the lack of
scalability of some methods and a strong divergence in their performance with
respect to the different quality metrics used.
",computer-science
"  Distributions of anthropogenic signatures (impacts and activities) are
mathematically analysed. The aim is to understand the Anthropocene and to see
whether anthropogenic signatures could be used to determine its beginning. A
total of 23 signatures were analysed and results are presented in 31 diagrams.
Some of these signatures contain undistinguishable natural components but most
of them are of purely anthropogenic origin. Great care was taken to identify
abrupt accelerations, which could be used to determine the beginning of the
Anthropocene. Results of the analysis can be summarised in three conclusions.
1. Anthropogenic signatures cannot be used to determine the beginning of the
Anthropocene. 2. There was no abrupt Great Acceleration around 1950 or around
any other time. 3. Anthropogenic signatures are characterised by the Great
Deceleration in the second half of the 20th century. The second half of the
20th century does not mark the beginning of the Anthropocene but most likely
the beginning of the end of the strong anthropogenic impacts, maybe even the
beginning of a transition to a sustainable future. The Anthropocene is a unique
stage in human experience but it has no clearly marked beginning and it is
probably not a new geological epoch.
",quantitative-biology
"  When performing a time series analysis of continuous data, for example from
climate or environmental problems, the assumption that the process is Gaussian
is often violated. Therefore, we introduce two non-Gaussian autoregressive time
series models that are able to fit skewed and heavy-tailed time series data.
Our two models are based on the Tukey g-and-h transformation. We discuss
parameter estimation, order selection, and forecasting procedures for our
models and examine their performances in a simulation study. We demonstrate the
usefulness of our models by applying them to two sets of wind speed data.
",statistics
"  We report all phases and corresponding critical lines of the quantum
anisotropic transverse XY model with Dzyaloshinskii-Moriya (DM) interaction
along with uniform and alternating transverse magnetic fields (ATXY) by using
appropriately chosen order parameters. We prove that when DM interaction is
weaker than the anisotropy parameter, it has no effect at all on the
zero-temperature states of the XY model with uniform transverse magnetic field
which is not the case for the ATXY model. However, when DM interaction is
stronger than the anisotropy parameter, we show appearance of a new gapless
phase - a chiral phase - in the XY model with uniform as well as alternating
field. We further report that first derivatives of nearest neighbor two-site
entanglement with respect to magnetic fields can detect all the critical lines
present in the system. We also observe that the factorization surface at
zero-temperature present in this model without DM interaction becomes a volume
on the introduction of the later. We find that DM interaction can generate
bipartite entanglement sustainable at large times, leading to a proof of
ergodic nature of bipartite entanglement in this system, and can induce a
transition from non-monotonicity of entanglement with temperature to a
monotonic one.
",physics
"  Segmental duplications (SDs), or low-copy repeats (LCR), are segments of DNA
greater than 1 Kbp with high sequence identity that are copied to other regions
of the genome. SDs are among the most important sources of evolution, a common
cause of genomic structural variation, and several are associated with diseases
of genomic origin. Despite their functional importance, SDs present one of the
major hurdles for de novo genome assembly due to the ambiguity they cause in
building and traversing both state-of-the-art overlap-layout-consensus and de
Bruijn graphs. This causes SD regions to be misassembled, collapsed into a
unique representation, or completely missing from assembled reference genomes
for various organisms. In turn, this missing or incorrect information limits
our ability to fully understand the evolution and the architecture of the
genomes. Despite the essential need to accurately characterize SDs in
assemblies, there is only one tool that has been developed for this purpose,
called Whole Genome Assembly Comparison (WGAC). WGAC is comprised of several
steps that employ different tools and custom scripts, which makes it difficult
and time consuming to use. Thus there is still a need for algorithms to
characterize within-assembly SDs quickly, accurately, and in a user friendly
manner.
Here we introduce a SEgmental Duplication Evaluation Framework (SEDEF) to
rapidly detect SDs through sophisticated filtering strategies based on Jaccard
similarity and local chaining. We show that SEDEF accurately detects SDs while
maintaining substantial speed up over WGAC that translates into practical run
times of minutes instead of weeks. Notably, our algorithm captures up to 25%
pairwise error between segments, where previous studies focused on only 10%,
allowing us to more deeply track the evolutionary history of the genome.
SEDEF is available at this https URL
",quantitative-biology
"  Data shaping is a coding technique that has been proposed to increase the
lifetime of flash memory devices. Several data shaping codes have been
described in recent work, including endurance codes and direct shaping codes
for structured data. In this paper, we study information-theoretic properties
of a general class of data shaping codes and prove a separation theorem stating
that optimal data shaping can be achieved by the concatenation of optimal
lossless compression with optimal endurance coding. We also determine the
expansion factor that minimizes the total wear cost. Finally, we analyze the
performance of direct shaping codes and establish a condition for their
optimality.
",computer-science
"  Non-stationary domains, where unforeseen changes happen, present a challenge
for agents to find an optimal policy for a sequential decision making problem.
This work investigates a solution to this problem that combines Markov Decision
Processes (MDP) and Reinforcement Learning (RL) with Answer Set Programming
(ASP) in a method we call ASP(RL). In this method, Answer Set Programming is
used to find the possible trajectories of an MDP, from where Reinforcement
Learning is applied to learn the optimal policy of the problem. Results show
that ASP(RL) is capable of efficiently finding the optimal solution of an MDP
representing non-stationary domains.
",computer-science
"  Three dimensional galaxy clustering measurements provide a wealth of
cosmological information. However, obtaining spectra of galaxies is expensive,
and surveys often only measure redshifts for a subsample of a target galaxy
population. Provided that the spectroscopic data is representative, we argue
that angular pair upweighting should be used in these situations to improve the
3D clustering measurements. We present a toy model showing mathematically how
such a weighting can improve measurements, and provide a practical example of
its application using mocks created for the Baryon Oscillation Spectroscopic
Survey (BOSS). Our analysis of mocks suggests that, if an angular clustering
measurement is available over twice the area covered spectroscopically,
weighting gives a $\sim$10-20% reduction of the variance of the monopole
correlation function on the BAO scale.
",physics
"  Intrinsic stochasticity can induce highly non-trivial effects on dynamical
systems, including stochastic and coherence resonance, noise induced
bistability, noise-induced oscillations, to name but a few. In this paper we
revisit a mechanism first investigated in the context of neuroscience by which
relatively small demographic (intrinsic) fluctuations can lead to the emergence
of avalanching behavior in systems that are deterministically characterized by
a single stable fixed point (up state). The anomalously large response of such
systems to stochasticity stems (or is strongly associated with) the existence
of a ""non-normal"" stability matrix at the deterministic fixed point, which may
induce the system to be ""reactive"". Here, we further investigate this mechanism
by exploring the interplay between non-normality and intrinsic (demographic)
stochasticity, by employing a number of analytical and computational
approaches. We establish, in particular, that the resulting dynamics in this
type of systems cannot be simply derived from a scalar potential but,
additionally, one needs to consider a curl flux which describes the essential
non-equilibrium nature of this type of noisy non-normal systems. Moreover, we
shed further light on the origin of the phenomenon, introduce the novel concept
of ""non-linear reactivity"", and rationalize of the observed the value of the
emerging avalanche exponents.
",quantitative-biology
"  Consider an infinite chain of masses, each connected to its nearest neighbors
by a (nonlinear) spring. This is a Fermi-Pasta-Ulam-Tsingou lattice. We prove
the existence of traveling waves in the setting where the masses alternate in
size. In particular we address the limit where the mass ratio tends to zero.
The problem is inherently singular and we find that the traveling waves are not
true solitary waves but rather ""nanopterons"", which is to say, waves which
asymptotic at spatial infinity to very small amplitude periodic waves.
Moreover, we can only find solutions when the mass ratio lies in a certain open
set. The difficulties in the problem all revolve around understanding Jost
solutions of a nonlocal Schrödinger operator in its semi-classical limit.
",mathematics
"  To investigate whether training load monitoring data could be used to predict
injuries in elite Australian football players, data were collected from elite
athletes over 3 seasons at an Australian football club. Loads were quantified
using GPS devices, accelerometers and player perceived exertion ratings.
Absolute and relative training load metrics were calculated for each player
each day (rolling average, exponentially weighted moving average, acute:chronic
workload ratio, monotony and strain). Injury prediction models (regularised
logistic regression, generalised estimating equations, random forests and
support vector machines) were built for non-contact, non-contact time-loss and
hamstring specific injuries using the first two seasons of data. Injury
predictions were generated for the third season and evaluated using the area
under the receiver operator characteristic (AUC). Predictive performance was
only marginally better than chance for models of non-contact and non-contact
time-loss injuries (AUC$<$0.65). The best performing model was a multivariate
logistic regression for hamstring injuries (best AUC=0.76). Learning curves
suggested logistic regression was underfitting the load-injury relationship and
that using a more complex model or increasing the amount of model building data
may lead to future improvements. Injury prediction models built using training
load data from a single club showed poor ability to predict injuries when
tested on previously unseen data, suggesting they are limited as a daily
decision tool for practitioners. Focusing the modelling approach on specific
injury types and increasing the amount of training data may lead to the
development of improved predictive models for injury prevention.
",statistics
"  Feed-forward convolutional neural networks (CNNs) are currently
state-of-the-art for object classification tasks such as ImageNet. Further,
they are quantitatively accurate models of temporally-averaged responses of
neurons in the primate brain's visual system. However, biological visual
systems have two ubiquitous architectural features not shared with typical
CNNs: local recurrence within cortical areas, and long-range feedback from
downstream areas to upstream areas. Here we explored the role of recurrence in
improving classification performance. We found that standard forms of
recurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the
ImageNet task. In contrast, novel cells that incorporated two structural
features, bypassing and gating, were able to boost task accuracy substantially.
We extended these design principles in an automated search over thousands of
model architectures, which identified novel local recurrent cells and
long-range feedback connections useful for object recognition. Moreover, these
task-optimized ConvRNNs matched the dynamics of neural activity in the primate
visual system better than feedforward networks, suggesting a role for the
brain's recurrent connections in performing difficult visual behaviors.
",quantitative-biology
"  We examine dense self-gravitating stellar systems dominated by a central
potential, such as nuclear star clusters hosting a central supermassive black
hole. Different dynamical properties of these systems evolve on vastly
different timescales. In particular, the orbital-plane orientations are
typically driven into internal thermodynamic equilibrium by vector resonant
relaxation before the orbital eccentricities or semimajor axes relax. We show
that the statistical mechanics of such systems exhibit a striking resemblance
to liquid crystals, with analogous ordered-nematic and disordered-isotropic
phases. The ordered phase consists of bodies orbiting in a disk in both
directions, with the disk thickness depending on temperature, while the
disordered phase corresponds to a nearly isotropic distribution of the orbit
normals. We show that below a critical value of the total angular momentum, the
system undergoes a first-order phase transition between the ordered and
disordered phases. At the critical point the phase transition becomes
second-order while for higher angular momenta there is a smooth crossover. We
also find metastable equilibria containing two identical disks with mutual
inclinations between $90^{\circ}$ and $180^\circ$.
",physics
"  We study Swendsen--Wang dynamics for the critical $q$-state Potts model on
the square lattice. For $q=2,3,4$, where the phase transition is continuous,
the mixing time $t_{\textrm{mix}}$ is expected to obey a universal power-law
independent of the boundary conditions. On the other hand, for large $q$, where
the phase transition is discontinuous, the authors recently showed that
$t_{\textrm{mix}}$ is highly sensitive to boundary conditions:
$t_{\textrm{mix}} \geq \exp(cn)$ on an $n\times n$ box with periodic boundary,
yet under free or monochromatic boundary conditions, $t_{\textrm{mix}}
\leq\exp(n^{o(1)})$.
In this work we classify this effect under boundary conditions that
interpolate between these two (torus vs. free/monochromatic). Specifically, if
one of the $q$ colors is red, mixed boundary conditions such as
red-free-red-free on the 4 sides of the box induce $t_{\textrm{mix}} \geq
\exp(cn)$, yet Dobrushin boundary conditions such as red-red-free-free, as well
as red-periodic-red-periodic, induce sub-exponential mixing.
",mathematics
"  We present a co-segmentation technique for space-time co-located image
collections. These prevalent collections capture various dynamic events,
usually by multiple photographers, and may contain multiple co-occurring
objects which are not necessarily part of the intended foreground object,
resulting in ambiguities for traditional co-segmentation techniques. Thus, to
disambiguate what the common foreground object is, we introduce a
weakly-supervised technique, where we assume only a small seed, given in the
form of a single segmented image. We take a distributed approach, where local
belief models are propagated and reinforced with similar images. Our technique
progressively expands the foreground and background belief models across the
entire collection. The technique exploits the power of the entire set of image
without building a global model, and thus successfully overcomes large
variability in appearance of the common foreground object. We demonstrate that
our method outperforms previous co-segmentation techniques on challenging
space-time co-located collections, including dense benchmark datasets which
were adapted for our novel problem setting.
",computer-science
"  Let $ \mathbb{A}$ be a cellular algebra over a field $\mathbb{F}$ with a
decomposition of the identity $ 1_{\mathbb{A}} $ into orthogonal idempotents $
e_i$, $i \in I$ (for some finite set $I$) satisfying some properties. We
describe the entire Loewy structure of cell modules of the algebra $ \mathbb{A}
$ by using the representation theory of the algebra $ e_i \mathbb{A} e_i $ for
each $ i $. Moreover, we also study the block theory of $\mathbb{A}$ by using
this decomposition.
",mathematics
"  The recent detection of two faint and extended star clusters in the central
regions of two Local Group dwarf galaxies, Eridanus II and Andromeda XXV,
raises the question of whether clusters with such low densities can survive the
tidal field of cold dark matter haloes with central density cusps. Using both
analytic arguments and a suite of collisionless N-body simulations, I show that
these clusters are extremely fragile and quickly disrupted in the presence of
central cusps $\rho\sim r^{-\alpha}$ with $\alpha\gtrsim 0.2$. Furthermore, the
scenario in which the clusters where originally more massive and sank to the
center of the halo requires extreme fine tuning and does not naturally
reproduce the observed systems. In turn, these clusters are long lived in cored
haloes, whose central regions are safe shelters for $\alpha\lesssim 0.2$. The
only viable scenario for hosts that have preserved their primoridal cusp to the
present time is that the clusters formed at rest at the bottom of the
potential, which is easily tested by measurement of the clusters proper
velocity within the host. This offers means to readily probe the central
density profile of two dwarf galaxies as faint as $L_V\sim5\times 10^5 L_\odot$
and $L_V\sim6\times10^4 L_\odot$, in which stellar feedback is unlikely to be
effective.
",physics
"  Smart solar inverters can be used to store, monitor and manage a home's solar
energy. We describe a smart solar inverter system with battery which can either
operate in an automatic mode or receive commands over a network to charge and
discharge at a given rate. In order to make battery storage financially viable
and advantageous to the consumers, effective battery scheduling algorithms can
be employed. Particularly, when time-of-use tariffs are in effect in the region
of the inverter, it is possible in some cases to schedule the battery to save
money for the individual customer, compared to the ""automatic"" mode. Hence,
this paper presents and evaluates the performance of a novel battery scheduling
algorithm for residential consumers of solar energy. The proposed battery
scheduling algorithm optimizes the cost of electricity over next 24 hours for
residential consumers. The cost minimization is realized by controlling the
charging/discharging of battery storage system based on the predictions for
load and solar power generation values. The scheduling problem is formulated as
a linear programming problem. We performed computer simulations over 83
inverters using several months of hourly load and PV data. The simulation
results indicate that key factors affecting the viability of optimization are
the tariffs and the PV to Load ratio at each inverter. Depending on the tariff,
savings of between 1% and 10% can be expected over the automatic approach. The
prediction approach used in this paper is also shown to out-perform basic
""persistence"" forecasting approaches. We have also examined the approaches for
improving the prediction accuracy and optimization effectiveness.
",computer-science
"  Given a straight-line drawing $\Gamma$ of a graph $G=(V,E)$, for every vertex
$v$ the ply disk $D_v$ is defined as a disk centered at $v$ where the radius of
the disk is half the length of the longest edge incident to $v$. The ply number
of a given drawing is defined as the maximum number of overlapping disks at
some point in $\mathbb{R}^2$. Here we present a tool to explore and evaluate
the ply number for graphs with instant visual feedback for the user. We
evaluate our methods in comparison to an existing ply computation by De Luca et
al. [WALCOM'17]. We are able to reduce the computation time from seconds to
milliseconds for given drawings and thereby contribute to further research on
the ply topic by providing an efficient tool to examine graphs extensively by
user interaction as well as some automatic features to reduce the ply number.
",computer-science
"  Standard model-free deep reinforcement learning (RL) algorithms sample a new
initial state for each trial, allowing them to optimize policies that can
perform well even in highly stochastic environments. However, problems that
exhibit considerable initial state variation typically produce high-variance
gradient estimates for model-free RL, making direct policy or value function
optimization challenging. In this paper, we develop a novel algorithm that
instead partitions the initial state space into ""slices"", and optimizes an
ensemble of policies, each on a different slice. The ensemble is gradually
unified into a single policy that can succeed on the whole state space. This
approach, which we term divide-and-conquer RL, is able to solve complex tasks
where conventional deep RL methods are ineffective. Our results show that
divide-and-conquer RL greatly outperforms conventional policy gradient methods
on challenging grasping, manipulation, and locomotion tasks, and exceeds the
performance of a variety of prior methods. Videos of policies learned by our
algorithm can be viewed at this http URL
",computer-science
"  We consider an extension of the contextual bandit setting, motivated by
several practical applications, where an unlabeled history of contexts can
become available for pre-training before the online decision-making begins. We
propose an approach for improving the performance of contextual bandit in such
setting, via adaptive, dynamic representation learning, which combines offline
pre-training on unlabeled history of contexts with online selection and
modification of embedding functions. Our experiments on a variety of datasets
and in different nonstationary environments demonstrate clear advantages of our
approach over the standard contextual bandit.
",statistics
"  Let $L/K$ be a finite Galois extension of number fields with Galois group
$G$. Let $p$ be an odd prime and $r>1$ be an integer. Assuming a conjecture of
Schneider, we formulate a conjecture that relates special values of equivariant
Artin $L$-series at $s=r$ to the compact support cohomology of the étale
$p$-adic sheaf $\mathbb Z_p(r)$. We show that our conjecture is essentially
equivalent to the $p$-part of the equivariant Tamagawa number conjecture for
the pair $(h^0(\mathrm{Spec}(L))(r), \mathbb Z[G])$. We derive from this
explicit constraints on the Galois module structure of Banaszak's $p$-adic wild
kernels.
",mathematics
"  In the spirit of recent work of Lamm, Malchiodi and Micallef in the setting
of harmonic maps, we identify Yang-Mills connections obtained by approximations
with respect to the Yang-Mills {\alpha}-energy. More specifically, we show that
for the SU(2) Hopf fibration over the four sphere, for sufficiently small
{\alpha} values the SO(4) invariant ADHM instanton is the unique
{\alpha}-critical point which has Yang-Mills {\alpha}-energy lower than a
specific threshold.
",mathematics
"  The literature on Inverse Reinforcement Learning (IRL) typically assumes that
humans take actions in order to minimize the expected value of a cost function,
i.e., that humans are risk neutral. Yet, in practice, humans are often far from
being risk neutral. To fill this gap, the objective of this paper is to devise
a framework for risk-sensitive IRL in order to explicitly account for a human's
risk sensitivity. To this end, we propose a flexible class of models based on
coherent risk measures, which allow us to capture an entire spectrum of risk
preferences from risk-neutral to worst-case. We propose efficient
non-parametric algorithms based on linear programming and semi-parametric
algorithms based on maximum likelihood for inferring a human's underlying risk
measure and cost function for a rich class of static and dynamic
decision-making settings. The resulting approach is demonstrated on a simulated
driving game with ten human participants. Our method is able to infer and mimic
a wide range of qualitatively different driving styles from highly risk-averse
to risk-neutral in a data-efficient manner. Moreover, comparisons of the
Risk-Sensitive (RS) IRL approach with a risk-neutral model show that the RS-IRL
framework more accurately captures observed participant behavior both
qualitatively and quantitatively, especially in scenarios where catastrophic
outcomes such as collisions can occur.
",computer-science
"  Bilayer van der Waals (vdW) heterostructures such as MoS2/WS2 and MoSe2/WSe2
have attracted much attention recently, particularly because of their type II
band alignments and the formation of interlayer exciton as the lowest-energy
excitonic state. In this work, we calculate the electronic and optical
properties of such heterostructures with the first-principles GW+Bethe-Salpeter
Equation (BSE) method and reveal the important role of interlayer coupling in
deciding the excited-state properties, including the band alignment and
excitonic properties. Our calculation shows that due to the interlayer
coupling, the low energy excitons can be widely tunable by a vertical gate
field. In particular, the dipole oscillator strength and radiative lifetime of
the lowest energy exciton in these bilayer heterostructures is varied by over
an order of magnitude within a practical external gate field. We also build a
simple model that captures the essential physics behind this tunability and
allows the extension of the ab initio results to a large range of electric
fields. Our work clarifies the physical picture of interlayer excitons in
bilayer vdW heterostructures and predicts a wide range of gate-tunable
excited-state properties of 2D optoelectronic devices.
",physics
"  In the present paper we use the theory of exact completions to study
categorical properties of small setoids in Martin-Löf type theory and, more
generally, of models of the Constructive Elementary Theory of the Category of
Sets, in terms of properties of their subcategories of choice objects (i.e.
objects satisfying the axiom of choice). Because of these intended
applications, we deal with categories that lack equalisers and just have weak
ones, but whose objects can be regarded as collections of global elements. In
this context, we study the internal logic of the categories involved, and
employ this analysis to give a sufficient condition for the local cartesian
closure of an exact completion. Finally, we apply these results to show when an
exact completion produces a model of CETCS.
",mathematics
"  This research was to design a 2.4 GHz class E Power Amplifier (PA) for health
care, with 0.18um Semiconductor Manufacturing International Corporation CMOS
technology by using Cadence software. And also RF switch was designed at
cadence software with power Jazz 180nm SOI process. The ultimate goal for such
application is to reach high performance and low cost, and between high
performance and low power consumption design. This paper introduces the design
of a 2.4GHz class E power amplifier and RF switch design. PA consists of
cascade stage with negative capacitance. This power amplifier can transmit
16dBm output power to a 50{\Omega} load. The performance of the power amplifier
and switch meet the specification requirements of the desired.
",computer-science
"  We report on the observation of magnon thermal conductivity $\kappa_m\sim$ 70
W/mK near 5 K in the helimagnetic insulator Cu$_2$OSeO$_3$, exceeding that
measured in any other ferromagnet by almost two orders of magnitude. Ballistic,
boundary-limited transport for both magnons and phonons is established below 1
K, and Poiseuille flow of magnons is proposed to explain a magnon mean-free
path substantially exceeding the specimen width for the least defective
specimens in the range 2 K $<T<$ 10 K. These observations establish
Cu$_2$OSeO$_3$ as a model system for studying long-wavelength magnon dynamics.
",physics
"  In this paper, we present a new algorithm for parallel Monte Carlo tree
search (MCTS). It is based on the pipeline pattern and allows flexible
management of the control flow of the operations in parallel MCTS. The pipeline
pattern provides for the first structured parallel programming approach to
MCTS. Moreover, we propose a new lock-free tree data structure for parallel
MCTS which removes synchronization overhead. The Pipeline Pattern for Parallel
MCTS algorithm (called 3PMCTS), scales very well to higher numbers of cores
when compared to the existing methods.
",computer-science
"  Our ability to model the shapes and strengths of iron lines in the solar
spectrum is a critical test of the accuracy of the solar iron abundance, which
sets the absolute zero-point of all stellar metallicities. We use an extensive
463-level Fe atom with new photoionisation cross-sections for FeI as well as
quantum mechanical calculations of collisional excitation and charge transfer
with neutral hydrogen; the latter effectively remove a free parameter that has
hampered all previous line formation studies of Fe in non-local thermodynamic
equilibrium (NLTE). For the first time, we use realistic 3D NLTE calculations
of Fe for a quantitative comparison to solar observations. We confront our
theoretical line profiles with observations taken at different viewing angles
across the solar disk with the Swedish 1-m Solar Telescope. We find that 3D
modelling well reproduces the observed centre-to-limb behaviour of spectral
lines overall, but highlight aspects that may require further work, especially
cross-sections for inelastic collisions with electrons. Our inferred solar iron
abundance is log(eps(Fe))=7.48+-0.04.
",physics
"  We study the asymptotic behavior of the homotopy groups of simply connected
finite $p$-local complexes, and define a space to be locally hyperbolic if its
homotopy groups have exponential growth. Under some certain conditions related
to the functorial decomposition of loop suspension, we prove that the suspended
finite complexes are locally hyperbolic if suitable but accessible information
of the homotopy groups is known. In particular, we prove that Moore spaces are
locally hyperbolic, and other candidates are also given.
",mathematics
"  We study the changes of opinions about vaccination together with the
evolution of a disease. In our model we consider a multiplex network consisting
of two layers. One of the layers corresponds to a social network where people
share their opinions and influence others opinions. The social model that rules
the dynamic is the M-model, which takes into account two different processes
that occurs in a society: persuasion and compromise. This two processes are
related through a parameter $r$, $r<1$ describes a moderate and committed
society, for $r>1$ the society tends to have extremist opinions, while $r=1$
represents a neutral society. This social network may be of real or virtual
contacts. On the other hand, the second layer corresponds to a network of
physical contacts where the disease spreading is described by the SIR-Model. In
this model the individuals may be in one of the following four states:
Susceptible ($S$), Infected($I$), Recovered ($R$) or Vaccinated ($V$). A
Susceptible individual can: i) get vaccinated, if his opinion in the other
layer is totally in favor of the vaccine, ii) get infected, with probability
$\beta$ if he is in contact with an infected neighbor. Those $I$ individuals
recover after a certain period $t_r=6$. Vaccinated individuals have an
extremist positive opinion that does not change. We consider that the vaccine
has a certain effectiveness $\omega$ and as a consequence vaccinated nodes can
be infected with probability $\beta (1 - \omega)$ if they are in contact with
an infected neighbor. In this case, if the infection process is successful, the
new infected individual changes his opinion from extremist positive to totally
against the vaccine. We find that depending on the trend in the opinion of the
society, which depends on $r$, different behaviors in the spread of the
epidemic occurs. An epidemic threshold was found.
",physics
"  A remarkable discovery of NASA's Kepler mission is the wide diversity in the
average densities of planets of similar mass. After gas disk dissipation, fully
formed planets could interact with nearby planetesimals from a remnant
planetesimal disk. These interactions would often lead to planetesimal
accretion due to the relatively high ratio between the planet size and the hill
radius for typical planets. We present calculations using the open-source
stellar evolution toolkit MESA (Modules for Experiments in Stellar
Astrophysics) modified to include the deposition of planetesimals into the H/He
envelopes of sub-Neptunes (~1-20 MEarth). We show that planetesimal accretion
can alter the mass-radius isochrones for these planets. The same initial planet
as a result of the same total accreted planetesimal mass can have up to ~5%
difference in mean densities several Gyr after the last accretion due to
inherent stochasticity of the accretion process. During the phase of rapid
accretion these differences are more dramatic. The additional energy deposition
from the accreted planetesimals increase the ratio between the planet's radius
to that of the core during rapid accretion, which in turn leads to enhanced
loss of atmospheric mass. As a result, the same initial planet can end up with
very different envelope mass fractions. These differences manifest as
differences in mean densities long after accretion stops. These effects are
particularly important for planets initially less massive than ~10 MEarth and
with envelope mass fraction less than ~10%, thought to be the most common type
of planets discovered by Kepler.
",physics
"  This paper characterizes the capacity region of Gaussian MIMO broadcast
channels (BCs) with per-antenna power constraint (PAPC). While the capacity
region of MIMO BCs with a sum power constraint (SPC) was extensively studied,
that under PAPC has received less attention. A reason is that efficient
solutions for this problem are hard to find. The goal of this paper is to
devise an efficient algorithm for determining the capacity region of Gaussian
MIMO BCs subject to PAPC, which is scalable to the problem size. To this end,
we first transform the weighted sum capacity maximization problem, which is
inherently nonconvex with the input covariance matrices, into a convex
formulation in the dual multiple access channel by minimax duality. Then we
derive a computationally efficient algorithm combining the concept of
alternating optimization and successive convex approximation. The proposed
algorithm achieves much lower complexity compared to an existing interiorpoint
method. Moreover, numerical results demonstrate that the proposed algorithm
converges very fast under various scenarios.
",computer-science
"  From [Problem 1729, Groups of prime power order, Vol. 3], Berkovich et al.
asked to obtain the Schur multiplier and the representation of a group $G$,
when $G$ is a special $p$-group minimally generated by $d$ elements and
$|G'|=p^{\frac{1}{2}d(d-1)}$. Since there are analogies between groups and Lie
algebras, we intend to give an answer to this question similarly for nilpotent
Lie algebras. Furthermore, we give some results about the tensor square and the
Schur multiplier of some nilpotent Lie algebras of class two.
",mathematics
"  This paper presents a laser amplifier based on an antireflection coated laser
diode. This laser amplifier operates without active temperature stabilisation
at any wavelength within its gain profile without restrictions on the injection
current. Using a active feedback from an external detector to the laser current
the power stabilized to better than $10^{-4}$, even after additional optical
elements such as an optical fiber and/or a polarization cleaner. This power can
also be modulated and tuned arbitrarily. In the absence of the seeding light,
the laser amplifier does not lase, thus resulting in an extremely simple setup,
which requires neither an external Fabry Perot cavity for monitoring the mode
purity nor a temperature stabilization.
",physics
"  We study continuum Schrödinger operators on the real line whose potentials
are comprised of two compactly supported square-integrable functions
concatenated according to an element of the Fibonacci substitution subshift
over two letters. We show that the Hausdorff dimension of the spectrum tends to
one in the small-coupling and high-energy regimes, regardless of the shape of
the potential pieces.
",mathematics
"  Short electron pulses are demonstrated to trigger and control magnetic
excitations, even at low electron current densities.
We show that the tangential magnetic field surrounding a picosecond electron
pulse can imprint topologically protected magnetic textures such as skyrmions
in a sample with a residual Dzyaloshinskii-Moriya spin-orbital coupling.
Characteristics of the created excitations such as the topological charge can
be steered via the duration and the strength of the electron pulses. The study
points to a possible way for a spatio-temporally controlled generation of
skyrmionic excitations.
",physics
"  Joint analysis of multiple phenotypes can increase statistical power in
genetic association studies. Principal component analysis, as a popular
dimension reduction method, especially when the number of phenotypes is
high-dimensional, has been proposed to analyze multiple correlated phenotypes.
It has been empirically observed that the first PC, which summarizes the
largest amount of variance, can be less powerful than higher order PCs and
other commonly used methods in detecting genetic association signals. In this
paper, we investigate the properties of PCA-based multiple phenotype analysis
from a geometric perspective by introducing a novel concept called principal
angle. A particular PC is powerful if its principal angle is $0^o$ and is
powerless if its principal angle is $90^o$. Without prior knowledge about the
true principal angle, each PC can be powerless. We propose linear, non-linear
and data-adaptive omnibus tests by combining PCs. We show that the omnibus PC
test is robust and powerful in a wide range of scenarios. We study the
properties of the proposed methods using power analysis and eigen-analysis. The
subtle differences and close connections between these combined PC methods are
illustrated graphically in terms of their rejection boundaries. Our proposed
tests have convex acceptance regions and hence are admissible. The $p$-values
for the proposed tests can be efficiently calculated analytically and the
proposed tests have been implemented in a publicly available R package {\it
MPAT}. We conduct simulation studies in both low and high dimensional settings
with various signal vectors and correlation structures. We apply the proposed
tests to the joint analysis of metabolic syndrome related phenotypes with data
sets collected from four international consortia to demonstrate the
effectiveness of the proposed combined PC testing procedures.
",statistics
"  Game Theory (GT) has been used with significant success to formulate, and
either design or optimize, the operation of many representative communications
and networking scenarios. The games in these scenarios involve, as usual,
diverse players with conflicting goals. This paper primarily surveys the
literature that has applied theoretical games to wireless networks, emphasizing
use cases of upcoming Multi-Access Edge Computing (MEC). MEC is relatively new
and offers cloud services at the network periphery, aiming to reduce service
latency backhaul load, and enhance relevant operational aspects such as Quality
of Experience or security. Our presentation of GT is focused on the major
challenges imposed by MEC services over the wireless resources. The survey is
divided into classical and evolutionary games. Then, our discussion proceeds to
more specific aspects which have a considerable impact on the game usefulness,
namely: rational vs. evolving strategies, cooperation among players, available
game information, the way the game is played (single turn, repeated), the game
model evaluation, and how the model results can be applied for both optimizing
resource-constrained resources and balancing diverse trade-offs in real edge
networking scenarios. Finally, we reflect on lessons learned, highlighting
future trends and research directions for applying theoretical model games in
upcoming MEC services, considering both network design issues and usage
scenarios.
",computer-science
"  We study interfacial magnetocrystalline anisotropies in various
Fe/semiconductor heterostructures by means of first-principles calculations. We
find that many of those systems show perpendicular magnetic anisotropy (PMA)
with a positive value of the interfacial anisotropy constant $K_{\rm i}$. In
particular, the Fe/CuInSe$_2$ interface has a large $K_{\rm i}$ of $\sim
2.3\,{\rm mJ/m^2}$, which is about 1.6 times larger than that of Fe/MgO known
as a typical system with relatively large PMA. We also find that the values of
$K_{\rm i}$ in almost all the systems studied in this work follow the
well-known Bruno's relation, which indicates that minority-spin states around
the Fermi level provide dominant contributions to the interfacial
magnetocrystalline anisotropies. Detailed analyses of the local density of
states and wave-vector-resolved anisotropy energy clarify that the large
$K_{\rm i}$ in Fe/CuInSe$_2$ is attributed to the preferable $3d$-orbital
configurations around the Fermi level in the minority-spin states of the
interfacial Fe atoms. Moreover, we have shown that the locations of interfacial
Se atoms are the key for such orbital configurations of the interfacial Fe
atoms.
",physics
"  Generating large quantities of quality labeled data in medical imaging is
very time consuming and expensive. The performance of supervised algorithms for
various tasks on imaging has improved drastically over the years, however the
availability of data to train these algorithms have become one of the main
bottlenecks for implementation. To address this, we propose a semi-supervised
learning method where pseudo-negative labels from unlabeled data are used to
further refine the performance of a pulmonary nodule detection network in chest
radiographs. After training with the proposed network, the false positive rate
was reduced to 0.1266 from 0.4864 while maintaining sensitivity at 0.89.
",statistics
"  Consider an undirected mixed membership network with $n$ nodes and $K$
communities. For each node $1 \leq i \leq n$, we model the membership by
$\pi_{i} = (\pi_{i}(1), \pi_{i}(2), \ldots$, $\pi_{i}(K))'$, where $\pi_{i}(k)$
is the probability that node $i$ belongs to community $k$, $1 \leq k \leq K$.
We call node $i$ ""pure"" if $\pi_i$ is degenerate and ""mixed"" otherwise. The
primary interest is to estimate $\pi_i$, $1 \leq i \leq n$.
We model the adjacency matrix $A$ with a Degree Corrected Mixed Membership
(DCMM) model. Let $\hat{\xi}_1, \hat{\xi}_2, \ldots, \hat{\xi}_K$ be the first
$K$ eigenvectors of $A$. We define a matrix $\hat{R} \in \mathbb{R}^{n, K-1}$
by $\hat{R}(i,k) = \hat{\xi}_{k+1}(i)/\hat{\xi}_1(i)$, $1 \leq k \leq K-1$, $1
\leq i \leq n$. The matrix can be viewed as a distorted version of its
non-stochastic counterpart $R \in \mathbb{R}^{n, K-1}$, which is unknown but
contains all information we need for the memberships.
We reveal an interesting insight: There is a simplex ${\cal S}$ in
$\mathbb{R}^{K-1}$ such that row $i$ of $R$ corresponds to a vertex of ${\cal
S}$ if node $i$ is pure, and corresponds to an interior point of ${\cal S}$
otherwise. Vertex Hunting (i.e., estimating the vertices of ${\cal S}$) is thus
the key to our problem.
The matrix $\hat{R}$ is a row-wise normalization on the matrix of
eigenvectors $\hat{\Xi}=[\hat{\xi}_1,\ldots,\hat{\xi}_K]$, first proposed by
Jin (2015). Alternatively, we may normalize $\hat{\Xi}$ by the row-wise
$\ell^q$-norms (e.g., Supplement of Jin (2015)), but it won't give rise to a
simplex so is less convenient.
We propose a new approach $\textit{Mixed-SCORE}$ to estimating the
memberships, at the heart of which is an easy-to-use Vertex Hunting algorithm.
The approach is successfully applied to $4$ network data sets. We also derive
the rate of convergence for Mixed-SCORE.
",statistics
"  We study a multi-period demand response problem in the smart grid with
multiple companies and their consumers. We model the interactions by a
Stackelberg game, where companies are the leaders and consumers are the
followers. It is shown that this game has a unique equilibrium at which the
companies set prices to maximize their revenues while the consumers respond
accordingly to maximize their utilities subject to their local constraints.
Billing minimization is achieved as an outcome of our method. Closed-form
expressions are provided for the strategies of all players. Based on these
solutions, a power allocation game has been formulated, and which is shown to
admit a unique pure-strategy Nash equilibrium, for which closed-form
expressions are provided. For privacy, we provide a distributed algorithm for
the computation of all strategies. We study the asymptotic behavior of
equilibrium strategies when the numbers of periods and consumers grow. We find
an appropriate company-to-user ratio for the large population regime.
Furthermore, it is shown, both analytically and numerically, that the
multi-period scheme, compared with the single-period one, provides more
incentives for energy consumers to participate in demand response. We have also
carried out case studies on real life data to demonstrate the benefits of our
approach, including billing savings of up to 30\%.
",computer-science
"  In the seminal work [9], several macroscopic market observables have been
introduced, in an attempt to find characteristics capturing the diversity of a
financial market. Despite the crucial importance of such observables for
investment decisions, a concise mathematical description of their dynamics has
been missing. We fill this gap in the setting of rank-based models and expect
our ideas to extend to other models of large financial markets as well. The
results are then used to study the performance of multiplicatively and
additively functionally generated portfolios, in particular, over short-term
and medium-term horizons.
",quantitative-finance
"  We investigate the properties of entanglement in one-dimensional fermionic
lattices at the Fulde-Ferrell-Larkin-Ovchinnikov (FFLO) superfluid regime. By
analyzing occupation probabilities, which are concepts closely related to FFLO
and entanglement, we obtain approximate analytical expressions for the
spin-flip processes at the FFLO regime. We also apply density matrix
renormalization group calculations to obtain the exact ground-state
entanglement of the system in superfluid and non-superfluid regimes. Our
results reveal a breaking pairs avalanche appearing precisely at the
FFLO-normal phase transition. We find that entanglement is non-monotonic in
superfluid regimes, feature that could be used as a signature of exotic
superfluidity.
",physics
"  There has been great interest in realizing quantum simulators of charged
particles in artificial gauge fields. Here, we perform the first quantum
simulation explorations of the combination of artificial gauge fields and
disorder. Using synthetic lattice techniques based on parametrically-coupled
atomic momentum states, we engineer zigzag chains with a tunable homogeneous
flux. The breaking of time-reversal symmetry by the applied flux leads to
analogs of spin-orbit coupling and spin-momentum locking, which we observe
directly through the chiral dynamics of atoms initialized to single lattice
sites. We additionally introduce precisely controlled disorder in the site
energy landscape, allowing us to explore the interplay of disorder and large
effective magnetic fields. The combination of correlated disorder and
controlled intra- and inter-row tunneling in this system naturally supports
energy-dependent localization, relating to a single-particle mobility edge. We
measure the localization properties of the extremal eigenstates of this system,
the ground state and the most-excited state, and demonstrate clear evidence for
a flux-dependent mobility edge. These measurements constitute the first direct
evidence for energy-dependent localization in a lower-dimensional system, as
well as the first explorations of the combined influence of artificial gauge
fields and engineered disorder. Moreover, we provide direct evidence for
interaction shifts of the localization transitions for both low- and
high-energy eigenstates in correlated disorder, relating to the presence of a
many-body mobility edge. The unique combination of strong interactions,
controlled disorder, and tunable artificial gauge fields present in this
synthetic lattice system should enable myriad explorations into intriguing
correlated transport phenomena.
",physics
"  We develop a one-dimensional notion of affine processes under parameter
uncertainty, which we call non-linear affine processes. This is done as
follows: given a set of parameters for the process, we construct a
corresponding non-linear expectation on the path space of continuous processes.
By a general dynamic programming principle we link this non-linear expectation
to a variational form of the Kolmogorov equation, where the generator of a
single affine process is replaced by the supremum over all corresponding
generators of affine processes with parameters in the parameter set. This
non-linear affine process yields a tractable model for Knightian uncertainty,
especially for modelling interest rates under ambiguity.
We then develop an appropriate Ito-formula, the respective term-structure
equations and study the non-linear versions of the Vasicek and the
Cox-Ingersoll-Ross (CIR) model. Thereafter we introduce the non-linear
Vasicek-CIR model. This model is particularly suitable for modelling interest
rates when one does not want to restrict the state space a priori and hence the
approach solves this modelling issue arising with negative interest rates.
",quantitative-finance
"  The pseudo-marginal algorithm is a variant of the Metropolis-Hastings
algorithm which samples asymptotically from a probability distribution when it
is only possible to estimate unbiasedly an unnormalized version of its density.
Practically, one has to trade-off the computational resources used to obtain
this estimator against the asymptotic variances of the ergodic averages
obtained by the pseudo-marginal algorithm. Recent works optimizing this
trade-off rely on some strong assumptions which can cast doubts over their
practical relevance. In particular, they all assume that the distribution of
the additive error in the log-likelihood estimator is independent of the
parameter value at which it is evaluated. Under weak regularity conditions we
show here that, as the number of data points tends to infinity, a
space-rescaled version of the pseudo-marginal chain converges weakly towards
another pseudo-marginal chain for which this assumption indeed holds. A study
of this limiting chain allows us to provide parameter dimension-dependent
guidelines on how to optimally scale a normal random walk proposal and the
number of Monte Carlo samples for the pseudo-marginal method in the large
sample regime. This complements and validates currently available results.
",statistics
"  The Sharing Economy (SE) is a growing ecosystem focusing on peer-to-peer
enterprise. In the SE the information available to assist individuals (users)
in making decisions focuses predominantly on community generated trust and
reputation information. However, how such information impacts user judgement is
still being understood. To explore such effects, we constructed an artificial
SE accommodation platform where we varied the elements related to hosts'
digital identity, measuring users' perceptions and decisions to interact.
Across three studies, we find that trust and reputation information increases
not only the users' perceived trustworthiness, credibility, and sociability of
hosts, but also the propensity to rent a private room in their home. This
effect is seen when providing users both with complete profiles and profiles
with partial user-selected information. Closer investigations reveal that three
elements relating to the host's digital identity are sufficient to produce such
positive perceptions and increased rental decisions, regardless of which three
elements are presented. Our findings have relevant implications for human
judgment and privacy in the SE, and question its current culture of ever
increasing information-sharing.
",computer-science
"  Let $\mathbb{K}$ be the algebraic closure of a finite field $\mathbb{F}_q$ of
odd characteristic $p$. For a positive integer $m$ prime to $p$, let
$F=\mathbb{K}(x,y)$ be the transcendency degree $1$ function field defined by
$y^q+y=x^m+x^{-m}$. Let $t=x^{m(q-1)}$ and $H=\mathbb{K}(t)$. The extension
$F|H$ is a non-Galois extension. Let $K$ be the Galois closure of $F$ with
respect to $H$. By a result of Stichtenoth, $K$ has genus $g(K)=(qm-1)(q-1)$,
$p$-rank (Hasse-Witt invariant) $\gamma(K)=(q-1)^2$ and a
$\mathbb{K}$-automorphism group of order at least $2q^2m(q-1)$. In this paper
we prove that this subgroup is the full $\mathbb{K}$-automorphism group of $K$;
more precisely $Aut_{\mathbb {K}}(K)=Q\rtimes D$ where $Q$ is an elementary
abelian $p$-group of order $q^2$ and $D$ has a index $2$ cyclic subgroup of
order $m(q-1)$. In particular, $\sqrt{m}|Aut_{\mathbb{K}}(K)|> g(K)^{3/2}$, and
if $K$ is ordinary (i.e. $g(K)=\gamma(K)$) then
$|Aut_{\mathbb{K}}(K)|>g^{3/2}$. On the other hand, if $G$ is a solvable
subgroup of the $\mathbb{K}$-automorphism group of an ordinary, transcendency
degree $1$ function field $L$ of genus $g(L)\geq 2$ defined over $\mathbb{K}$,
then by a result due to Korchmáros and Montanucci, $|Aut_{\mathbb{K}}(K)|\le
34 (g(L)+1)^{3/2}<68\sqrt{2}g(L)^{3/2}$. This shows that $K$ hits this bound up
to the constant $68\sqrt{2}$.
Since $Aut_{\mathbb{K}}(K)$ has several subgroups, the fixed subfield $F^N$
of such a subgroup $N$ may happen to have many automorphisms provided that the
normalizer of $N$ in $Aut_{\mathbb{K}}(K)$ is large enough. This possibility is
worked out for subgroups of $Q$.
",mathematics
"  Internal gravity waves play a primary role in geophysical fluids: they
contribute significantly to mixing in the ocean and they redistribute energy
and momentum in the middle atmosphere. Until recently, most studies were
focused on plane wave solutions. However, these solutions are not a
satisfactory description of most geophysical manifestations of internal gravity
waves, and it is now recognized that internal wave beams with a confined
profile are ubiquitous in the geophysical context.
We will discuss the reason for the ubiquity of wave beams in stratified
fluids, related to the fact that they are solutions of the nonlinear governing
equations. We will focus more specifically on situations with a constant
buoyancy frequency. Moreover, in light of recent experimental and analytical
studies of internal gravity beams, it is timely to discuss the two main
mechanisms of instability for those beams. i) The Triadic Resonant Instability
generating two secondary wave beams. ii) The streaming instability
corresponding to the spontaneous generation of a mean flow.
",physics
"  Despite the wealth of $Planck$ results, there are difficulties in
disentangling the primordial non-Gaussianity of the Cosmic Microwave Background
(CMB) from the secondary and the foreground non-Gaussianity (NG). For each of
these forms of NG the lack of complete data introduces model-dependencies.
Aiming at detecting the NGs of the CMB temperature anisotropy $\delta T$, while
paying particular attention to a model-independent quantification of NGs, our
analysis is based upon statistical and morphological univariate descriptors,
respectively: the probability density function $P(\delta T)$, related to
${\mathrm v}_{0}$, the first Minkowski Functional (MF), and the two other MFs,
${\mathrm v}_{1}$ and ${\mathrm v}_{2}$. From their analytical Gaussian
predictions we build the discrepancy functions $\Delta_{k}$ ($k=P,0,1,2$) which
are applied to an ensemble of $10^{5}$ CMB realization maps of the $\Lambda$CDM
model and to the $Planck$ CMB maps. In our analysis we use general Hermite
expansions of the $\Delta_{k}$ up to the $12^{th}$ order, where the
coefficients are explicitly given in terms of cumulants. Assuming hierarchical
ordering of the cumulants, we obtain the perturbative expansions generalizing
the $2^{nd}$ order expansions of Matsubara to arbitrary order in the standard
deviation $\sigma_0$ for $P(\delta T)$ and ${\mathrm v}_0$, where the
perturbative expansion coefficients are explicitly given in terms of complete
Bell polynomials. The comparison of the Hermite expansions and the perturbative
expansions is performed for the $\Lambda$CDM map sample and the $Planck$ data.
We confirm the weak level of non-Gaussianity ($1$-$2$)$\sigma$ of the
foreground corrected masked $Planck$ $2015$ maps.
",physics
"  Next investigations in our program of transition from the He atom to the
complex atoms description have been presented. The method of interacting
configurations in the complex number representation is under consideration. The
spectroscopic characteristics of the Mg and Ca atoms in the problem of the
electron-impact ionization of these atoms are investigated. The energies and
the widths of the lowest autoionizing states of Mg and Ca atoms are calculated.
Few results in the photoionization problem on the autoionizing states above the
n=2 threshold of helium-like Be ion are presented.
",physics
"  Word obfuscation or substitution means replacing one word with another word
in a sentence to conceal the textual content or communication. Word obfuscation
is used in adversarial communication by terrorist or criminals for conveying
their messages without getting red-flagged by security and intelligence
agencies intercepting or scanning messages (such as emails and telephone
conversations). ConceptNet is a freely available semantic network represented
as a directed graph consisting of nodes as concepts and edges as assertions of
common sense about these concepts. We present a solution approach exploiting
vast amount of semantic knowledge in ConceptNet for addressing the technically
challenging problem of word substitution in adversarial communication. We frame
the given problem as a textual reasoning and context inference task and utilize
ConceptNet's natural-language-processing tool-kit for determining word
substitution. We use ConceptNet to compute the conceptual similarity between
any two given terms and define a Mean Average Conceptual Similarity (MACS)
metric to identify out-of-context terms. The test-bed to evaluate our proposed
approach consists of Enron email dataset (having over 600000 emails generated
by 158 employees of Enron Corporation) and Brown corpus (totaling about a
million words drawn from a wide variety of sources). We implement word
substitution techniques used by previous researches to generate a test dataset.
We conduct a series of experiments consisting of word substitution methods used
in the past to evaluate our approach. Experimental results reveal that the
proposed approach is effective.
",computer-science
"  A study of the intersection theory on the moduli space of Riemann surfaces
with boundary was recently initiated in a work of R. Pandharipande, J. P.
Solomon and the third author, where they introduced open intersection numbers
in genus 0. Their construction was later generalized to all genera by J. P.
Solomon and the third author. In this paper we consider a refinement of the
open intersection numbers by distinguishing contributions from surfaces with
different numbers of boundary components, and we calculate all these numbers.
We then construct a matrix model for the generating series of the refined open
intersection numbers and conjecture that it is equivalent to the
Kontsevich-Penner matrix model. An evidence for the conjecture is presented.
Another refinement of the open intersection numbers, which describes the
distribution of the boundary marked points on the boundary components, is also
discussed.
",mathematics
"  The architectures of debris disks encode the history of planet formation in
these systems. Studies of debris disks via their spectral energy distributions
(SEDs) have found infrared excesses arising from cold dust, warm dust, or a
combination of the two. The cold outer belts of many systems have been imaged,
facilitating their study in great detail. Far less is known about the warm
components, including the origin of the dust. The regularity of the disk
temperatures indicates an underlying structure that may be linked to the water
snow line. If the dust is generated from collisions in an exo-asteroid belt,
the dust will likely trace the location of the water snow line in the
primordial protoplanetary disk where planetesimal growth was enhanced. If
instead the warm dust arises from the inward transport from a reservoir of icy
material farther out in the system, the dust location is expected to be set by
the current snow line. We analyze the SEDs of a large sample of debris disks
with warm components. We find that warm components in single-component systems
(those without detectable cold components) follow the primordial snow line
rather than the current snow line, so they likely arise from exo-asteroid
belts. While the locations of many warm components in two-component systems are
also consistent with the primordial snow line, there is more diversity among
these systems, suggesting additional effects play a role.
",physics
"  We discuss the effect of ram pressure on the cold clouds in the centers of
cool-core galaxy clusters, and in particular, how it reduces cloud velocity and
sometimes causes an offset between the cold gas and young stars. The velocities
of the molecular gas in both observations and our simulations fall in the range
of $100-400$ km/s, much lower than expected if they fall from a few tens of kpc
ballistically. If the intra-cluster medium (ICM) is at rest, the ram pressure
of the ICM only slightly reduces the velocity of the clouds. When we assume
that the clouds are actually ""fluffier"" because they are co-moving with a
warm-hot layer, the velocity becomes smaller. If we also consider the AGN wind
in the cluster center by adding a wind profile measured from the simulation,
the clouds are further slowed down at small radii, and the resulting velocities
are in general agreement with the observations and simulations. Because ram
pressure only affects gas but not stars, it can cause a separation between a
filament and young stars that formed in the filament as they move through the
ICM together. This separation has been observed in Perseus and also exists in
our simulations. We show that the star-filament offset combined with
line-of-sight velocity measurements can help determine the true motion of the
cold gas, and thus distinguish between inflows and outflows.
",physics
"  We prove that for $1<c<4/3$ the subsequence of the Thue--Morse sequence
$\mathbf t$ indexed by $\lfloor n^c\rfloor$ defines a normal sequence, that is,
each finite sequence $(\varepsilon_0,\ldots,\varepsilon_{T-1})\in \{0,1\}^T$
occurs as a contiguous subsequence of the sequence $n\mapsto \mathbf
t\left(\lfloor n^c\rfloor\right)$ with asymptotic frequency $2^{-T}$.
",mathematics
"  Unlike the Web where each web page has a global URL to reach, a specific
""content page"" inside a mobile app cannot be opened unless the user explores
the app with several operations from the landing page. Recently, deep links
have been advocated by major companies to enable targeting and opening a
specific page of an app externally with an accessible uniform resource
identifier (URI). To empirically investigate the state of the practice on
adopting deep links, in this article, we present the largest empirical study of
deep links over 20,000 Android apps, and find that deep links do not get wide
adoption among current Android apps, and non-trivial manual efforts are
required for app developers to support deep links. To address such an issue, we
propose the Aladdin approach and supporting tool to release deep links to
access arbitrary location of existing apps. Aladdin instantiates our novel
cooperative framework to synergically combine static analysis and dynamic
analysis while minimally engaging developers to provide inputs to the framework
for automation, without requiring any coding efforts or additional deployment
efforts. We evaluate Aladdin with popular apps and demonstrate its
effectiveness and performance.
",computer-science
"  Complexity analysis becomes a common task in supervisory control. However,
many results of interest are spread across different topics. The aim of this
paper is to bring several interesting results from complexity theory and to
illustrate their relevance to supervisory control by proving new nontrivial
results concerning nonblockingness in modular supervisory control of discrete
event systems modeled by finite automata.
",computer-science
"  Slater's condition -- existence of a ""strictly feasible solution"" -- is a
common assumption in conic optimization. Without strict feasibility,
first-order optimality conditions may be meaningless, the dual problem may
yield little information about the primal, and small changes in the data may
render the problem infeasible. Hence, failure of strict feasibility can
negatively impact off-the-shelf numerical methods, such as primal-dual interior
point methods, in particular. New optimization modelling techniques and convex
relaxations for hard nonconvex problems have shown that the loss of strict
feasibility is a more pronounced phenomenon than has previously been realized.
In this text, we describe various reasons for the loss of strict feasibility,
whether due to poor modelling choices or (more interestingly) rich underlying
structure, and discuss ways to cope with it and, in many pronounced cases, how
to use it as an advantage. In large part, we emphasize the facial reduction
preprocessing technique due to its mathematical elegance, geometric
transparency, and computational potential.
",mathematics
"  Many debris discs reveal a two-component structure, with a cold outer and a
warm inner component. While the former are likely massive analogues of the
Kuiper belt, the origin of the latter is still a matter of debate. In this work
we investigate whether the warm dust may be a signature of asteroid belt
analogues. In the scenario tested here the current two-belt architecture stems
from an originally extended protoplanetary disc, in which planets have opened a
gap separating it into the outer and inner discs which, after the gas
dispersal, experience a steady-state collisional decay. This idea is explored
with an analytic collisional evolution model for a sample of 225 debris discs
from a Spitzer/IRS catalogue that are likely to possess a two-component
structure. We find that the vast majority of systems (220 out of 225, or 98%)
are compatible with this scenario. For their progenitors, original
protoplanetary discs, we find an average surface density slope of
$-0.93\pm0.06$ and an average initial mass of
$\left(3.3^{+0.4}_{-0.3}\right)\times 10^{-3}$ solar masses, both of which are
in agreement with the values inferred from submillimetre surveys. However, dust
production by short-period comets and - more rarely - inward transport from the
outer belts may be viable, and not mutually excluding, alternatives to the
asteroid belt scenario. The remaining five discs (2% of the sample: HIP 11486,
HIP 23497, HIP 57971, HIP 85790, HIP 89770) harbour inner components that
appear inconsistent with dust production in an ""asteroid belt."" Warm dust in
these systems must either be replenished from cometary sources or represent an
aftermath of a recent rare event, such as a major collision or planetary system
instability.
",physics
"  In this paper, we develop a system for the low-cost indoor localization and
tracking problem using radio signal strength indicator, Inertial Measurement
Unit (IMU), and magnetometer sensors. We develop a novel and simplified
probabilistic IMU motion model as the proposal distribution of the sequential
Monte-Carlo technique to track the robot trajectory. Our algorithm can globally
localize and track a robot with a priori unknown location, given an informative
prior map of the Bluetooth Low Energy (BLE) beacons. Also, we formulate the
problem as an optimization problem that serves as the Back-end of the algorithm
mentioned above (Front-end). Thus, by simultaneously solving for the robot
trajectory and the map of BLE beacons, we recover a continuous and smooth
trajectory of the robot, corrected locations of the BLE beacons, and the
time-varying IMU bias. The evaluations achieved using hardware show that
through the proposed closed-loop system the localization performance can be
improved; furthermore, the system becomes robust to the error in the map of
beacons by feeding back the optimized map to the Front-end.
",computer-science
"  In this paper we perform a formal asymptotic analysis on a kinetic model for
reactive mixtures in order to derive a reaction-diffusion system of
Maxwell-Stefan type. More specifically, we start from the kinetic model of
simple reacting spheres for a quaternary mixture of monatomic ideal gases that
undergoes a reversible chemical reaction of bimolecular type. Then, we consider
a scaling describing a physical situation in which mechanical collisions play a
dominant role in the evolution process, while chemical reactions are slow, and
compute explicitly the production terms associated to the concentration and
momentum balance equations for each species in the reactive mixture. Finally,
we prove that, under isothermal assumptions, the limit equations for the scaled
kinetic model is the reaction diffusion system of Maxwell-Stefan type.
",physics
"  Dietzfelbinger and Weidling [DW07] proposed a natural variation of cuckoo
hashing where each of $cn$ objects is assigned $k = 2$ intervals of size $\ell$
in a linear (or cyclic) hash table of size $n$ and both start points are chosen
independently and uniformly at random. Each object must be placed into a table
cell within its intervals, but each cell can only hold one object. Experiments
suggested that this scheme outperforms the variant with blocks in which
intervals are aligned at multiples of $\ell$. In particular, the load threshold
is higher, i.e. the load $c$ that can be achieved with high probability. For
instance, Lehman and Panigrahy [LP09] empirically observed the threshold for
$\ell = 2$ to be around $96.5\%$ as compared to roughly $89.7\%$ using blocks.
They managed to pin down the asymptotics of the thresholds for large $\ell$,
but the precise values resisted rigorous analysis.
We establish a method to determine these load thresholds for all $\ell \geq
2$, and, in fact, for general $k \geq 2$. For instance, for $k = \ell = 2$ we
get $\approx 96.4995\%$. The key tool we employ is an insightful and general
theorem due to Leconte, Lelarge, and Massoulié [LLM13], which adapts methods
from statistical physics to the world of hypergraph orientability. In effect,
the orientability thresholds for our graph families are determined by belief
propagation equations for certain graph limits. As a side note we provide
experimental evidence suggesting that placements can be constructed in linear
time with loads close to the threshold using an adapted version of an algorithm
by Khosla [Kho13].
",computer-science
"  We present numerical evidence that most two-dimensional surface states of a
bulk topological superconductor (TSC) sit at an integer quantum Hall plateau
transition. We study TSC surface states in class CI with quenched disorder.
Low-energy (finite-energy) surface states were expected to be critically
delocalized (Anderson localized). We confirm the low-energy picture, but find
instead that finite-energy states are also delocalized, with universal
statistics that are independent of the TSC winding number, and consistent with
the spin quantum Hall plateau transition (percolation).
",physics
"  A simple recurrence relation for the even order moments of the Fabius
function is proven. Also, a very similar formula for the odd order moments in
terms of the even order moments is proved. The matrices corresponding to these
formulas (and their inverses) are multiplied so as to obtain a matrix that
correspond to a recurrence relation for the odd order moments in terms of
themselves. The theorem at the end gives a closed-form for the coefficients.
",mathematics
"  We report a study on spin conductance in ultra-thin films of Yttrium Iron
Garnet (YIG), where spin transport is provided by propagating spin waves, that
are generated and detected by direct and inverse spin Hall effects in two Pt
wires deposited on top. While at low current the spin conductance is dominated
by transport of thermal magnons, at high current, the spin conductance is
dominated by low-damping non-equilibrium magnons thermalized near the spectral
bottom by magnon-magnon interaction, with consequent a sensitivity to the
applied magnetic field and a longer decay length. This picture is supported by
microfocus Brillouin Light Scattering spectroscopy.
",physics
"  We report results from twelve simulations of the collapse of a molecular
cloud core to form one or more protostars, comprising three field strengths
(mass-to-flux ratios, {\mu}, of 5, 10, and 20) and four field geometries (with
values of the angle between the field and rotation axes, {\theta}, of 0°,
20°, 45°, and 90°), using a smoothed particle
magnetohydrodynamics method. We find that the values of both parameters have a
strong effect on the resultant protostellar system and outflows. This ranges
from the formation of binary systems when {\mu} = 20 to strikingly differing
outflow structures for differing values of {\theta}, in particular highly
suppressed outflows when {\theta} = 90°. Misaligned magnetic fields can
also produce warped pseudo-discs where the outer regions align perpendicular to
the magnetic field but the innermost region re-orientates to be perpendicular
to the rotation axis. We follow the collapse to sizes comparable to those of
first cores and find that none of the outflow speeds exceed 8 km s$^{-1}$.
These results may place constraints on both observed protostellar outflows, and
also on which molecular cloud cores may eventually form either single stars and
binaries: a sufficiently weak magnetic field may allow for disc fragmentation,
whilst conversely the greater angular momentum transport of a strong field may
inhibit disc fragmentation.
",physics
"  We consider the hypothesis that dark matter and dark energy consists of
ultra-light self-interacting scalar particles. It is found that the
Klein-Gordon equation with only two free parameters (mass and self-coupling) on
a Schwarzschild background, at the galactic length-scales has the solution
which corresponds to Bose-Einstein condensate, behaving as dark matter, while
the constant solution at supra-galactic scales can explain dark energy.
",physics
"  One of the most challenging tasks for a flying robot is to autonomously
navigate between target locations quickly and reliably while avoiding obstacles
in its path, and with little to no a-priori knowledge of the operating
environment. This challenge is addressed in the present paper. We describe the
system design and software architecture of our proposed solution, and showcase
how all the distinct components can be integrated to enable smooth robot
operation. We provide critical insight on hardware and software component
selection and development, and present results from extensive experimental
testing in real-world warehouse environments. Experimental testing reveals that
our proposed solution can deliver fast and robust aerial robot autonomous
navigation in cluttered, GPS-denied environments.
",computer-science
"  In elastic-wave turbulence, strong turbulence appears in small wave numbers
while weak turbulence does in large wave numbers. Energy transfers in the
coexistence of these turbulent states are numerically investigated in both of
the Fourier space and the real space. An analytical expression of a detailed
energy balance reveals from which mode to which mode energy is transferred in
the triad interaction. Stretching energy excited by external force is
transferred nonlocally and intermittently to large wave numbers as the kinetic
energy in the strong turbulence. In the weak turbulence, the resonant
interactions according to the weak turbulence theory produces cascading net
energy transfer to large wave numbers. Because the system's nonlinearity shows
strong temporal intermittency, the energy transfers are investigated at active
and moderate phases separately. The nonlocal interactions in the Fourier space
are characterized by the intermittent bundles of fibrous structures in the real
space.
",physics
"  The formation and the interaction of multiple cavities, induced by tightly
focused femtosecond laser pulses, are studied by using a developed numerical
tool, including the thermo-elasto-plastic material response. Simulations are
performed in fused silica in cases of one, two, and four spots of laser energy
deposition. The relaxation of the heated matter, launching shock waves in the
surrounding cold material, leads to cavity formation and emergence of areas
where cracks may be induced. Results show that the laser-induced structure
shape depends on the energy deposition configuration and demonstrate the
potential of the used numerical tool to obtain the desired designed structure
or technological process.
",physics
"  A class of Actively Calibrated Line Mounted Capacitive Voltage Transducers
(LMCVT) are introduced as a viable line mountable instrumentation option for
deploying large numbers of voltage transducers onto the medium and high voltage
systems. Active Calibration is shown to reduce the error of line mounted
voltage measurements by an order of magnitude from previously published
techniques. The instrument physics and sensing method is presented and the
performance is evaluated in a laboratory setting. Finally, a roadmap to a fully
deployable prototype is shown.
",physics
"  We present the results of an optical spectroscopic monitoring program
targeting NGC 5548 as part of a larger multi-wavelength reverberation mapping
campaign. The campaign spanned six months and achieved an almost daily cadence
with observations from five ground-based telescopes. The H$\beta$ and He II
$\lambda$4686 broad emission-line light curves lag that of the 5100 $\AA$
optical continuum by $4.17^{+0.36}_{-0.36}$ days and $0.79^{+0.35}_{-0.34}$
days, respectively. The H$\beta$ lag relative to the 1158 $\AA$ ultraviolet
continuum light curve measured by the Hubble Space Telescope is roughly
$\sim$50% longer than that measured against the optical continuum, and the lag
difference is consistent with the observed lag between the optical and
ultraviolet continua. This suggests that the characteristic radius of the
broad-line region is $\sim$50% larger than the value inferred from optical data
alone. We also measured velocity-resolved emission-line lags for H$\beta$ and
found a complex velocity-lag structure with shorter lags in the line wings,
indicative of a broad-line region dominated by Keplerian motion. The responses
of both the H$\beta$ and He II $\lambda$4686 emission lines to the driving
continuum changed significantly halfway through the campaign, a phenomenon also
observed for C IV, Ly $\alpha$, He II(+O III]), and Si IV(+O IV]) during the
same monitoring period. Finally, given the optical luminosity of NGC 5548
during our campaign, the measured H$\beta$ lag is a factor of five shorter than
the expected value implied by the $R_\mathrm{BLR} - L_\mathrm{AGN}$ relation
based on the past behavior of NGC 5548.
",physics
"  In this paper, we present a set of simulation models to more realistically
mimic the behaviour of users reading messages. We propose a User Behaviour
Model, where a simulated user reacts to a message by a flexible set of possible
reactions (e.g. ignore, read, like, save, etc.) and a mobility-based reaction
(visit a place, run away from danger, etc.). We describe our models and their
implementation in OMNeT++. We strongly believe that these models will
significantly contribute to the state of the art of simulating realistically
opportunistic networks.
",computer-science
"  Once a failure is observed, the primary concern of the developer is to
identify what caused it in order to repair the code that induced the incorrect
behavior. Until a permanent repair is afforded, code repair patches are
invaluable. The aim of this work is to devise an automated patch generation
technique that proceeds as follows: Step1) It identifies a set of
failure-causing control dependence chains that are minimal in terms of number
and length. Step2) It identifies a set of predicates within the chains along
with associated execution instances, such that negating the predicates at the
given instances would exhibit correct behavior. Step3) For each candidate
predicate, it creates a classifier that dictates when the predicate should be
negated to yield correct program behavior. Step4) Prior to each candidate
predicate, the faulty program is injected with a call to its corresponding
classifier passing it the program state and getting a return value predictively
indicating whether to negate the predicate or not. The role of the classifiers
is to ensure that: 1) the predicates are not negated during passing runs; and
2) the predicates are negated at the appropriate instances within failing runs.
We implemented our patch generation approach for the Java platform and
evaluated our toolset using 148 defects from the Introclass and Siemens
benchmarks. The toolset identified 56 full patches and another 46 partial
patches, and the classification accuracy averaged 84%.
",computer-science
"  We address the problem of constructing of coding schemes for the channels
with high-order modulations. It is known, that non-binary LDPC codes are
especially good for such channels and significantly outperform their binary
counterparts. Unfortunately, their decoding complexity is still large. In order
to reduce the decoding complexity we consider multilevel coding schemes based
on non-binary LDPC codes (NB-LDPC-MLC schemes) over smaller fields. The use of
such schemes gives us a reasonable gain in complexity. At the same time the
performance of NB-LDPC-MLC schemes is practically the same as the performance
of LDPC codes over the field matching the modulation order. In particular by
means of simulations we showed that the performance of NB-LDPC-MLC schemes over
GF(16) is the same as the performance of non-binary LDPC codes over GF(64) and
GF(256) in AWGN channel with QAM64 and QAM256 accordingly. We also perform a
comparison with binary LDPC codes.
",computer-science
"  We present a new method that combines alchemical transformation with physical
pathway to accurately and efficiently compute the absolute binding free energy
of receptor-ligand complex. Currently, the double decoupling method (DDM) and
the potential of mean force approach (PMF) methods are widely used to compute
the absolute binding free energy of biomolecules. The DDM relies on
alchemically decoupling the ligand from its environments, which can be
computationally challenging for large ligands and charged ligands because of
the large magnitude of the decoupling free energies involved. On the other
hand, the PMF approach uses physical pathway to extract the ligand out of the
binding site, thus avoids the alchemical decoupling of the ligand. However, the
PMF method has its own drawback because of the reliance on a ligand
binding/unbinding pathway free of steric obstruction from the receptor atoms.
Therefore, in the presence of deeply buried ligand functional groups the
convergence of the PMF calculation can be very slow leading to large errors in
the computed binding free energy. Here we develop a new method called AlchemPMF
by combining alchemical transformation with physical pathway to overcome the
major drawback in the PMF method. We have tested the new approach on the
binding of a charged ligand to an allosteric site on HIV-1 Integrase. After 20
ns of simulation per umbrella sampling window, the new method yields absolute
binding free energies within ~1 kcal/mol from the experimental result, whereas
the standard PMF approach and the DDM calculations result in errors of ~5
kcal/mol and > 2 kcal/mol, respectively. Furthermore, the binding free energy
computed using the new method is associated with smaller statistical error
compared with those obtained from the existing methods.
",quantitative-biology
"  LPMLN is a recent addition to probabilistic logic programming languages. Its
main idea is to overcome the rigid nature of the stable model semantics by
assigning a weight to each rule in a way similar to Markov Logic is defined. We
present two implementations of LPMLN, $\text{LPMLN2ASP}$ and
$\text{LPMLN2MLN}$. System $\text{LPMLN2ASP}$ translates LPMLN programs into
the input language of answer set solver $\text{CLINGO}$, and using weak
constraints and stable model enumeration, it can compute most probable stable
models as well as exact conditional and marginal probabilities. System
$\text{LPMLN2MLN}$ translates LPMLN programs into the input language of Markov
Logic solvers, such as $\text{ALCHEMY}$, $\text{TUFFY}$, and $\text{ROCKIT}$,
and allows for performing approximate probabilistic inference on LPMLN
programs. We also demonstrate the usefulness of the LPMLN systems for computing
other languages, such as ProbLog and Pearl's Causal Models, that are shown to
be translatable into LPMLN. (Under consideration for acceptance in TPLP)
",computer-science
"  Artificial Intelligence methods to solve continuous- control tasks have made
significant progress in recent years. However, these algorithms have important
limitations and still need significant improvement to be used in industry and
real- world applications. This means that this area is still in an active
research phase. To involve a large number of research groups, standard
benchmarks are needed to evaluate and compare proposed algorithms. In this
paper, we propose a physical environment benchmark framework to facilitate
collaborative research in this area by enabling different research groups to
integrate their designed benchmarks in a unified cloud-based repository and
also share their actual implemented benchmarks via the cloud. We demonstrate
the proposed framework using an actual implementation of the classical
mountain-car example and present the results obtained using a Reinforcement
Learning algorithm.
",computer-science
"  Rapid popularity of Internet of Things (IoT) and cloud computing permits
neuroscientists to collect multilevel and multichannel brain data to better
understand brain functions, diagnose diseases, and devise treatments. To ensure
secure and reliable data communication between end-to-end (E2E) devices
supported by current IoT and cloud infrastructure, trust management is needed
at the IoT and user ends. This paper introduces a Neuro-Fuzzy based
Brain-inspired trust management model (TMM) to secure IoT devices and relay
nodes, and to ensure data reliability. The proposed TMM utilizes node
behavioral trust and data trust estimated using Adaptive Neuro-Fuzzy Inference
System and weighted-additive methods respectively to assess the nodes
trustworthiness. In contrast to the existing fuzzy based TMMs, the NS2
simulation results confirm the robustness and accuracy of the proposed TMM in
identifying malicious nodes in the communication network. With the growing
usage of cloud based IoT frameworks in Neuroscience research, integrating the
proposed TMM into the existing infrastructure will assure secure and reliable
data communication among the E2E devices.
",quantitative-biology
"  The barocaloric effect is still an incipient scientific topic, but it has
been attracting an increasing attention in the last years due to the promising
perspectives for its application in alternative cooling devices. Here, we
present giant values of barocaloric entropy change and temperature change
induced by low pressures in PDMS elastomer around room temperature. Adiabatic
temperature changes of 12.0 K and 28.5 K were directly measured for pressure
changes of 173 MPa and 390 MPa, respectively, associated with large normalized
temperature changes (~70 K GPa-1). From adiabatic temperature change data, we
obtained entropy change values larger than 140 J kg-1 K-1. We found barocaloric
effect values that exceed those previously reported for any promising
barocaloric materials from direct measurements of temperature change around
room temperature. Our results stimulate the study of the barocaloric effect in
elastomeric polymers and broaden the pathway to use this effect in solid-state
cooling technologies.
",physics
"  Small bodies of the Solar system, like asteroids, trans-Neptunian objects,
cometary nuclei, planetary satellites, with diameters smaller than one thousand
kilometers usually have irregular shapes, often resembling dumb-bells, or
contact binaries. The spinning of such a gravitating dumb-bell creates around
it a zone of chaotic orbits. We determine its extent analytically and
numerically. We find that the chaotic zone swells significantly if the rotation
rate is decreased, in particular, the zone swells more than twice if the
rotation rate is decreased ten times with respect to the ""centrifugal breakup""
threshold. We illustrate the properties of the chaotic orbital zones in
examples of the global orbital dynamics about asteroid 243 Ida (which has a
moon, Dactyl, orbiting near the edge of the chaotic zone) and asteroid 25143
Itokawa.
",physics
"  Estimating cascade size and nodes' influence is a fundamental task in social,
technological, and biological networks. Yet this task is extremely challenging
due to the sheer size and the structural heterogeneity of networks. We
investigate a new influence measure, termed outward influence (OI), defined as
the (expected) number of nodes that a subset of nodes $S$ will activate,
excluding the nodes in S. Thus, OI equals, the de facto standard measure,
influence spread of S minus |S|. OI is not only more informative for nodes with
small influence, but also, critical in designing new effective sampling and
statistical estimation methods.
Based on OI, we propose SIEA/SOIEA, novel methods to estimate influence
spread/outward influence at scale and with rigorous theoretical guarantees. The
proposed methods are built on two novel components 1) IICP an important
sampling method for outward influence, and 2) RSA, a robust mean estimation
method that minimize the number of samples through analyzing variance and range
of random variables. Compared to the state-of-the art for influence estimation,
SIEA is $\Omega(\log^4 n)$ times faster in theory and up to several orders of
magnitude faster in practice. For the first time, influence of nodes in the
networks of billions of edges can be estimated with high accuracy within a few
minutes. Our comprehensive experiments on real-world networks also give
evidence against the popular practice of using a fixed number, e.g. 10K or 20K,
of samples to compute the ""ground truth"" for influence spread.
",computer-science
"  A novel approach towards the spectral analysis of stationary random bivariate
signals is proposed. Using the Quaternion Fourier Transform, we introduce a
quaternion-valued spectral representation of random bivariate signals seen as
complex-valued sequences. This makes possible the definition of a scalar
quaternion-valued spectral density for bivariate signals. This spectral density
can be meaningfully interpreted in terms of frequency-dependent polarization
attributes. A natural decomposition of any random bivariate signal in terms of
unpolarized and polarized components is introduced. Nonparametric spectral
density estimation is investigated, and we introduce the polarization
periodogram of a random bivariate signal. Numerical experiments support our
theoretical analysis, illustrating the relevance of the approach on synthetic
data.
",statistics
"  802.11p based V2X communication uses stochastic medium access control, which
cannot prevent broadcast packet collision, in particular during high channel
load. Wireless congestion control has been designed to keep the channel load at
an optimal point. However, vehicles' lack of precise and granular knowledge
about true channel activity, in time and space, makes it impossible to fully
avoid packet collisions. In this paper, we propose a machine learning approach
using deep neural network for learning the vehicles' transmit patterns, and as
such predicting future channel activity in space and time. We evaluate the
performance of our proposal via simulation considering multiple safety-related
V2X services involving heterogeneous transmit patterns. Our results show that
predicting channel activity, and transmitting accordingly, reduces collisions
and significantly improves communication performance.
",computer-science
"  In this work, we propose a goal-driven collaborative task that contains
language, vision, and action in a virtual environment as its core components.
Specifically, we develop a Collaborative image-Drawing game between two agents,
called CoDraw. Our game is grounded in a virtual world that contains movable
clip art objects. The game involves two players: a Teller and a Drawer. The
Teller sees an abstract scene containing multiple clip art pieces in a
semantically meaningful configuration, while the Drawer tries to reconstruct
the scene on an empty canvas using available clip art pieces. The two players
communicate via two-way communication using natural language. We collect the
CoDraw dataset of ~10K dialogs consisting of ~138K messages exchanged between
human agents. We define protocols and metrics to evaluate the effectiveness of
learned agents on this testbed, highlighting the need for a novel crosstalk
condition which pairs agents trained independently on disjoint subsets of the
training data for evaluation. We present models for our task, including simple
but effective nearest-neighbor techniques and neural network approaches trained
using a combination of imitation learning and goal-driven training. All models
are benchmarked using both fully automated evaluation and by playing the game
with live human agents.
",computer-science
"  Artificial Intelligence federates numerous scientific fields in the aim of
developing machines able to assist human operators performing complex
treatments -- most of which demand high cognitive skills (e.g. learning or
decision processes). Central to this quest is to give machines the ability to
estimate the likeness or similarity between things in the way human beings
estimate the similarity between stimuli.
In this context, this book focuses on semantic measures: approaches designed
for comparing semantic entities such as units of language, e.g. words,
sentences, or concepts and instances defined into knowledge bases. The aim of
these measures is to assess the similarity or relatedness of such semantic
entities by taking into account their semantics, i.e. their meaning --
intuitively, the words tea and coffee, which both refer to stimulating
beverage, will be estimated to be more semantically similar than the words
toffee (confection) and coffee, despite that the last pair has a higher
syntactic similarity. The two state-of-the-art approaches for estimating and
quantifying semantic similarities/relatedness of semantic entities are
presented in detail: the first one relies on corpora analysis and is based on
Natural Language Processing techniques and semantic models while the second is
based on more or less formal, computer-readable and workable forms of knowledge
such as semantic networks, thesaurus or ontologies. (...) Beyond a simple
inventory and categorization of existing measures, the aim of this monograph is
to convey novices as well as researchers of these domains towards a better
understanding of semantic similarity estimation and more generally semantic
measures.
",computer-science
"  Neural networks are known to be vulnerable to adversarial examples. Carefully
chosen perturbations to real images, while imperceptible to humans, induce
misclassification and threaten the reliability of deep learning systems in the
wild. To guard against adversarial examples, we take inspiration from game
theory and cast the problem as a minimax zero-sum game between the adversary
and the model. In general, for such games, the optimal strategy for both
players requires a stochastic policy, also known as a mixed strategy. In this
light, we propose Stochastic Activation Pruning (SAP), a mixed strategy for
adversarial defense. SAP prunes a random subset of activations (preferentially
pruning those with smaller magnitude) and scales up the survivors to
compensate. We can apply SAP to pretrained networks, including adversarially
trained models, without fine-tuning, providing robustness against adversarial
examples. Experiments demonstrate that SAP confers robustness against attacks,
increasing accuracy and preserving calibration.
",statistics
"  We provide justifications for two questions on special maps on subgroups of
the reals. We will show that the questions can be treated from different points
of view. We also discuss two versions of Anderson's Involution Conjecture.
",mathematics
"  Abstract separation logics are a family of extensions of Hoare logic for
reasoning about programs that manipulate resources such as memory locations.
These logics are ""abstract"" because they are independent of any particular
concrete resource model. Their assertion languages, called propositional
abstract separation logics (PASLs), extend the logic of (Boolean) Bunched
Implications (BBI) in various ways. In particular, these logics contain the
connectives $*$ and $-\!*$, denoting the composition and extension of resources
respectively.
This added expressive power comes at a price since the resulting logics are
all undecidable. Given their wide applicability, even a semi-decision procedure
for these logics is desirable. Although several PASLs and their relationships
with BBI are discussed in the literature, the proof theory and automated
reasoning for these logics were open problems solved by the conference version
of this paper, which developed a modular proof theory for various PASLs using
cut-free labelled sequent calculi. This paper non-trivially improves upon this
previous work by giving a general framework of calculi on which any new axiom
in the logic satisfying a certain form corresponds to an inference rule in our
framework, and the completeness proof is generalised to consider such axioms.
Our base calculus handles Calcagno et al.'s original logic of separation
algebras by adding sound rules for partial-determinism and cancellativity,
while preserving cut-elimination. We then show that many important properties
in separation logic, such as indivisible unit, disjointness, splittability, and
cross-split, can be expressed in our general axiom form. Thus our framework
offers inference rules and completeness for these properties for free. Finally,
we show how our calculi reduce to calculi with global label substitutions,
enabling more efficient implementation.
",computer-science
"  Over almost three decades the TAUP conference has seen a remarkable momentum
gain in direct dark matter search. An important accelerator were first
indications for a modulating signal rate in the DAMA/NaI experiment reported in
1997. Today the presence of an annual modulation, which matches in period and
phase the expectation for dark matter, is supported at > 9$\sigma$ confidence.
The underlying nature of dark matter, however, is still considered an open and
fundamental question of particle physics. No other direct dark matter search
could confirm the DAMA claim up to now; moreover, numerous null-results are in
clear contradiction under so-called standard assumptions for the dark matter
halo and the interaction mechanism of dark with ordinary matter. As both bear a
dependence on the target material, resolving this controversial situation will
convincingly only be possible with an experiment using sodium iodide (NaI) as
target. COSINUS aims to even go a step further by combining NaI with a novel
detection approach. COSINUS aims to operate NaI as a cryogenic calorimeter
reading scintillation light and phonon/heat signal. Two distinct advantages
arise from this approach, a substantially lower energy threshold for nuclear
recoils and particle identification on an event-by-event basis. These key
benefits will allow COSINUS to clarify a possible nuclear recoil origin of the
DAMA signal with comparatively little exposure of O(100kg days) and, thereby,
answer a long-standing question of particle physics. Today COSINUS is in R&D
phase; in this contribution we show results from the 2nd prototype, albeit the
first one of the final foreseen detector design. The key finding of this
measurement is that pure, undoped NaI is a truly excellent scintillator at low
temperatures: We measure 13.1% of the total deposited energy in the NaI crystal
in the form of scintillation light (in the light detector).
",physics
"  We study the Morse-Novikov cohomology and its almost-symplectic counterpart
on manifolds admitting locally conformally symplectic structures. More
precisely, we introduce lcs cohomologies and we study elliptic Hodge theory,
dualities, Hard Lefschetz Condition. We consider solvmanifolds and
Oeljeklaus-Toma manifolds. In particular, we prove that Oeljeklaus-Toma
manifolds with precisely one complex place, and under an additional arithmetic
condition, satisfy the Mostow property. This holds in particular for the Inoue
surface of type $S^0$.
",mathematics
"  Recent advances in 3D fully convolutional networks (FCN) have made it
feasible to produce dense voxel-wise predictions of full volumetric images. In
this work, we show that a multi-class 3D FCN trained on manually labeled CT
scans of seven abdominal structures (artery, vein, liver, spleen, stomach,
gallbladder, and pancreas) can achieve competitive segmentation results, while
avoiding the need for handcrafting features or training organ-specific models.
To this end, we propose a two-stage, coarse-to-fine approach that trains an FCN
model to roughly delineate the organs of interest in the first stage (seeing
$\sim$40% of the voxels within a simple, automatically generated binary mask of
the patient's body). We then use these predictions of the first-stage FCN to
define a candidate region that will be used to train a second FCN. This step
reduces the number of voxels the FCN has to classify to $\sim$10% while
maintaining a recall high of $>$99%. This second-stage FCN can now focus on
more detailed segmentation of the organs. We respectively utilize training and
validation sets consisting of 281 and 50 clinical CT images. Our hierarchical
approach provides an improved Dice score of 7.5 percentage points per organ on
average in our validation set. We furthermore test our models on a completely
unseen data collection acquired at a different hospital that includes 150 CT
scans with three anatomical labels (liver, spleen, and pancreas). In such
challenging organs as the pancreas, our hierarchical approach improves the mean
Dice score from 68.5 to 82.2%, achieving the highest reported average score on
this dataset.
",computer-science
"  A temperature (T)-dependent coarse-grained (CG) Hamiltonian of polyethylene
glycol/oxide (PEG/PEO) in aqueous solution is reported to be used in
implicit-solvent material models in a wide temperature (i.e., solvent quality)
range. The T-dependent nonbonded CG interactions are derived from a combined
""bottom-up"" and ""top-down"" approach. The pair potentials calculated from
atomistic replica-exchange molecular dynamics simulations in combination with
the iterative Boltzmann inversion are post-refined by benchmarking to
experimental data of the radius of gyration. For better handling and a fully
continuous transferability in T-space, the pair potentials are conveniently
truncated and mapped to an analytic formula with three structural parameters
expressed as explicit continuous functions of T. It is then demonstrated that
this model without further adjustments successfully reproduces other
experimentally known key thermodynamic properties of semi-dilute PEG solutions
such as the full equation of state (i.e., T-dependent osmotic pressure) for
various chain lengths as well as their cloud point (or collapse) temperature.
",physics
"  Heterogeneous information networks (HINs) are ubiquitous in real-world
applications. Due to the heterogeneity in HINs, the typed edges may not fully
align with each other. In order to capture the semantic subtlety, we propose
the concept of aspects with each aspect being a unit representing one
underlying semantic facet. Meanwhile, network embedding has emerged as a
powerful method for learning network representation, where the learned
embedding can be used as features in various downstream applications.
Therefore, we are motivated to propose a novel embedding learning
framework---AspEm---to preserve the semantic information in HINs based on
multiple aspects. Instead of preserving information of the network in one
semantic space, AspEm encapsulates information regarding each aspect
individually. In order to select aspects for embedding purpose, we further
devise a solution for AspEm based on dataset-wide statistics. To corroborate
the efficacy of AspEm, we conducted experiments on two real-words datasets with
two types of applications---classification and link prediction. Experiment
results demonstrate that AspEm can outperform baseline network embedding
learning methods by considering multiple aspects, where the aspects can be
selected from the given HIN in an unsupervised manner.
",computer-science
"  This paper studies the dynamics of a network-based SIRS epidemic model with
vaccination and a nonmonotone incidence rate. This type of nonlinear incidence
can be used to describe the psychological or inhibitory effect from the
behavioral change of the susceptible individuals when the number of infective
individuals on heterogeneous networks is getting larger. Using the analytical
method, epidemic threshold $R_0$ is obtained. When $R_0$ is less than one, we
prove the disease-free equilibrium is globally asymptotically stable and the
disease dies out, while $R_0$ is greater than one, there exists a unique
endemic equilibrium. By constructing a suitable Lyapunov function, we also
prove the endemic equilibrium is globally asymptotically stable if the
inhibitory factor $\alpha$ is sufficiently large. Numerical experiments are
also given to support the theoretical results. It is shown both theoretically
and numerically a larger $\alpha$ can accelerate the extinction of the disease
and reduce the level of disease.
",quantitative-biology
"  Convolutional neural networks (CNNs) have been successfully applied on both
discriminative and generative modeling for music-related tasks. For a
particular task, the trained CNN contains information representing the decision
making or the abstracting process. One can hope to manipulate existing music
based on this 'informed' network and create music with new features
corresponding to the knowledge obtained by the network. In this paper, we
propose a method to utilize the stored information from a CNN trained on
musical genre classification task. The network was composed of three
convolutional layers, and was trained to classify five-second song clips into
five different genres. After training, randomly selected clips were modified by
maximizing the sum of outputs from the network layers. In addition to the
potential of such CNNs to produce interesting audio transformation, more
information about the network and the original music could be obtained from the
analysis of the generated features since these features indicate how the
network 'understands' the music.
",computer-science
"  This paper examines the Standard Model under the strong-electroweak gauge
group $SU_S(3)\times U_{EW}(2)$ subject to the condition $u_{EW}(2)\not\cong
su_I(2)\oplus u_Y(1)$. Physically, the condition ensures that all electroweak
gauge bosons interact with each other prior to symmetry breaking --- as one
might expect from $U(2)$ invariance. This represents a crucial shift in the
notion of physical gauge bosons: Unlike the Standard Model which posits a
change of Lie algebra basis induced by spontaneous symmetry breaking, here the
basis is unaltered and $A,\,Z^0,\,W^\pm$ represent (modulo $U_{EW}(2)$ gauge
transformations) the physical bosons both \emph{before} and after spontaneous
symmetry breaking.
Our choice of $u_{EW}(2)$ basis requires some modification of the matter
field sector of the Standard Model. Careful attention to the product group
structure calls for strong-electroweak degrees of freedom in the
$(\mathbf{3},\mathbf{2})$ and the $(\mathbf{3},\overline{\mathbf{2}})$ of
$SU_S(3)\times U_{EW}(2)$ that possess integer electric charge just like
leptons. These degrees of freedom play the role of quarks, and they lead to a
modified Lagrangian that nevertheless reproduces transition rates and cross
sections equivalent to the Standard Model.
The close resemblance between quark and lepton electroweak doublets in this
picture suggests a mechanism for a phase transition between quarks and leptons
that stems from the product structure of the gauge group. Our hypothesis is
that the strong and electroweak bosons see each other as a source of
decoherence. In effect, leptons get identified with the $SU_S(3)$-trace of
quark representations. This mechanism allows for possible extensions of the
Standard Model that don't require large inclusive multiplets of matter fields.
As an example, we propose and investigate a model that turns out to have some
promising cosmological implications.
",physics
"  We study the large time behaviour of the mass (size) of particles described
by the fragmentation equation with homogeneous breakup kernel. We give
necessary and sufficient conditions for the convergence of solutions to the
unique self-similar solution.
",mathematics
"  This paper presents an interconnected control-planning strategy for redundant
manipulators, subject to system and environmental constraints. The method
incorporates low-level control characteristics and high-level planning
components into a robust strategy for manipulators acting in complex
environments, subject to joint limits. This strategy is formulated using an
adaptive control rule, the estimated dynamic model of the robotic system and
the nullspace of the linearized constraints. A path is generated that takes
into account the capabilities of the platform. The proposed method is
computationally efficient, enabling its implementation on a real multi-body
robotic system. Through experimental results with a 7 DOF manipulator, we
demonstrate the performance of the method in real-world scenarios.
",computer-science
"  We consider continuous-time Markov chains which display a family of wells at
the same depth. We provide sufficient conditions which entail the convergence
of the finite-dimensional distributions of the order parameter to the ones of a
finite state Markov chain. We also show that the state of the process can be
represented as a time-dependent convex combination of metastable states, each
of which is supported on one well.
",mathematics
"  Surface scattering is the key limiting factor to thermal transport in
dielectric crystals as the length scales are reduced or when temperature is
lowered. To explain this phenomenon, it is commonly assumed that the mean free
paths of heat carriers are bound by the crystal size and that thermal
conductivity is reduced in a manner proportional to such mean free paths. We
show here that these conclusions rely on simplifying assumptions and
approximated transport models. Instead, starting from the linearized Boltzmann
transport equation in the relaxon basis, we show how the problem can be reduced
to a set of decoupled linear differential equations. Then, the heat flow can be
interpreted as a hydrodynamic phenomenon, with the relaxon gas being slowed
down in proximity of a surface by friction effects, similar to the flux of a
viscous fluid in a pipe. As an example, we study a ribbon and a trench of
monolayer molybdenum disulphide, describing the procedure to reconstruct the
temperature and thermal conductivity profile in the sample interior and showing
how to estimate the effect of nanostructuring. The approach is general and
could be extended to other transport carriers, such as electrons, or extended
to materials of higher dimensionality and to different geometries, such as thin
films.
",physics
"  We study methods to estimate drivers' posture in vehicles using acceleration
data of wearable sensor and conduct field tests. To prevent fatal accidents,
demands for safety management of bus and taxi are high. However, acceleration
of vehicles is added to wearable sensor in vehicles. Therefore, we study
methods to estimate driving posture using acceleration data acquired from shirt
type wearable sensor hitoe and conduct field tests.
",computer-science
"  Type 2 diabetes mellitus (T2DM) is a chronic disease that often results in
multiple complications. Risk prediction and profiling of T2DM complications is
critical for healthcare professionals to design personalized treatment plans
for patients in diabetes care for improved outcomes. In this paper, we study
the risk of developing complications after the initial T2DM diagnosis from
longitudinal patient records. We propose a novel multi-task learning approach
to simultaneously model multiple complications where each task corresponds to
the risk modeling of one complication. Specifically, the proposed method
strategically captures the relationships (1) between the risks of multiple T2DM
complications, (2) between the different risk factors, and (3) between the risk
factor selection patterns. The method uses coefficient shrinkage to identify an
informative subset of risk factors from high-dimensional data, and uses a
hierarchical Bayesian framework to allow domain knowledge to be incorporated as
priors. The proposed method is favorable for healthcare applications because in
additional to improved prediction performance, relationships among the
different risks and risk factors are also identified. Extensive experimental
results on a large electronic medical claims database show that the proposed
method outperforms state-of-the-art models by a significant margin.
Furthermore, we show that the risk associations learned and the risk factors
identified lead to meaningful clinical insights.
",statistics
"  In this paper we have generalized the notion of $\lambda$-radial contraction
in complete Riemannian manifold and developed the concept of $p^\lambda$-convex
function. We have also given a counter example proving the fact that in general
$\lambda$-radial contraction of a geodesic is not necessarily a geodesic. We
have also deduced some relations between geodesic convex sets and
$p^\lambda$-convex sets and showed that under certain conditions they are
equivalent.
",mathematics
"  Kinetic Inductance Detectors (KIDs) have become an attractive alternative to
traditional bolometers in the sub-mm and mm observing community due to their
innate frequency multiplexing capabilities and simple lithographic processes.
These advantages make KIDs a viable option for the $O(500,000)$ detectors
needed for the upcoming Cosmic Microwave Background - Stage 4 (CMB-S4)
experiment. We have fabricated antenna-coupled MKID array in the 150GHz band
optimized for CMB detection. Our design uses a twin slot antenna coupled to
inverted microstrip made from a superconducting Nb/Al bilayer and SiN$_x$,
which is then coupled to an Al KID grown on high resistivity Si. We present the
fabrication process and measurements of SiN$_x$ microstrip resonators.
",physics
"  There has been relatively little attention to incorporating linguistic prior
to neural machine translation. Much of the previous work was further
constrained to considering linguistic prior on the source side. In this paper,
we propose a hybrid model, called NMT+RNNG, that learns to parse and translate
by combining the recurrent neural network grammar into the attention-based
neural machine translation. Our approach encourages the neural machine
translation model to incorporate linguistic prior during training, and lets it
translate on its own afterward. Extensive experiments with four language pairs
show the effectiveness of the proposed NMT+RNNG.
",computer-science
"  Expressive variations of tempo and dynamics are an important aspect of music
performances, involving a variety of underlying factors. Previous work has
showed a relation between such expressive variations (in particular expressive
tempo) and perceptual characteristics derived from the musical score, such as
musical expectations, and perceived tension. In this work we use a
computational approach to study the role of three measures of tonal tension
proposed by Herremans and Chew (2016) in the prediction of expressive
performances of classical piano music. These features capture tonal
relationships of the music represented in Chew's spiral array model, a three
dimensional representation of pitch classes, chords and keys constructed in
such a way that spatial proximity represents close tonal relationships. We use
non-linear sequential models (recurrent neural networks) to assess the
contribution of these features to the prediction of expressive dynamics and
expressive tempo using a dataset of Mozart piano sonatas performed by a
professional concert pianist. Experiments of models trained with and without
tonal tension features show that tonal tension helps predict change of tempo
and dynamics more than absolute tempo and dynamics values. Furthermore, the
improvement is stronger for dynamics than for tempo.
",computer-science
"  We prove that for any partially hyperbolic diffeomorphism with one
dimensional neutral center on a 3-manifold, the center stable and center
unstable foliations are complete; moreover, each leaf of center stable and
center unstable foliations is a cylinder, a M$\ddot{o}$bius band or a plane.
Further properties of the Bonatti-Parwani-Potrie type of partially hyperbolic
diffeomorphisms are studied. Such examples are obtained by composing the time
$m$-map (for $m>0$ large) of a non-transitive Anosov flow $\phi_t$ on an
orientable 3-manifold with Dehn twists along some transverse tori, and the
examples are partially hyperbolic with one-dimensional neutral center. We prove
that the center foliation gives a topologically Anosov flow which is
topologically equivalent to $\phi_t$. We also prove that for the precise
example constructed by Bonatti-Parwani-Potrie, the center stable and center
unstable foliations are robustly complete.
",mathematics
"  Determination of the energy and flux of the gamma photons by Imaging
Atmospheric Cherenkov Technique is strongly dependent on optical properties of
the atmosphere. Therefore, atmospheric monitoring during the future
observations of the Cherenkov Telescope Array (CTA) as well as anticipated
long-term monitoring in order to characterize overal properties and annual
variation of atmospheric conditions are very important. Several instruments are
already installed at the CTA sites in order to monitor atmospheric conditions
on long-term. One of them is a Sun/Moon photometer CE318-T, installed at the
Southern CTA site. Since the photometer is installed at a place with very
stable atmospheric conditions, it can be also used for characterization of its
performance and testing of new methods of aerosol optical depth (AOD)
retrieval, cloud-screening and calibration. In this work, we describe our
calibration method for nocturnal measurements and the modification of
cloud-screening for purposes of nocturnal AOD retrieval. We applied these
methods on two months of observations and present the distribution of AODs in
four photometric passbands together with their uncertainties.
",physics
"  We refine a result of the last two Authors of [8] on a Diophantine
approximation problem with two primes and a $k$-th power of a prime which was
only proved to hold for $1<k<4/3$. We improve the $k$-range to $1<k\le 3$ by
combining Harman's technique on the minor arc with a suitable estimate for the
$L^4$-norm of the relevant exponential sum over primes $S_k$. In the common
range we also give a stronger bound for the approximation.
",mathematics
"  We introduce simulations aimed at assessing how well weak gravitational
lensing of 21cm radiation from the Epoch of Reionization ($z \sim 8$) can be
measured by an SKA-like radio telescope. A simulation pipeline has been
implemented to study the performance of lensing reconstruction techniques. We
show how well the lensing signal can be reconstructed using the
three-dimensional quadratic lensing estimator in Fourier space assuming
different survey strategies. The numerical code introduced in this work is
capable of dealing with issues that can not be treated analytically such as the
discreteness of visibility measurements and the inclusion of a realistic model
for the antennae distribution. This paves the way for future numerical studies
implementing more realistic reionization models, foreground subtraction
schemes, and testing the performance of lensing estimators that take into
account the non-Gaussian distribution of HI after reionization. If multiple
frequency channels covering $z \sim 7-11.6$ are combined, Phase 1 of SKA-Low
should be able to obtain good quality images of the lensing potential with a
total resolution of $\sim 1.6$ arcmin. The SKA-Low Phase 2 should be capable of
providing images with high-fidelity even using data from $z\sim 7.7 - 8.3$. We
perform tests aimed at evaluating the numerical implementation of the mapping
reconstruction. We also discuss the possibility of measuring an accurate
lensing power spectrum. Combining data from $z \sim 7-11.6$ using the SKA2-Low
telescope model, we find constraints comparable to sample variance in the range
$L<1000$, even for survey areas as small as $25\mbox{ deg}^2$.
",physics
"  We investigate the minimum cases for realtime probabilistic machines that can
define uncountably many languages with bounded error. We show that logarithmic
space is enough for realtime PTMs on unary languages. On binary case, we follow
the same result for double logarithmic space, which is tight. When replacing
the worktape with some limited memories, we can follow uncountable results on
unary languages for two counters.
",computer-science
"  We present the procedure to build and validate the bright-star masks for the
Hyper-Suprime-Cam Strategic Subaru Proposal (HSC-SSP) survey. To identify and
mask the saturated stars in the full HSC-SSP footprint, we rely on the Gaia and
Tycho-2 star catalogues. We first assemble a pure star catalogue down to
$G_{\rm Gaia} < 18$ after removing $\sim1.5\%$ of sources that appear extended
in the Sloan Digital Sky Survey (SDSS). We perform visual inspection on the
early data from the S16A internal release of HSC-SSP, finding that our star
catalogue is $99.2\%$ pure down to $G_{\rm Gaia} < 18$. Second, we build the
mask regions in an automated way using stacked detected source measurements
around bright stars binned per $G_{\rm Gaia}$ magnitude. Finally, we validate
those masks from visual inspection and comparison with the literature of galaxy
number counts and angular two-point correlation functions. This version
(Arcturus) supersedes the previous version (Sirius) used in the S16A internal
and DR1 public releases. We publicly release the full masks and tools to flag
objects in the entire footprint of the planned HSC-SSP observations at this
address: this ftp URL.
",physics
"  In this paper we will deal with Lipschitz continuous perturbations of
Morse-Smale semigroups with only equilibrium points as critical elements. We
study the behavior of the structure of equilibrium points and their connections
when subjected to non-differentiable perturbations. To this end we define more
general notions of \emph{hyperbolicity} and \emph{transversality}, which do not
require differentiability.
",mathematics
"  Nowadays, the Security Information and Event Management (SIEM) systems take
on great relevance in handling security issues for critical infrastructures as
Internet Service Providers. Basically, a SIEM has two main functions: i) the
collection and the aggregation of log data and security information from
disparate network devices (routers, firewalls, intrusion detection systems, ad
hoc probes and others) and ii) the analysis of the gathered data by
implementing a set of correlation rules aimed at detecting potential suspicious
events as the presence of encrypted real-time traffic. In the present work, the
authors propose an enhanced implementation of a SIEM where a particular focus
is given to the detection of encrypted Skype traffic by using an ad-hoc
developed enhanced probe (ESkyPRO) conveniently governed by the SIEM itself.
Such enhanced probe, able to interact with an agent counterpart deployed into
the SIEM platform, is designed by exploiting some machine learning concepts.
The main purpose of the proposed ad-hoc SIEM is to correlate the information
received by ESkyPRO and other types of data obtained by an Intrusion Detection
System (IDS) probe in order to make the encrypted Skype traffic detection as
accurate as possible.
",computer-science
"  In this paper, we propose a dynamical systems perspective of the
Expectation-Maximization (EM) algorithm. More precisely, we can analyze the EM
algorithm as a nonlinear state-space dynamical system. The EM algorithm is
widely adopted for data clustering and density estimation in statistics,
control systems, and machine learning. This algorithm belongs to a large class
of iterative algorithms known as proximal point methods. In particular, we
re-interpret limit points of the EM algorithm and other local maximizers of the
likelihood function it seeks to optimize as equilibria in its dynamical system
representation. Furthermore, we propose to assess its convergence as asymptotic
stability in the sense of Lyapunov. As a consequence, we proceed by leveraging
recent results regarding discrete-time Lyapunov stability theory in order to
establish asymptotic stability (and thus, convergence) in the dynamical system
representation of the EM algorithm.
",computer-science
"  Early attempts to apply asteroseismology to study the Galaxy have already
shown unexpected discrepancies for the mass distribution of stars between the
Galactic models and the data; a result that is still unexplained. Here, we
revisit the analysis of the asteroseismic sample of dwarf and subgiant stars
observed by Kepler and investigate in detail the possible causes for the
reported discrepancy. We investigate two models of the Milky Way based on
stellar population synthesis, Galaxia and TRILEGAL. In agreement with previous
results, we find that TRILEGAL predicts more massive stars compared to Galaxia,
and that TRILEGAL predicts too many blue stars compared to 2MASS observations.
Both models fail to match the distribution of the stellar sample in $(\log
g,T_{\rm eff})$ space, pointing to inaccuracies in the models and/or the
assumed selection function. When corrected for this mismatch in $(\log g,T_{\rm
eff})$ space, the mass distribution calculated by Galaxia is broader and the
mean is shifted toward lower masses compared to that of the observed stars.
This behaviour is similar to what has been reported for the Kepler red giant
sample. The shift between the mass distributions is equivalent to a change of
2\% in $\nu_{\rm max}$, which is within the current uncertainty in the
$\nu_{\rm max}$ scaling relation. Applying corrections to the $\Delta \nu$
scaling relation predicted by the stellar models makes the observed mass
distribution significantly narrower, but there is no change to the mean.
",physics
"  In this paper, we study the $\mu$-ordinary locus of a Shimura variety with
parahoric level structure. Under the axioms in \cite{HR}, we show that
$\mu$-ordinary locus is a union of some maximal Ekedahl-Kottwitz-Oort-Rapoport
strata introduced in \cite{HR} and we give criteria on the density of the
$\mu$-ordinary locus.
",mathematics
"  Derived geometry can be defined as the universal way to adjoin finite
homotopical limits to a given category of manifolds compatibly with products
and glueing. The point of this paper is to show that a construction closely
resembling existing approaches to derived geometry in fact produces a geometry
with this universal property.
I also investigate consequences of this definition in particular in the
differentiable setting, and compare the theory so obtained to D. Spivak's
axioms for derived C-infinity geometry.
",mathematics
"  Establishing metallic hydrogen is a goal of intensive theoretical and
experimental work since 1935 when Wigner and Hungtinton [1] predicted that
insulating molecular hydrogen will dissociate at high pressures and transform
to a metal. This metal is predicted to be a superconductor with very high
critical temperature [2]. In another scenario, the metallization can be
realized through overlapping of electronic bands in molecular hydrogen in the
similar 400 - 500 GPa pressure range [3-5]. The calculations are not accurate
enough to predict which option will be realized. Our data are consistent with
transforms of hydrogen to semimetal by closing the indirect band gap in the
molecular phase III at pressure ~ 360 GPa. Above this pressure, the metallic
behaviour in the electrical conductivity appears, the reflection significantly
increases. With pressure, the electrical conductivity strongly increases as
measured up to 440 GPa. The Raman measurements evidence that hydrogen is in the
molecular phase III at pressures at least up to 440 GPa. At higher pressures
measured up to 480 GPa, the Raman signal gradually disappears indicating
further transformation to a good molecular metal or to an atomic state.
",physics
"  We investigate perturbative thermodynamic geometry of nonextensive ideal
Classical, Bose and Fermi gases.We show that the intrinsic statistical
interaction of nonextensive Bose (Fermi) gas is attractive (repulsive) similar
to the extensive case but the value of thermodynamic curvature is changed by
nonextensive parameter. In contrary to the extensive ideal classical gas, the
nonextensive one may be divided to two different regimes. According to
deviation parameter of the system to the nonextensive case, one can find a
special value of fugacity, $z^{*}$, where the sign of thermodynamic curvature
is changed. Therefore, we argue that the nonextensive parameter induces an
attractive (repulsive) statistical interaction for $z<z^{*}$ ($z>z^{*}$) for an
ideal classical gas. Also, according to the singular point of thermodynamic
curvature, we consider the condensation of nonextensive Boson gas.
",physics
"  A model-based approach to forecasting chaotic dynamical systems utilizes
knowledge of the physical processes governing the dynamics to build an
approximate mathematical model of the system. In contrast, machine learning
techniques have demonstrated promising results for forecasting chaotic systems
purely from past time series measurements of system state variables (training
data), without prior knowledge of the system dynamics. The motivation for this
paper is the potential of machine learning for filling in the gaps in our
underlying mechanistic knowledge that cause widely-used knowledge-based models
to be inaccurate. Thus we here propose a general method that leverages the
advantages of these two approaches by combining a knowledge-based model and a
machine learning technique to build a hybrid forecasting scheme. Potential
applications for such an approach are numerous (e.g., improving weather
forecasting). We demonstrate and test the utility of this approach using a
particular illustrative version of a machine learning known as reservoir
computing, and we apply the resulting hybrid forecaster to a low-dimensional
chaotic system, as well as to a high-dimensional spatiotemporal chaotic system.
These tests yield extremely promising results in that our hybrid technique is
able to accurately predict for a much longer period of time than either its
machine-learning component or its model-based component alone.
",statistics
"  In this paper the Chua circuit with five linear elements and saturation
non-linearity is studied. Numerical localization of self-excited attractor in
the Chua circuit model can be done by computation of trajectory with initial
data in a vicinity of an unstable equilibrium. For a hidden attractor its basin
of attraction does not overlap with a small vicinity of equilibria, so it is
difficult to find the corresponding initial data for localization. This survey
is devoted to the application of describing function method for localization of
hidden periodic and chaotic attractors in the Chua model. We use a rigorous
justification of the describing function method, based on the method of small
parameter, to get the initial data for the visualization of the hidden
attractors. A new configuration of hidden Chua attractors is presented.
",physics
"  In this paper, some algebraic and combinatorial characterizations of the
spanning simplicial complex $\Delta_s(\mathcal{J}_{n,m})$ of the Jahangir's
graph $\mathcal{J}_{n,m}$ are explored. We show that
$\Delta_s(\mathcal{J}_{n,m})$ is pure, present the formula for $f$-vectors
associated to it and hence deduce a recipe for computing the Hilbert series of
the Face ring $k[\Delta_s(\mathcal{J}_{n,m})]$. Finaly, we show that the face
ring of $\Delta_s(\mathcal{J}_{n,m})$ is Cohen-Macaulay and give some open
scopes of the current work.
",mathematics
"  All previous experiments in open turbulent flows (e.g. downstream of grids,
jet and atmospheric boundary layer) have produced quantitatively consistent
values for the scaling exponents of velocity structure functions. The only
measurement in closed turbulent flow (von Kármán swirling flow) using
Taylor-hypothesis, however, produced scaling exponents that are significantly
smaller, suggesting that the universality of these exponents are broken with
respect to change of large scale geometry of the flow. Here, we report
measurements of longitudinal structure functions of velocity in a von
Kármán setup without the use of Taylor-hypothesis. The measurements are
made using Stereo Particle Image Velocimetry at 4 different ranges of spatial
scales, in order to observe a combined inertial subrange spanning roughly one
and a half order of magnitude. We found scaling exponents (up to 9th order)
that are consistent with values from open turbulent flows, suggesting that they
might be in fact universal.
",physics
"  We show that the partial transposes of complex Wishart random matrices are
asymptotically free. We also investigate regimes where the number of blocks is
fixed but the size of the blocks increases. This gives a example where the
partial transpose produces freeness at the operator level. Finally we
investigate the case of real Wishart matrices.
",mathematics
"  We prove the Gaschütz Lemma holds for all metrisable compact groups.
",mathematics
"  The minimum volume enclosing ellipsoid (MVEE) problem is an optimization
problem in the basis of many practical problems. This paper describes some new
properties of this model and proposes a first-order oracle algorithm, the
Adjusted Coordinate Descent (ACD) algorithm, to address the MVEE problem. The
ACD algorithm is globally linear convergent and has an overwhelming advantage
over the other algorithms in cases where the dimension of the data is large.
Moreover, as a byproduct of the convergence property of the ACD algorithm, we
prove the global linear convergence of the Frank-Wolfe type algorithm
(illustrated by the case of Wolfe-Atwood's algorithm), which supports the
conjecture of Todd. Furthermore, we provide a new interpretation for the means
of choosing the coordinate axis of the Frank-Wolfe type algorithm from the
perspective of the smoothness of the coordinate axis, i.e., the algorithm
chooses the coordinate axis with the worst smoothness at each iteration. This
finding connects the first-order oracle algorithm and the linear optimization
oracle algorithm on the MVEE problem. The numerical tests support our
theoretical results.
",mathematics
"  Let $L$ be the $n$-th order linear differential operator $Ly = \phi_0y^{(n)}
+ \phi_1y^{(n-1)} + \cdots + \phi_ny$ with variable coefficients. A
representation is given for $n$ linearly independent solutions of $Ly=\lambda r
y$ as power series in $\lambda$, generalizing the SPPS (spectral parameter
power series) solution which has been previously developed for $n=2$. The
coefficient functions in these series are obtained by recursively iterating a
simple integration process, begining with a solution system for $\lambda=0$. It
is shown how to obtain such an initializing system working upwards from
equations of lower order. The values of the successive derivatives of the power
series solutions at the basepoint of integration are given, which provides a
technique for numerical solution of $n$-th order initial value problems and
spectral problems.
",mathematics
"  For parabolic equations of the form $$ \frac{\partial u}{\partial t} -
\sum_{i,j=1}^n a_{ij} (x, u) \frac{\partial^2 u}{\partial x_i \partial x_j} + f
(x, u, D u) = 0 \quad \mbox{in } {\mathbb R}_+^{n+1}, $$ where ${\mathbb
R}_+^{n+1} = {\mathbb R}^n \times (0, \infty)$, $n \ge 1$, $D = (\partial /
\partial x_1, \ldots, \partial / \partial x_n)$ is the gradient operator, and
$f$ is some function, we obtain conditions guaranteeing that every solution
tends to zero as $t \to \infty$.
",mathematics
"  Phase transitions in isotropic quantum antiferromagnets are associated with
the condensation of bosonic triplet excitations. In three dimensional quantum
antiferromagnets, such as TlCuCl$_3$, condensation can be either pressure or
magnetic field induced. The corresponding magnetic order obeys universal
scaling with thermal critical exponent $\phi$. Employing a relativistic quantum
field theory, the present work predicts the emergence of multiple (three)
universalities under combined pressure and field tuning. Changes of
universality are signalled by changes of the critical exponent $\phi$.
Explicitly, we predict the existence of two new exponents $\phi=1$ and $1/2$ as
well as recovering the known exponent $\phi=3/2$. We also predict logarithmic
corrections to the power law scaling.
",physics
"  One of the primary questions when characterizing Earth-sized and
super-Earth-sized exoplanets is whether they have a substantial atmosphere like
Earth and Venus or a bare-rock surface like Mercury. Phase curves of the
planets in thermal emission provide clues to this question, because a
substantial atmosphere would transport heat more efficiently than a bare-rock
surface. Analyzing phase curve photometric data around secondary eclipse has
previously been used to study energy transport in the atmospheres of hot
Jupiters. Here we use phase curve, Spitzer time-series photometry to study the
thermal emission properties of the super-Earth exoplanet 55 Cancri e. We
utilize a semi-analytical framework to fit a physical model to the infrared
photometric data at 4.5 micron. The model uses parameters of planetary
properties including Bond albedo, heat redistribution efficiency (i.e., ratio
between radiative timescale and advective timescale of the atmosphere), and
atmospheric greenhouse factor. The phase curve of 55 Cancri e is dominated by
thermal emission with an eastward-shifted hot spot. We determine the heat
redistribution efficiency to be ~1.47, which implies that the advective
timescale is on the same order as the radiative timescale. This requirement
cannot be met by the bare-rock planet scenario because heat transport by
currents of molten lava would be too slow. The phase curve thus favors the
scenario with a substantial atmosphere. Our constraints on the heat
redistribution efficiency translate to an atmospheric pressure of ~1.4 bar. The
Spitzer 4.5-micron band is thus a window into the deep atmosphere of the planet
55 Cancri e.
",physics
"  The X-ray spectra of the neutron stars located in the centers of supernova
remnants Cas A and HESS J1731-347 are well fit with carbon atmosphere models.
These fits yield plausible neutron star sizes for the known or estimated
distances to these supernova remnants. The evidence in favor of the presence of
a pure carbon envelope at the neutron star surface is rather indirect and is
based on the assumption that the emission is generated uniformly by the entire
stellar surface. Although this assumption is supported by the absence of
pulsations, the observational upper limit on the pulsed fraction is not very
stringent. In an attempt to quantify this evidence, we investigate the
possibility that the observed spectrum of the neutron star in HESS J1731-347 is
a combination of the spectra produced in a hydrogen atmosphere of the hotspots
and of the cooler remaining part of the neutron star surface. The lack of
pulsations in this case has to be explained either by a sufficiently small
angle between the neutron star spin axis and the line of sight, or by a
sufficiently small angular distance between the hotspots and the neutron star
rotation poles. As the observed flux from a non-uniformly emitting neutron star
depends on the angular distribution of the radiation emerging from the
atmosphere, we have computed two new grids of pure carbon and pure hydrogen
atmosphere model spectra accounting for Compton scattering. Using new hydrogen
models, we have evaluated the probability of a geometry that leads to a pulsed
fraction below the observed upper limit to be about 8.2 %. Such a geometry thus
seems to be rather improbable but cannot be excluded at this stage.
",physics
"  It is shown that the adiabatic Born-Oppenheimer expansion does not satisfy
the necessary condition for the applicability of perturbation theory. A simple
example of an exact solution of a problem that can not be obtained from the
Born-Oppenheimer expansion is given. A new version of perturbation theory for
molecular systems is proposed.
",physics
"  Context: The success of Stack Overflow and other community-based
question-and-answer (Q&A) sites depends mainly on the will of their members to
answer others' questions. In fact, when formulating requests on Q&A sites, we
are not simply seeking for information. Instead, we are also asking for other
people's help and feedback. Understanding the dynamics of the participation in
Q&A communities is essential to improve the value of crowdsourced knowledge.
Objective: In this paper, we investigate how information seekers can increase
the chance of eliciting a successful answer to their questions on Stack
Overflow by focusing on the following actionable factors: affect, presentation
quality, and time.
Method: We develop a conceptual framework of factors potentially influencing
the success of questions in Stack Overflow. We quantitatively analyze a set of
over 87K questions from the official Stack Overflow dump to assess the impact
of actionable factors on the success of technical requests. The information
seeker reputation is included as a control factor. Furthermore, to understand
the role played by affective states in the success of questions, we
qualitatively analyze questions containing positive and negative emotions.
Finally, a survey is conducted to understand how Stack Overflow users perceive
the guideline suggestions for writing questions.
Results: We found that regardless of user reputation, successful questions
are short, contain code snippets, and do not abuse with uppercase characters.
As regards affect, successful questions adopt a neutral emotional style.
Conclusion: We provide evidence-based guidelines for writing effective
questions on Stack Overflow that software engineers can follow to increase the
chance of getting technical help. As for the role of affect, we empirically
confirmed community guidelines that suggest avoiding rudeness in question
writing.
",computer-science
"  We propose a new formal criterion for secure compilation, providing strong
security guarantees for components written in unsafe, low-level languages with
C-style undefined behavior. Our criterion goes beyond recent proposals, which
protect the trace properties of a single component against an adversarial
context, to model dynamic compromise in a system of mutually distrustful
components. Each component is protected from all the others until it receives
an input that triggers an undefined behavior, causing it to become compromised
and attack the remaining uncompromised components. To illustrate this model, we
demonstrate a secure compilation chain for an unsafe language with buffers,
procedures, and components, compiled to a simple RISC abstract machine with
built-in compartmentalization. The protection guarantees offered by this
abstract machine can be achieved at the machine-code level using either
software fault isolation or tag-based reference monitoring. We are working on
machine-checked proofs showing that this compiler satisfies our secure
compilation criterion.
",computer-science
"  We report a method to control the positions of ellipsoidal magnets in flowing
channels of rectangular or circular cross section at low Reynolds number.A
static uniform magnetic field is used to pin the particle orientation, and the
particles move with translational drift velocities resulting from hydrodynamic
interactions with the channel walls which can be described using Blake's image
tensor.Building on his insights, we are able to present a far-field theory
predicting the particle motion in rectangular channels, and validate the
accuracy of the theory by comparing to numerical solutions using the boundary
element method.We find that, by changing the direction of the applied magnetic
field, the motion can be controlled so that particles move either to a curved
focusing region or to the channel walls.We also use simulations to show that
the particles are focused to a single line in a circular channel.Our results
suggest ways to focus and segregate magnetic particles in lab-on-a-chip
devices.
",physics
"  Image-guided radiation therapy can benefit from accurate motion tracking by
ultrasound imaging, in order to minimize treatment margins and radiate moving
anatomical targets, e.g., due to breathing. One way to formulate this tracking
problem is the automatic localization of given tracked anatomical landmarks
throughout a temporal ultrasound sequence. For this, we herein propose a
fully-convolutional Siamese network that learns the similarity between pairs of
image regions containing the same landmark. Accordingly, it learns to localize
and thus track arbitrary image features, not only predefined anatomical
structures. We employ a temporal consistency model as a location prior, which
we combine with the network-predicted location probability map to track a
target iteratively in ultrasound sequences. We applied this method on the
dataset of the Challenge on Liver Ultrasound Tracking (CLUST) with competitive
results, where our work is the first to effectively apply CNNs on this tracking
problem, thanks to our temporal regularization.
",computer-science
"  Single-user multiple-input / multiple-output (SU-MIMO) communication systems
have been successfully used over the years and have provided a significant
increase on a wireless link's capacity by enabling the transmission of multiple
data streams. Assuming channel knowledge at the transmitter, the maximization
of the mutual information of a MIMO link is achieved by finding the optimal
power allocation under a given sum-power constraint, which is in turn obtained
by the water-filling (WF) algorithm. However, in spectrum sharing setups, such
as Licensed Shared Access (LSA), where a primary link (PL) and a secondary link
(SL) coexist, the power transmitted by the SL transmitter may induce harmful
interference to the PL receiver. While such co-existing links have been
considered extensively in various spectrum sharing setups, the mutual
information of the SL under a constraint on the interference it may cause to
the PL receiver has, quite astonishingly, not been evaluated so far. In this
paper, we solve this problem, find its unique optimal solution and provide the
power allocation policy and corresponding precoding solution that achieves the
optimal capacity under the imposed constraint. The performance of the optimal
solution and the penalty due to the interference constraint are evaluated over
some indicative Rayleigh fading channel conditions and interference thresholds.
We believe that the obtained results are of general nature and that they may
apply, beyond spectrum sharing, to a variety of applications that admit a
similar setup.
",computer-science
"  The theoretical description of the thermodynamics of water is challenged by
the structural transition towards tetrahedral symmetry at ambient conditions.
As perturbation theories typically assume a spherically symmetric reference
fluid, they are incapable of accurately describing the liquid properties of
water at ambient conditions. In this paper we solve this problem, by
introducing the concept of an associated reference perturbation theory (APT).
In APT we treat the reference fluid as an associating hard sphere fluid which
transitions to tetrahedral symmetry in the fully hydrogen bonded limit. We
calculate this transition in a theoretically self-consistent manner without
appealing to molecular simulations. This associated reference provides the
reference fluid for a second order Barker-Hendersen perturbative treatment of
the long-range attractions. We demonstrate that this new approach gives a
significantly improved description of water as compared to standard
perturbation theories.
",physics
"  We consider the nonunitary quantum dynamics of neutral massless scalar
particles used to model photons around a massive gravitational lens. The
gravitational interaction between the lensing mass and asymptotically free
particles is described by their second-quantized scattering wavefunctions.
Remarkably, the zero-point spacetime fluctuations can induce significant
decoherence of the scattered states with spontaneous emission of gravitons,
thereby reducing the particles' coherence as well as energy. This new effect
suggests that, when photon polarizations are negligible, such quantum gravity
phenomena could lead to measurable anomalous redshift of recently studied
astrophysical lasers through a gravitational lens in the range of black holes
and galaxy clusters.
",physics
"  We develop a strong diagnostic for bubbles and crashes in bitcoin, by
analyzing the coincidence (and its absence) of fundamental and technical
indicators. Using a generalized Metcalfe's law based on network properties, a
fundamental value is quantified and shown to be heavily exceeded, on at least
four occasions, by bubbles that grow and burst. In these bubbles, we detect a
universal super-exponential unsustainable growth. We model this universal
pattern with the Log-Periodic Power Law Singularity (LPPLS) model, which
parsimoniously captures diverse positive feedback phenomena, such as herding
and imitation. The LPPLS model is shown to provide an ex-ante warning of market
instabilities, quantifying a high crash hazard and probabilistic bracket of the
crash time consistent with the actual corrections; although, as always, the
precise time and trigger (which straw breaks the camel's back) being exogenous
and unpredictable. Looking forward, our analysis identifies a substantial but
not unprecedented overvaluation in the price of bitcoin, suggesting many months
of volatile sideways bitcoin prices ahead (from the time of writing, March
2018).
",quantitative-finance
"  We present a novel framework for addressing the nonlinear Landau collision
integral in terms of finite element and other subspace projection methods. We
employ the underlying metriplectic structure of the Landau collision integral
and, using a Galerkin discretization for the velocity space, we transform the
infinite-dimensional system into a finite-dimensional, time-continuous
metriplectic system. Temporal discretization is accomplished using the concept
of discrete gradients. The conservation of energy, momentum, and particle
densities, as well as the production of entropy is demonstrated algebraically
for the fully discrete system. Due to the generality of our approach, the
conservation properties and the monotonic behavior of entropy are guaranteed
for finite element discretizations in general, independently of the mesh
configuration.
",physics
"  We determine three invariants: Arnold's $J^+$-invariant as well as
$\mathcal{J}_1$ and $\mathcal{J}_2$ invariants, which were introduced by
Cieliebak-Frauenfelder-van Koert, of periodic orbits of the second kind near
the heavier primary in the restricted three-body problem, provided that the
mass ratio is sufficiently small.
",mathematics
"  Change point analysis is a statistical tool to identify homogeneity within
time series data. We propose a pruning approach for approximate nonparametric
estimation of multiple change points. This general purpose change point
detection procedure `cp3o' applies a pruning routine within a dynamic program
to greatly reduce the search space and computational costs. Existing
goodness-of-fit change point objectives can immediately be utilized within the
framework. We further propose novel change point algorithms by applying cp3o to
two popular nonparametric goodness of fit measures: `e-cp3o' uses E-statistics,
and `ks-cp3o' uses Kolmogorov-Smirnov statistics. Simulation studies highlight
the performance of these algorithms in comparison with parametric and other
nonparametric change point methods. Finally, we illustrate these approaches
with climatological and financial applications.
",statistics
"  Two new high-precision measurements of the deuterium abundance from absorbers
along the line of sight to the quasar PKS1937--1009 were presented. The
absorbers have lower neutral hydrogen column densities (N(HI) $\approx$
18\,cm$^{-2}$) than for previous high-precision measurements, boding well for
further extensions of the sample due to the plenitude of low column density
absorbers. The total high-precision sample now consists of 12 measurements with
a weighted average deuterium abundance of D/H = $2.55\pm0.02\times10^{-5}$. The
sample does not favour a dipole similar to the one detected for the fine
structure constant. The increased precision also calls for improved
nucleosynthesis predictions. For that purpose we have updated the public
AlterBBN code including new reactions, updated nuclear reaction rates, and the
possibility of adding new physics such as dark matter. The standard Big Bang
Nucleosynthesis prediction of D/H = $2.456\pm0.057\times10^{-5}$ is consistent
with the observed value within 1.7 standard deviations.
",physics
"  Interpreting the performance of deep learning models beyond test set accuracy
is challenging. Characteristics of individual data points are often not
considered during evaluation, and each data point is treated equally. We
examine the impact of a test set question's difficulty to determine if there is
a relationship between difficulty and performance. We model difficulty using
well-studied psychometric methods on human response patterns. Experiments on
Natural Language Inference (NLI) and Sentiment Analysis (SA) show that the
likelihood of answering a question correctly is impacted by the question's
difficulty. As DNNs are trained with more data, easy examples are learned more
quickly than hard examples.
",computer-science
"  We construct, for imaginary quadratic number fields with class number 1, an
arithmetic site of Connes-Consani type. The main difficulty here is that the
constructions of Connes and Consani and part of their results strongly rely on
the natural order existing on real numbers which is compatible with basic
arithmetic operations. Of course nothing of this sort exists in the case of
imaginary quadratic number fields with class number 1. We first define what we
call arithmetic site for such number fields, we then calculate the points of
those arithmetic sites and we express them in terms of the adèles class space
considered by Connes to give a spectral interpretation of zeroes of Hecke L
functions of number fields. We get therefore that for a fixed imaginary
quadratic number field with class number 1, that the points of our arithmetic
site are related to the zeroes of the Dedekind zeta function of the number
field considered and to the zeroes of some Hecke L functions. We then study the
relation between the spectrum of the ring of integers of the number field and
the arithmetic site. Finally we construct the square of the arithmetic site.
",mathematics
"  For monomial special multiserial algebras, which in general are of wild
representation type, we construct radical embeddings into algebras of finite
representation type. As a consequence, we show that the representation
dimension of monomial and self-injective special multiserial algebras is less
or equal to three. This implies that the finitistic dimension conjecture holds
for all special multiserial algebras.
",mathematics
"  Let $\pi$ be a Hecke-Maass cusp form for $SL(3,\mathbb{Z})$ and
$\chi=\chi_1\chi_2$ a Dirichlet character with $\chi_i$ primitive modulo $M_i$.
Suppose that $M_1$, $M_2$ are primes such that
$\max\{(M|t|)^{1/3+2\delta/3},M^{2/5}|t|^{-9/20},
M^{1/2+2\delta}|t|^{-3/4+2\delta}\}(M|t|)^{\varepsilon}<M_1< \min\{
(M|t|)^{2/5},(M|t|)^{1/2-8\delta}\}(M|t|)^{-\varepsilon}$ for any
$\varepsilon>0$, where $M=M_1M_2$, $|t|\geq 1$ and $0<\delta< 1/52$. Then we
have $$ L\left(\frac{1}{2}+it,\pi\otimes \chi\right)\ll_{\pi,\varepsilon}
(M|t|)^{3/4-\delta+\varepsilon}. $$
",mathematics
"  Detection of a planetary ring of exoplanets remains as one of the most
attractive but challenging goals in the field. We present a methodology of a
systematic search for exoplanetary rings via transit photometry of long-period
planets. The methodology relies on a precise integration scheme we develop to
compute a transit light curve of a ringed planet. We apply the methodology to
89 long-period planet candidates from the Kepler data so as to estimate, and/or
set upper limits on, the parameters of possible rings. While a majority of our
samples do not have a sufficiently good signal-to-noise ratio for meaningful
constraints on ring parameters, we find that six systems with a higher
signal-to-noise ratio are inconsistent with the presence of a ring larger than
1.5 times the planetary radius assuming a grazing orbit and a tilted ring.
Furthermore, we identify five preliminary candidate systems whose light curves
exhibit ring-like features. After removing four false positives due to the
contamination from nearby stars, we identify KIC 10403228 as a reasonable
candidate for a ringed planet. A systematic parameter fit of its light curve
with a ringed planet model indicates two possible solutions corresponding to a
Saturn-like planet with a tilted ring. There also remain other two possible
scenarios accounting for the data; a circumstellar disk and a hierarchical
triple. Due to large uncertain factors, we cannot choose one specific model
among the three.
",physics
"  Pharmaco-epidemiology (PE) is the study of uses and effects of drugs in well
defined populations. As medico-administrative databases cover a large part of
the population, they have become very interesting to carry PE studies. Such
databases provide longitudinal care pathways in real condition containing
timestamped care events, especially drug deliveries. Temporal pattern mining
becomes a strategic choice to gain valuable insights about drug uses. In this
paper we propose DCM, a new discriminant temporal pattern mining algorithm. It
extracts chronicle patterns that occur more in a studied population than in a
control population. We present results on the identification of possible
associations between hospitalizations for seizure and anti-epileptic drug
switches in care pathway of epileptic patients.
",computer-science
"  e-ASTROGAM (enhanced ASTROGAM) is a breakthrough Observatory space mission,
with a detector composed by a Silicon tracker, a calorimeter, and an
anticoincidence system, dedicated to the study of the non-thermal Universe in
the photon energy range from 0.3 MeV to 3 GeV - the lower energy limit can be
pushed to energies as low as 150 keV for the tracker, and to 30 keV for
calorimetric detection. The mission is based on an advanced space-proven
detector technology, with unprecedented sensitivity, angular and energy
resolution, combined with polarimetric capability. Thanks to its performance in
the MeV-GeV domain, substantially improving its predecessors, e-ASTROGAM will
open a new window on the non-thermal Universe, making pioneering observations
of the most powerful Galactic and extragalactic sources, elucidating the nature
of their relativistic outflows and their effects on the surroundings. With a
line sensitivity in the MeV energy range one to two orders of magnitude better
than previous generation instruments, e-ASTROGAM will determine the origin of
key isotopes fundamental for the understanding of supernova explosion and the
chemical evolution of our Galaxy. The mission will provide unique data of
significant interest to a broad astronomical community, complementary to
powerful observatories such as LIGO-Virgo-GEO600-KAGRA, SKA, ALMA, E-ELT, TMT,
LSST, JWST, Athena, CTA, IceCube, KM3NeT, and LISA.
",physics
"  In this paper we study the systole growth of arithmetic locally symmetric
spaces up congruence covers and show that this growth is at least logarithmic
in volume. This generalizes previous work of Buser and Sarnak as well as Katz,
Schaps and Vishne where the case of compact hyperbolic 2- and 3-manifolds was
considered.
",mathematics
"  We study learning problems involving arbitrary classes of functions $F$,
distributions $X$ and targets $Y$. Because proper learning procedures, i.e.,
procedures that are only allowed to select functions in $F$, tend to perform
poorly unless the problem satisfies some additional structural property (e.g.,
that $F$ is convex), we consider unrestricted learning procedures that are free
to choose functions outside the given class.
We present a new unrestricted procedure that is optimal in a very strong
sense: the required sample complexity is essentially the best one can hope for,
and the estimate holds for (almost) any problem, including heavy-tailed
situations. Moreover, the sample complexity coincides with the what one would
expect if $F$ were convex, even when $F$ is not. And if $F$ is convex, the
procedure turns out to be proper. Thus, the unrestricted procedure is actually
optimal in both realms, for convex classes as a proper procedure and for
arbitrary classes as an unrestricted procedure.
",statistics
"  We calculate 3-loop master integrals for heavy quark correlators and the
3-loop QCD corrections to the $\rho$-parameter. They obey non-factorizing
differential equations of second order with more than three singularities,
which cannot be factorized in Mellin-$N$ space either. The solution of the
homogeneous equations is possible in terms of convergent close integer power
series as $_2F_1$ Gau\ss{} hypergeometric functions at rational argument. In
some cases, integrals of this type can be mapped to complete elliptic integrals
at rational argument. This class of functions appears to be the next one
arising in the calculation of more complicated Feynman integrals following the
harmonic polylogarithms, generalized polylogarithms, cyclotomic harmonic
polylogarithms, square-root valued iterated integrals, and combinations
thereof, which appear in simpler cases. The inhomogeneous solution of the
corresponding differential equations can be given in terms of iterative
integrals, where the new innermost letter itself is not an iterative integral.
A new class of iterative integrals is introduced containing letters in which
(multiple) definite integrals appear as factors. For the elliptic case, we also
derive the solution in terms of integrals over modular functions and also
modular forms, using $q$-product and series representations implied by Jacobi's
$\vartheta_i$ functions and Dedekind's $\eta$-function. The corresponding
representations can be traced back to polynomials out of Lambert--Eisenstein
series, having representations also as elliptic polylogarithms, a $q$-factorial
$1/\eta^k(\tau)$, logarithms and polylogarithms of $q$ and their $q$-integrals.
Due to the specific form of the physical variable $x(q)$ for different
processes, different representations do usually appear. Numerical results are
also presented.
",computer-science
"  We present two different approaches to model power grids as interconnected
networks of networks. Both models are derived from a model for spatially
embedded mono-layer networks and are generalised to handle an arbitrary number
of network layers. The two approaches are distinguished by their use case. The
static glue stick construction model yields a multi-layer network from a
predefined layer interconnection scheme, i.e. different layers are attached
with transformer edges. It is especially suited to construct multi-layer power
grids with a specified number of nodes in and transformers between layers. We
contrast it with a genuine growth model which we label interconnected layer
growth model.
",physics
"  We consider a gas of independent Brownian particles on a bounded interval in
contact with two particle reservoirs at the endpoints. Due to the Brownian
nature of the particles, infinitely many particles enter and leave the system
in each time interval. Nonetheless, the dynamics can be constructed as a Markov
process with continuous paths on a suitable space. If $\lambda_0$ and
$\lambda_1$ are the chemical potentials of the boundary reservoirs, the
stationary distribution (reversible if and only if $\lambda_0=\lambda_1$) is a
Poisson point process with intensity given by the linear interpolation between
$\lambda_0$ and $\lambda_1$. We then analyze the empirical flow that it is
defined by counting, in a time interval $[0,t]$, the net number of particles
crossing a given point $x$. In the stationary regime we identify its statistics
and show that it is given, apart an $x$ dependent correction that is bounded
for large $t$, by the difference of two independent Poisson processes with
parameters $\lambda_0$ and $\lambda_1$.
",mathematics
"  We examine the conditions under which material from the martian moons Phobos
and Deimos could reach our planet in the form of meteorites. We find that the
necessary ejection speeds from these moons (900 and 600 m/s for Phobos and
Deimos respectively) are much smaller than from Mars' surface (5000 m/s). These
speeds are below typical impact speeds for asteroids and comets (10-40 km/s) at
Mars' orbit, and we conclude that the delivery of meteorites from Phobos and
Deimos to the Earth can occur.
",physics
"  Motion planning for underwater vehicles must consider the effect of ocean
currents. We present an efficient method to compute reachability and cost
between sample points in sampling-based motion planning that supports
long-range planning over hundreds of kilometres in complicated flows. The idea
is to search a reduced space of control inputs that consists of stream
functions whose level sets, or streamlines, optimally connect two given points.
Such stream functions are generated by superimposing a control input onto the
underlying current flow. A streamline represents the resulting path that a
vehicle would follow as it is carried along by the current given that control
input. We provide rigorous analysis that shows how our method avoids exhaustive
search of the control space, and demonstrate simulated examples in complicated
flows including a traversal along the east coast of Australia, using actual
current predictions, between Sydney and Brisbane.
",computer-science
"  We investigate some extremal problems in Fourier analysis and their
connection to a problem in prime number theory. In particular, we improve the
current bounds for the largest possible gap between consecutive primes assuming
the Riemann hypothesis.
",mathematics
"  Modeled along the truncated approach in Panigrahi (2016), selection-adjusted
inference in a Bayesian regime is based on a selective posterior. Such a
posterior is determined together by a generative model imposed on data and the
selection event that enforces a truncation on the assumed law. The effective
difference between the selective posterior and the usual Bayesian framework is
reflected in the use of a truncated likelihood. The normalizer of the truncated
law in the adjusted framework is the probability of the selection event; this
is typically intractable and it leads to the computational bottleneck in
sampling from such a posterior. The current work lays out a primal-dual
approach of solving an approximating optimization problem to provide valid
post-selective Bayesian inference. The selection procedures are posed as
data-queries that solve a randomized version of a convex learning program which
have the advantage of preserving more left-over information for inference. We
propose a randomization scheme under which the optimization has separable
constraints that result in a partially separable objective in lower dimensions
for many commonly used selective queries to approximate the otherwise
intractable selective posterior. We show that the approximating optimization
under a Gaussian randomization gives a valid exponential rate of decay for the
selection probability on a large deviation scale. We offer a primal-dual method
to solve the optimization problem leading to an approximate posterior; this
allows us to exploit the usual merits of a Bayesian machinery in both low and
high dimensional regimes where the underlying signal is effectively sparse. We
show that the adjusted estimates empirically demonstrate better frequentist
properties in comparison to the unadjusted estimates based on the usual
posterior, when applied to a wide range of constrained, convex data queries.
",statistics
"  The interplay between geometric frustration (GF) and bond disorder is studied
in the Ising kagome lattice within a cluster approach. The model considers
antiferromagnetic (AF) short-range couplings and long-range intercluster
disordered interactions. The replica formalism is used to obtain an effective
single cluster model from where the thermodynamics is analyzed by exact
diagonalization. We found that the presence of GF can introduce cluster
freezing at very low levels of disorder. The system exhibits an entropy plateau
followed by a large entropy drop close to the freezing temperature. In this
scenario, a spin-liquid (SL) behavior prevents conventional long-range order,
but an infinitesimal disorder picks out uncompensated cluster states from the
multi degenerate SL regime, potentializing the intercluster disordered coupling
and bringing the cluster spin-glass state. To summarize, our results suggest
that the SL state combined with low levels of disorder can activate small
clusters, providing hypersensitivity to the freezing process in geometrically
frustrated materials and playing a key role in the glassy stabilization. We
propose that this physical mechanism could be present in several geometrically
frustrated materials. In particular, we discuss our results in connection to
the recent experimental investigations of the Ising kagome compound
Co$_3$Mg(OH)$_6$Cl$_2$.
",physics
"  Action potentials are the basic unit of information in the nervous system and
their reliable detection and decoding holds the key to understanding how the
brain generates complex thought and behavior. Transducing these signals into
microwave field oscillations can enable wireless sensors that report on brain
activity through magnetic induction. In the present work we demonstrate that
action potentials from crayfish lateral giant neuron can trigger microwave
oscillations in spin-torque nano-oscillators. These nanoscale devices take as
input small currents and convert them to microwave current oscillations that
can wirelessly broadcast neuronal activity, opening up the possibility for
compact neuro-sensors. We show that action potentials activate microwave
oscillations in spin-torque nano-oscillators with an amplitude that follows the
action potential signal, demonstrating that the device has both the sensitivity
and temporal resolution to respond to action potentials from a single neuron.
The activation of magnetic oscillations by action potentials, together with the
small footprint and the high frequency tunability, makes these devices
promising candidates for high resolution sensing of bioelectric signals from
neural tissues. These device attributes may be useful for design of
high-throughput bi-directional brain-machine interfaces.
",physics
"  Despite rapid advances in face recognition, there remains a clear gap between
the performance of still image-based face recognition and video-based face
recognition, due to the vast difference in visual quality between the domains
and the difficulty of curating diverse large-scale video datasets. This paper
addresses both of those challenges, through an image to video feature-level
domain adaptation approach, to learn discriminative video frame
representations. The framework utilizes large-scale unlabeled video data to
reduce the gap between different domains while transferring discriminative
knowledge from large-scale labeled still images. Given a face recognition
network that is pretrained in the image domain, the adaptation is achieved by
(i) distilling knowledge from the network to a video adaptation network through
feature matching, (ii) performing feature restoration through synthetic data
augmentation and (iii) learning a domain-invariant feature through a domain
adversarial discriminator. We further improve performance through a
discriminator-guided feature fusion that boosts high-quality frames while
eliminating those degraded by video domain-specific factors. Experiments on the
YouTube Faces and IJB-A datasets demonstrate that each module contributes to
our feature-level domain adaptation framework and substantially improves video
face recognition performance to achieve state-of-the-art accuracy. We
demonstrate qualitatively that the network learns to suppress diverse artifacts
in videos such as pose, illumination or occlusion without being explicitly
trained for them.
",computer-science
"  The paper presents a novel analysis of a transmission problem for a network
of flexural beams incorporating conventional Euler-Bernoulli beams as well as
Rayleigh beams with the enhanced rotational inertia. Although, in the
low-frequency regime, these beams have a similar dynamic response, we have
demonstrated novel features which occur in the transmission at higher
frequencies across the layer of the Rayleigh beams.
",physics
"  We extend the homotopy theories based on point reduction for finite spaces
and simplicial complexes to finite acyclic categories and $\Delta$-complexes,
respectively. The functors of classifying spaces and face posets are compatible
with these homotopy theories. In contrast with the classical settings of finite
spaces and simplicial complexes, the universality of morphisms and simplices
plays a central role in this paper.
",mathematics
"  We present an introduction to periodic and stochastic homogenization of
ellip- tic partial differential equations. The first part is concerned with the
qualitative theory, which we present for equations with periodic and random
coefficients in a unified approach based on Tartar's method of oscillating test
functions. In partic- ular, we present a self-contained and elementary argument
for the construction of the sublinear corrector of stochastic homogenization.
(The argument also applies to elliptic systems and in particular to linear
elasticity). In the second part we briefly discuss the representation of the
homogenization error by means of a two- scale expansion. In the last part we
discuss some results of quantitative stochastic homogenization in a discrete
setting. In particular, we discuss the quantification of ergodicity via
concentration inequalities, and we illustrate that the latter in combi- nation
with elliptic regularity theory leads to a quantification of the growth of the
sublinear corrector and the homogenization error.
",mathematics
"  A new coding technique, based on \textit{fixed block-length} codes, is
proposed for the problem of communicating a pair of correlated sources over a
$2-$user interference channel. Its performance is analyzed to derive a new set
of sufficient conditions. The latter is proven to be strictly less binding than
the current known best, which is due to Liu and Chen [Dec, 2011]. Our findings
are inspired by Dueck's example [March, 1981].
",computer-science
"  In the last three decades, we have seen a significant increase in trading
goods and services through online auctions. However, this business created an
attractive environment for malicious moneymakers who can commit different types
of fraud activities, such as Shill Bidding (SB). The latter is predominant
across many auctions but this type of fraud is difficult to detect due to its
similarity to normal bidding behaviour. The unavailability of SB datasets makes
the development of SB detection and classification models burdensome.
Furthermore, to implement efficient SB detection models, we should produce SB
data from actual auctions of commercial sites. In this study, we first scraped
a large number of eBay auctions of a popular product. After preprocessing the
raw auction data, we build a high-quality SB dataset based on the most reliable
SB strategies. The aim of our research is to share the preprocessed auction
dataset as well as the SB training (unlabelled) dataset, thereby researchers
can apply various machine learning techniques by using authentic data of
auctions and fraud.
",statistics
"  Recently, the authors of the present work (together with M. N. Kolountzakis)
introduced a new version of the non-commutative Delsarte scheme and applied it
to the problem of mutually unbiased bases. Here we use this method to
investigate the existence of a finite projective plane of a given order d. In
particular, a short new proof is obtained for the nonexistence of a projective
plane of order 6. For higher orders like 10 and 12, the method is non decisive
but could turn out to give important supplementary informations.
",mathematics
"  The detection of molecular species in the atmospheres of earth-like
exoplanets orbiting nearby stars requires an optical system that suppresses
starlight and maximizes the sensitivity to the weak planet signals at small
angular separations. Achieving sufficient contrast performance on a segmented
aperture space telescope is particularly challenging due to unwanted
diffraction within the telescope from amplitude and phase discontinuities in
the pupil. Apodized vortex coronagraphs are a promising solution that
theoretically meet the performance needs for high contrast imaging with future
segmented space telescopes. We investigate the sensitivity of apodized vortex
coronagraphs to the expected aberrations, including segment co-phasing errors
in piston and tip/tilt as well as other low-order and mid-spatial frequency
aberrations. Coronagraph designs and their associated telescope requirements
are identified for conceptual HabEx and LUVOIR telescope designs.
",physics
"  The quality of experience (QoE) is known to be subjective and
context-dependent. Identifying and calculating the factors that affect QoE is
indeed a difficult task. Recently, a lot of effort has been devoted to estimate
the users QoE in order to improve video delivery. In the literature, most of
the QoE-driven optimization schemes that realize trade-offs among different
quality metrics have been addressed under the assumption of homogenous
populations. Nevertheless, people perceptions on a given video quality may not
be the same, which makes the QoE optimization harder. This paper aims at taking
a step further in order to address this limitation and meet users profiles. To
do so, we propose a closed-loop control framework based on the
users(subjective) feedbacks to learn the QoE function and optimize it at the
same time. Our simulation results show that our system converges to a steady
state, where the resulting QoE function noticeably improves the users
feedbacks.
",computer-science
"  We define parahoric $\cG$--torsors for certain Bruhat--Tits group scheme
$\cG$ on a smooth complex projective curve $X$ when the weights are real, and
also define connections on them. We prove that a $\cG$--torsor is given by a
homomorphism from $\pi_1(X\setminus D)$ to a maximal compact subgroup of $G$,
where $D\, \subset\, X$ is the parabolic divisor, if and only if the torsor is
polystable.
",mathematics
"  Scientific publishing conveys the outputs of an academic or research
activity, in this sense; it also reflects the efforts and issues in which
people engage. To identify potential collaborative networks one of the simplest
approaches is to leverage the co-authorship relations. In this approach,
semantic and hierarchic relationships defined by a Knowledge Organization
System are used in order to improve the system's ability to recommend potential
networks beyond the lexical or syntactic analysis of the topics or concepts
that are of interest to academics.
",computer-science
"  Long-lead forecasting for spatio-temporal systems can often entail complex
nonlinear dynamics that are difficult to specify it a priori. Current
statistical methodologies for modeling these processes are often highly
parameterized and thus, challenging to implement from a computational
perspective. One potential parsimonious solution to this problem is a method
from the dynamical systems and engineering literature referred to as an echo
state network (ESN). ESN models use so-called {\it reservoir computing} to
efficiently compute recurrent neural network (RNN) forecasts. Moreover,
so-called ""deep"" models have recently been shown to be successful at predicting
high-dimensional complex nonlinear processes, particularly those with multiple
spatial and temporal scales of variability (such as we often find in
spatio-temporal environmental data). Here we introduce a deep ensemble ESN
(D-EESN) model. We present two versions of this model for spatio-temporal
processes that both produce forecasts and associated measures of uncertainty.
The first approach utilizes a bootstrap ensemble framework and the second is
developed within a hierarchical Bayesian framework (BD-EESN). This more general
hierarchical Bayesian framework naturally accommodates non-Gaussian data types
and multiple levels of uncertainties. The methodology is first applied to a
data set simulated from a novel non-Gaussian multiscale Lorenz-96 dynamical
system simulation model and then to a long-lead United States (U.S.) soil
moisture forecasting application.
",statistics
"  A non-equilibrium theory of optical conductivity of dirty-limit
superconductors and commensurate charge density wave is presented. We discuss
the current response to different experimentally relevant light-field probe
pulses and show that a single frequency definition of the optical conductivity
$\sigma(\omega)\equiv j(\omega)/E(\omega)$ is difficult to interpret out of the
adiabatic limit. We identify characteristic time domain signatures
distinguishing between superconducting, normal metal and charge density wave
states. We also suggest a route to directly address the instantaneous
superfluid stiffness of a superconductor by shaping the probe light field.
",physics
"  Studies of the response of the SiD silicon-tungsten electromagnetic
calorimeter (ECal) are presented. Layers of highly granular (13 mm^2 pixels)
silicon detectors embedded in thin gaps (~ 1 mm) between tungsten alloy plates
give the SiD ECal the ability to separate electromagnetic showers in a crowded
environment. A nine-layer prototype has been built and tested in a 12.1 GeV
electron beam at the SLAC National Accelerator Laboratory. This data was
simulated with a Geant4 model. Particular attention was given to the separation
of nearby incident electrons, which demonstrated a high (98.5%) separation
efficiency for two electrons at least 1 cm from each other. The beam test study
will be compared to a full SiD detector simulation with a realistic geometry,
where the ECal calibration constants must first be established. This work is
continuing, as the geometry requires that the calibration constants depend upon
energy, angle, and absorber depth. The derivation of these constants is being
developed from first principles.
",physics
"  Reaction-diffusion equations appear in biology and chemistry, and combine
linear diffusion with different kind of reaction terms. Some of them are
remarkable from the mathematical point of view, since they admit families of
travelling waves that describe the asymptotic behaviour of a larger class of
solutions $0\leq u(x,t)\leq 1$ of the problem posed in the real line. We
investigate here the existence of waves with constant propagation speed, when
the linear diffusion is replaced by the ""slow"" doubly nonlinear diffusion. In
the present setting we consider bistable reaction terms, which present
interesting differences w.r.t. the Fisher-KPP framework recently studied in
\cite{AA-JLV:art}. We find different families of travelling waves that are
employed to describe the wave propagation of more general solutions and to
study the stability/instability of the steady states, even when we extend the
study to several space dimensions. A similar study is performed in the critical
case that we call ""pseudo-linear"", i.e., when the operator is still nonlinear
but has homogeneity one. With respect to the classical model and the
""pseudo-linear"" case, the travelling waves of the ""slow"" diffusion setting
exhibit free boundaries. \\ Finally, as a complement of \cite{AA-JLV:art}, we
study the asymptotic behaviour of more general solutions in the presence of a
""heterozygote superior"" reaction function and doubly nonlinear diffusion
(""slow"" and ""pseudo-linear"").
",mathematics
"  Process Monitoring involves tracking a system's behaviors, evaluating the
current state of the system, and discovering interesting events that require
immediate actions. In this paper, we consider monitoring temporal system state
sequences to help detect the changes of dynamic systems, check the divergence
of the system development, and evaluate the significance of the deviation. We
begin with discussions of data reduction, symbolic data representation, and the
anomaly detection in temporal discrete sequences. Time-series representation
methods are also discussed and used in this paper to discretize raw data into
sequences of system states. Markov Chains and stationary state distributions
are continuously generated from temporal sequences to represent snapshots of
the system dynamics in different time frames. We use generalized Jensen-Shannon
Divergence as the measure to monitor changes of the stationary symbol
probability distributions and evaluate the significance of system deviations.
We prove that the proposed approach is able to detect deviations of the systems
we monitor and assess the deviation significance in probabilistic manner.
",statistics
"  Penalized regression models such as the lasso have been extensively applied
to analyzing high-dimensional data sets. However, due to memory limitations,
existing R packages like glmnet and ncvreg are not capable of fitting
lasso-type models for ultrahigh-dimensional, multi-gigabyte data sets that are
increasingly seen in many areas such as genetics, genomics, biomedical imaging,
and high-frequency finance. In this research, we implement an R package called
biglasso that tackles this challenge. biglasso utilizes memory-mapped files to
store the massive data on the disk, only reading data into memory when
necessary during model fitting, and is thus able to handle out-of-core
computation seamlessly. Moreover, it's equipped with newly proposed, more
efficient feature screening rules, which substantially accelerate the
computation. Benchmarking experiments show that our biglasso package, as
compared to existing popular ones like glmnet, is much more memory- and
computation-efficient. We further analyze a 31 GB real data set on a laptop
with only 16 GB RAM to demonstrate the out-of-core computation capability of
biglasso in analyzing massive data sets that cannot be accommodated by existing
R packages.
",statistics
"  For convex co-compact subgroups of SL2(Z) we consider the ""congruence
subgroups"" for p prime. We prove a factorization formula for the Selberg zeta
function in term of L-functions related to irreducible representations of the
Galois group SL2(Fp) of the covering, together with a priori bounds and
analytic continuation. We use this factorization property combined with an
averaging technique over representations to prove a new existence result of
non-trivial resonances in an effective low frequency strip.
",mathematics
"  Distributed word representations are widely used for modeling words in NLP
tasks. Most of the existing models generate one representation per word and do
not consider different meanings of a word. We present two approaches to learn
multiple topic-sensitive representations per word by using Hierarchical
Dirichlet Process. We observe that by modeling topics and integrating topic
distributions for each document we obtain representations that are able to
distinguish between different meanings of a given word. Our models yield
statistically significant improvements for the lexical substitution task
indicating that commonly used single word representations, even when combined
with contextual information, are insufficient for this task.
",computer-science
"  A revolution in galaxy cluster science is only a few years away. The survey
machines eROSITA and Euclid will provide cluster samples of never-before-seen
statistical quality. XMM-Newton will be the key instrument to exploit these
rich datasets in terms of detailed follow-up of the cluster hot gas content,
systematically characterizing sub-samples as well as exotic new objects.
",physics
"  Clusters of galaxies gravitationally lens the cosmic microwave background
(CMB) radiation, resulting in a distinct imprint in the CMB on arcminute
scales. Measurement of this effect offers a promising way to constrain the
masses of galaxy clusters, particularly those at high redshift. We use CMB maps
from the South Pole Telescope Sunyaev-Zel'dovich (SZ) survey to measure the CMB
lensing signal around galaxy clusters identified in optical imaging from first
year observations of the Dark Energy Survey. The cluster catalog used in this
analysis contains 3697 members with mean redshift of $\bar{z} = 0.45$. We
detect lensing of the CMB by the galaxy clusters at $8.1\sigma$ significance.
Using the measured lensing signal, we constrain the amplitude of the relation
between cluster mass and optical richness to roughly $17\%$ precision, finding
good agreement with recent constraints obtained with galaxy lensing. The error
budget is dominated by statistical noise but includes significant contributions
from systematic biases due to the thermal SZ effect and cluster miscentering.
",physics
"  We present a straightforward source-to-source transformation that introduces
justifications for user-defined constraints into the CHR programming language.
Then a scheme of two rules suffices to allow for logical retraction (deletion,
removal) of constraints during computation. Without the need to recompute from
scratch, these rules remove not only the constraint but also undo all
consequences of the rule applications that involved the constraint. We prove a
confluence result concerning the rule scheme and show its correctness. When
algorithms are written in CHR, constraints represent both data and operations.
CHR is already incremental by nature, i.e. constraints can be added at runtime.
Logical retraction adds decrementality. Hence any algorithm written in CHR with
justifications will become fully dynamic. Operations can be undone and data can
be removed at any point in the computation without compromising the correctness
of the result. We present two classical examples of dynamic algorithms, written
in our prototype implementation of CHR with justifications that is available
online: maintaining the minimum of a changing set of numbers and shortest paths
in a graph whose edges change.
",computer-science
"  An RNA secondary structure is designable if there is an RNA sequence which
can attain its maximum number of base pairs only by adopting that structure.
The combinatorial RNA design problem, introduced by Haleš et al. in 2016,
is to determine whether or not a given RNA secondary structure is designable.
Haleš et al. identified certain classes of designable and non-designable
secondary structures by reference to their corresponding rooted trees. We
introduce an infinite class of rooted trees containing unpaired nucleotides at
the greatest height, and prove constructively that their corresponding
secondary structures are designable. This complements previous results for the
combinatorial RNA design problem.
",computer-science
"  High-availability of software systems requires automated handling of crashes
in presence of errors. Failure-oblivious computing is one technique that aims
to achieve high availability. We note that failure-obliviousness has not been
studied in depth yet, and there is very few study that helps understand why
failure-oblivious techniques work. In order to make failure-oblivious computing
to have an impact in practice, we need to deeply understand failure-oblivious
behaviors in software. In this paper, we study, design and perform an
experiment that analyzes the size and the diversity of the failure-oblivious
behaviors. Our experiment consists of exhaustively computing the search space
of 16 field failures of large-scale open-source Java software. The outcome of
this experiment is a much better understanding of what really happens when
failure-oblivious computing is used, and this opens new promising research
directions.
",computer-science
"  In this paper, we introduce a design principle to develop novel soft modular
robots based on tensegrity structures and inspired by the cytoskeleton of
living cells. We describe a novel strategy to realize tensegrity structures
using planar manufacturing techniques, such as 3D printing. We use this
strategy to develop icosahedron tensegrity structures with programmable
variable stiffness that can deform in a three-dimensional space. We also
describe a tendon-driven contraction mechanism to actively control the
deformation of the tensegrity mod-ules. Finally, we validate the approach in a
modular locomotory worm as a proof of concept.
",computer-science
"  We explore the problem of intersection classification using monocular
on-board passive vision, with the goal of classifying traffic scenes with
respect to road topology. We divide the existing approaches into two broad
categories according to the type of input data: (a) first person vision (FPV)
approaches, which use an egocentric view sequence as the intersection is
passed; and (b) third person vision (TPV) approaches, which use a single view
immediately before entering the intersection. The FPV and TPV approaches each
have advantages and disadvantages. Therefore, we aim to combine them into a
unified deep learning framework. Experimental results show that the proposed
FPV-TPV scheme outperforms previous methods and only requires minimal FPV/TPV
measurements.
",computer-science
"  During the last decades, public policies become a central pillar in
supporting and stabilising agricultural sector. In 1962, EU policy-makers
developed the so-called Common Agricultural Policy (CAP) to ensure
competitiveness and a common market organisation for agricultural products,
while 2003 reform decouple the CAP from the production to focus only on income
stabilization and the sustainability of agricultural sector. Notwithstanding
farmers are highly dependent to public support, literature on the role played
by the CAP in fostering agricultural performances is still scarce and
fragmented. Actual CAP policies increases performance differentials between
Northern Central EU countries and peripheral regions. This paper aims to
evaluate the effectiveness of CAP in stimulate performances by focusing on
Italian lagged Regions. Moreover, agricultural sector is deeply rooted in
place-based production processes. In this sense, economic analysis which omit
the presence of spatial dependence produce biased estimates of the
performances. Therefore, this paper, using data on subsidies and economic
results of farms from the RICA dataset which is part of the Farm Accountancy
Data Network (FADN), proposes a spatial Augmented Cobb-Douglas Production
Function to evaluate the effects of subsidies on farm's performances. The major
innovation in this paper is the implementation of a micro-founded quantile
version of a spatial lag model to examine how the impact of the subsidies may
vary across the conditional distribution of agricultural performances. Results
show an increasing shape which switch from negative to positive at the median
and becomes statistical significant for higher quantiles. Additionally, spatial
autocorrelation parameter is positive and significant across all the
conditional distribution, suggesting the presence of significant spatial
spillovers in agricultural performances.
",statistics
"  Current tools for exploratory data analysis (EDA) require users to manually
select data attributes, statistical computations and visual encodings. This can
be daunting for large-scale, complex data. We introduce Foresight, a
visualization recommender system that helps the user rapidly explore large
high-dimensional datasets through ""guideposts."" A guidepost is a visualization
corresponding to a pronounced instance of a statistical descriptor of the
underlying data, such as a strong linear correlation between two attributes,
high skewness or concentration about the mean of a single attribute, or a
strong clustering of values. For each descriptor, Foresight initially presents
visualizations of the ""strongest"" instances, based on an appropriate ranking
metric. Given these initial guideposts, the user can then look at ""nearby""
guideposts by issuing ""guidepost queries"" containing constraints on metric
type, metric strength, data attributes, and data values. Thus, the user can
directly explore the network of guideposts, rather than the overwhelming space
of data attributes and visual encodings. Foresight also provides for each
descriptor a global visualization of ranking-metric values to both help orient
the user and ensure a thorough exploration process. Foresight facilitates
interactive exploration of large datasets using fast, approximate sketching to
compute ranking metrics. We also contribute insights on EDA practices of data
scientists, summarizing results from an interview study we conducted to inform
the design of Foresight.
",computer-science
"  The Venusian surface has been studied by measuring radar reflections and
thermal radio emission over a wide spectral region of several centimeters to
meter wavelengths from the Earth-based as well as orbiter platforms. The
radiometric observations, in the decimeter (dcm) wavelength regime showed a
decreasing trend in the observed brightness temperature (Tb) with increasing
wavelength. The thermal emission models available at present have not been able
to explain the radiometric observations at longer wavelength (dcm) to a
satisfactory level. This paper reports the first interferometric imaging
observations of Venus below 620 MHz. They were carried out at 606, 332.9 and
239.9 MHz using the Giant Meterwave Radio Telescope (GMRT). The Tb values
derived at the respective frequencies are 526 K, 409 K and < 426 K, with errors
of ~7% which are generally consistent with the reported Tb values at 608 MHz
and 430 MHz by previous investigators, but are much lower than those derived
from high-frequency observations at 1.38-22.46 GHz using the VLA.
",physics
"  Ground-based observations at thermal infrared wavelengths suffer from large
background radiation due to the sky, telescope and warm surfaces in the
instrument. This significantly limits the sensitivity of ground-based
observations at wavelengths longer than 3 microns. We analyzed this background
emission in infrared high contrast imaging data, show how it can be modelled
and subtracted and demonstrate that it can improve the detection of faint
sources, such as exoplanets. We applied principal component analysis to model
and subtract the thermal background emission in three archival high contrast
angular differential imaging datasets in the M and L filter. We describe how
the algorithm works and explain how it can be applied. The results of the
background subtraction are compared to the results from a conventional mean
background subtraction scheme. Finally, both methods for background subtraction
are also compared by performing complete data reductions. We analyze the
results from the M dataset of HD100546 qualitatively. For the M band dataset of
beta Pic and the L band dataset of HD169142, which was obtained with an annular
groove phase mask vortex vector coronagraph, we also calculate and analyze the
achieved signal to noise (S/N). We show that applying PCA is an effective way
to remove spatially and temporarily varying thermal background emission down to
close to the background limit. The procedure also proves to be very successful
at reconstructing the background that is hidden behind the PSF. In the complete
data reductions, we find at least qualitative improvements for HD100546 and
HD169142, however, we fail to find a significant increase in S/N of beta Pic b.
We discuss these findings and argue that in particular datasets with strongly
varying observing conditions or infrequently sampled sky background will
benefit from the new approach.
",physics
"  Analysis of solar magnetic fields using observations as well as theoretical
interpretations of the scattering polarization is commonly designated as a high
priority area of the solar research. The interpretation of the observed
polarization raises a serious theoretical challenge to the researchers involved
in this field. In fact, realistic interpretations need detailed investigations
of the depolarizing role of isotropic collisions with neutral hydrogen. The
goal of this paper is to determine new relationships which allow the
calculation of any collisional rates of the d-levels of ions by simply
determining the value of n^* and $E_p$ without the need of determining the
interaction potentials and treating the dynamics of collisions. The
determination of n^* and E_p is easy and based on atomic data usually available
online. Accurate collisional rates allow a reliable diagnostics of solar
magnetic fields. In this work we applied our collisional FORTRAN code to a
large number of cases involving complex and simple ions. After that, the
results are utilized and injected in a genetic programming code developed with
C-langugae in order to infer original relationships which will be of great help
to solar applications. We discussed the accurarcy of our collisional rates in
the cases of polarized complex atoms and atoms with hyperfine structure. The
relationships are expressed on the tensorial basis and we explain how to
include their contributions in the master equation giving the variation of the
density matrix elements. As a test, we compared the results obtained through
the general relationships provided in this work with the results obtained
directly by running our code of collisions. These comparisons show a percentage
of error of about 10% in the average value.
",physics
"  We analytically derive the elastic, dielectric, piezoelectric, and the
flexoelectric phenomenological coefficients as functions of microscopic model
parameters such as ionic positions and spring constants in the two-dimensional
square-lattice model with rock-salt-type ionic arrangement. Monte-Carlo
simulation reveals that a difference in the given elastic constants of the
diagonal springs, each of which connects the same cations or anions, is
responsible for the linear flexoelectric effect in the model. We show the
quadratic flexoelectric effect is present only in non-centrosymmetric systems
and it can overwhelm the linear effect in feasibly large strain gradients.
",physics
"  This paper presents the submissions by the University of Zurich to the
SIGMORPHON 2017 shared task on morphological reinflection. The task is to
predict the inflected form given a lemma and a set of morpho-syntactic
features. We focus on neural network approaches that can tackle the task in a
limited-resource setting. As the transduction of the lemma into the inflected
form is dominated by copying over lemma characters, we propose two recurrent
neural network architectures with hard monotonic attention that are strong at
copying and, yet, substantially different in how they achieve this. The first
approach is an encoder-decoder model with a copy mechanism. The second approach
is a neural state-transition system over a set of explicit edit actions,
including a designated COPY action. We experiment with character alignment and
find that naive, greedy alignment consistently produces strong results for some
languages. Our best system combination is the overall winner of the SIGMORPHON
2017 Shared Task 1 without external resources. At a setting with 100 training
samples, both our approaches, as ensembles of models, outperform the next best
competitor.
",computer-science
"  With Bell's inequalities one has a formal expression to show how essentially
all local theories of natural phenomena that are formulated within the
framework of realism may be tested using a simple experimental arrangement. For
the case of entangled pairs of spin-1/2 particles we propose an alternative
measurement setup which is consistent to the necessary assumptions
corresponding to the derivation of the Bell inequalities. We find that the Bell
inequalities are never violated with respect to our suggested measurement
process.
",physics
"  Existing shape estimation methods for deformable object manipulation suffer
from the drawbacks of being off-line, model dependent, noise-sensitive or
occlusion-sensitive, and thus are not appropriate for manipulation tasks
requiring high precision. In this paper, we present a real-time shape
estimation approach for autonomous robotic manipulation of 3D deformable
objects. Our method fulfills all the requirements necessary for the
high-quality deformable object manipulation in terms of being real-time,
model-free and robust to noise and occlusion. These advantages are accomplished
using a joint tracking and reconstruction framework, in which we track the
object deformation by aligning a reference shape model with the stream input
from the RGB-D camera, and simultaneously upgrade the reference shape model
according to the newly captured RGB-D data. We have evaluated the quality and
robustness of our real-time shape estimation pipeline on a set of deformable
manipulation tasks implemented on physical robots. Videos are available at
this https URL
",computer-science
"  Proxies for regulatory reforms based on categorical variables are
increasingly used in empirical evaluation models. We surveyed 63 studies that
rely on such indices to analyze the effects of entry liberalization,
privatization, unbundling, and independent regulation of the electricity,
natural gas, and telecommunications sectors. We highlight methodological issues
related to the use of these proxies. Next, taking stock of the literature, we
provide practical advice for the design of the empirical strategy and discuss
the selection of control and instrumental variables to attenuate endogeneity
problems undermining identification of the effects of regulatory reforms.
",quantitative-finance
"  Using deep multi-wavelength photometry of galaxies from ZFOURGE, we group
galaxies at $2.5<z<4.0$ by the shape of their spectral energy distributions
(SEDs). We identify a population of galaxies with excess emission in the
$K_s$-band, which corresponds to [OIII]+H$\beta$ emission at $2.95<z<3.65$.
This population includes 78% of the bluest galaxies with UV slopes steeper than
$\beta = -2$. We de-redshift and scale this photometry to build two composite
SEDs, enabling us to measure equivalent widths of these Extreme [OIII]+H$\beta$
Emission Line Galaxies (EELGs) at $z\sim3.5$. We identify 60 galaxies that
comprise a composite SED with [OIII]+H$\beta$ rest-frame equivalent width of
$803\pm228$\AA\ and another 218 galaxies in a composite SED with equivalent
width of $230\pm90$\AA. These EELGs are analogous to the `green peas' found in
the SDSS, and are thought to be undergoing their first burst of star formation
due to their blue colors ($\beta < -1.6$), young ages
($\log(\rm{age}/yr)\sim7.2$), and low dust attenuation values. Their strong
nebular emission lines and compact sizes (typically $\sim1.4$ kpc) are
consistent with the properties of the star-forming galaxies possibly
responsible for reionizing the universe at $z>6$. Many of the EELGs also
exhibit Lyman-$\alpha$ emission. Additionally, we find that many of these
sources are clustered in an overdensity in the Chandra Deep Field South, with
five spectroscopically confirmed members at $z=3.474 \pm 0.004$. The spatial
distribution and photometric redshifts of the ZFOURGE population further
confirm the overdensity highlighted by the EELGs.
",physics
"  In this study, we determine all modular curves $X_0(N)$ that admit infinitely
many cubic points.
",mathematics
"  We calculate the specific heat of a weakly interacting dilute system of
bosons on a lattice and show that it is consistent with the measured electronic
specific heat in the superconducting state of underdoped cuprates with boson
concentration $\rho \sim x/2$, where $x$ is the hole (dopant) concentration. As
usual, the $T^3$ term is due to Goldstone phonons. The zero-point energy,
through its dependence on the condensate density $\rho_0(T)$, accounts for the
anomalous $T$-linear term. These results support the split-pairing mechanism,
in which spinons (pure spin) are paired at $T^*$ and holons (pure charge) form
real-space pairs at $T_p < T^*$, creating a gauge-coupled physical pair of
charge $+2e$ and concentration $x/2$ which Bose condenses below $T_c$,
accounting for the observed phases.
",physics
"  In further study of the application of crossed-product functors to the
Baum-Connes Conjecture, Buss, Echterhoff, and Willett introduced various other
properties that crossed-product functors may have. Here we introduce and study
analogues of these properties for coaction functors, making sure that the
properties are preserved when the coaction functors are composed with the full
crossed product to make a crossed-product functor. The new properties for
coaction functors studied here are functoriality for generalized homomorphisms
and the correspondence property. We particularly study the connections with the
ideal property. The study of functoriality for generalized homomorphisms
requires a detailed development of the Fischer construction of maximalization
of coactions with regard to possibly degenerate homomorphisms into multiplier
algebras. We verify that all ""KLQ"" functors arising from large ideals of the
Fourier-Stieltjes algebra $B(G)$ have all the properties we study, and at the
opposite extreme we give an example of a coaction functor having none of the
properties.
",mathematics
"  Motivated by station-keeping applications in various unmanned settings, this
paper introduces a steering control law for a pair of agents operating in the
vicinity of a fixed beacon in a three-dimensional environment. This feedback
law is a modification of the previously studied three-dimensional constant
bearing (CB) pursuit law, in the sense that it incorporates an additional term
to allocate attention to the beacon. We investigate the behavior of the
closed-loop dynamics for a two agent mutual pursuit system in which each agent
employs the beacon-referenced CB pursuit law with regards to the other agent
and a stationary beacon. Under certain assumptions on the associated control
parameters, we demonstrate that this problem admits circling equilibria wherein
the agents move on circular orbits with a common radius, in planes
perpendicular to a common axis passing through the beacon. As the common radius
and distances from the beacon are determined by choice of parameters in the
feedback law, this approach provides a means to engineer desired formations in
a three-dimensional setting.
",computer-science
"  Let $\xi(t\,,x)$ denote space-time white noise and consider a
reaction-diffusion equation of the form \[
\dot{u}(t\,,x)=\tfrac12 u""(t\,,x) + b(u(t\,,x)) + \sigma(u(t\,,x))
\xi(t\,,x), \] on $\mathbb{R}_+\times[0\,,1]$, with homogeneous Dirichlet
boundary conditions and suitable initial data, in the case that there exists
$\varepsilon>0$ such that $\vert b(z)\vert \ge|z|(\log|z|)^{1+\varepsilon}$ for
all sufficiently-large values of $|z|$. When $\sigma\equiv 0$, it is well known
that such PDEs frequently have non-trivial stationary solutions. By contrast,
Bonder and Groisman (2009) have recently shown that there is finite-time blowup
when $\sigma$ is a non-zero constant. In this paper, we prove that the
Bonder--Groisman condition is unimproveable by showing that the
reaction-diffusion equation with noise is ""typically"" well posed when $\vert
b(z) \vert =O(|z|\log_+|z|)$ as $|z|\to\infty$. We interpret the word
""typically"" in two essentially-different ways without altering the conclusions
of our assertions.
",mathematics
"  V. Nestoridis conjectured that if $\Omega$ is a simply connected subset of
$\mathbb{C}$ that does not contain $0$ and $S(\Omega)$ is the set of all
functions $f\in \mathcal{H}(\Omega)$ with the property that the set
$\left\{T_N(f)(z)\coloneqq\sum_{n=0}^N\dfrac{f^{(n)}(z)}{n!} (-z)^n : N =
0,1,2,\dots \right\}$ is dense in $\mathcal{H}(\Omega)$, then $S(\Omega)$ is a
dense $G_\delta$ set in $\mathcal{H}(\Omega)$. We answer the conjecture in the
affirmative in the special case where $\Omega$ is an open disc $D(z_0,r)$ that
does not contain $0$.
",mathematics
"  We study the annealing stability of bottom-pinned perpendicularly magnetized
magnetic tunnel junctions based on dual MgO free layers and thin fixed systems
comprising a hard [Co/Ni] multilayer antiferromagnetically coupled to thin a Co
reference layer and a FeCoB polarizing layer. Using conventional magnetometry
and advanced broadband ferromagnetic resonance, we identify the properties of
each sub-unit of the magnetic tunnel junction and demonstrate that this
material option can ensure a satisfactory resilience to the 400$^\circ$C
thermal annealing needed in solid-state magnetic memory applications. The dual
MgO free layer possesses an anneal-robust 0.4 T effective anisotropy and
suffers only a minor increase of its Gilbert damping from 0.007 to 0.010 for
the toughest annealing conditions. Within the fixed system, the ferro-coupler
and texture-breaking TaFeCoB layer keeps an interlayer exchange above 0.8
mJ/m$^2$, while the Ru antiferrocoupler layer within the synthetic
antiferromagnet maintains a coupling above -0.5 mJ/m$^2$. These two strong
couplings maintain the overall functionality of the tunnel junction upon the
toughest annealing despite the gradual degradation of the thin Co layer
anisotropy that may reduce the operation margin in spin torque memory
applications. Based on these findings, we propose further optimization routes
for the next generation magnetic tunnel junctions.
",physics
"  The Column Subset Selection Problem provides a natural framework for
unsupervised feature selection. Despite being a hard combinatorial optimization
problem, there exist efficient algorithms that provide good approximations. The
drawback of the problem formulation is that it incorporates no form of
regularization, and is therefore very sensitive to noise when presented with
scarce data. In this paper we propose a regularized formulation of this
problem, and derive a correct greedy algorithm that is similar in efficiency to
existing greedy methods for the unregularized problem. We study its adequacy
for feature selection and propose suitable formulations. Additionally, we
derive a lower bound for the error of the proposed problems. Through various
numerical experiments on real and synthetic data, we demonstrate the
significantly increased robustness and stability of our method, as well as the
improved conditioning of its output, all while remaining efficient for
practical use.
",statistics
"  One of the most basic skills a robot should possess is predicting the effect
of physical interactions with objects in the environment. This enables optimal
action selection to reach a certain goal state. Traditionally, dynamics are
approximated by physics-based analytical models. These models rely on specific
state representations that may be hard to obtain from raw sensory data,
especially if no knowledge of the object shape is assumed. More recently, we
have seen learning approaches that can predict the effect of complex physical
interactions directly from sensory input. It is however an open question how
far these models generalize beyond their training data. In this work, we
investigate the advantages and limitations of neural network based learning
approaches for predicting the effects of actions based on sensory input and
show how analytical and learned models can be combined to leverage the best of
both worlds. As physical interaction task, we use planar pushing, for which
there exists a well-known analytical model and a large real-world dataset. We
propose to use a convolutional neural network to convert raw depth images or
organized point clouds into a suitable representation for the analytical model
and compare this approach to using neural networks for both, perception and
prediction. A systematic evaluation of the proposed approach on a very large
real-world dataset shows two main advantages of the hybrid architecture.
Compared to a pure neural network, it significantly (i) reduces required
training data and (ii) improves generalization to novel physical interaction.
",computer-science
"  We study two colored operads of configurations of little $n$-disks in a unit
$n$-disk, with the centers of the small disks of one color restricted to an
$m$-plane, $m<n$. We compute the rational homotopy type of these \emph{extended
Swiss Cheese operads} and show how they are connected to the rational homotopy
types of the inclusion maps from the little $m$-disks to the little $n$-disks
operad.
",mathematics
"  Skyrmions are localized magnetic spin textures whose stability has been shown
theoretically to depend on material parameters including bulk Dresselhaus spin
orbit coupling (SOC), interfacial Rashba SOC, and magnetic anisotropy. Here, we
establish the growth of a new class of artificial skyrmion materials, namely
B20 superlattices, where these parameters could be systematically tuned.
Specifically, we report the successful growth of B20 superlattices comprised of
single crystal thin films of FeGe, MnGe, and CrGe on Si(111) substrates. Thin
films and superlattices are grown by molecular beam epitaxy and are
characterized through a combination of reflection high energy electron
diffraction, x-ray diffraction, and cross-sectional scanning transmission
electron microscopy (STEM). X-ray energy dispersive spectroscopy (XEDS)
distinguishes layers by elemental mapping and indicates good interface quality
with relatively low levels of intermixing in the [CrGe/MnGe/FeGe] superlattice.
This demonstration of epitaxial, single-crystalline B20 superlattices is a
significant advance toward tunable skyrmion systems for fundamental scientific
studies and applications in magnetic storage and logic.
",physics
"  In this paper, we present a novel deep fusion architecture for audio
classification tasks. The multi-channel model presented is formed using deep
convolution layers where different acoustic features are passed through each
channel. To enable dissemination of information across the channels, we
introduce attention feature maps that aid in the alignment of frames. The
output of each channel is merged using interaction parameters that non-linearly
aggregate the representative features. Finally, we evaluate the performance of
the proposed architecture on three benchmark datasets:- DCASE-2016 and LITIS
Rouen (acoustic scene recognition), and CHiME-Home (tagging). Our experimental
results suggest that the architecture presented outperforms the standard
baselines and achieves outstanding performance on the task of acoustic scene
recognition and audio tagging.
",computer-science
"  We investigate the long-time stability of the Sun-Jupiter-Saturn-Uranus
system by considering a planar secular model, that can be regarded as a major
refinement of the approach first introduced by Lagrange. Indeed, concerning the
planetary orbital revolutions, we improve the classical circular approximation
by replacing it with a solution that is invariant up to order two in the
masses; therefore, we investigate the stability of the secular system for
rather small values of the eccentricities. First, we explicitly construct a
Kolmogorov normal form, so as to find an invariant KAM torus which approximates
very well the secular orbits. Finally, we adapt the approach that is at basis
of the analytic part of the Nekhoroshev's theorem, so as to show that there is
a neighborhood of that torus for which the estimated stability time is larger
than the lifetime of the Solar System. The size of such a neighborhood,
compared with the uncertainties of the astronomical observations, is about ten
times smaller.
",mathematics
"  Realization of a short bunch beam by manipulating the longitudinal phase
space distribution with a finite longitudinal dispersion following an off-crest
accelera- tion is a widely used technique. The technique was applied in a
compact test accelerator of an energy-recovery linac scheme for compressing the
bunch length at the return loop. A diagnostic system utilizing coherent
transition radiation was developed for the beam tuning and for estimating the
bunch length. By scanning the beam parameters, we experimentally found the best
condition for the bunch compression. The RMS bunch length of 250+-50 fs was
obtained at a bunch charge of 2 pC. This result confirmed the design and the
tuning pro- cedure of the bunch compression operation for the future
energy-recovery linac (ERL).
",physics
"  In the present paper we demonstrate the results of a statistical analysis of
some characteristics of precipitation events and propose a kind of a
theoretical explanation of the proposed models in terms of mixed Poisson and
mixed exponential distributions based on the information-theoretical entropy
reasoning. The proposed models can be also treated as the result of following
the popular Bayesian approach.
",statistics
"  It is well-known that the verification of partial correctness properties of
imperative programs can be reduced to the satisfiability problem for
constrained Horn clauses (CHCs). However, state-of-the-art solvers for CHCs
(CHC solvers) based on predicate abstraction are sometimes unable to verify
satisfiability because they look for models that are definable in a given class
A of constraints, called A-definable models. We introduce a transformation
technique, called Predicate Pairing (PP), which is able, in many interesting
cases, to transform a set of clauses into an equisatisfiable set whose
satisfiability can be proved by finding an A-definable model, and hence can be
effectively verified by CHC solvers. We prove that, under very general
conditions on A, the unfold/fold transformation rules preserve the existence of
an A-definable model, i.e., if the original clauses have an A-definable model,
then the transformed clauses have an A-definable model. The converse does not
hold in general, and we provide suitable conditions under which the transformed
clauses have an A-definable model iff the original ones have an A-definable
model. Then, we present the PP strategy which guides the application of the
transformation rules with the objective of deriving a set of clauses whose
satisfiability can be proved by looking for A-definable models. PP introduces a
new predicate defined by the conjunction of two predicates together with some
constraints. We show through some examples that an A-definable model may exist
for the new predicate even if it does not exist for its defining atomic
conjuncts. We also present some case studies showing that PP plays a crucial
role in the verification of relational properties of programs (e.g., program
equivalence and non-interference). Finally, we perform an experimental
evaluation to assess the effectiveness of PP in increasing the power of CHC
solving.
",computer-science
"  Given a positive function $u\in W^{1,n}$, we define its John-Nirenberg radius
at point $x$ to be the supreme of the radius such that $\int_{B_t}|\nabla\log
u|^n<\epsilon_0^n$ when $n>2$, and $\int_{B_t}|\nabla u|^2<\epsilon_0^2$ when
$n=2$. We will show that for a collapsing sequence in a fixed conformal class
under some curvature conditions, the radius is bounded below by a positive
constant. As applications, we will study the convergence of a conformal metric
sequence on a $4$-manifold with bounded $\|K\|_{W^{1,2}}$, and prove a
generalized Hélein's Convergence Theorem.
",mathematics
"  Objective: The Learning Health System (LHS) requires integration of research
into routine practice. eSource or embedding clinical trial functionalities into
routine electronic health record (EHR) systems has long been put forward as a
solution to the rising costs of research. We aimed to create and validate an
eSource solution that would be readily extensible as part of a LHS.
Materials and Methods: The EU FP7 TRANSFoRm project's approach is based on
dual modelling, using the Clinical Research Information Model (CRIM) and the
Clinical Data Integration Model of meaning (CDIM) to bridge the gap between
clinical and research data structures, using the CDISC Operational Data Model
(ODM) standard. Validation against GCP requirements was conducted in a clinical
site, and a cluster randomised evaluation by site nested into a live clinical
trial.
Results: Using the form definition element of ODM, we linked precisely
modelled data queries to data elements, constrained against CDIM concepts, to
enable automated patient identification for specific protocols and
prepopulation of electronic case report forms (e-CRF). Both control and eSource
sites recruited better than expected with no significant difference.
Completeness of clinical forms was significantly improved by eSource, but
Patient Related Outcome Measures (PROMs) were less well completed on
smartphones than paper in this population.
Discussion: The TRANSFoRm approach provides an ontologically-based approach
to eSource in a low-resource, heterogeneous, highly distributed environment,
that allows precise prospective mapping of data elements in the EHR.
Conclusion: Further studies using this approach to CDISC should optimise the
delivery of PROMS, whilst building a sustainable infrastructure for eSource
with research networks, trials units and EHR vendors.
",computer-science
"  For a group $H$ and a non empty subset $\Gamma\subseteq H$, the commuting
graph $G=\mathcal{C}(H,\Gamma)$ is the graph with $\Gamma$ as the node set and
where any $x,y \in \Gamma$ are joined by an edge if $x$ and $y$ commute in $H$.
We prove that any simple graph can be obtained as a commuting graph of a
Coxeter group, solving the realizability problem in this setup. In particular
we can recover every Dynkin diagram of ADE type as a commuting graph. Thanks to
the relation between the ADE classification and finite subgroups of
$\SL(2,\C)$, we are able to rephrase results from the {\em McKay
correspondence} in terms of generators of the corresponding Coxeter groups. We
finish the paper studying commuting graphs $\mathcal{C}(H,\Gamma)$ for every
finite subgroup $H\subset\SL(2,\C)$ for different subsets $\Gamma\subseteq H$,
and investigating metric properties of them when $\Gamma=H$.
",mathematics
"  Stratum, the de-facto mining communication protocol used by blockchain based
cryptocurrency systems, enables miners to reliably and efficiently fetch jobs
from mining pool servers. In this paper we exploit Stratum's lack of encryption
to develop passive and active attacks on Bitcoin's mining protocol, with
important implications on the privacy, security and even safety of mining
equipment owners. We introduce StraTap and ISP Log attacks, that infer miner
earnings if given access to miner communications, or even their logs. We
develop BiteCoin, an active attack that hijacks shares submitted by miners, and
their associated payouts. We build BiteCoin on WireGhost, a tool we developed
to hijack and surreptitiously maintain Stratum connections. Our attacks reveal
that securing Stratum through pervasive encryption is not only undesirable (due
to large overheads), but also ineffective: an adversary can predict miner
earnings even when given access to only packet timestamps. Instead, we devise
Bedrock, a minimalistic Stratum extension that protects the privacy and
security of mining participants. We introduce and leverage the mining cookie
concept, a secret that each miner shares with the pool and includes in its
puzzle computations, and that prevents attackers from reconstructing or
hijacking the puzzles. We have implemented our attacks and collected 138MB of
Stratum protocol traffic from mining equipment in the US and Venezuela. We show
that Bedrock is resilient to active attacks even when an adversary breaks the
crypto constructs it uses. Bedrock imposes a daily overhead of 12.03s on a
single pool server that handles mining traffic from 16,000 miners.
",computer-science
"  An adversarial example is an example that has been adjusted to produce a
wrong label when presented to a system at test time. To date, adversarial
example constructions have been demonstrated for classifiers, but not for
detectors. If adversarial examples that could fool a detector exist, they could
be used to (for example) maliciously create security hazards on roads populated
with smart vehicles. In this paper, we demonstrate a construction that
successfully fools two standard detectors, Faster RCNN and YOLO. The existence
of such examples is surprising, as attacking a classifier is very different
from attacking a detector, and that the structure of detectors - which must
search for their own bounding box, and which cannot estimate that box very
accurately - makes it quite likely that adversarial patterns are strongly
disrupted. We show that our construction produces adversarial examples that
generalize well across sequences digitally, even though large perturbations are
needed. We also show that our construction yields physical objects that are
adversarial.
",computer-science
"  Quantifying and estimating wildlife population sizes is a foundation of
wildlife management. However, many carnivore species are cryptic, leading to
innate difficulties in estimating their populations. We evaluated the potential
for using more rigorous statistical models to estimate the populations of black
bears (Ursus americanus) in Wisconisin, and their applicability to other
furbearers such as bobcats (Lynx rufus). To estimate black bear populations, we
developed an AAH state-space model in a Bayesian framework based on Norton
(2015) that can account for variation in harvest and population demographics
over time. Our state-space model created an accurate estimate of the black bear
population in Wisconsin based on age-at-harvest data and improves on previous
models by using little demographic data, no auxiliary data, and not being
sensitive to initial population size. The increased accuracy of the AAH
state-space models should allow for better management by being able to set
accurate quotas to ensure a sustainable harvest for the black bear population
in each zone. We also evaluated the demography and annual harvest data of
bobcats in Wisconsin to determine trends in harvest, method, and hunter
participation in Wisconsin. Each trait of harvested bobcats that we tested
(mass, male:female sex ratio, and age) changed over time, and because these
were interrelated, and we inferred that harvest selection for larger size
biased harvests in favor of a) larger, b) older, and c) male bobcats by hound
hunters. We also found an increase in the proportion of bobcats that were
harvested by hound hunting compared to trapping, and that hound hunters were
more likely to create taxidermy mounts than trappers. We also found that
decreasing available bobcat tags and increasing success have occurred over
time, and correlate with substantially increasing both hunter populations and
hunter interest.
",quantitative-biology
"  The combination of strong correlation and emergent lattice can be achieved
when quantum gases are confined in a superradiant Fabry-Perot cavity. In
addition to the discoveries of exotic phases, such as density wave ordered Mott
insulator and superfluid, a surprising kink structure is found in the slope of
the cavity strength as a function of the pumping strength. In this Letter, we
show that the appearance of such a kink is a manifestation of a liquid-gas like
transition between two superfluids with different densities. The slopes in the
immediate neighborhood of the kink become divergent at the liquid-gas critical
points and display a critical scaling law with a critical exponent 1 in the
quantum critical region. Our predictions could be tested in current
experimental set-up.
",physics
"  {\it Victory}, i.e. \underline{vi}enna \underline{c}omputational
\underline{to}ol deposito\underline{ry}, is a collection of numerical tools for
solving the parquet equations for the Hubbard model and similar many body
problems. The parquet formalism is a self-consistent theory at both the single-
and two-particle levels, and can thus describe individual fermions as well as
their collective behavior on equal footing. This is essential for the
understanding of various emergent phases and their transitions in many-body
systems, in particular for cases in which a single-particle description fails.
Our implementation of {\it victory} is in modern Fortran and it fully respects
the structure of various vertex functions in both momentum and Matsubara
frequency space. We found the latter to be crucial for the convergence of the
parquet equations, as well as for the correct determination of various physical
observables. In this release, we thoroughly explain the program structure and
the controlled approximations to efficiently solve the parquet equations, i.e.
the two-level kernel approximation and the high-frequency regulation.
",physics
"  When a vortex refracts surface waves, the momentum flux carried by the waves
changes direction and the waves induce a reaction force on the vortex. We study
experimentally the resulting vortex distortion. Incoming surface gravity waves
impinge on a steady vortex of velocity $U_0$ driven magneto-hydrodynamically at
the bottom of a fluid layer. The waves induce a shift of the vortex center in
the direction transverse to wave propagation, together with a decrease in
surface vorticity. We interpret these two phenomena in the framework introduced
by Craik and Leibovich (1976): we identify the dimensionless Stokes drift
$S=U_s/U_0$ as the relevant control parameter, $U_s$ being the Stokes drift
velocity of the waves. We propose a simple vortex line model which indicates
that the shift of the vortex center originates from a balance between vorticity
advection by the Stokes drift and self-advection of the vortex. The decrease in
surface vorticity is interpreted as a consequence of vorticity expulsion by the
fast Stokes drift, which confines it at depth. This purely hydrodynamic process
is analogous to the magnetohydrodynamic expulsion of magnetic field by a
rapidly moving conductor through the electromagnetic skin effect. We study
vorticity expulsion in the limit of fast Stokes drift and deduce that the
surface vorticity decreases as $1/S$, a prediction which is compatible with the
experimental data. Such wave-induced vortex distortions have important
consequences for the nonlinear regime of wave refraction: the refraction angle
rapidly decreases with wave intensity.
",physics
"  In this paper, we prove pointwise convergence of heat kernels for
mGH-convergent sequences of $RCD^*(K,N)$-spaces. We obtain as a corollary
results on the short-time behavior of the heat kernel in $RCD^*(K,N)$-spaces.
We use then these results to initiate the study of Weyl's law in the $RCD$
setting
",mathematics
"  Heart disease is one of leading causes of mortality worldwide. Healthy heart
valves are key for proper heart function. When these valves dysfunction, a
replacement is often necessary in severe cases. The current study presents an
investigation of the pulsatile blood flow through a bileaflet mechanical heart
valve (BMHV) where one leaflet is healthy and can fully open and the other
leaflet cannot fully open with different levels of dysfunction. To better
understand the implications that a dysfunctional leaflet has on the blood flow
through these valves, analysis of flow characteristics such as velocity,
pressure drop, wall shear stress and vorticity profiles was performed. Results
suggested that leaflet dysfunction caused increased local velocities,
separation regions and wall shear stresses. For example, the maximum velocity
increased from 2.53 m/s to 4.9 m/s when dysfunction increased from 0% to 100%.
The pressure drop increased (by up to 300%) with dysfunctionality. Results
suggested that leaflet dysfunction also caused increased wall shear stresses on
the valve frame where higher stresses developed around the hinges (at 75% and
100% dysfunctions). Analysis also showed that increased dysfunctionality of one
leaflet led to higher net shear forces on both the healthy and dysfunctional
leaflets (by up to 200% and 600%, respectively).
",physics
"  The purpose of this note is to point out that simplicial methods and the
well-known Dold-Kan construction in simplicial homotopy theory can be
fruitfully applied to convert link homology theories into homotopy theories.
Dold and Kan prove that there is a functor from the category of chain complexes
over a commutative ring with unit to the category of simplicial objects over
that ring such that chain homotopic maps go to homotopic maps in the simplicial
category. Furthermore, this is an equivalence of categories. In this way, given
a link homology theory, we construct a mapping taking link diagrams to a
category of simplicial objects such that up to looping or delooping, link
diagrams related by Reidemeister moves will give rise to homotopy equivalent
simplicial objects, and the homotopy groups of these objects will be equal to
the link homology groups of the original link homology theory. The construction
is independent of the particular link homology theory. A simplifying point in
producing a homotopy simplicial object in relation to a chain complex occurs
when the chain complex is itself derived (via face maps) from a simplicial
object that satisfies the Kan extension condition. Under these circumstances
one can use that simplicial object rather than apply the Dold-Kan functor to
the chain complex. We will give examples of this situation in regard to
Khovanov homology. We will investigate detailed working out of this
correspondence in separate papers. The purpose of this note is to announce the
basic relationships for using simplicial methods in this domain. Thus we do
more than just quote the Dold-Kan Theorem. We give a review of simplicial
theory and we point to specific constructions, particularly in relation to
Khovanov homology, that can be used to make simplicial homotopy types directly.
",mathematics
"  Given two independent sets $I, J$ of a graph $G$, and imagine that a token
(coin) is placed at each vertex of $I$. The Sliding Token problem asks if one
could transform $I$ to $J$ via a sequence of elementary steps, where each step
requires sliding a token from one vertex to one of its neighbors so that the
resulting set of vertices where tokens are placed remains independent. This
problem is $\mathsf{PSPACE}$-complete even for planar graphs of maximum degree
$3$ and bounded-treewidth. In this paper, we show that Sliding Token can be
solved efficiently for cactus graphs and block graphs, and give upper bounds on
the length of a transformation sequence between any two independent sets of
these graph classes. Our algorithms are designed based on two main
observations. First, all structures that forbid the existence of a sequence of
token slidings between $I$ and $J$, if exist, can be found in polynomial time.
A sufficient condition for determining no-instances can be easily derived using
this characterization. Second, without such forbidden structures, a sequence of
token slidings between $I$ and $J$ does exist. In this case, one can indeed
transform $I$ to $J$ (and vice versa) using a polynomial number of
token-slides.
",computer-science
"  A model of incentive salience as a function of stimulus value and
interoceptive state has been previously proposed. In that model, the function
differs depending on whether the stimulus is appetitive or aversive; it is
multiplicative for appetitive stimuli and additive for aversive stimuli. The
authors argued it was necessary to capture data on how extreme changes in salt
appetite could move evaluation of an extreme salt solution from negative to
positive. We demonstrate that arbitrarily varying this function is unnecessary,
and that a multiplicative function is sufficient if one assumes the incentive
salience function for an incentive (such as salt) is comprised of multiple
stimulus features and multiple interoceptive signals. We show that it is also
unnecessary considering the dual-structure approach-aversive nature of the
reward system, which results in separate weighting of appetitive and aversive
stimulus features.
",quantitative-biology
"  We give a new proof of the strong Arnold conjecture for $1$-periodic
solutions of Hamiltonian systems on tori, that was first shown by C. Conley and
E. Zehnder in 1983. Our proof uses other methods and is shorter than the
previous one. We first show that the $E$-cohomological Conley index, that was
introduced by the first author recently, has a natural module structure. This
yields a new cup-length and a lower bound for the number of critical points of
functionals. Then an existence result for the $E$-cohomological Conley index,
which applies to the setting of the Arnold conjecture, paves the way to a new
proof of it on tori.
",mathematics
"  Fast-declining Type Ia supernovae (SN Ia) separate into two categories based
on their bolometric and near-infrared (NIR) properties. The peak bolometric
luminosity ($\mathrm{L_{max}}$), the phase of the first maximum relative to the
optical, the NIR peak luminosity and the occurrence of a second maximum in the
NIR distinguish a group of very faint SN Ia. Fast-declining supernovae show a
large range of peak bolometric luminosities ($\mathrm{L_{max}}$ differing by up
to a factor of $\sim$ 8). All fast-declining SN Ia with $\mathrm{L_{max}} < 0.3
\cdot$ 10$^{43}\mathrm{erg s}^{-1}$ are spectroscopically classified as
91bg-like and show only a single NIR peak. SNe with $\mathrm{L_{max}} > 0.5
\cdot$ 10$^{43}\mathrm{erg s}^{-1}$ appear to smoothly connect to normal SN Ia.
The total ejecta mass (M$_{ej}$) values for SNe with enough late time data are
$\lesssim$1 $M_{\odot}$, indicating a sub-Chandrasekhar mass progenitor for
these SNe.
",physics
"  Recent technological development has enabled researchers to study social
phenomena scientifically in detail and financial markets has particularly
attracted physicists since the Brownian motion has played the key role as in
physics. In our previous report (arXiv:1703.06739; to appear in Phys. Rev.
Lett.), we have presented a microscopic model of trend-following high-frequency
traders (HFTs) and its theoretical relation to the dynamics of financial
Brownian motion, directly supported by a data analysis of tracking trajectories
of individual HFTs in a financial market. Here we show the mathematical
foundation for the HFT model paralleling to the traditional kinetic theory in
statistical physics. We first derive the time-evolution equation for the
phase-space distribution for the HFT model exactly, which corresponds to the
Liouville equation in conventional analytical mechanics. By a systematic
reduction of the Liouville equation for the HFT model, the
Bogoliubov-Born-Green-Kirkwood-Yvon hierarchal equations are derived for
financial Brownian motion. We then derive the Boltzmann-like and Langevin-like
equations for the order-book and the price dynamics by making the assumption of
molecular chaos. The qualitative behavior of the model is asymptotically
studied by solving the Boltzmann-like and Langevin-like equations for the large
number of HFTs, which is numerically validated through the Monte-Carlo
simulation. Our kinetic description highlights the parallel mathematical
structure between the financial Brownian motion and the physical Brownian
motion.
",quantitative-finance
"  We present a family of mutually orthogonal polynomials on the unit ball with
respect to an inner product which includes a mass uniformly distributed on the
sphere. First, connection formulas relating these multivariate orthogonal
polynomials and the classical ball polynomials are obtained. Then, using the
representation formula for these polynomials in terms of spherical harmonics
analytic properties will be deduced. Finally, we analyze the asymptotic
behaviour of the Christoffel functions.
",mathematics
"  We prove that for any dimension function $h$ with $h \prec x^d$ and for any
countable set of linear patterns, there exists a compact set $E$ with
$\mathcal{H}^h(E)>0$ avoiding all the given patterns. We also give several
applications and recover results of Keleti, Maga, and Máthé.
",mathematics
"  Proteins are only moderately stable. It has long been debated whether this
narrow range of stabilities is solely a result of neutral drift towards lower
stability or purifying selection against excess stability is also at work - for
which no experimental evidence was found so far. Here we show that mutations
outside the active site in the essential E. coli enzyme adenylate kinase result
in stability-dependent increase in substrate inhibition by AMP, thereby
impairing overall enzyme activity at high stability. Such inhibition caused
substantial fitness defects not only in the presence of excess substrate but
also under physiological conditions. In the latter case, substrate inhibition
caused differential accumulation of AMP in the stationary phase for the
inhibition prone mutants. Further, we show that changes in flux through Adk
could accurately describe the variation in fitness effects. Taken together,
these data suggest that selection against substrate inhibition and hence excess
stability may have resulted in a narrow range of optimal stability observed for
modern proteins.
",quantitative-biology
"  In this paper, we consider the final state problem for the nonlinear
Schrödinger equation with a homogeneous nonlinearity of the critical order
which is not necessarily a polynomial. In [10], the first and the second
authors consider one- and two-dimensional cases and gave a sufficient condition
on the nonlinearity for that the corresponding equation admits a solution that
behaves like a free solution with or without a logarithmic phase correction.
The present paper is devoted to the study of the three-dimensional case, in
which it is required that a solution converges to a given asymptotic profile in
a faster rate than in the lower dimensional cases. To obtain the necessary
convergence rate, we employ the end-point Strichartz estimate and modify a
time-dependent regularizing operator, introduced in [10]. Moreover, we present
a candidate of the second asymptotic profile to the solution.
",mathematics
"  We present a state interaction spin-orbit coupling method to calculate
electron paramagnetic resonance (EPR) $g$-tensors from density matrix
renormalization group wavefunctions. We apply the technique to compute
$g$-tensors for the \ce{TiF3} and \ce{CuCl4^2-} complexes, a [2Fe-2S] model of
the active center of ferredoxins, and a \ce{Mn4CaO5} model of the S2 state of
the oxygen evolving complex. These calculations raise the prospects of
determining $g$-tensors in multireference calculations with a large number of
open shells.
",physics
"  We give the sharp conditions for boundedness of Hausdorff operators on
certain modulation and Wiener amalgam spaces.
",mathematics
"  We propose an exploration method that incorporates look-ahead search over
basic learnt skills and their dynamics, and use it for reinforcement learning
(RL) of manipulation policies . Our skills are multi-goal policies learned in
isolation in simpler environments using existing multigoal RL formulations,
analogous to options or macroactions. Coarse skill dynamics, i.e., the state
transition caused by a (complete) skill execution, are learnt and are unrolled
forward during lookahead search. Policy search benefits from temporal
abstraction during exploration, though itself operates over low-level primitive
actions, and thus the resulting policies does not suffer from suboptimality and
inflexibility caused by coarse skill chaining. We show that the proposed
exploration strategy results in effective learning of complex manipulation
policies faster than current state-of-the-art RL methods, and converges to
better policies than methods that use options or parametrized skills as
building blocks of the policy itself, as opposed to guiding exploration. We
show that the proposed exploration strategy results in effective learning of
complex manipulation policies faster than current state-of-the-art RL methods,
and converges to better policies than methods that use options or parameterized
skills as building blocks of the policy itself, as opposed to guiding
exploration.
",computer-science
"  It is becoming increasingly clear that complex interactions among genes and
environmental factors play crucial roles in triggering complex diseases. Thus,
understanding such interactions is vital, which is possible only through
statistical models that adequately account for such intricate, albeit unknown,
dependence structures. Bhattacharya & Bhattacharya (2016b) attempt such
modeling, relating finite mixtures composed of Dirichlet processes that
represent unknown number of genetic sub-populations through a hierarchical
matrix-normal structure that incorporates gene-gene interactions, and possible
mutations, induced by environmental variables. However, the product dependence
structure implied by their matrix-normal model seems to be too simple to be
appropriate for general complex, realistic situations. In this article, we
propose and develop a novel nonparametric Bayesian model for case-control
genotype data using hierarchies of Dirichlet processes that offers a more
realistic and nonparametric dependence structure between the genes, induced by
the environmental variables. In this regard, we propose a novel and highly
parallelisable MCMC algorithm that is rendered quite efficient by the
combination of modern parallel computing technology, effective Gibbs sampling
steps, retrospective sampling and Transformation based Markov Chain Monte Carlo
(TMCMC). We use appropriate Bayesian hypothesis testing procedures to detect
the roles of genes and environment in case-control studies. We apply our ideas
to 5 biologically realistic case-control genotype datasets simulated under
distinct set-ups, and obtain encouraging results in each case. We finally apply
our ideas to a real, myocardial infarction dataset, and obtain interesting
results on gene-gene and gene-environment interaction, while broadly agreeing
with the results reported in the literature.
",statistics
"  A quasi-relativistic two-component approach for an efficient calculation of
$\mathcal{P,T}$-odd interactions caused by a permanent electric dipole moment
of the electron (eEDM) is presented. The approach uses a (two-component)
complex generalized Hartree-Fock (cGHF) and a complex generalized Kohn-Sham
(cGKS) scheme within the zeroth order regular approximation (ZORA). In
applications to select heavy-elemental polar diatomic molecular radicals, which
are promising candidates for an eEDM experiment, the method is compared to
relativistic four-component electron-correlation calculations and confirms
values for the effective electrical field acting on the unpaired electron for
RaF, BaF, YbF and HgF. The calculations show that purely relativistic effects,
involving only the lower component of the Dirac bi-spinor, are well described
by treating only the upper component explicitly.
",physics
"  In this note we describe how some objects from generalized geometry appear in
the qualitative analysis and numerical simulation of mechanical systems. In
particular we discuss double vector bundles and Dirac structures. It turns out
that those objects can be naturally associated to systems with constraints --
we recall the mathematical construction in the context of so called implicit
Lagrangian systems. We explain how they can be used to produce new numerical
methods, that we call Dirac integrators.
On a test example of a simple pendulum in a gravity field we compare the
Dirac integrators with classical explicit and implicit methods, we pay special
attention to conservation of constrains. Then, on a more advanced example of
the Ziegler column we show that the choice of numerical methods can indeed
affect the conclusions of qualitative analysis of the dynamics of mechanical
systems. We also tell why we think that Dirac integrators are appropriate for
this kind of systems by explaining the relation with the notions of geometric
degree of non-conservativity and kinematic structural stability.
",computer-science
"  In this paper, we investigate the behavior of the thermoelectric DC
conductivities in the presence of Weyl corrections with momentum dissipation in
the incoherent limit. Moreover, we compute the butterfly velocity and study the
charge and energy diffusion with broken translational symmetry. Our results
show that the Weyl coupling $\gamma$, violates the bounds on the charge and
energy diffusivity. It is also shown that the Weyl corrections violate the
bound on the DC electrical conductivity in the incoherent limit.
",physics
"  We consider Schrödinger operators with periodic potentials in the positive
quadrant for dim $>1$ with Dirichlet boundary condition. We show that for any
integer $N$ and any interval $I$ there exists a periodic potential such that
the Schrödinger operator has $N$ eigenvalues counted with the multiplicity on
this interval and there is no other spectrum on the interval. Furthermore, to
the right and to the left of it there is a essential spectrum.
Moreover, we prove similar results for Schrödinger operators for other
domains. The proof is based on the inverse spectral theory for Hill operators
on the real line.
",mathematics
"  Recurrent Neural Networks (RNNs) with sophisticated units that implement a
gating mechanism have emerged as powerful technique for modeling sequential
signals such as speech or electroencephalography (EEG). The latter is the focus
on this paper. A significant big data resource, known as the TUH EEG Corpus
(TUEEG), has recently become available for EEG research, creating a unique
opportunity to evaluate these recurrent units on the task of seizure detection.
In this study, we compare two types of recurrent units: long short-term memory
units (LSTM) and gated recurrent units (GRU). These are evaluated using a state
of the art hybrid architecture that integrates Convolutional Neural Networks
(CNNs) with RNNs. We also investigate a variety of initialization methods and
show that initialization is crucial since poorly initialized networks cannot be
trained. Furthermore, we explore regularization of these convolutional gated
recurrent networks to address the problem of overfitting. Our experiments
revealed that convolutional LSTM networks can achieve significantly better
performance than convolutional GRU networks. The convolutional LSTM
architecture with proper initialization and regularization delivers 30%
sensitivity at 6 false alarms per 24 hours.
",statistics
"  We describe the neutrino flavor (e = electron, u = muon, t = tau) masses as
m(i=e;u;t)= m + [Delta]mi with |[Delta]mij|/m < 1 and probably |[Delta]mij|/m
<< 1. The quantity m is the degenerate neutrino mass. Because neutrino flavor
is not a quantum number, this degenerate mass appears in the neutrino equation
of state. We apply a Monte Carlo computational physics technique to the Local
Group (LG) of galaxies to determine an approximate location for a Dark Matter
embedding condensed neutrino object(CNO). The calculation is based on the
rotational properties of the only spiral galaxies within the LG: M31, M33 and
the Milky Way. CNOs could be the Dark Matter everyone is looking for and we
estimate the CNO embedding the LG to have a mass 5.17x10^15 Mo and a radius
1.316 Mpc, with the estimated value of m ~= 0.8 eV/c2. The up-coming KATRIN
experiment will either be the definitive result or eliminate condensed
neutrinos as a Dark Matter candidate.
",physics
"  In this paper, we demonstrate the connection between a magnetic storage ring
with additional sextupole fields set so that the x and y chromaticities vanish
and the maximizing of the lifetime of in-plane polarization (IPP) for a
0.97-GeV/c deuteron beam. The IPP magnitude was measured by continuously
monitoring the down-up scattering asymmetry (sensitive to sideways
polarization) in an in-beam, carbon-target polarimeter and unfolding the
precession of the IPP due to the magnetic anomaly of the deuteron. The optimum
operating conditions for a long IPP lifetime were made by scanning the field of
the storage ring sextupole magnet families while observing the rate of IPP loss
during storage of the beam. The beam was bunched and electron cooled. The IPP
losses appear to arise from the change of the orbit circumference, and
consequently the particle speed and spin tune, due to the transverse betatron
oscillations of individual particles in the beam. The effects of these changes
are canceled by an appropriate sextupole field setting.
",physics
"  Layer normalization is a recently introduced technique for normalizing the
activities of neurons in deep neural networks to improve the training speed and
stability. In this paper, we introduce a new layer normalization technique
called Dynamic Layer Normalization (DLN) for adaptive neural acoustic modeling
in speech recognition. By dynamically generating the scaling and shifting
parameters in layer normalization, DLN adapts neural acoustic models to the
acoustic variability arising from various factors such as speakers, channel
noises, and environments. Unlike other adaptive acoustic models, our proposed
approach does not require additional adaptation data or speaker information
such as i-vectors. Moreover, the model size is fixed as it dynamically
generates adaptation parameters. We apply our proposed DLN to deep
bidirectional LSTM acoustic models and evaluate them on two benchmark datasets
for large vocabulary ASR experiments: WSJ and TED-LIUM release 2. The
experimental results show that our DLN improves neural acoustic models in terms
of transcription accuracy by dynamically adapting to various speakers and
environments.
",computer-science
"  Software-driven cloud networking is a new paradigm in orchestrating physical
resources (CPU, network bandwidth, energy, storage) allocated to network
functions, services, and applications, which is commonly modeled as a
cross-layer network. This model carries a physical network representing the
physical infrastructure, a logical network showing demands, and
logical-to-physical node/link mappings. In such networks, a single failure in
the physical network may trigger cascading failures in the logical network and
disable network services and connectivity. In this paper, we propose an
evaluation metric, survivable probability, to evaluate the reliability of such
networks under random physical link failure(s). We propose the concept of base
protecting spanning tree and prove the necessary and sufficient conditions for
its existence and relation to survivability. We then develop mathematical
programming formulations for reliable cross-layer network routing design with
the maximal reliable probability. Computation results demonstrate the viability
of our approach.
",computer-science
"  Walking quadruped robots face challenges in positioning their feet and
lifting their legs during gait cycles over uneven terrain. The robot Laika is
under development as a quadruped with a flexible, actuated spine designed to
assist with foot movement and balance during these gaits. This paper presents
the first set of hardware designs for the spine of Laika, a physical prototype
of those designs, and tests in both hardware and simulations that show the
prototype's capabilities. Laika's spine is a tensegrity structure, used for its
advantages with weight and force distribution, and represents the first working
prototype of a tensegrity spine for a quadruped robot. The spine bends by
adjusting the lengths of the cables that separate its vertebrae, and twists
using an actuated rotating vertebra at its center. The current prototype of
Laika has stiff legs attached to the spine, and is used as a test setup for
evaluation of the spine itself. This work shows the advantages of Laika's spine
by demonstrating the spine lifting each of the robot's four feet, both as a
form of balancing and as a precursor for a walking gait. These foot motions,
using specific combinations of bending and rotation movements of the spine, are
measured in both simulation and hardware experiments. Hardware data are used to
calibrate the simulations, such that the simulations can be used for control of
balancing or gait cycles in the future. Future work will attach actuated legs
to Laika's spine, and examine balancing and gait cycles when combined with leg
movements.
",computer-science
"  In recent works we have constructed axisymmetric solutions to the
Euler-Poisson equations which give mathematical models of slowly uniformly
rotating gaseous stars. We try to extend this result to the study of solutions
of the Einstein-Euler equations in the framework of the general theory of
relativity. Although many interesting studies have been done about axisymmetric
metric in the general theory of relativity, they are restricted to the region
of the vacuum. Mathematically rigorous existence theorem of the axisymmetric
interior solutions of the stationary metric corresponding to the
energy-momentum tensor of the perfect fluid with non-zero pressure may be not
yet established until now except only one found in the pioneering work by U.
Heilig done in 1993. In this article, along a different approach to that of
Heilig's work, axisymmetric stationary solutions of the Einstein-Euler
equations are constructed near those of the Euler-Poisson equations when the
speed of light is sufficiently large in the considered system of units, or,
when the gravitational field is sufficiently weak.
",mathematics
"  Using the ""enthalpy-based thermal evolution of loops"" (EBTEL) model, we
investigate the hydrodynamics of the plasma in a flaring coronal loop in which
heat conduction is limited by turbulent scattering of the electrons that
transport the thermal heat flux. The EBTEL equations are solved analytically in
each of the two (conduction-dominated and radiation-dominated) cooling phases.
Comparison of the results with typical observed cooling times in solar flares
shows that the turbulent mean free-path $\lambda_T$ lies in a range
corresponding to a regime in which classical (collision-dominated) conduction
plays at most a limited role. We also consider the magnitude and duration of
the heat input that is necessary to account for the enhanced values of
temperature and density at the beginning of the cooling phase and for the
observed cooling times. We find through numerical modeling that in order to
produce a peak temperature $\simeq 1.5 \times 10^7$~K and a 200~s cooling time
consistent with observations, the flare heating profile must extend over a
significant period of time; in particular, its lingering role must be taken
into consideration in any description of the cooling phase. Comparison with
observationally-inferred values of post-flare loop temperatures, densities, and
cooling times thus leads to useful constraints on both the magnitude and
duration of the magnetic energy release in the loop, as well as on the value of
the turbulent mean free-path $\lambda_T$.
",physics
"  The Markoff group of transformations is a group $\Gamma$ of affine integral
morphisms, which is known to act transitively on the set of all positive
integer solutions to the equation $x^{2}+y^{2}+z^{2}=xyz$. The fundamental
strong approximation conjecture for the Markoff equation states that for every
prime $p$, the group $\Gamma$ acts transitively on the set
$X^{*}\left(p\right)$ of non-zero solutions to the same equation over
$\mathbb{Z}/p\mathbb{Z}$. Recently, Bourgain, Gamburd and Sarnak proved this
conjecture for all primes outside a small exceptional set.
In the current paper, we study a group of permutations obtained by the action
of $\Gamma$ on $X^{*}\left(p\right)$, and show that for most primes, it is the
full symmetric or alternating group. We use this result to deduce that $\Gamma$
acts transitively also on the set of non-zero solutions in a big class of
composite moduli.
Our result is also related to a well-known theorem of Gilman, stating that
for any finite non-abelian simple group $G$ and $r\ge3$, the group
$\mathrm{Aut}\left(F_{r}\right)$ acts on at least one $T_{r}$-system of $G$ as
the alternating or symmetric group. In this language, our main result
translates to that for most primes $p$, the group
$\mathrm{Aut}\left(F_{2}\right)$ acts on a particular $T_{2}$-system of
$\mathrm{PSL}\left(2,p\right)$ as the alternating or symmetric group.
",mathematics
"  We present a dataset with models of 14 articulated objects commonly found in
human environments and with RGB-D video sequences and wrenches recorded of
human interactions with them. The 358 interaction sequences total 67 minutes of
human manipulation under varying experimental conditions (type of interaction,
lighting, perspective, and background). Each interaction with an object is
annotated with the ground truth poses of its rigid parts and the kinematic
state obtained by a motion capture system. For a subset of 78 sequences (25
minutes), we also measured the interaction wrenches. The object models contain
textured three-dimensional triangle meshes of each link and their motion
constraints. We provide Python scripts to download and visualize the data. The
data is available at this https URL and hosted
at this https URL.
",computer-science
"  Resonant x-ray scattering at the Dy $M_5$ and Ni $L_3$ absorption edges was
used to probe the temperature and magnetic field dependence of magnetic order
in epitaxial LaNiO$_3$-DyScO$_3$ superlattices. For superlattices with 2 unit
cell thick LaNiO$_3$ layers, a commensurate spiral state develops in the Ni
spin system below 100 K. Upon cooling below $T_{ind} = 18$ K, Dy-Ni exchange
interactions across the LaNiO$_3$-DyScO$_3$ interfaces induce collinear
magnetic order of interfacial Dy moments as well as a reorientation of the Ni
spins to a direction dictated by the strong magneto-crystalline anisotropy of
Dy. This transition is reversible by an external magnetic field of 3 T.
Tailored exchange interactions between rare-earth and transition-metal ions
thus open up new perspectives for the manipulation of spin structures in
metal-oxide heterostructures and devices.
",physics
"  The monomorphism category $\mathscr{S}(A, M, B)$ induced by a bimodule
$_AM_B$ is the subcategory of $\Lambda$-mod consisting of
$\left[\begin{smallmatrix} X\\ Y\end{smallmatrix}\right]_{\phi}$ such that
$\phi: M\otimes_B Y\rightarrow X$ is a monic $A$-map, where
$\Lambda=\left[\begin{smallmatrix} A&M\\0&B \end{smallmatrix}\right]$. In
general, it is not the monomorphism categories induced by quivers. It could
describe the Gorenstein-projective $\m$-modules. This monomorphism category is
a resolving subcategory of $\modcat{\Lambda}$ if and only if $M_B$ is
projective. In this case, it has enough injective objects and Auslander-Reiten
sequences, and can be also described as the left perpendicular category of a
unique basic cotilting $\Lambda$-module. If $M$ satisfies the condition ${\rm
(IP)}$, then the stable category of $\mathscr{S}(A, M, B)$ admits a recollement
of additive categories, which is in fact a recollement of singularity
categories if $\mathscr{S}(A, M, B)$ is a {\rm Frobenius} category.
Ringel-Schmidmeier-Simson equivalence between $\mathscr{S}(A, M, B)$ and its
dual is introduced. If $M$ is an exchangeable bimodule, then an {\rm RSS}
equivalence is given by a $\Lambda$-$\Lambda$ bimodule which is a two-sided
cotilting $\Lambda$-module with a special property; and the Nakayama functor
$\mathcal N_\m$ gives an {\rm RSS} equivalence if and only if both $A$ and $B$
are Frobenius algebras.
",mathematics
"  Estimating covariances between financial assets plays an important role in
risk management and optimal portfolio allocation. In practice, when the sample
size is small compared to the number of variables, i.e. when considering a wide
universe of assets over just a few years, this poses considerable challenges
and the empirical estimate is known to be very unstable.
Here, we propose a novel covariance estimator based on the Gaussian Process
Latent Variable Model (GP-LVM). Our estimator can be considered as a non-linear
extension of standard factor models with readily interpretable parameters
reminiscent of market betas. Furthermore, our Bayesian treatment naturally
shrinks the sample covariance matrix towards a more structured matrix given by
the prior and thereby systematically reduces estimation errors.
",quantitative-finance
"  We develop an ac-biased shift register introduced in our previous work (V.K.
Semenov et al., IEEE Trans. Appl. Supercond., vol. 25, no. 3, 1301507, June
2015) into a benchmark circuit for evaluation of superconductor electronics
fabrication technology. The developed testing technique allows for extracting
margins of all individual cells in the shift register, which in turn makes it
possible to estimate statistical distribution of Josephson junctions in the
circuit. We applied this approach to successfully test registers having 8, 16,
36, and 202 thousand cells and, respectively, about 33000, 65000, 144000, and
809000 Josephson junctions. The circuits were fabricated at MIT Lincoln
Laboratory, using a fully planarized process, 0.4 {\mu}m inductor linewidth,
and 1.33x10^6 cm^-2 junction density. They are presently the largest
operational superconducting SFQ circuits ever made. The developed technique
distinguishes between hard defects (fabrication-related) and soft defects
(measurement-related) and locates them in the circuit. The soft defects are
specific to superconducting circuits and caused by magnetic flux trapping
either inside the active cells or in the dedicated flux-trapping moats near the
cells. The number and distribution of soft defects depend on the ambient
magnetic field and vary with thermal cycling even if done in the same magnetic
environment.
",physics
"  Modern operating systems such as Android, iOS, Windows Phone, and Chrome OS
support a cooperating program abstraction. Instead of placing all functionality
into a single program, programs cooperate to complete tasks requested by users.
However, untrusted programs may exploit interactions with other programs to
obtain unauthorized access to system sensors either directly or through
privileged services. Researchers have proposed that programs should only be
authorized to access system sensors on a user-approved input event, but these
methods do not account for possible delegation done by the program receiving
the user input event. Furthermore, proposed delegation methods do not enable
users to control the use of their input events accurately. In this paper, we
propose ENTRUST, a system that enables users to authorize sensor operations
that follow their input events, even if the sensor operation is performed by a
program different from the program receiving the input event. ENTRUST tracks
user input as well as delegation events and restricts the execution of such
events to compute unambiguous delegation paths to enable accurate and reusable
authorization of sensor operations. To demonstrate this approach, we implement
the ENTRUST authorization system for Android. We find, via a laboratory user
study, that attacks can be prevented at a much higher rate (54-64%
improvement); and via a field user study, that ENTRUST requires no more than
three additional authorizations per program with respect to the first-use
approach, while incurring modest performance (<1%) and memory overheads (5.5 KB
per program).
",computer-science
"  Classification is one of the widely used analytical techniques in data
science domain across different business to associate a pattern which
contribute to the occurrence of certain event which is predicted with some
likelihood. This Paper address a lacuna of creating some time window before the
prediction actually happen to enable organizations some space to act on the
prediction. There are some really good state of the art machine learning
techniques to optimally identify the possible churners in either customer base
or employee base, similarly for fault prediction too if the prediction does not
come with some buffer time to act on the fault it is very difficult to provide
a seamless experience to the user. New concept of reference frame creation is
introduced to solve this problem in this paper
",statistics
"  We present experimental results on the controlled de-excitation of Rydberg
states in a cold gas of Rb atoms. The effect of the van der Waals interactions
between the Rydberg atoms is clearly seen in the de-excitation spectrum and
dynamics. Our observations are confirmed by numerical simulations. In
particular, for off-resonant (facilitated) excitation we find that the
de-excitation spectrum reflects the spatial arrangement of the atoms in the
quasi one-dimensional geometry of our experiment. We discuss future
applications of this technique and implications for detection and controlled
dissipation schemes.
",physics
"  This paper develops an online inverse reinforcement learning algorithm aimed
at efficiently recovering a reward function from ongoing observations of an
agent's actions. To reduce the computation time and storage space in reward
estimation, this work assumes that each observed action implies a change of the
Q-value distribution, and relates the change to the reward function via the
gradient of Q-value with respect to reward function parameter. The gradients
are computed with a novel Bellman Gradient Iteration method that allows the
reward function to be updated whenever a new observation is available. The
method's convergence to a local optimum is proved.
This work tests the proposed method in two simulated environments, and
evaluates the algorithm's performance under a linear reward function and a
non-linear reward function. The results show that the proposed algorithm only
requires a limited computation time and storage space, but achieves an
increasing accuracy as the number of observations grows. We also present a
potential application to robot cleaners at home.
",computer-science
"  We describe the MERger-event Gamma-Ray (MERGR) Telescope intended for
deployment by ~2021. MERGR will cover from 20 keV to 2 MeV with a wide field of
view (6 sr) using nineteen gamma-ray detectors arranged on a section of a
sphere. The telescope will work as a standalone system or as part of a network
of sensors, to increase by ~50% the current sky coverage to detect short
Gamma-Ray Burst (SGRB) counterparts to neutron-star binary mergers within the
~200 Mpc range of gravitational wave detectors in the early 2020's. Inflight
software will provide realtime burst detections with mean localization
uncertainties of 6 deg for a photon fluence of 5 ph cm^-2 (the mean fluence of
Fermi-GBM SGRBs) and <3 deg for the brightest ~5% of SGRBs to enable rapid
multi-wavelength follow-up to identify a host galaxy and its redshift. To
minimize cost and time to first light, MERGR is directly derived from
demonstrators designed and built at NRL for the DoD Space Test Program (STP).
We argue that the deployment of a network that provides all-sky coverage for
SGRB detection is of immediate urgency to the multi-messenger astrophysics
community.
",physics
"  The momentum conservation law is applied to analyse the dynamics of pulsejet
engine in vertical motion in a uniform gravitational field in the absence of
friction. The model predicts existence of a terminal speed given frequency of
the short pulses. The conditions that the engine does not return to the
starting position are identified. The number of short periodic pulses after
which the engine returns to the starting position is found to be independent of
the exhaust velocity and gravitational field intensity for certain frequency of
the pulses. The pulsejet engine and turbojet engine aircraft models of dynamics
are compared. Also the octopus dynamics is modelled. The paper is addressed to
intermediate undergraduate students of classical mechanics and aerospace
engineering.
",physics
"  Precipitation hardening, which relies on a high density of intermetallic
precipitates, is a commonly utilized technique for strengthening structural
alloys. Structural alloys are commonly strengthened through a high density of
small size intermetallic precipitates. At high temperatures, however, the
precipitates coarsen to reduce the excess energy of the interface, resulting in
a significant reduction in the strengthening provided by the precipitates. In
certain ternary alloys, the secondary solute segregates to the interface and
results in the formation of a high density of nanosize precipitates that
provide enhanced strength and are resistant to coarsening. To understand the
chemical effects involved, and to identify such systems, we develop a
thermodynamic model using the framework of the regular nanocrystalline solution
model. For various global compositions, temperatures and thermodynamic
parameters, equilibrium configuration of Mg-Sn-Zn alloy is evaluated by
minimizing the Gibbs free energy function with respect to the region-specific
(bulk solid-solution, interface and precipitate) concentrations and sizes. The
results show that Mg$_2$Sn precipitates can be stabilized to nanoscale sizes
through Zn segregation to Mg/Mg$_2$Sn interface, and the precipitates can be
stabilized against coarsening at high-temperatures by providing a larger Zn
concentration in the system. Together with the inclusion of elastic strain
energy effects and the input of computationally informed interface
thermodynamic parameters in the future, the model is expected to provide a more
realistic prediction of segregation and precipitate stabilization in ternary
alloys of structural importance.
",physics
"  This paper studies problems on locally stopping distributed consensus
algorithms over networks where each node updates its state by interacting with
its neighbors and decides by itself whether certain level of agreement has been
achieved among nodes. Since an individual node is unable to access the states
of those beyond its neighbors, this problem becomes challenging. In this work,
we first define the stopping problem for generic distributed algorithms. Then,
a distributed algorithm is explicitly provided for each node to stop consensus
updating by exploring the relationship between the so-called local and global
consensus. Finally, we show both in theory and simulation that its
effectiveness depends both on the network size and the structure.
",computer-science
"  Using theorems of Eliashberg and McDuff, Etnyre [Et] proved that the
intersection form of a symplectic filling of a contact 3-manifold supported by
planar open book is negative definite.
In this paper, we prove a signature formula for allowable Lefschetz
fibrations over $D^2$ with planar fiber by computing Maslov index appearing in
Wall's non-additivity formula.
The signature formula leads to an alternative proof of Etnyre's theorem via
works of Niederkrüger and Wendl [NWe] and Wendl [We].
Conversely, Etnyre's theorem, together with the existence theorem of Stein
structures on Lefschetz fibrations over $D^2$ with bordered fiber by Loi and
Piergallini [LP], implies the formula.
",mathematics
"  This paper presents two visual trackers from the different paradigms of
learning and registration based tracking and evaluates their application in
image based visual servoing. They can track object motion with four degrees of
freedom (DoF) which, as we will show here, is sufficient for many fine
manipulation tasks. One of these trackers is a newly developed learning based
tracker that relies on learning discriminative correlation filters while the
other is a refinement of a recent 8 DoF RANSAC based tracker adapted with a new
appearance model for tracking 4 DoF motion.
Both trackers are shown to provide superior performance to several state of
the art trackers on an existing dataset for manipulation tasks. Further, a new
dataset with challenging sequences for fine manipulation tasks captured from
robot mounted eye-in-hand (EIH) cameras is also presented. These sequences have
a variety of challenges encountered during real tasks including jittery camera
movement, motion blur, drastic scale changes and partial occlusions.
Quantitative and qualitative results on these sequences are used to show that
these two trackers are robust to failures while providing high precision that
makes them suitable for such fine manipulation tasks.
",computer-science
"  An integral power series is called lacunary modulo $M$ if almost all of its
coefficients are divisible by $M$. Motivated by the parity problem for the
partition function, $p(n)$, Gordon and Ono studied the generating functions for
$t$-regular partitions, and determined conditions for when these functions are
lacunary modulo powers of primes. We generalize their results in a number of
ways by studying infinite products called Dedekind eta-quotients and
generalized Dedekind eta-quotients. We then apply our results to the generating
functions for the partition functions considered by Nekrasov, Okounkov, and
Han.
",mathematics
"  We consider a hyperkähler reduction and describe it via frame bundles.
Tracing the connection through the various reductions, we recover the results
of Gocho and Nakajima. In addition, we show that the fibers of such a reduction
are necessarily totally geodesic. As an independent result, we describe
O'Neill's submersion tensors on principal bundles.
",mathematics
"  Early approaches to multiple-output Gaussian processes (MOGPs) relied on
linear combinations of independent, latent, single-output Gaussian processes
(GPs). This resulted in cross-covariance functions with limited parametric
interpretation, thus conflicting with the ability of single-output GPs to
understand lengthscales, frequencies and magnitudes to name a few. On the
contrary, current approaches to MOGP are able to better interpret the
relationship between different channels by directly modelling the
cross-covariances as a spectral mixture kernel with a phase shift. We extend
this rationale and propose a parametric family of complex-valued cross-spectral
densities and then build on Cramér's Theorem (the multivariate version of
Bochner's Theorem) to provide a principled approach to design multivariate
covariance functions. The so-constructed kernels are able to model delays among
channels in addition to phase differences and are thus more expressive than
previous methods, while also providing full parametric interpretation of the
relationship across channels. The proposed method is first validated on
synthetic data and then compared to existing MOGP methods on two real-world
examples.
",statistics
"  We investigate possible links between the large-scale and small-scale
features of solar wind fluctuations across the frequency break separating fluid
and kinetic regimes. The aim is to correlate the magnetic field fluctuations
polarization at dissipative scales with the particular state of turbulence
within the inertial range of fluctuations. We found clear correlations between
each type of polarization within the kinetic regime and fluid parameters within
the inertial range. Moreover, for the first time in literature, we showed that
left-handed and right-handed polarized fluctuations occupy different areas of
the plasma instabilities-temperature anisotropy plot, as expected for
Alfv$\acute{\textrm{e}}$n Ion Cyclotron and Kinetic Alfv$\acute{\textrm{e}}$n
waves, respectively.
",physics
"  In this paper, we study joint functional calculus for commuting $n$-tuple of
Ritt operators. We provide an equivalent characterisation of boundedness for
joint functional calculus for Ritt operators on $L^p$-spaces, $1< p<\infty$. We
also investigate joint similarity problem and joint bounded functional calculus
on non-commutative $L^p$-spaces for $n$-tuple of Ritt operators. We get our
results by proving a suitable multivariable transfer principle between
sectorial and Ritt operators as well as an appropriate joint dilation result in
a general setting.
",mathematics
"  Processing sequential data of variable length is a major challenge in a wide
range of applications, such as speech recognition, language modeling,
generative image modeling and machine translation. Here, we address this
challenge by proposing a novel recurrent neural network (RNN) architecture, the
Fast-Slow RNN (FS-RNN). The FS-RNN incorporates the strengths of both
multiscale RNNs and deep transition RNNs as it processes sequential data on
different timescales and learns complex transition functions from one time step
to the next. We evaluate the FS-RNN on two character level language modeling
data sets, Penn Treebank and Hutter Prize Wikipedia, where we improve state of
the art results to $1.19$ and $1.25$ bits-per-character (BPC), respectively. In
addition, an ensemble of two FS-RNNs achieves $1.20$ BPC on Hutter Prize
Wikipedia outperforming the best known compression algorithm with respect to
the BPC measure. We also present an empirical investigation of the learning and
network dynamics of the FS-RNN, which explains the improved performance
compared to other RNN architectures. Our approach is general as any kind of RNN
cell is a possible building block for the FS-RNN architecture, and thus can be
flexibly applied to different tasks.
",computer-science
"  The celebrated Time Hierarchy Theorem for Turing machines states, informally,
that more problems can be solved given more time. The extent to which a time
hierarchy-type theorem holds in the distributed LOCAL model has been open for
many years. It is consistent with previous results that all natural problems in
the LOCAL model can be classified according to a small constant number of
complexities, such as $O(1),O(\log^* n), O(\log n), 2^{O(\sqrt{\log n})}$, etc.
In this paper we establish the first time hierarchy theorem for the LOCAL
model and prove that several gaps exist in the LOCAL time hierarchy.
1. We define an infinite set of simple coloring problems called Hierarchical
$2\frac{1}{2}$-Coloring}. A correctly colored graph can be confirmed by simply
checking the neighborhood of each vertex, so this problem fits into the class
of locally checkable labeling (LCL) problems. However, the complexity of the
$k$-level Hierarchical $2\frac{1}{2}$-Coloring problem is $\Theta(n^{1/k})$,
for $k\in\mathbb{Z}^+$. The upper and lower bounds hold for both general graphs
and trees, and for both randomized and deterministic algorithms.
2. Consider any LCL problem on bounded degree trees. We prove an
automatic-speedup theorem that states that any randomized $n^{o(1)}$-time
algorithm solving the LCL can be transformed into a deterministic $O(\log
n)$-time algorithm. Together with a previous result, this establishes that on
trees, there are no natural deterministic complexities in the ranges
$\omega(\log^* n)$---$o(\log n)$ or $\omega(\log n)$---$n^{o(1)}$.
3. We expose a gap in the randomized time hierarchy on general graphs. Any
randomized algorithm that solves an LCL problem in sublogarithmic time can be
sped up to run in $O(T_{LLL})$ time, which is the complexity of the distributed
Lovasz local lemma problem, currently known to be $\Omega(\log\log n)$ and
$O(\log n)$.
",computer-science
"  We investigate the collective behavior of magnetic swimmers, which are
suspended in a Poiseuille flow and placed under an external magnetic field,
using analytical techniques and Brownian dynamics simulations. We find that the
interplay between intrinsic activity, external alignment, and magnetic
dipole-dipole interactions leads to longitudinal structure formation. Our work
sheds light on a recent experimental observation of a clustering instability in
this system.
",physics
"  The presence of ubiquitous magnetic fields in the universe is suggested from
observations of radiation and cosmic ray from galaxies or the intergalactic
medium (IGM). One possible origin of cosmic magnetic fields is the
magnetogenesis in the primordial universe. Such magnetic fields are called
primordial magnetic fields (PMFs), and are considered to affect the evolution
of matter density fluctuations and the thermal history of the IGM gas. Hence
the information of PMFs is expected to be imprinted on the anisotropies of the
cosmic microwave background (CMB) through the thermal Sunyaev-Zel'dovich (tSZ)
effect in the IGM. In this study, given an initial power spectrum of PMFs as
$P(k)\propto B_{\rm 1Mpc}^2 k^{n_{B}}$, we calculate dynamical and thermal
evolutions of the IGM under the influence of PMFs, and compute the resultant
angular power spectrum of the Compton $y$-parameter on the sky. As a result, we
find that two physical processes driven by PMFs dominantly determine the power
spectrum of the Compton $y$-parameter; (i) the heating due to the ambipolar
diffusion effectively works to increase the temperature and the ionization
fraction, and (ii) the Lorentz force drastically enhances the density contrast
just after the recombination epoch. These facts result in making the tSZ
angular power spectrum induced by the PMFs more remarkable at $\ell >10^4$ than
that by galaxy clusters even with $B_{\rm 1Mpc}=0.1$ nG and $n_{B}=-1.0$
because the contribution from galaxy clusters decreases with increasing $\ell$.
The measurement of the tSZ angular power spectrum on high $\ell$ modes can
provide the stringent constraint on PMFs.
",physics
"  Shape information is of great importance in many applications. For example,
the oil-bearing capacity of sand bodies, the subterranean remnants of ancient
rivers, is related to their cross-sectional shapes. The analysis of these
shapes is therefore of some interest, but current classifications are
simplistic and ad hoc. In this paper, we describe the first steps towards a
coherent statistical analysis of these shapes by deriving the integrated
likelihood for data shapes given class parameters. The result is of interest
beyond this particular application.
",statistics
"  We have developed a computational code, DynaPhoPy, that allow us to extract
the microscopic anharmonic phonon properties from molecular dynamics (MD)
simulations using the normal-mode-decomposition technique as presented by Sun
et al. [T. Sun, D. Zhang, R. Wentzcovitch, 2014]. Using this code we calculated
the quasiparticle phonon frequencies and linewidths of crystalline silicon at
different temperatures using both of first-principles and the Tersoff empirical
potential approaches. In this work we show the dependence of these properties
on the temperature using both approaches and compare them with reported
experimental data obtained by Raman spectroscopy [M. Balkanski, R. Wallis, E.
Haro, 1983 and R. Tsu, J. G. Hernandez, 1982].
",physics
"  Let M be ternary, homogeneous and simple. We prove that if M is finitely
constrained, then it is supersimple with finite SU-rank and dependence is
$k$-trivial for some $k < \omega$ and for finite sets of real elements. Now
suppose that, in addition, M is supersimple with SU-rank 1. If M is finitely
constrained then algebraic closure in M is trivial. We also find connections
between the nature of the constraints of M, the nature of the amalgamations
allowed by the age of M, and the nature of definable equivalence relations. A
key method of proof is to ""extract"" constraints (of M) from instances of
dividing and from definable equivalence relations. Finally, we give new
examples, including an uncountable family, of ternary homogeneous supersimple
structures of SU-rank 1.
",mathematics
"  Planning problems in partially observable environments cannot be solved
directly with convolutional networks and require some form of memory. But, even
memory networks with sophisticated addressing schemes are unable to learn
intelligent reasoning satisfactorily due to the complexity of simultaneously
learning to access memory and plan. To mitigate these challenges we introduce
the Memory Augmented Control Network (MACN). The proposed network architecture
consists of three main parts. The first part uses convolutions to extract
features and the second part uses a neural network-based planning module to
pre-plan in the environment. The third part uses a network controller that
learns to store those specific instances of past information that are necessary
for planning. The performance of the network is evaluated in discrete grid
world environments for path planning in the presence of simple and complex
obstacles. We show that our network learns to plan and can generalize to new
environments.
",computer-science
"  Despite the progress in high performance computing, Computational Fluid
Dynamics (CFD) simulations are still computationally expensive for many
practical engineering applications such as simulating large computational
domains and highly turbulent flows. One of the major reasons of the high
expense of CFD is the need for a fine grid to resolve phenomena at the relevant
scale, and obtain a grid-independent solution. The fine grid requirements often
drive the computational time step size down, which makes long transient
problems prohibitively expensive. In the research presented, the feasibility of
a Coarse Grid CFD (CG-CFD) approach is investigated by utilizing Machine
Learning (ML) algorithms. Relying on coarse grids increases the discretization
error. Hence, a method is suggested to produce a surrogate model that predicts
the CG-CFD local errors to correct the variables of interest. Given
high-fidelity data, a surrogate model is trained to predict the CG-CFD local
errors as a function of the coarse grid local features. ML regression
algorithms are utilized to construct a surrogate model that relates the local
error and the coarse grid features. This method is applied to a
three-dimensional flow in a lid driven cubic cavity domain. The performance of
the method was assessed by training the surrogate model on the flow full field
spatial data and tested on new data (from flows of different Reynolds number
and/or computed by different grid sizes). The proposed method maximizes the
benefit of the available data and shows potential for a good predictive
capability.
",physics
"  Past work in relation extraction has focused on binary relations in single
sentences. Recent NLP inroads in high-value domains have sparked interest in
the more general setting of extracting n-ary relations that span multiple
sentences. In this paper, we explore a general relation extraction framework
based on graph long short-term memory networks (graph LSTMs) that can be easily
extended to cross-sentence n-ary relation extraction. The graph formulation
provides a unified way of exploring different LSTM approaches and incorporating
various intra-sentential and inter-sentential dependencies, such as sequential,
syntactic, and discourse relations. A robust contextual representation is
learned for the entities, which serves as input to the relation classifier.
This simplifies handling of relations with arbitrary arity, and enables
multi-task learning with related relations. We evaluate this framework in two
important precision medicine settings, demonstrating its effectiveness with
both conventional supervised learning and distant supervision. Cross-sentence
extraction produced larger knowledge bases. and multi-task learning
significantly improved extraction accuracy. A thorough analysis of various LSTM
approaches yielded useful insight the impact of linguistic analysis on
extraction accuracy.
",computer-science
"  We present a prototype of a software tool for exploration of multiple
combinatorial optimisation problems in large real-world and synthetic complex
networks. Our tool, called GraphCombEx (an acronym of Graph Combinatorial
Explorer), provides a unified framework for scalable computation and
presentation of high-quality suboptimal solutions and bounds for a number of
widely studied combinatorial optimisation problems. Efficient representation
and applicability to large-scale graphs and complex networks are particularly
considered in its design. The problems currently supported include maximum
clique, graph colouring, maximum independent set, minimum vertex clique
covering, minimum dominating set, as well as the longest simple cycle problem.
Suboptimal solutions and intervals for optimal objective values are estimated
using scalable heuristics. The tool is designed with extensibility in mind,
with the view of further problems and both new fast and high-performance
heuristics to be added in the future. GraphCombEx has already been successfully
used as a support tool in a number of recent research studies using
combinatorial optimisation to analyse complex networks, indicating its promise
as a research software tool.
",computer-science
"  The peculiar band structure of semimetals exhibiting Dirac and Weyl crossings
can lead to spectacular electronic properties such as large mobilities
accompanied by extremely high magnetoresistance. In particular, two closely
neighbouring Weyl points of the same chirality are protected from annihilation
by structural distortions or defects, thereby significantly reducing the
scattering probability between them. Here we present the electronic properties
of the transition metal diphosphides, WP2 and MoP2, that are type-II Weyl
semimetals with robust Weyl points. We present transport and angle resolved
photoemission spectroscopy measurements, and first principles calculations. Our
single crystals of WP2 display an extremely low residual low-temperature
resistivity of 3 nohm-cm accompanied by an enormous and highly anisotropic
magnetoresistance above 200 million % at 63 T and 2.5 K. These properties are
likely a consequence of the novel Weyl fermions expressed in this compound. We
observe a large suppression of charge carrier backscattering in WP2 from
transport measurements.
",physics
"  Line bundles of rational degree are defined using Perfectoid spaces, and
their co-homology computed via standard Čech complex along with Kunneth
formula. A new concept of `braided dimension' is introduced, which helps
convert the curse of infinite dimensionality into a boon, which is then used to
do Bezout type computations, define euler characters, describe ampleness and
link integer partitions with geometry. This new concept of 'Braided dimension'
gives a space within a space within a space an infinite tower of spaces, all
intricately braided into each other. Finally, the concept of Blow Up over
perfectoid space is introduced.
",mathematics
"  The Subaru Strategic Program (SSP) is an ambitious multi-band survey using
the Hyper Suprime-Cam (HSC) on the Subaru telescope. The Wide layer of the SSP
is both wide and deep, reaching a detection limit of i~26.0 mag. At these
depths, it is challenging to achieve accurate, unbiased, and consistent
photometry across all five bands. The HSC data are reduced using a pipeline
that builds on the prototype pipeline for the Large Synoptic Survey Telescope.
We have developed a Python-based, flexible framework to inject synthetic
galaxies into real HSC images called SynPipe. Here we explain the design and
implementation of SynPipe and generate a sample of synthetic galaxies to
examine the photometric performance of the HSC pipeline. For stars, we achieve
1% photometric precision at i~19.0 mag and 6% precision at i~25.0 in the
i-band. For synthetic galaxies with single-Sersic profiles, forced CModel
photometry achieves 13% photometric precision at i~20.0 mag and 18% precision
at i~25.0 in the i-band. We show that both forced PSF and CModel photometry
yield unbiased color estimates that are robust to seeing conditions. We
identify several caveats that apply to the version of HSC pipeline used for the
first public HSC data release (DR1) that need to be taking into consideration.
First, the degree to which an object is blended with other objects impacts the
overall photometric performance. This is especially true for point sources.
Highly blended objects tend to have larger photometric uncertainties,
systematically underestimated fluxes and slightly biased colors. Second, >20%
of stars at 22.5< i < 25.0 mag can be misclassified as extended objects. Third,
the current CModel algorithm tends to strongly underestimate the half-light
radius and ellipticity of galaxy with i>21.5 mag.
",physics
"  Is perfect matching in NC? That is, is there a deterministic fast parallel
algorithm for it? This has been an outstanding open question in theoretical
computer science for over three decades, ever since the discovery of RNC
matching algorithms. Within this question, the case of planar graphs has
remained an enigma: On the one hand, counting the number of perfect matchings
is far harder than finding one (the former is #P-complete and the latter is in
P), and on the other, for planar graphs, counting has long been known to be in
NC whereas finding one has resisted a solution.
In this paper, we give an NC algorithm for finding a perfect matching in a
planar graph. Our algorithm uses the above-stated fact about counting matchings
in a crucial way. Our main new idea is an NC algorithm for finding a face of
the perfect matching polytope at which $\Omega(n)$ new conditions, involving
constraints of the polytope, are simultaneously satisfied. Several other ideas
are also needed, such as finding a point in the interior of the minimum weight
face of this polytope and finding a balanced tight odd set in NC.
",computer-science
"  Persistent homology studies the evolution of k-dimensional holes along a
nested sequence of simplicial complexes (called a filtration). The set of bars
(i.e. intervals) representing birth and death times of k-dimensional holes
along such sequence is called the persistence barcode. k-Dimensional holes with
short lifetimes are informally considered to be ""topological noise"", and those
with long lifetimes are considered to be ""topological features"" associated to
the filtration. Persistent entropy is defined as the Shannon entropy of the
persistence barcode of a given filtration. In this paper we present new
important properties of persistent entropy of Cech and Vietoris-Rips
filtrations. Among the properties, we put a focus on the stability theorem that
allows to use persistent entropy for comparing persistence barcodes. Later, we
derive a simple method for separating topological noise from features in
Vietoris-Rips filtrations.
",computer-science
"  We report results of isothermal magnetotransport and susceptibility
measurements at elevated magnetic fields B down to very low temperatures T on
high-quality single crystals of the frustrated Kondo-lattice system CePdAl.
They reveal a B*(T) line within the paramagnetic part of the phase diagram.
This line denotes a thermally broadened 'small'-to-'large' Fermi surface
crossover which substantially narrows upon cooling. At B_0* = B*(T=0) = (4.6
+/- 0.1) T, this B*(T) line merges with two other crossover lines, viz. Tp(B)
below and T_FL(B) above B_0*. Tp characterizes a frustration-dominated
spin-liquid state, while T_FL is the Fermi-liquid temperature associated with
the lattice Kondo effect. Non-Fermi-liquid phenomena which are commonly
observed near a 'Kondo destruction' quantum critical point cannot be resolved
in CePdAl. Our observations reveal a rare case where Kondo coupling,
frustration and quantum criticality are closely intertwined.
",physics
"  Most current results on coverage control using mobile sensors require that
one partitioned cell is associated with precisely one sensor. In this paper, we
consider a class of coverage control problems involving higher order Voronoi
partitions, motivated by applications where more than one sensor is required to
monitor and cover one cell. Such applications are frequent in scenarios
requiring the sensors to localize targets. We introduce a framework depending
on a coverage performance function incorporating higher order Voronoi cells and
then design a gradient-based controller which allows the multi-sensor system to
achieve a local equilibrium in a distributed manner. The convergence properties
are studied and related to Lloyd algorithm. We study also the extension to
coverage of a discrete set of points. In addition, we provide a number of real
world scenarios where our framework can be applied. Simulation results are also
provided to show the controller performance.
",computer-science
"  We present a deep generative model for learning to predict classes not seen
at training time. Unlike most existing methods for this problem, that represent
each class as a point (via a semantic embedding), we represent each seen/unseen
class using a class-specific latent-space distribution, conditioned on class
attributes. We use these latent-space distributions as a prior for a supervised
variational autoencoder (VAE), which also facilitates learning highly
discriminative feature representations for the inputs. The entire framework is
learned end-to-end using only the seen-class training data. The model infers
corresponding attributes of a test image by maximizing the VAE lower bound; the
inferred attributes may be linked to labels not seen when training. We further
extend our model to a (1) semi-supervised/transductive setting by leveraging
unlabeled unseen-class data via an unsupervised learning module, and (2)
few-shot learning where we also have a small number of labeled inputs from the
unseen classes. We compare our model with several state-of-the-art methods
through a comprehensive set of experiments on a variety of benchmark data sets.
",computer-science
"  Contingent Convertible bonds (CoCos) are debt instruments that convert into
equity or are written down in times of distress. Existing pricing models assume
conversion triggers based on market prices and on the assumption that markets
can always observe all relevant firm information. But all Cocos issued so far
have triggers based on accounting ratios and/or regulatory intervention. We
incorporate that markets receive information through noisy accounting reports
issued at discrete time instants, which allows us to distinguish between market
and accounting values, and between automatic triggers and regulator-mandated
conversions. Our second contribution is to incorporate that coupon payments are
contingent too: their payment is conditional on the Maximum Distributable
Amount not being exceeded. We examine the impact of CoCo design parameters,
asset volatility and accounting noise on the price of a CoCo; and investigate
the interaction between CoCo design features, the capital structure of the
issuing bank and their implications for risk taking and investment incentives.
Finally, we use our model to explain the crash in CoCo prices after Deutsche
Bank's profit warning in February 2016.
",quantitative-finance
"  We show that the any nonempty open set on a hyperbolic surface provides
observability and control for the time dependent Schrödinger equation. The
only other manifolds for which this was previously known are flat tori. The
proof is based on the main estimate in Dyatlov-Jin and standard arguments of
control theory.
",mathematics
"  We introduce a dynamic model of the default waterfall of derivatives CCPs and
propose a risk sensitive method for sizing the initial margin (IM), and the
default fund (DF) and its allocation among clearing members. Using a Markovian
structure model of joint credit migrations, our evaluation of DF takes into
account the joint credit quality of clearing members as they evolve over time.
Another important aspect of the proposed methodology is the use of the time
consistent dynamic risk measures for computation of IM and DF. We carry out a
comprehensive numerical study, where, in particular, we analyze the advantages
of the proposed methodology and its comparison with the currently prevailing
methods used in industry.
",quantitative-finance
"  The magnetic signature of an urban environment is investigated using a
geographically distributed network of fluxgate magnetometers deployed in and
around Berkeley, California. The system hardware and software are described and
results from initial operation of the network are reported. The sensors sample
the vector magnetic field with a 4 kHz resolution and are sensitive to
fluctuations below 0.1 $\textrm{nT}/\sqrt{\textrm{Hz}}$. Data from separate
stations are synchronized to around $\pm100$ $\mu{s}$ using GPS and computer
system clocks. Data from all sensors are automatically uploaded to a central
server. Anomalous events, such as lightning strikes, have been observed. A
wavelet analysis is used to study observations over a wide range of temporal
scales up to daily variations that show strong differences between weekend and
weekdays. The Bay Area Rapid Transit (BART) is identified as the most dominant
signal from these observations and a superposed epoch analysis is used to study
and extract the BART signal. Initial results of the correlation between sensors
are also presented.
",physics
"  Witten's Gauged Linear $\sigma$-Model (GLSM) unifies the Gromov-Witten theory
and the Landau-Ginzburg theory, and provides a global perspective on mirror
symmetry. In this article, we summarize a mathematically rigorous construction
of the GLSM in the geometric phase using methods from symplectic geometry.
",mathematics
"  A method is presented for solving the discrete-time finite-horizon Linear
Quadratic Regulator (LQR) problem subject to auxiliary linear equality
constraints, such as fixed end-point constraints. The method explicitly
determines an affine relationship between the control and state variables, as
in standard Riccati recursion, giving rise to feedback control policies that
account for constraints. Since the linearly-constrained LQR problem arises
commonly in robotic trajectory optimization, having a method that can
efficiently compute these solutions is important. We demonstrate some of the
useful properties and interpretations of said control policies, and we compare
the computation time of our method against existing methods.
",computer-science
"  In this paper, a new Smartphone sensor based algorithm is proposed to detect
accurate distance estimation. The algorithm consists of two phases, the first
phase is for detecting the peaks from the Smartphone accelerometer sensor. The
other one is for detecting the step length which varies from step to step. The
proposed algorithm is tested and implemented in real environment and it showed
promising results. Unlike the conventional approaches, the error of the
proposed algorithm is fixed and is not affected by the long distance.
Keywords distance estimation, peaks, step length, accelerometer.
",computer-science
"  This paper presents a feature encoding method of complex 3D objects for
high-level semantic features. Recent approaches to object recognition methods
become important for semantic simultaneous localization and mapping (SLAM).
However, there is a lack of consideration of the probabilistic observation
model for 3D objects, as the shape of a 3D object basically follows a complex
probability distribution. Furthermore, since the mobile robot equipped with a
range sensor observes only a single view, much information of the object shape
is discarded. These limitations are the major obstacles to semantic SLAM and
view-independent loop closure using 3D object shapes as features. In order to
enable the numerical analysis for the Bayesian inference, we approximate the
true observation model of 3D objects to tractable distributions. Since the
observation likelihood can be obtained from the generative model, we formulate
the true generative model for 3D object with the Bayesian networks. To capture
these complex distributions, we apply a variational auto-encoder. To analyze
the approximated distributions and encoded features, we perform classification
with maximum likelihood estimation and shape retrieval.
",computer-science
"  We consider a theory of a two-component Dirac fermion localized on a (2+1)
dimensional brane coupled to a (3+1) dimensional bulk. Using the fermionic
particle-vortex duality, we show that the theory has a strong-weak duality that
maps the coupling $e$ to $\tilde e=(8\pi)/e$. We explore the theory at
$e^2=8\pi$ where it is self-dual. The electrical conductivity of the theory is
a constant independent of frequency. When the system is at finite density and
magnetic field at filling factor $\nu=\frac12$, the longitudinal and Hall
conductivity satisfies a semicircle law, and the ratio of the longitudinal and
Hall thermal electric coefficients is completely determined by the Hall angle.
The thermal Hall conductivity is directly related to the thermal electric
coefficients.
",physics
"  We analyze new far-ultraviolet spectra of 13 quasars from the z~0.2 COS-Halos
survey that cover the HI Lyman limit of 14 circumgalactic medium (CGM) systems.
These data yield precise estimates or more constraining limits than previous
COS-Halos measurements on the HI column densities NHI. We then apply a
Monte-Carlo Markov Chain approach on 32 systems from COS-Halos to estimate the
metallicity of the cool (T~10^4K) CGM gas that gives rise to low-ionization
state metal lines, under the assumption of photoionization equilibrium with the
extragalactic UV background. The principle results are: (1) the CGM of field L*
galaxies exhibits a declining HI surface density with impact parameter Rperp
(at >99.5%$ confidence), (2) the transmission of ionizing radiation through CGM
gas alone is 70+/-7%; (3) the metallicity distribution function of the cool CGM
is unimodal with a median of 1/3 Z_Sun and a 95% interval from ~1/50 Z_Sun to
over 3x solar. The incidence of metal poor (<1/100 Z_Sun) gas is low, implying
any such gas discovered along quasar sightlines is typically unrelated to L*
galaxies; (4) we find an unexpected increase in gas metallicity with declining
NHI (at >99.9% confidence) and, therefore, also with increasing Rperp. The high
metallicity at large radii implies early enrichment; (5) A non-parametric
estimate of the cool CGM gas mass is M_CGM_cool = 9.2 +/- 4.3 10^10 Msun, which
together with new mass estimates for the hot CGM may resolve the galactic
missing baryons problem. Future analyses of halo gas should focus on the
underlying astrophysics governing the CGM, rather than processes that simply
expel the medium from the halo.
",physics
"  The superposition of temporal point processes has been studied for many
years, although the usefulness of such models for practical applications has
not be fully developed. We investigate superposed Hawkes process as an
important class of such models, with properties studied in the framework of
least squares estimation. The superposition of Hawkes processes is demonstrated
to be beneficial for tightening the upper bound of excess risk under certain
conditions, and we show the feasibility of the benefit in typical situations.
The usefulness of superposed Hawkes processes is verified on synthetic data,
and its potential to solve the cold-start problem of recommendation systems is
demonstrated on real-world data.
",statistics
"  Let $M$ be a compact 3-manifold and $\Gamma=\pi_1(M)$. The work of Thurston
and Culler--Shalen established the $\mathrm{SL}_2(\mathbb{C})$ character
variety $X(\Gamma)$ as fundamental tool in the study of the geometry and
topology of $M$. This is particularly so in the case when $M$ is the exterior
of a hyperbolic knot $K$ in $S^3$. The main goals of this paper are to bring to
bear tools from algebraic and arithmetic geometry to understand algebraic and
number theoretic properties of the so-called canonical component of $X(\Gamma)$
as well as distinguished points on the canonical component when $\Gamma$ is a
knot group. In particular, we study how the theory of quaternion Azumaya
algebras can be used to obtain algebraic and arithmetic information about Dehn
surgeries, and perhaps of most interest, to construct new knot invariants that
lie in the Brauer groups of curves over number fields.
",mathematics
"  The importance of being able to verify quantum computation delegated to
remote servers increases with recent development of quantum technologies. In
some of the proposed protocols for this task, a client delegates her quantum
computation to non-communicating servers. The fact that the servers do not
communicate is not physically justified and it is essential for the proof of
security of such protocols. For the best of our knowledge, we present in this
work the first verifiable delegation scheme where a classical client delegates
her quantum computation to two entangled servers that are allowed to
communicate, but respecting the plausible assumption that information cannot be
propagated faster than speed of light. We achieve this result by proposing the
first one-round two-prover game for the Local Hamiltonian problem where provers
only need polynomial time quantum computation and access to copies of the
groundstate of the Hamiltonian.
",computer-science
"  In this paper, we consider the weak convergence of the Euler-Maruyama
approximation for one dimensional stochastic differential equations involving
the local times of the unknown process. We use a transformation in order to
remove the local time from the stochastic differential equations and we provide
the approximation of Euler-maruyama for the stochastic differential equations
without local time. After that, we conclude the approximation of Euler-maruyama
for one dimensional stochastic differential equations involving the local times
of the unknown process , and we provide the rate of weak convergence for any
function G in a certain class.
",mathematics
"  In Lithium ion batteries (LIBs), proper design of cathode materials
influences its intercalation behavior, overall cost, structural stability, and
its impact on environment. At present, the most common type of cathode
materials, NCA , has very high cobalt concentration. Since cobalt is toxic and
expensive, the existing design of cathode materials is not cost-effective, and
environmentally benign. However, these immensely important issues have not yet
been properly addressed. Therefore, we have performed density functional theory
(DFT) calculations to investigate three types of NCA cathode materials
NCA_(Co=0.15), NCA_(Co=0.10), NCA_(Co=0.05). Our results show that even if the
cobalt concentration is significantly decreased from NCA_(Co=0.15) to
NCA_(Co=0.05), variation in intercalation potential and specific capacity is
negligible. For example, in case of 50% Li concentration, voltage drop is
~0.12V while change in specific capacity is negligible. Moreover, decrease in
cobalt concentration doesn't influence the structural stability. We have also
explored the influence of sodium doping on the electrochemical and structural
properties of these three structures. Our results provide insight into the
design of cathode materials with reduced cobalt concentration, environmentally
benign, low-cost cathode materials.
",physics
"  The quest for large and low frequency band gaps is one of the principal
objectives pursued in a number of engineering applications, ranging from noise
absorption to vibration control, to seismic wave abatement. For this purpose, a
plethora of complex architectures (including multi-phase materials) and
multi-physics approaches have been proposed in the past, often involving
difficulties in their practical realization.
To address this issue, in this work we propose an easy-to-manufacture design
able to open large, low frequency complete Lamb band gaps exploiting a suitable
arrangement of masses and stiffnesses produced by cavities in a monolithic
material. The performance of the designed structure is evaluated by numerical
simulations and confirmed by Scanning Laser Doppler Vibrometer (SLDV)
measurements on an isotropic polyvinyl chloride plate in which a square ring
region of cross-like cavities is fabricated. The full wave field reconstruction
clearly confirms the ability of even a limited number of unit cell rows of the
proposed design to efficiently attenuate Lamb waves. In addition, numerical
simulations show that the structure allows to shift of the central frequency of
the BG through geometrical modifications. The design may be of interest for
applications in which large BGs at low frequencies are required.
",physics
"  Absolutely Koszul algebras are a class of rings over which any finite graded
module has a rational Poincaré series. We provide a criterion to detect
non-absolutely Koszul rings. Combining the criterion with machine computations,
we identify large families of Veronese subrings and Segre products of
polynomial rings which are not absolutely Koszul. In particular, we classify
completely the absolutely Koszul algebras among Segre products of polynomial
rings, at least in characteristic $0$.
",mathematics
"  Beyond traditional security methods, unmanned aerial vehicles (UAVs) have
become an important surveillance tool used in security domains to collect the
required annotated data. However, collecting annotated data from videos taken
by UAVs efficiently, and using these data to build datasets that can be used
for learning payoffs or adversary behaviors in game-theoretic approaches and
security applications, is an under-explored research question. This paper
presents VIOLA, a novel labeling application that includes (i) a workload
distribution framework to efficiently gather human labels from videos in a
secured manner; (ii) a software interface with features designed for labeling
videos taken by UAVs in the domain of wildlife security. We also present the
evolution of VIOLA and analyze how the changes made in the development process
relate to the efficiency of labeling, including when seemingly obvious
improvements did not lead to increased efficiency. VIOLA enables collecting
massive amounts of data with detailed information from challenging security
videos such as those collected aboard UAVs for wildlife security. VIOLA will
lead to the development of new approaches that integrate deep learning for
real-time detection and response.
",computer-science
"  Variational approaches for the calculation of vibrational wave functions and
energies are a natural route to obtain highly accurate results with
controllable errors. However, the unfavorable scaling and the resulting high
computational cost of standard variational approaches limit their application
to small molecules with only few vibrational modes. Here, we demonstrate how
the density matrix renormalization group (DMRG) can be exploited to optimize
vibrational wave functions (vDMRG) expressed as matrix product states. We study
the convergence of these calculations with respect to the size of the local
basis of each mode, the number of renormalized block states, and the number of
DMRG sweeps required. We demonstrate the high accuracy achieved by vDMRG for
small molecules that were intensively studied in the literature. We then
proceed to show that the complete fingerprint region of the sarcosyn-glycin
dipeptide can be calculated with vDMRG.
",physics
"  In this paper, we study how to predict the results of LTL model checking
using some machine learning algorithms. Some Kripke structures and LTL formulas
and their model checking results are made up data set. The approaches based on
the Random Forest (RF), K-Nearest Neighbors (KNN), Decision tree (DT), and
Logistic Regression (LR) are used to training and prediction. The experiment
results show that the average computation efficiencies of the RF, LR, DT, and
KNN-based approaches are 2066181, 2525333, 1894000 and 294 times than that of
the existing approach, respectively.
",computer-science
"  Every Constraint Programming (CP) solver exposes a library of constraints for
solving combinatorial problems. In order to be useful, CP solvers need to be
bug-free. Therefore the testing of the solver is crucial to make developers and
users confident. We present a Java library allowing any JVM based solver to
test that the implementations of the individual constraints are correct. The
library can be used in a test suite executed in a continuous integration tool
or it can also be used to discover minimalist instances violating some
properties (arc-consistency, etc) in order to help the developer to identify
the origin of the problem using standard debuggers.
",computer-science
"  A proper semantic representation for encoding side information is key to the
success of zero-shot learning. In this paper, we explore two alternative
semantic representations especially for zero-shot human action recognition:
textual descriptions of human actions and deep features extracted from still
images relevant to human actions. Such side information are accessible on Web
with little cost, which paves a new way in gaining side information for
large-scale zero-shot human action recognition. We investigate different
encoding methods to generate semantic representations for human actions from
such side information. Based on our zero-shot visual recognition method, we
conducted experiments on UCF101 and HMDB51 to evaluate two proposed semantic
representations . The results suggest that our proposed text- and image-based
semantic representations outperform traditional attributes and word vectors
considerably for zero-shot human action recognition. In particular, the
image-based semantic representations yield the favourable performance even
though the representation is extracted from a small number of images per class.
",computer-science
"  Geospatial semantics is a broad field that involves a variety of research
areas. The term semantics refers to the meaning of things, and is in contrast
with the term syntactics. Accordingly, studies on geospatial semantics usually
focus on understanding the meaning of geographic entities as well as their
counterparts in the cognitive and digital world, such as cognitive geographic
concepts and digital gazetteers. Geospatial semantics can also facilitate the
design of geographic information systems (GIS) by enhancing the
interoperability of distributed systems and developing more intelligent
interfaces for user interactions. During the past years, a lot of research has
been conducted, approaching geospatial semantics from different perspectives,
using a variety of methods, and targeting different problems. Meanwhile, the
arrival of big geo data, especially the large amount of unstructured text data
on the Web, and the fast development of natural language processing methods
enable new research directions in geospatial semantics. This chapter,
therefore, provides a systematic review on the existing geospatial semantic
research. Six major research areas are identified and discussed, including
semantic interoperability, digital gazetteers, geographic information
retrieval, geospatial Semantic Web, place semantics, and cognitive geographic
concepts.
",computer-science
"  Modern corporations physically separate their sensitive computational
infrastructure from public or other accessible networks in order to prevent
cyber-attacks. However, attackers still manage to infect these networks, either
by means of an insider or by infiltrating the supply chain. Therefore, an
attacker's main challenge is to determine a way to command and control the
compromised hosts that are isolated from an accessible network (e.g., the
Internet).
In this paper, we propose a new adversarial model that shows how an air
gapped network can receive communications over a covert thermal channel.
Concretely, we show how attackers may use a compromised air-conditioning system
(connected to the internet) to send commands to infected hosts within an
air-gapped network. Since thermal communication protocols are a rather
unexplored domain, we propose a novel line-encoding and protocol suitable for
this type of channel. Moreover, we provide experimental results to demonstrate
the covert channel's feasibility, and to calculate the channel's bandwidth.
Lastly, we offer a forensic analysis and propose various ways this channel can
be detected and prevented.
We believe that this study details a previously unseen vector of attack that
security experts should be aware of.
",computer-science
"  A stochastic model of excitatory and inhibitory interactions which bears
universality traits is introduced and studied. The endogenous component of
noise, stemming from finite size corrections, drives robust inter-nodes
correlations, that persist at large large distances. Anti-phase synchrony at
small frequencies is resolved on adjacent nodes and found to promote the
spontaneous generation of long-ranged stochastic patterns, that invade the
network as a whole. These patterns are lacking under the idealized
deterministic scenario, and could provide novel hints on how living systems
implement and handle a large gallery of delicate computational tasks.
",physics
"  Topological nodal line (DNL) semimetals, formed by a closed loop of the
inverted bands in the bulk, result in the nearly flat drumhead-like surface
states with a high electronic density near the Fermi level. The high catalytic
active sites associated with the high electronic densities, the good carrier
mobility, and the proper thermodynamic stabilities with $\Delta
G_{H^*}$$\approx$0 are currently the prerequisites to seek the alternative
candidates to precious platinum for catalyzing electrochemical hydrogen (HER)
production from water. Within this context, it is natural to consider whether
or not the DNLs are a good candidate for the HER because its non-trivial
surface states provide a robust platform to activate possibly chemical
reactions. Here, through first-principles calculations we reported on a new DNL
TiSi-type family with a closed Dirac nodal line consisting of the linear band
crossings in the $k_y$ = 0 plane. The hydrogen adsorption on the (010) and
(110) surfaces yields the $\Delta G_{H^*}$ to be almost zero. The topological
charge carries have been revealed to participate in this HER. The results are
highlighting that TiSi not only is a promising catalyst for the HER but also
paves a new routine to design topological quantum catalyst utilizing the
topological DNL-induced surface bands as active sites, rather than edge sites-,
vacancy-, dopant-, strain-, or heterostructure-created active sites.
",physics
"  The question in this paper is whether R&D efforts affect education
performance in small classes. Merging two datasets collected from the PISA
studies and the World Development Indicators and using Learning Bayesian
Networks, we prove the existence of a statistical causal relationship between
investment in R&D of a country and its education performance (PISA scores). We
also prove that the effect of R\&D on Education is long term as a country has
to invest at least 10 years before beginning to improve the level of young
pupils.
",statistics
"  Automated vehicles can change the society by improved safety, mobility and
fuel efficiency. However, due to the higher cost and change in business model,
over the coming decades, the highly automated vehicles likely will continue to
interact with many human-driven vehicles. In the past, the control/design of
the highly automated (robotic) vehicles mainly considers safety and efficiency
but failed to address the ""driving culture"" of surrounding human-driven
vehicles. Thus, the robotic vehicles may demonstrate behaviors very different
from other vehicles. We study this ""driving etiquette"" problem in this paper.
As the first step, we report the key behavior parameters of human driven
vehicles derived from a large naturalistic driving database. The results can be
used to guide future algorithm design of highly automated vehicles or to
develop realistic human-driven vehicle behavior model in simulations.
",computer-science
"  Using age of information as the freshness metric, we examine a multicast
network in which real-time status updates are generated by the source and sent
to a group of $n$ interested receivers. We show that in order to keep the
information freshness at each receiver, the source should terminate the
transmission of the current update and start sending a new update packet as
soon as it receives the acknowledgements back from any $k$ out of $n$ nodes. As
the source stopping threshold $k$ increases, a node is more likely to get the
latest generated update, but the age of the most recent update is more likely
to become outdated. We derive the age minimized stopping threshold $k$ that
balances the likelihood of getting the latest update and the freshness of the
latest update for shifted exponential link delay. Through numerical evaluations
for different stopping strategies, we find that waiting for the
acknowledgements from the earliest $k$ out of $n$ nodes leads to lower average
age than waiting for a pre-selected group of $k$ nodes. We also observe that a
properly chosen threshold $k$ can prevent information staleness for increasing
number of nodes $n$ in the multicast network.
",computer-science
"  Gender inequality starts before birth. Parents tend to prefer boys over
girls, which is manifested in reproductive behavior, marital life, and parents'
pastimes and investments in their children. While social media and sharing
information about children (so-called ""sharenting"") have become an integral
part of parenthood, it is not well-known if and how gender preference shapes
online behavior of users. In this paper, we investigate public mentions of
daughters and sons on social media. We use data from a popular social
networking site on public posts from 635,665 users. We find that both men and
women mention sons more often than daughters in their posts. We also find that
posts featuring sons get more ""likes"" on average. Our results indicate that
girls are underrepresented in parents' digital narratives about their children.
This gender imbalance may send a message that girls are less important than
boys, or that they deserve less attention, thus reinforcing gender inequality.
",computer-science
"  Ultraviolet self-interaction energies in field theory sometimes contain
meaningful physical quantities. The self-energies in such as classical
electrodynamics are usually subtracted from the rest mass. For the consistent
treatment of energies as sources of curvature in the Einstein field equations,
this study includes these subtracted self-energies into vacuum energy expressed
by the constant Lambda (used in such as Lambda-CDM). In this study, the
self-energies in electrodynamics and macroscopic classical Einstein field
equations are examined, using the formalisms with the ultraviolet cutoff
scheme. One of the cutoff formalisms is the field theory in terms of the
step-function-type basis functions, developed by the present authors. The other
is a continuum theory of a fundamental particle with the same cutoff length.
Based on the effectiveness of the continuum theory with the cutoff length shown
in the examination, the dominant self-energy is the quadratic term of the Higgs
field at a quantum level (classical self-energies are reduced to logarithmic
forms by quantum corrections). The cutoff length is then determined to
reproduce today's tiny value of Lambda for vacuum energy. Additionally, a field
with nonperiodic vanishing boundary conditions is treated, showing that the
field has no zero-point energy.
",physics
"  The collapse of a collisionless self-gravitating system, with the fast
achievement of a quasi-stationary state, is driven by violent relaxation, with
a typical particle interacting with the time-changing collective potential. It
is traditionally assumed that this evolution is governed by the Vlasov-Poisson
equation, in which case entropy must be conserved. We run N-body simulations of
isolated self-gravitating systems, using three simulation codes: NBODY-6
(direct summation without softening), NBODY-2 (direct summation with softening)
and GADGET-2 (tree code with softening), for different numbers of particles and
initial conditions. At each snapshot, we estimate the Shannon entropy of the
distribution function with three different techniques: Kernel, Nearest Neighbor
and EnBiD. For all simulation codes and estimators, the entropy evolution
converges to the same limit as N increases. During violent relaxation, the
entropy has a fast increase followed by damping oscillations, indicating that
violent relaxation must be described by a kinetic equation other than the
Vlasov-Poisson, even for N as large as that of astronomical structures. This
indicates that violent relaxation cannot be described by a time-reversible
equation, shedding some light on the so-called ""fundamental paradox of stellar
dynamics"". The long-term evolution is well described by the orbit-averaged
Fokker-Planck model, with Coulomb logarithm values in the expected range 10-12.
By means of NBODY-2, we also study the dependence of the 2-body relaxation
time-scale on the softening length. The approach presented in the current work
can potentially provide a general method for testing any kinetic equation
intended to describe the macroscopic evolution of N-body systems.
",physics
"  The mean objective cost of uncertainty (MOCU) quantifies the performance cost
of using an operator that is optimal across an uncertainty class of systems as
opposed to using an operator that is optimal for a particular system.
MOCU-based experimental design selects an experiment to maximally reduce MOCU,
thereby gaining the greatest reduction of uncertainty impacting the operational
objective. The original formulation applied to finding optimal system
operators, where optimality is with respect to a cost function, such as
mean-square error; and the prior distribution governing the uncertainty class
relates directly to the underlying physical system. Here we provide a
generalized MOCU and the corresponding experimental design. We then demonstrate
how this new formulation includes as special cases MOCU-based experimental
design methods developed for materials science and genomic networks when there
is experimental error. Most importantly, we show that the classical Knowledge
Gradient and Efficient Global Optimization experimental design procedures are
actually implementations of MOCU-based experimental design under their modeling
assumptions.
",statistics
"  We study the following nonlocal diffusion equation in the Heisenberg group
$\mathbb{H}_n$, \[ u_t(z,s,t)=J\ast u(z,s,t)-u(z,s,t), \] where $\ast$ denote
convolution product and $J$ satisfies appropriated hypothesis. For the Cauchy
problem we obtain that the asymptotic behavior of the solutions is the same
form that the one for the heat equation in the Heisenberg group. To obtain this
result we use the spherical transform related to the pair
$(U(n),\mathbb{H}_n)$. Finally we prove that solutions of properly rescaled
nonlocal Dirichlet problem converge uniformly to the solution of the
corresponding Dirichlet problem for the classical heat equation in the
Heisenberg group.
",mathematics
"  We present a randomization-based inferential framework for experiments
characterized by a strongly ignorable assignment mechanism where units have
independent probabilities of receiving treatment. Previous works on
randomization tests often assume these probabilities are equal within blocks of
units. We consider the general case where they differ across units and show how
to perform randomization tests and obtain point estimates and confidence
intervals. Furthermore, we develop a rejection-sampling algorithm to conduct
randomization-based inference conditional on ancillary statistics, covariate
balance, or other statistics of interest. Through simulation we demonstrate how
our algorithm can yield powerful randomization tests and thus precise
inference. Our work also has implications for observational studies, which
commonly assume a strongly ignorable assignment mechanism. Most methodologies
for observational studies make additional modeling or asymptotic assumptions,
while our framework only assumes the strongly ignorable assignment mechanism,
and thus can be considered a minimal-assumption approach.
",statistics
"  Monolayer films of FeSe grown on SrTiO$_3$ substrates exhibit significantly
higher superconducting transition temperatures than those of bulk FeSe.
Interaction of electrons in the FeSe layer with dipolar SrTiO$_3$ phonons has
been suggested as the cause of the enhanced transition temperature. In this
paper we systematically study the coupling of SrTiO$_3$ longitudinal optical
phonons to the FeSe electron, including also electron-electron Coulomb
interactions at the random phase approximation level. We find that the
electron-phonon interaction between FeSe and SrTiO$_3$ substrate is almost
entirely screened by the electronic fluctuations in the FeSe monolayer, so that
the net electron-phonon interaction is very weak and unlikely to lead to
superconductivity.
",physics
"  auDeep is a Python toolkit for deep unsupervised representation learning from
acoustic data. It is based on a recurrent sequence to sequence autoencoder
approach which can learn representations of time series data by taking into
account their temporal dynamics. We provide an extensive command line interface
in addition to a Python API for users and developers, both of which are
comprehensively documented and publicly available at
this https URL. Experimental results indicate that auDeep
features are competitive with state-of-the art audio classification.
",computer-science
"  The structure and nature of water confined between hydrophobic molybdenum
disulfide (MoS2) and graphene (Gr) are investigated at room temperature by
means of atomic force microscopy. We find the formation of two-dimensional (2D)
crystalline ice layers. In contrast to the hexagonal ice 'bilayers' of bulk
ice, these 2D crystalline ice phases consist of two planar hexagonal layers.
Additional water condensation leads to either lateral expansion of the ice
layers or to the formation of three-dimensional water droplets on top or at the
edges of the two-layer ice, indicating that water does not wet these planar ice
films. The results presented here are in line with a recent theory suggesting
that water confined between hydrophobic walls forms 2D crystalline two-layer
ice with a nontetrahedral geometry and intrahydrogen bonding. The lack of
dangling bonds on either surface of the ice film gives rise to a hydrophobic
character. The unusual geometry of these ice films is of great potential
importance in biological systems with water in direct contact with hydrophobic
surfaces.
",physics
"  We study the two-dimensional topology of the galactic distribution when
projected onto two-dimensional spherical shells. Using the latest Horizon Run 4
simulation data, we construct the genus of the two-dimensional field and
consider how this statistic is affected by late-time nonlinear effects --
principally gravitational collapse and redshift space distortion (RSD). We also
consider systematic and numerical artifacts such as shot noise, galaxy bias,
and finite pixel effects. We model the systematics using a Hermite polynomial
expansion and perform a comprehensive analysis of known effects on the
two-dimensional genus, with a view toward using the statistic for cosmological
parameter estimation. We find that the finite pixel effect is dominated by an
amplitude drop and can be made less than $1\%$ by adopting pixels smaller than
$1/3$ of the angular smoothing length. Nonlinear gravitational evolution
introduces time-dependent coefficients of the zeroth, first, and second Hermite
polynomials, but the genus amplitude changes by less than $1\%$ between $z=1$
and $z=0$ for smoothing scales $R_{\rm G} > 9 {\rm Mpc/h}$. Non-zero terms are
measured up to third order in the Hermite polynomial expansion when studying
RSD. Differences in shapes of the genus curves in real and redshift space are
small when we adopt thick redshift shells, but the amplitude change remains a
significant $\sim {\cal O}(10\%)$ effect. The combined effects of galaxy
biasing and shot noise produce systematic effects up to the second Hermite
polynomial. It is shown that, when sampling, the use of galaxy mass cuts
significantly reduces the effect of shot noise relative to random sampling.
",physics
"  Tungsten (W) is widely considered as the most promising plasma facing
material, which is used in nuclear fusion devices. During the operation of the
nuclear fusion devices, transmutation elements, such as Re, Os, and Ta, are
generated in W due to the transmutation reaction under fusion neutron
irradiation. In this paper, we investigated the effects of the transmutation
elements on the mechanical properties of W and the behavior of hydrogen/helium
(H/He) atom in W by using the rst principles calculation method. The results
are that the generation of the transmutation elements can enhance the ductility
of W without considering the dislocation and other defects, and this phenomenon
is called as solution toughen. However, there is not a strict linear
relationship between the change of the mechanical properties and the
transmutation elements concentration. Compared with the H/He atom in pure W,
the formation energy of the H/He in W are decreased by the transmutation
elements, but the transmutation elements does not change the most favorable
sites for H/He in W. An attractive interaction exists between the transmutation
elements and H/He in W, while a repulsive interaction exists between Ta and He
in W. The best diffusion path H/He in W is changed due to the interaction
between the transmutation elements and H/He. All of the above results provide
important information for application of W as the plasma facing material in the
nuclear fusion devices.
",physics
"  Observations of stars in the the solar vicinity show a clear tendency for old
stars to have larger velocity dispersions. This relation is called the
age-velocity dispersion relation (AVR) and it is believed to provide insight
into the heating history of the Milky Way galaxy. Here, in order to investigate
the origin of the AVR, we performed smoothed particle hydrodynamic simulations
of the self-gravitating multiphase gas disks in the static disk-halo
potentials. Star formation from cold and dense gas is taken into account, and
we analyze the evolution of these star particles. We find that exponents of
simulated AVR and the ratio of the radial to vertical velocity dispersion are
close to the observed values. We also find that the simulated AVR is not a
simple consequence of dynamical heating. The evolution tracks of stars with
different epochs evolve gradually in the age-velocity dispersion plane as a
result of: (1) the decrease in velocity dispersion in star forming regions, and
(2) the decrease in the number of cold/dense/gas as scattering sources. These
results suggest that the AVR involves not only the heating history of a stellar
disk, but also the historical evolution of the ISM in a galaxy.
",physics
"  In concurrent systems, some form of synchronisation is typically needed to
achieve data-race freedom, which is important for correctness and safety. In
actor-based systems, messages are exchanged concurrently but executed
sequentially by the receiving actor. By relying on isolation and non-sharing,
an actor can access its own state without fear of data-races, and the internal
behavior of an actor can be reasoned about sequentially.
However, actor isolation is sometimes too strong to express useful patterns.
For example, letting the iterator of a data-collection alias the internal
structure of the collection allows a more efficient implementation than if each
access requires going through the interface of the collection. With full
isolation, in order to maintain sequential reasoning the iterator must be made
part of the collection, which bloats the interface of the collection and means
that a client must have access to the whole data-collection in order to use the
iterator.
In this paper, we propose a programming language construct that enables a
relaxation of isolation but without sacrificing sequential reasoning. We
formalise the mechanism in a simple lambda calculus with actors and passive
objects, and show how an actor may leak parts of its internal state while
ensuring that any interaction with this data is still synchronised.
",computer-science
"  We characterize the response of the quiet time (no substorms or storms)
large-scale ionospheric transient equivalent currents to north-south and
south-north IMF turnings by using a dynamical network of ground-based
magnetometers. Canonical correlation between all pairs of SuperMAG magnetometer
stations in the Northern Hemisphere (magnetic latitude (MLAT) 50-82$^{\circ}$)
is used to establish the extent of near-simultaneous magnetic response between
regions of magnetic local time-MLAT. Parameters and maps that describe
spatial-temporal correlation are used to characterize the system and its
response to the turnings aggregated over several hundred events. We find that
regions that experience large increases in correlation post turning coincide
with typical locations of a two-cell convection system and are influenced by
the interplanetary magnetic field $\mathit{B}_{y}$. The time between the
turnings reaching the magnetopause and a network response is found to be
$\sim$8-10 min and correlation in the dayside occurs 2-8 min before that in the
nightside.
",physics
"  We present the complete optical transmission spectrum of the hot Jupiter
WASP-4b from 440-940 nm at R ~ 400-1500 obtained with the Gemini Multi-Object
Spectrometers (GMOS); this is the first result from a comparative
exoplanetology survey program of close-in gas giants conducted with GMOS.
WASP-4b has an equilibrium temperature of 1700 K and is favorable to study in
transmission due to a large scale height (370 km). We derive the transmission
spectrum of WASP-4b using 4 transits observed with the MOS technique. We
demonstrate repeatable results across multiple epochs with GMOS, and derive a
combined transmission spectrum at a precision about twice above photon noise,
which is roughly equal to to one atmospheric scale height. The transmission
spectrum is well fitted with a uniform opacity as a function of wavelength. The
uniform opacity and absence of a Rayleigh slope from molecular hydrogen suggest
that the atmosphere is dominated by clouds with condensate grain size of ~1 um.
This result is consistent with previous observations of hot Jupiters since
clouds have been seen in planets with similar equilibrium temperatures to
WASP-4b. We describe a custom pipeline that we have written to reduce GMOS
time-series data of exoplanet transits, and present a thorough analysis of the
dominant noise sources in GMOS, which primarily consist of wavelength- and
time- dependent displacements of the spectra on the detector, mainly due to a
lack of atmospheric dispersion correction.
",physics
"  Inspired by the tremendous success of deep Convolutional Neural Networks as
generic feature extractors for images, we propose TimeNet: a deep recurrent
neural network (RNN) trained on diverse time series in an unsupervised manner
using sequence to sequence (seq2seq) models to extract features from time
series. Rather than relying on data from the problem domain, TimeNet attempts
to generalize time series representation across domains by ingesting time
series from several domains simultaneously. Once trained, TimeNet can be used
as a generic off-the-shelf feature extractor for time series. The
representations or embeddings given by a pre-trained TimeNet are found to be
useful for time series classification (TSC). For several publicly available
datasets from UCR TSC Archive and an industrial telematics sensor data from
vehicles, we observe that a classifier learned over the TimeNet embeddings
yields significantly better performance compared to (i) a classifier learned
over the embeddings given by a domain-specific RNN, as well as (ii) a nearest
neighbor classifier based on Dynamic Time Warping.
",computer-science
"  E. Opdam introduced the tool of spectral transfer morphism (STM) of affine
Hecke algebras to study the formal degrees of unipotent discrete series
representations. He established a uniqueness property of STM for the affine
Hecke algebras associated of unipotent discrete series representations. Based
on this result, Opdam gave an explanation for Lusztig's arithmetic/geometric
correspondence (in Lusztig's classification of unipotent representations of
$p$-adic adjoint simple groups) in terms of harmonic analysis, and partitioned
the unipotent discrete series representations into $L$-packets based on the
Lusztig-Langlands parameters. The present paper provides some omitted details
for the argument of the uniqueness property of STM. In the last section, we
prove that three finite morphisms of algebraic tori are spectral transfer
morphisms, and hence complete the proof of the uniqueness property.
",mathematics
"  We revisit the relation between the neutrino masses and the spontaneous
breaking of the B-L gauge symmetry. We discuss the main scenarios for Dirac and
Majorana neutrinos and point out two simple mechanisms for neutrino masses. In
this context the neutrino masses can be generated either at tree level or at
quantum level and one predicts the existence of very light sterile neutrinos
with masses below the eV scale. The predictions for lepton number violating
processes such as mu to e and mu to e gamma are discussed in detail. The impact
from the cosmological constraints on the effective number of relativistic
degree of freedom is investigated.
",physics
"  Personalized search has been a hot research topic for many years and has been
widely used in e-commerce. This paper describes our solution to tackle the
challenge of personalized e-commerce search at CIKM Cup 2016. The goal of this
competition is to predict search relevance and re-rank the result items in SERP
according to the personalized search, browsing and purchasing preferences.
Based on a detailed analysis of the provided data, we extract three different
types of features, i.e., statistic features, query-item features and session
features. Different models are used on these features, including logistic
regression, gradient boosted decision trees, rank svm and a novel deep match
model. With the blending of multiple models, a stacking ensemble model is built
to integrate the output of individual models and produce a more accurate
prediction result. Based on these efforts, our solution won the champion of the
competition on all the evaluation metrics.
",computer-science
"  Modelling information cascades over online social networks is important in
fields from marketing to civil unrest prediction, however the underlying
network structure strongly affects the probability and nature of such cascades.
Even with simple cascade dynamics the probability of large cascades are almost
entirely dictated by network properties, with well-known networks such as
Erdos-Renyi and Barabasi-Albert producing wildly different cascades from the
same model. Indeed, the notion of 'superspreaders' has arisen to describe
highly influential nodes promoting global cascades in a social network. Here we
use a simple model of global cascades to show that the presence of locality in
the network increases the probability of a global cascade due to the increased
vulnerability of connecting nodes. Rather than 'super-spreaders', we find that
the presence of these highly connected 'super-blockers' in heavy-tailed
networks in fact reduces the probability of global cascades, while promoting
information spread when targeted as the initial spreader.
",computer-science
"  Calibration of the Advanced LIGO detectors is the quantification of the
detectors' response to gravitational waves. Gravitational waves incident on the
detectors cause phase shifts in the interferometer laser light which are read
out as intensity fluctuations at the detector output. Understanding this
detector response to gravitational waves is crucial to producing accurate and
precise gravitational wave strain data. Estimates of binary black hole and
neutron star parameters and tests of general relativity require well-calibrated
data, as miscalibrations will lead to biased results. We describe the method of
producing calibration uncertainty estimates for both LIGO detectors in the
first and second observing runs.
",physics
"  A mapping of the process on a continuous configuration space to the symbolic
representation of the motion on a discrete state space will be combined with an
iterative aggregation and disaggregation (IAD) procedure to obtain steady state
distributions of the process. The IAD speeds up the convergence to the unit
eigenvector, which is the steady state distribution, by forming smaller
aggregated matrices whose unit eigenvector solutions are used to refine
approximations of the steady state vector until convergence is reached. This
method works very efficiently and can be used together with distributed or
parallel computing methods to obtain high resolution images of the steady state
distribution of complex atomistic or energy landscape type problems. The method
is illustrated in two numerical examples. In the first example the transition
matrix is assumed to be known. The second example represents an overdamped
Brownian motion process subject to a dichotomously changing external potential.
",physics
"  Networked system often relies on distributed algorithms to achieve a global
computation goal with iterative local information exchanges between neighbor
nodes. To preserve data privacy, a node may add a random noise to its original
data for information exchange at each iteration. Nevertheless, a neighbor node
can estimate other's original data based on the information it received. The
estimation accuracy and data privacy can be measured in terms of $(\epsilon,
\delta)$-data-privacy, defined as the probability of $\epsilon$-accurate
estimation (the difference of an estimation and the original data is within
$\epsilon$) is no larger than $\delta$ (the disclosure probability). How to
optimize the estimation and analyze data privacy is a critical and open issue.
In this paper, a theoretical framework is developed to investigate how to
optimize the estimation of neighbor's original data using the local information
received, named optimal distributed estimation. Then, we study the disclosure
probability under the optimal estimation for data privacy analysis. We further
apply the developed framework to analyze the data privacy of the
privacy-preserving average consensus algorithm and identify the optimal noises
for the algorithm.
",computer-science
"  Fraenkel and Simpson showed that the number of distinct squares in a word of
length n is bounded from above by 2n, since at most two distinct squares have
their rightmost, or last, occurrence begin at each position. Improvements by
Ilie to $2n-\Theta(\log n)$ and by Deza et al. to 11n/6 rely on the study of
combinatorics of FS-double-squares, when the maximum number of two last
occurrences of squares begin. In this paper, we first study how to maximize
runs of FS-double-squares in the prefix of a word. We show that for a given
positive integer m, the minimum length of a word beginning with m
FS-double-squares, whose lengths are equal, is 7m+3. We construct such a word
and analyze its distinct-square-sequence as well as its
distinct-square-density. We then generalize our construction. We also construct
words with high distinct-square-densities that approach 5/6.
",computer-science
"  The OSIRIS-REx Visible and Infrared Spectrometer (OVIRS) is a point
spectrometer covering the spectral range of 0.4 to 4.3 microns (25,000-2300
cm-1). Its primary purpose is to map the surface composition of the asteroid
Bennu, the target asteroid of the OSIRIS-REx asteroid sample return mission.
The information it returns will help guide the selection of the sample site. It
will also provide global context for the sample and high spatial resolution
spectra that can be related to spatially unresolved terrestrial observations of
asteroids. It is a compact, low-mass (17.8 kg), power efficient (8.8 W
average), and robust instrument with the sensitivity needed to detect a 5%
spectral absorption feature on a very dark surface (3% reflectance) in the
inner solar system (0.89-1.35 AU). It, in combination with the other
instruments on the OSIRIS-REx Mission, will provide an unprecedented view of an
asteroid's surface.
",physics
"  Previous studies have demonstrated the empirical success of word embeddings
in various applications. In this paper, we investigate the problem of learning
distributed representations for text documents which many machine learning
algorithms take as input for a number of NLP tasks.
We propose a neural network model, KeyVec, which learns document
representations with the goal of preserving key semantics of the input text. It
enables the learned low-dimensional vectors to retain the topics and important
information from the documents that will flow to downstream tasks. Our
empirical evaluations show the superior quality of KeyVec representations in
two different document understanding tasks.
",computer-science
"  This work presents an algorithm to generate depth, quantum gate and qubit
optimized circuits for $GF(2^m)$ squaring in the polynomial basis. Further, to
the best of our knowledge the proposed quantum squaring circuit algorithm is
the only work that considers depth as a metric to be optimized. We compared
circuits generated by our proposed algorithm against the state of the art and
determine that they require $50 \%$ fewer qubits and offer gates savings that
range from $37 \%$ to $68 \%$. Further, existing quantum exponentiation are
based on either modular or integer arithmetic. However, Galois arithmetic is a
useful tool to design resource efficient quantum exponentiation circuit
applicable in quantum cryptanalysis. Therefore, we present the quantum circuit
implementation of Galois field exponentiation based on the proposed quantum
Galois field squaring circuit. We calculated a qubit savings ranging between
$44\%$ to $50\%$ and quantum gate savings ranging between $37 \%$ to $68 \%$
compared to identical quantum exponentiation circuit based on existing squaring
circuits.
",computer-science
"  We review instrumentation for nuclear magnetic resonance (NMR) in zero and
ultra-low magnetic field (ZULF, below 0.1 $\mu$T) where detection is based on a
low-cost, non-cryogenic, spin-exchange relaxation free (SERF) $^{87}$Rb atomic
magnetometer. The typical sensitivity is 20-30 fT/Hz$^{1/2}$ for signal
frequencies below 1 kHz and NMR linewidths range from Hz all the way down to
tens of mHz. These features enable precision measurements of chemically
informative nuclear spin-spin couplings as well as nuclear spin precession in
ultra-low magnetic fields.
",physics
"  The dueling bandits problem is an online learning framework for learning from
pairwise preference feedback, and is particularly well-suited for modeling
settings that elicit subjective or implicit human feedback. In this paper, we
study the problem of multi-dueling bandits with dependent arms, which extends
the original dueling bandits setting by simultaneously dueling multiple arms as
well as modeling dependencies between arms. These extensions capture key
characteristics found in many real-world applications, and allow for the
opportunity to develop significantly more efficient algorithms than were
possible in the original setting. We propose the \selfsparring algorithm, which
reduces the multi-dueling bandits problem to a conventional bandit setting that
can be solved using a stochastic bandit algorithm such as Thompson Sampling,
and can naturally model dependencies using a Gaussian process prior. We present
a no-regret analysis for multi-dueling setting, and demonstrate the
effectiveness of our algorithm empirically on a wide range of simulation
settings.
",computer-science
"  Primordial Black Holes (PBH) could be the cold dark matter of the universe.
They could have arisen from large (order one) curvature fluctuations produced
during inflation that reentered the horizon in the radiation era. At reentry,
these fluctuations source gravitational waves (GW) via second order anisotropic
stresses. These GW, together with those (possibly) sourced during inflation by
the same mechanism responsible for the large curvature fluctuations, constitute
a primordial stochastic GW background (SGWB) that unavoidably accompanies the
PBH formation. We study how the amplitude and the range of frequencies of this
signal depend on the statistics (Gaussian versus $\chi^2$) of the primordial
curvature fluctuations, and on the evolution of the PBH mass function due to
accretion and merging. We then compare this signal with the sensitivity of
present and future detectors, at PTA and LISA scales. We find that this SGWB
will help to probe, or strongly constrain, the early universe mechanism of PBH
production. The comparison between the peak mass of the PBH distribution and
the peak frequency of this SGWB will provide important information on the
merging and accretion evolution of the PBH mass distribution from their
formation to the present era. Different assumptions on the statistics and on
the PBH evolution also result in different amounts of CMB $\mu$-distortions.
Therefore the above results can be complemented by the detection (or the
absence) of $\mu$-distortions with an experiment such as PIXIE.
",physics
"  With the advent of deep learning, object detection drifted from a bottom-up
to a top-down recognition problem. State of the art algorithms enumerate a
near-exhaustive list of object locations and classify each into: object or not.
In this paper, we show that bottom-up approaches still perform competitively.
We detect four extreme points (top-most, left-most, bottom-most, right-most)
and one center point of objects using a standard keypoint estimation network.
We group the five keypoints into a bounding box if they are geometrically
aligned. Object detection is then a purely appearance-based keypoint estimation
problem, without region classification or implicit feature learning. The
proposed method performs on-par with the state-of-the-art region based
detection methods, with a bounding box AP of 43.2% on COCO test-dev. In
addition, our estimated extreme points directly span a coarse octagonal mask,
with a COCO Mask AP of 18.9%, much better than the Mask AP of vanilla bounding
boxes. Extreme point guided segmentation further improves this to 34.6% Mask
AP.
",computer-science
"  Data quality of Phasor Measurement Unit (PMU) is receiving increasing
attention as it has been identified as one of the limiting factors that affect
many wide-area measurement system (WAMS) based applications. In general,
existing PMU calibration methods include offline testing and model based
approaches. However, in practice, the effectiveness of both is limited due to
the very strong assumptions employed. This paper presents a novel framework for
online bias error detection and calibration of PMU measurement using
density-based spatial clustering of applications with noise (DBSCAN) based on
much relaxed assumptions. With a new problem formulation, the proposed data
mining based methodology is applicable across a wide spectrum of practical
conditions and one side-product of it is more accurate transmission line
parameters for EMS database and protective relay settings. Case studies
demonstrate the effectiveness of the proposed approach.
",computer-science
"  We present a search for CII emission over cosmological scales at
high-redshifts. The CII line is a prime candidate to be a tracer of star
formation over large-scale structure since it is one of the brightest emission
lines from galaxies. Redshifted CII emission appears in the submillimeter
regime, meaning it could potentially be present in the higher frequency
intensity data from the Planck satellite used to measure the cosmic infrared
background (CIB). We search for CII emission over redshifts z=2-3.2 in the
Planck 545 GHz intensity map by cross-correlating the 3 highest frequency
Planck maps with spectroscopic quasars and CMASS galaxies from the Sloan
Digital Sky Survey III (SDSS-III), which we then use to jointly fit for CII
intensity, CIB parameters, and thermal Sunyaev-Zeldovich (SZ) emission. We
report a measurement of an anomalous emission
$\mathrm{I_\nu}=6.6^{+5.0}_{-4.8}\times10^4$ Jy/sr at 95% confidence, which
could be explained by CII emission, favoring collisional excitation models of
CII emission that tend to be more optimistic than models based on CII
luminosity scaling relations from local measurements; however, a comparison of
Bayesian information criteria reveal that this model and the CIB & SZ only
model are equally plausible. Thus, more sensitive measurements will be needed
to confirm the existence of large-scale CII emission at high redshifts.
Finally, we forecast that intensity maps from Planck cross-correlated with
quasars from the Dark Energy Spectroscopic Instrument (DESI) would increase our
sensitivity to CII emission by a factor of 5, while the proposed Primordial
Inflation Explorer (PIXIE) could increase the sensitivity further.
",physics
"  I studied the non-equilibrium response of an initial Néel state under
time evolution with the Kitaev honeycomb model. This time evolution can be
computed using a random sampling over all relevant flux configurations. With
isotropic interactions the system quickly equilibrates into a steady state
valence bond solid. Anisotropy induces an exponentially long prethermal regime
whose dynamics are governed by an effective toric code. Signatures of topology
are absent, however, due to the high energy density nature of the initial
state.
",physics
"  We present our implementation of an automated VLBI data reduction pipeline
dedicated to interferometric data imaging and analysis. The pipeline can handle
massive VLBI data efficiently which makes it an appropriate tool to investigate
multi-epoch multiband VLBI data. Compared to traditional manual data reduction,
our pipeline provides more objective results since less human interference is
involved. Source extraction is done in the image plane, while deconvolution and
model fitting are done in both the image plane and the uv plane for parallel
comparison. The output from the pipeline includes catalogues of CLEANed images
and reconstructed models, polarisation maps, proper motion estimates, core
light curves and multi-band spectra. We have developed a regression strip
algorithm to automatically detect linear or non-linear patterns in the jet
component trajectories. This algorithm offers an objective method to match jet
components at different epochs and determine their proper motions.
",physics
"  The goal of unbounded program verification is to discover an inductive
invariant that safely over-approximates all possible program behaviors.
Functional languages featuring higher order and recursive functions become more
popular due to the domain-specific needs of big data analytics, web, and
security. We present Rosette/Unbound, the first program verifier for Racket
exploiting the automated constrained Horn solver on its backend. One of the key
features of Rosette/Unbound is the ability to synchronize recursive
computations over the same inputs allowing to verify programs that iterate over
unbounded data streams multiple times. Rosette/Unbound is successfully
evaluated on a set of non-trivial recursive and higher order functional
programs.
",computer-science
"  A streaming graph is a graph formed by a sequence of incoming edges with time
stamps. Unlike static graphs, the streaming graph is highly dynamic and time
related. In the real world, the high volume and velocity streaming graphs such
as internet traffic data, social network communication data and financial
transfer data are bringing challenges to the classic graph data structures. We
present a new data structure: double orthogonal list in hash table (Dolha)
which is a high speed and high memory efficiency graph structure applicable to
streaming graph. Dolha has constant time cost for single edge and near linear
space cost that we can contain billions of edges information in memory size and
process an incoming edge in nanoseconds. Dolha also has linear time cost for
neighborhood queries, which allow it to support most algorithms in graphs
without extra cost. We also present a persistent structure based on Dolha that
has the ability to handle the sliding window update and time related queries.
",computer-science
"  We present a machine learning based information retrieval system for
astronomical observatories that tries to address user defined queries related
to an instrument. In the modern instrumentation scenario where heterogeneous
systems and talents are simultaneously at work, the ability to supply with the
right information helps speeding up the detector maintenance operations.
Enhancing the detector uptime leads to increased coincidence observation and
improves the likelihood for the detection of astrophysical signals. Besides,
such efforts will efficiently disseminate technical knowledge to a wider
audience and will help the ongoing efforts to build upcoming detectors like the
LIGO-India etc even at the design phase to foresee possible challenges. The
proposed method analyses existing documented efforts at the site to
intelligently group together related information to a query and to present it
on-line to the user. The user in response can further go into interesting links
and find already developed solutions or probable ways to address the present
situation optimally. A web application that incorporates the above idea has
been implemented and tested for LIGO Livingston, LIGO Hanford and Virgo
observatories.
",physics
"  Our work presented in this paper focuses on the translation of terminological
expressions represented in semantically structured resources, like ontologies
or knowledge graphs. The challenge of translating ontology labels or
terminological expressions represented in knowledge bases lies in the highly
specific vocabulary and the lack of contextual information, which can guide a
machine translation system to translate ambiguous words into the targeted
domain. Due to these challenges, we evaluate the translation quality of
domain-specific expressions in the medical and financial domain with
statistical (SMT) as well as with neural machine translation (NMT) methods and
experiment domain adaptation of the translation models with terminological
expressions only. Furthermore, we perform experiments on the injection of
external terminological expressions into the translation systems. Through these
experiments, we observed a significant advantage in domain adaptation for the
domain-specific resource in the medical and financial domain and the benefit of
subword models over word-based NMT models for terminology translation.
",computer-science
"  Recently, inference about high-dimensional integrated covariance matrices
(ICVs) based on noisy high-frequency data has emerged as a challenging problem.
In the literature, a pre-averaging estimator (PA-RCov) is proposed to deal with
the microstructure noise. Using the large-dimensional random matrix theory, it
has been established that the eigenvalue distribution of the PA-RCov matrix is
intimately linked to that of the ICV through the Marcenko-Pastur equation.
Consequently, the spectrum of the ICV can be inferred from that of the PA-RCov.
However, extensive data analyses demonstrate that the spectrum of the PA-RCov
is spiked, that is, a few large eigenvalues (spikes) stay away from the others
which form a rather continuous distribution with a density function (bulk).
Therefore, any inference on the ICVs must take into account this spiked
structure. As a methodological contribution, we propose a spiked model for the
ICVs where spikes can be inferred from those of the available PA-RCov matrices.
The consistency of the inference procedure is established and is checked by
extensive simulation studies. In addition, we apply our method to the real data
from the US and Hong Kong markets. It is found that our model clearly
outperforms the existing one in predicting the existence of spikes and in
mimicking the empirical PA-RCov matrices.
",statistics
"  The main purpose of this paper is to propose a new preprocessing step in
order to improve local feature descriptors and texture classification.
Preprocessing is implemented by using transformations which help highlight
salient features that play a significant role in texture recognition. We
evaluate and compare four different competing methods: three different
anisotropic diffusion methods including the classical anisotropic Perona-Malik
diffusion and two subsequent regularizations of it and the application of a
Gaussian kernel, which is the classical multiscale approach in texture
analysis. The combination of the transformed images and the original ones are
analyzed. The results show that the use of the preprocessing step does lead to
improved texture recognition.
",computer-science
"  In this paper, we argue why and how the integration of recommender systems
for research can enhance the functionality and user experience in repositories.
We present the latest technical innovations in the CORE Recommender, which
provides research article recommendations across the global network of
repositories and journals. The CORE Recommender has been recently redeveloped
and released into production in the CORE system and has also been deployed in
several third-party repositories. We explain the design choices of this unique
system and the evaluation processes we have in place to continue raising the
quality of the provided recommendations. By drawing on our experience, we
discuss the main challenges in offering a state-of-the-art recommender solution
for repositories. We highlight two of the key limitations of the current
repository infrastructure with respect to developing research recommender
systems: 1) the lack of a standardised protocol and capabilities for exposing
anonymised user-interaction logs, which represent critically important input
data for recommender systems based on collaborative filtering and 2) the lack
of a voluntary global sign-on capability in repositories, which would enable
the creation of personalised recommendation and notification solutions based on
past user interactions.
",computer-science
"  This study focusses on self-balancing microgrids to smartly utilize and
prevent overdrawing of available power capacity of the grid. A distributed
framework for automated distribution of optimal power demand is proposed, where
all building in a microgrid dynamically and simultaneously adjusts their own
power consumption to reach their individual optimal power demands while
cooperatively striving to maintain the overall grid stable. Emphasis has been
given to aspects of algorithm that yields lower time of convergence and is
demonstrated through quantitative and qualitative analysis of simulation
results.
",computer-science
"  We report on thermodynamic, magnetization, and muon spin relaxation
measurements of the strong spin-orbit coupled iridate Ba$_3$IrTi$_2$O$_9$,
which constitutes a new frustration motif made up a mixture of edge- and
corner-sharing triangles. In spite of strong antiferromagnetic exchange
interaction of the order of 100~K, we find no hint for long-range magnetic
order down to 23 mK. The magnetic specific heat data unveil the $T$-linear and
-squared dependences at low temperatures below 1~K. At the respective
temperatures, the zero-field muon spin relaxation features a persistent spin
dynamics, indicative of unconventional low-energy excitations. A comparison to
the $4d$ isostructural compound Ba$_3$RuTi$_2$O$_9$ suggests that a concerted
interplay of compass-like magnetic interactions and frustrated geometry
promotes a dynamically fluctuating state in a triangle-based iridate.
",physics
"  Using quantum representations of mapping class groups we prove that profinite
completions of Burnside-type surface group quotients are not virtually
prosolvable, in general. Further, we construct infinitely many finite simple
characteristic quotients of surface groups.
",mathematics
"  We focus on two particular aspects of model risk: the inability of a chosen
model to fit observed market prices at a given point in time (calibration
error) and the model risk due to recalibration of model parameters (in
contradiction to the model assumptions). In this context, we follow the
approach of Glasserman and Xu (2014) and use relative entropy as a pre-metric
in order to quantify these two sources of model risk in a common framework, and
consider the trade-offs between them when choosing a model and the frequency
with which to recalibrate to the market. We illustrate this approach applied to
the models of Black and Scholes (1973) and Heston (1993), using option data for
Apple (AAPL) and Google (GOOG). We find that recalibrating a model more
frequently simply shifts model risk from one type to another, without any
substantial reduction of aggregate model risk. Furthermore, moving to a more
complicated stochastic model is seen to be counterproductive if one requires a
high degree of robustness, for example as quantified by a 99 percent quantile
of aggregate model risk.
",quantitative-finance
"  In this paper, a novel method using 3D Convolutional Neural Network (3D-CNN)
architecture has been proposed for speaker verification in the text-independent
setting. One of the main challenges is the creation of the speaker models. Most
of the previously-reported approaches create speaker models based on averaging
the extracted features from utterances of the speaker, which is known as the
d-vector system. In our paper, we propose an adaptive feature learning by
utilizing the 3D-CNNs for direct speaker model creation in which, for both
development and enrollment phases, an identical number of spoken utterances per
speaker is fed to the network for representing the speakers' utterances and
creation of the speaker model. This leads to simultaneously capturing the
speaker-related information and building a more robust system to cope with
within-speaker variation. We demonstrate that the proposed method significantly
outperforms the traditional d-vector verification system. Moreover, the
proposed system can also be an alternative to the traditional d-vector system
which is a one-shot speaker modeling system by utilizing 3D-CNNs.
",computer-science
"  The General AI Challenge is an initiative to encourage the wider artificial
intelligence community to focus on important problems in building intelligent
machines with more general scope than is currently possible. The challenge
comprises of multiple rounds, with the first round focusing on gradual
learning, i.e. the ability to re-use already learned knowledge for efficiently
learning to solve subsequent problems. In this article, we will present details
of the first round of the challenge, its inspiration and aims. We also outline
a more formal description of the challenge and present a preliminary analysis
of its curriculum, based on ideas from computational mechanics. We believe,
that such formalism will allow for a more principled approach towards
investigating tasks in the challenge, building new curricula and for
potentially improving consequent challenge rounds.
",computer-science
"  Regularization techniques such as the lasso (Tibshirani 1996) and elastic net
(Zou and Hastie 2005) can be used to improve regression model coefficient
estimation and prediction accuracy, as well as to perform variable selection.
Ordinal regression models are widely used in applications where the use of
regularization could be beneficial; however, these models are not included in
many popular software packages for regularized regression. We propose a
coordinate descent algorithm to fit a broad class of ordinal regression models
with an elastic net penalty. Furthermore, we demonstrate that each model in
this class generalizes to a more flexible form, for instance to accommodate
unordered categorical data. We introduce an elastic net penalty class that
applies to both model forms. Additionally, this penalty can be used to shrink a
non-ordinal model toward its ordinal counterpart. Finally, we introduce the R
package ordinalNet, which implements the algorithm for this model class.
",statistics
"  This paper investigates gradient recovery schemes for data defined on
discretized manifolds. The proposed method, parametric polynomial preserving
recovery (PPPR), does not require the tangent spaces of the exact manifolds
which have been assumed for some significant gradient recovery methods in the
literature. Another advantage is that superconvergence is guaranteed for PPPR
without the symmetric condition which has been asked in the existing
techniques. There is also numerical evidence that the superconvergence by PPPR
is high curvature stable, which distinguishes itself from the other methods. As
an application, we show that its capability of constructing an asymptotically
exact \textit{a posteriori} error estimator. Several numerical examples on
two-dimensional surfaces are presented to support the theoretical results and
make comparisons with state of the art methods.
",mathematics
"  We survey problems and results from combinatorial geometry in normed spaces,
concentrating on problems that involve distances. These include various
properties of unit-distance graphs, minimum-distance graphs, diameter graphs,
as well as minimum spanning trees and Steiner minimum trees. In particular, we
discuss translative kissing (or Hadwiger) numbers, equilateral sets, and the
Borsuk problem in normed spaces. We show how to use the angular measure of
Peter Brass to prove various statements about Hadwiger and blocking numbers of
convex bodies in the plane, including some new results. We also include some
new results on thin cones and their application to distinct distances and other
combinatorial problems for normed spaces.
",mathematics
"  We define an action of the extended affine d-strand braid group on the open
positroid stratum in the Grassmannian Gr(k,n), for d the greatest common
divisor of k and n. The action is by quasi-automorphisms of the cluster
structure on the Grassmannian, determining a homomorphism from the extended
affine braid group to the cluster modular group. We also define a
quasi-isomorphism between the Grassmannian Gr(k,rk) and the Fock-Goncharov
configuration space of 2r-tuples of affine flags for SL(k). This identifies the
cluster variables, clusters, and cluster modular groups, in these two cluster
structures.
Fomin and Pylyavskyy proposed a description of the cluster combinatorics for
Gr(3,n) in terms of Kuperberg's basis of non-elliptic webs. As our main
application, we prove many of their conjectures for Gr(3,9) and give a
presentation for its cluster modular group. We establish similar results for
Gr(4,8). These results rely on the fact that both of these Grassmannians have
finite mutation type.
",mathematics
"  The paper investigates the asymptotic behavior of a 2D overhead crane with
input delays in the boundary control. A linear boundary control is proposed.
The main feature of such a control lies in the facts that it solely depends on
the velocity but under the presence of time-delays. We end-up with a
closed-loop system where no displacement term is involved. It is shown that the
problem is well-posed in the sense of semigroups theory. LaSalle's invariance
principle is invoked in order to establish the asymptotic convergence for the
solutions of the system to a stationary position which depends on the initial
data. Using a resolvent method it is proved that the convergence is indeed
polynomial.
",mathematics
"  The pyrochlore magnet $\rm Yb_2Ti_2O_7$ has been proposed as a quantum spin
ice candidate, a spin liquid state expected to display emergent quantum
electrodynamics with gauge photons among its elementary excitations. However,
$\rm Yb_2Ti_2O_7$'s ground state is known to be very sensitive to its precise
stoichiometry. Powder samples, produced by solid state synthesis at relatively
low temperatures, tend to be stoichiometric, while single crystals grown from
the melt tend to display weak ""stuffing"" wherein $\mathrm{\sim 2\%}$ of the
$\mathrm{Yb^{3+}}$, normally at the $A$ site of the $A_2B_2O_7$ pyrochlore
structure, reside as well at the $B$ site. In such samples $\mathrm{Yb^{3+}}$
ions should exist in defective environments at low levels, and be subjected to
crystalline electric fields (CEFs) very different from those at the
stoichiometric $A$ sites. New neutron scattering measurements of
$\mathrm{Yb^{3+}}$ in four compositions of $\rm Yb_{2+x}Ti_{2-x}O_{7-y}$, show
the spectroscopic signatures for these defective $\mathrm{Yb^{3+}}$ ions and
explicitly demonstrate that the spin anisotropy of the $\mathrm{Yb^{3+}}$
moment changes from XY-like for stoichiometric $\mathrm{Yb^{3+}}$, to
Ising-like for ""stuffed"" $B$ site $\mathrm{Yb^{3+}}$, or for $A$ site
$\mathrm{Yb^{3+}}$ in the presence of an oxygen vacancy.
",physics
"  Considering its advantages in dealing with high-dimensional visual input and
learning control policies in discrete domain, Deep Q Network (DQN) could be an
alternative method of traditional auto-focus means in the future. In this
paper, based on Deep Reinforcement Learning, we propose an end-to-end approach
that can learn auto-focus policies from visual input and finish at a clear spot
automatically. We demonstrate that our method - discretizing the action space
with coarse to fine steps and applying DQN is not only a solution to auto-focus
but also a general approach towards vision-based control problems. Separate
phases of training in virtual and real environments are applied to obtain an
effective model. Virtual experiments, which are carried out after the virtual
training phase, indicates that our method could achieve 100% accuracy on a
certain view with different focus range. Further training on real robots could
eliminate the deviation between the simulator and real scenario, leading to
reliable performances in real applications.
",computer-science
"  This work focuses on reliable detection and segmentation of bird
vocalizations as recorded in the open field. Acoustic detection of avian sounds
can be used for the automatized monitoring of multiple bird taxa and querying
in long-term recordings for species of interest. These tasks are tackled in
this work, by suggesting two approaches: A) First, DenseNets are applied to
weekly labeled data to infer the attention map of the dataset (i.e. Salience
and CAM). We push further this idea by directing attention maps to the YOLO v2
Deepnet-based, detection framework to localize bird vocalizations. B) A deep
autoencoder, namely the U-net, maps the audio spectrogram of bird vocalizations
to its corresponding binary mask that encircles the spectral blobs of
vocalizations while suppressing other audio sources. We focus solely on
procedures requiring minimum human attendance, suitable to scan massive volumes
of data, in order to analyze them, evaluate insights and hypotheses and
identify patterns of bird activity. Hopefully, this approach will be valuable
to researchers, conservation practitioners, and decision makers that need to
design policies on biodiversity issues.
",computer-science
"  Robust reinforcement learning aims to produce policies that have strong
guarantees even in the face of environments/transition models whose parameters
have strong uncertainty. Existing work uses value-based methods and the usual
primitive action setting. In this paper, we propose robust methods for learning
temporally abstract actions, in the framework of options. We present a Robust
Options Policy Iteration (ROPI) algorithm with convergence guarantees, which
learns options that are robust to model uncertainty. We utilize ROPI to learn
robust options with the Robust Options Deep Q Network (RO-DQN) that solves
multiple tasks and mitigates model misspecification due to model uncertainty.
We present experimental results which suggest that policy iteration with linear
features may have an inherent form of robustness when using coarse feature
representations. In addition, we present experimental results which demonstrate
that robustness helps policy iteration implemented on top of deep neural
networks to generalize over a much broader range of dynamics than non-robust
policy iteration.
",statistics
"  We consider the multi-cell joint power control and scheduling problem in
cellular wireless networks as a weighted sum-rate maximization problem. This
formulation is very general and applies to a wide range of applications and QoS
requirements. The problem is inherently hard due to objective's non-convexity
and the knapsack-like constraints. Moreover, practical system requires a
distributed operation. We applied an existing algorithm proposed by Scutari et
al. in distributed optimization literature to our problem. The algorithm
performs local optimization followed by consensus update repeatedly. However,
it is not fully applicable to our problem, as it requires all decision
variables to be maintained at every base station (BS), which is impractical for
large-scale networks; also, it relies on the Lipschitz continuity of the
objective function's gradient, which does not hold here. We exploited the
nature of our objective function, and proposed a localized version of the
algorithm. Furthermore, we relaxed the requirements of Lipschitz continuity
with the proximal approximation. Convergence to local optimal solutions was
proved under some conditions. Future work includes proving the above results
from a stochastic approximation perspective, and investigating non-linear
consensus schemes to speed up the convergence.
",mathematics
"  Super-resolution fluorescence microscopy, with a resolution beyond the
diffraction limit of light, has become an indispensable tool to directly
visualize biological structures in living cells at a nanometer-scale
resolution. Despite advances in high-density super-resolution fluorescent
techniques, existing methods still have bottlenecks, including extremely long
execution time, artificial thinning and thickening of structures, and lack of
ability to capture latent structures. Here we propose a novel deep learning
guided Bayesian inference approach, DLBI, for the time-series analysis of
high-density fluorescent images. Our method combines the strength of deep
learning and statistical inference, where deep learning captures the underlying
distribution of the fluorophores that are consistent with the observed
time-series fluorescent images by exploring local features and correlation
along time-axis, and statistical inference further refines the ultrastructure
extracted by deep learning and endues physical meaning to the final image.
Comprehensive experimental results on both real and simulated datasets
demonstrate that our method provides more accurate and realistic local patch
and large-field reconstruction than the state-of-the-art method, the 3B
analysis, while our method is more than two orders of magnitude faster. The
main program is available at this https URL
",statistics
"  We consider the ground-state properties of Rashba spin-orbit-coupled
pseudo-spin-1/2 Bose-Einstein condensates (BECs) in a rotating two-dimensional
(2D) toroidal trap. In the absence of spin-orbit coupling (SOC), the increasing
rotation frequency enhances the creation of giant vortices for the initially
miscible BECs, while it can lead to the formation of semiring density patterns
with irregular hidden vortex structures for the initially immiscible BECs.
Without rotation, strong 2D isotropic SOC yields a heliciform-stripe phase for
the initially immiscible BECs. Combined effects of rotation, SOC, and
interatomic interactions on the vortex structures and typical spin textures of
the ground state of the system are discussed systematically. In particular, for
fixed rotation frequency above the critical value, the increasing isotropic SOC
favors a visible vortex ring in each component which is accompanied by a hidden
giant vortex plus a (several) hidden vortex ring(s) in the central region. In
the case of 1D anisotropic SOC, large SOC strength results in the generation of
hidden linear vortex string and the transition from initial phase separation
(phase mixing) to phase mixing (phase separation). Furthermore, the peculiar
spin textures including skyrmion lattice, skyrmion pair and skyrmion string are
revealed in this system.
",physics
"  Consider a spin manifold M, equipped with a line bundle L and an action of a
compact Lie group G. We can attach to this data a family Theta(k) of
distributions on the dual of the Lie algebra of G. The aim of this paper is to
study the asymptotic behaviour of Theta(k) when k is large, and M possibly non
compact, and to explore a functorial consequence of this formula for reduced
spaces.
",mathematics
"  Oeljeklaus-Toma (OT) manifolds are complex non-Kähler manifolds whose
construction arises from specific number fields. In this note, we compute their
de Rham cohomology in terms of invariants associated to the background number
field. This is done by two distinct approaches, one using invariant cohomology
and the other one using the Leray-Serre spectral sequence. In addition, we
compute also their Morse-Novikov cohomology. As an application, we show that
the low degree Chern classes of any complex vector bundle on an OT manifold
vanish in the real cohomology. Other applications concern the OT manifolds
admitting locally conformally Kähler (LCK) metrics: we show that there is
only one possible Lee class of an LCK metric, and we determine all the possible
Morse-Novikov classes of an LCK metric, which implies the nondegeneracy of
certain Lefschetz maps in cohomology.
",mathematics
"  With the volume of manuscripts submitted for publication growing every year,
the deficiencies of peer review (e.g. long review times) are becoming more
apparent. Editorial strategies, sets of guidelines designed to speed up the
process and reduce editors workloads, are treated as trade secrets by
publishing houses and are not shared publicly. To improve the effectiveness of
their strategies, editors in small publishing groups are faced with undertaking
an iterative trial-and-error approach. We show that Cartesian Genetic
Programming, a nature-inspired evolutionary algorithm, can dramatically improve
editorial strategies. The artificially evolved strategy reduced the duration of
the peer review process by 30%, without increasing the pool of reviewers (in
comparison to a typical human-developed strategy). Evolutionary computation has
typically been used in technological processes or biological ecosystems. Our
results demonstrate that genetic programs can improve real-world social systems
that are usually much harder to understand and control than physical systems.
",computer-science
"  Available possibilities to prevent a biped robot from falling down in the
presence of severe disturbances are mainly Center of Pressure (CoP) modulation,
step location and timing adjustment, and angular momentum regulation. In this
paper, we aim at designing a walking pattern generator which employs an optimal
combination of these tools to generate robust gaits. In this approach, first,
the next step location and timing are decided consistent with the commanded
walking velocity and based on the Divergent Component of Motion (DCM)
measurement. This stage which is done by a very small-size Quadratic Program
(QP) uses the Linear Inverted Pendulum Model (LIPM) dynamics to adapt the
switching contact location and time. Then, consistent with the first stage, the
LIPM with flywheel dynamics is used to regenerate the DCM and angular momentum
trajectories at each control cycle. This is done by modulating the CoP and
Centroidal Momentum Pivot (CMP) to realize a desired DCM at the end of current
step. Simulation results show the merit of this reactive approach in generating
robust and dynamically consistent walking patterns.
",computer-science
"  We present possible explanations of pulsations in early B-type main sequence
stars which arise purely from the excitation of gravity modes. There are three
stars with this type of oscillations detected from the BRITE light curves:
$\kappa$ Cen, a Car, $\kappa$ Vel. We show that by changing metallicity or the
opacity profile it is possible in some models to dump pressure modes keeping
gravity modes unstable. Other possible scenario involves pulsations of a lower
mass companion.
",physics
"  We study the angular dependence of the dissipation in the superconducting
state of FeSe and Fe(Se$_\text{1-x}$Te$_\text{x}$) through electrical transport
measurements, using crystalline intergrown materials. We reveal the key role of
the inclusions of the non superconducting magnetic phase
Fe$_\text{1-y}$(Se$_\text{1-x}$Te$_\text{x}$), growing into the
Fe(Se$_\text{1-x}$Te$_\text{x}$) pure $\beta$-phase, in the development of a
correlated defect structure. The matching of both atomic structures defines the
growth habit of the crystalline material as well as the correlated planar
defects orientation.
",physics
"  Since the concept of spin superconductor was proposed, all the related
studies concentrate on spin-polarized case. Here, we generalize the study to
spin-non-polarized case. The free energy of non-polarized spin superconductor
is obtained, and the Ginzburg-Landau-type equations are derived by using the
variational method. These Ginzburg-Landau-type equations can be reduced to the
spin-polarized case when the spin direction is fixed. Moreover, the expressions
of super linear and angular spin currents inside the superconductor are
derived. We demonstrate that the electric field induced by super spin current
is equal to the one induced by equivalent charge obtained from the second
Ginzburg-Landau-type equation, which shows self-consistency of our theory. By
applying these Ginzburg-Landau-type equations, the effect of electric field on
the superconductor is also studied. These results will help us get a better
understanding of the spin superconductor and the related topics such as
Bose-Einstein condensate of magnons and spin superfluidity.
",physics
"  Let $\mathcal{L}$ be a Schrödinger operator of the form $\mathcal{L} =
-\Delta+V$ acting on $L^2(\mathbb R^n)$ where the nonnegative potential $V$
belongs to the reverse Hölder class $B_q$ for some $q\geq n.$ Let
$L^{p,\lambda}(\mathbb{R}^{n})$, $0\le \lambda<n$ denote the Morrey space on
$\mathbb{R}^{n}$. In this paper, we will show that a function $f\in
L^{2,\lambda}(\mathbb{R}^{n})$ is the trace of the solution of ${\mathbb
L}u=u_{t}+{\mathcal{L}}u=0, u(x,0)= f(x),$ where $u$ satisfies a Carleson-type
condition \begin{eqnarray*} \sup_{x_B, r_B}
r_B^{-\lambda}\int_0^{r_B^2}\int_{B(x_B, r_B)} |\nabla u(x,t)|^2 {dx dt} \leq C
<\infty. \end{eqnarray*} Conversely, this Carleson-type condition characterizes
all the ${\mathbb L}$-carolic functions whose traces belong to the Morrey space
$L^{2,\lambda}(\mathbb{R}^{n})$ for all $0\le \lambda<n$. This result extends
the analogous characterization founded by Fabes and Neri for the classical BMO
space of John and Nirenberg.
",mathematics
"  We introduce intertwining operators among twisted modules or twisted
intertwining operators associated to not-necessarily-commuting automorphisms of
a vertex operator algebra. Let $V$ be a vertex operator algebra and let
$g_{1}$, $g_{2}$ and $g_{3}$ be automorphisms of $V$. We prove that for
$g_{1}$-, $g_{2}$- and $g_{3}$-twisted $V$-modules $W_{1}$, $W_{2}$ and
$W_{3}$, respectively, such that the vertex operator map for $W_{3}$ is
injective, if there exists a twisted intertwining operator of type
${W_{3}\choose W_{1}W_{2}}$ such that the images of its component operators
span $W_{3}$, then $g_{3}=g_{1}g_{2}$. We also construct what we call the
skew-symmetry and contragredient isomorphisms between spaces of twisted
intertwining operators among twisted modules of suitable types. The proofs of
these results involve careful analysis of the analytic extensions corresponding
to the actions of the not-necessarily-commuting automorphisms of the vertex
operator algebra.
",mathematics
"  The effect of spin-orbit coupling (SOC) on the electronic properties of
monolayer (ML) PtSe$_2$ is dictated by the presence of the crystal inversion
symmetry to exhibit spin polarized band without characteristic of spin
splitting. Through fully-relativistic density-functional theory calculations,
we show that large spin-orbit splitting can be induced by introducing point
defects. We calculate stability of native point defects such as a Se vacancy
(V$_{\texttt{Se}}$), a Se interstitial (Se$_{i}$), a Pt vacancy
(V$_{\texttt{Pt}}$), and a Pt interstitial (Pt$_{i}$), and find that both the
V$_{\texttt{Se}}$ and Se$_{i}$ have the lowest formation energy. We also find
that in contrast to the Se$_{i}$ case exhibiting spin degeneracy in the defect
states, the large spin-orbit splitting up to 152 meV is observed in the defect
states of the V$_{\texttt{Se}}$. Our analyses of orbital contributions to the
defect states show that the large spin splitting is originated from the strong
hybridization between Pt-$d_{x{^2}+y{^2}}+d_{xy}$ and Se-$p_{x}+p_{y}$
orbitals. Our study clarifies that the defects play an important role in the
spin splitting properties of the PtSe$_2$ ML, which is important for designing
future spintronic devices.
",physics
"  Multiple planet systems provide an ideal laboratory for probing exoplanet
composition, formation history and potential habitability. For the TRAPPIST-1
planets, the planetary radii are well established from transits (Gillon et al.,
2016, Gillon et al., 2017), with reasonable mass estimates coming from transit
timing variations (Gillon et al., 2017, Wang et al., 2017) and dynamical
modeling (Quarles et al., 2017). The low bulk densities of the TRAPPIST-1
planets demand significant volatile content. Here we show using
mass-radius-composition models, that TRAPPIST-1f and g likely contain
substantial ($\geq50$ wt\%) water/ice, with b and c being significantly drier
($\leq15$ wt\%). We propose this gradient of water mass fractions implies
planets f and g formed outside the primordial snow line whereas b and c formed
inside. We find that compared to planets in our solar system that also formed
within the snow line, TRAPPIST-1b and c contain hundreds more oceans worth of
water. We demonstrate the extent and timescale of migration in the TRAPPIST-1
system depends on how rapidly the planets formed and the relative location of
the primordial snow line. This work provides a framework for understanding the
differences between the protoplanetary disks of our solar system versus M
dwarfs. Our results provide key insights into the volatile budgets, timescales
of planet formation, and migration history of likely the most common planetary
host in the Galaxy.
",physics
"  We investigate the dynamics of a dilute suspension of hydrodynamically
interacting motile or immotile stress-generating swimmers or particles as they
invade a surrounding viscous fluid. Colonies of aligned pusher particles are
shown to elongate in the direction of particle orientation and undergo a
cascade of transverse concentration instabilities, governed at small times by
an equation which also describes the Saffman-Taylor instability in a Hele-Shaw
cell, or Rayleigh-Taylor instability in two-dimensional flow through a porous
medium. Thin sheets of aligned pusher particles are always unstable, while
sheets of aligned puller particles can either be stable (immotile particles),
or unstable (motile particles) with a growth rate which is non-monotonic in the
force dipole strength. We also prove a surprising ""no-flow theorem"": a
distribution initially isotropic in orientation loses isotropy immediately but
in such a way that results in no fluid flow everywhere and for all time.
",quantitative-biology
"  We performed a comparative study of extraction of large-scale flow structures
in Rayleigh Bénard convection using proper orthogonal decomposition (POD) and
{\em Fourier analysis}. We show that the free-slip basis functions capture the
flow profiles successfully for the no-slip boundary conditions. We observe that
the large-scale POD modes capture a larger fraction of total energy than the
Fourier modes. However, the Fourier modes capture the rarer flow structures
like flow reversals better. The flow profiles of the dominant POD and Fourier
modes are quite similar. Our results show that the Fourier analysis provides an
attractive alternative to POD analysis for capturing large-scale flow
structures.
",physics
"  We develop a finite element method for the Laplace--Beltrami operator on a
surface described by a set of patchwise parametrizations. The patches provide a
partition of the surface and each patch is the image by a diffeomorphism of a
subdomain of the unit square which is bounded by a number of smooth trim
curves. A patchwise tensor product mesh is constructed by using a structured
mesh in the reference domain. Since the patches are trimmed we obtain cut
elements in the vicinity of the interfaces. We discretize the Laplace--Beltrami
operator using a cut finite element method that utilizes Nitsche's method to
enforce continuity at the interfaces and a consistent stabilization term to
handle the cut elements. Several quantities in the method are conveniently
computed in the reference domain where the mappings impose a Riemannian metric.
We derive a priori estimates in the energy and $L^2$ norm and also present
several numerical examples confirming our theoretical results.
",mathematics
"  We devise an approach to the calculation of scaling dimensions of generic
operators describing scattering within multi-channel Luttinger liquid. The
local impurity scattering in an arbitrary configuration of conducting and
insulating channels is investigated and the problem is reduced to a single
algebraic matrix equation. In particular, the solution to this equation is
found for a finite array of chains described by Luttinger liquid models. It is
found that for a weak inter-chain hybridisation and intra-channel
electron-electron attraction the edge wires are robust against disorder whereas
bulk wires, on contrary, become insulating. Thus, the edge state may exist in a
finite sliding Luttinger liquid without time-reversal symmetry breaking
(quantum Hall systems) or spin-orbit interaction (topological insulators).
",physics
"  Superconducting detectors are now well-established tools for low-light
optics, and in particular quantum optics, boasting high-efficiency, fast
response and low noise. Similarly, lithium niobate is an important platform for
integrated optics given its high second-order nonlinearity, used for high-speed
electro-optic modulation and polarization conversion, as well as frequency
conversion and sources of quantum light. Combining these technologies addresses
the requirements for a single platform capable of generating, manipulating and
measuring quantum light in many degrees of freedom, in a compact and
potentially scalable manner. We will report on progress integrating tungsten
transition-edge sensors (TESs) and amorphous tungsten silicide superconducting
nanowire single-photon detectors (SNSPDs) on titanium in-diffused lithium
niobate waveguides. The travelling-wave design couples the evanescent field
from the waveguides into the superconducting absorber. We will report on
simulations and measurements of the absorption, which we can characterize at
room temperature prior to cooling down the devices. Independently, we show how
the detectors respond to flood illumination, normally incident on the devices,
demonstrating their functionality.
",physics
"  In this work exact solutions for the equation that describes anomalous heat
propagation in 1D harmonic lattices are obtained. Rectangular, triangular, and
sawtooth initial perturbations of the temperature field are considered. The
solution for an initially rectangular temperature profile is investigated in
detail. It is shown that the decay of the solution near the wavefront is
proportional to $1/ \sqrt{t}$. In the center of the perturbation zone the decay
is proportional to $1/t$. Thus the solution decays slower near the wavefront,
leaving clearly visible peaks that can be detected experimentally.
",physics
"  Bibliometrics offers a particular representation of science. Through
bibliometric methods a bibliometrician will always highlight particular
elements of publications, and through these elements operationalize particular
representations of science, while obscuring other possible representations from
view. Understanding bibliometrics as representation implies that a bibliometric
analysis is always performative: a bibliometric analysis brings a particular
representation of science into being that potentially influences the science
system itself. In this review we analyze the ways the humanities have been
represented throughout the history of bibliometrics, often in comparison to
other scientific domains or to a general notion of the sciences. Our review
discusses bibliometric scholarship between 1965 and 2016 that studies the
humanities empirically. We distinguish between two periods of bibliometric
scholarship. The first period, between 1965 and 1989, is characterized by a
sociological theoretical framework, the development and use of the Price index,
and small samples of journal publications as data sources. The second period,
from the mid-1980s up until the present day, is characterized by a new
hinterland, that of science policy and research evaluation, in which
bibliometric methods become embedded.
",computer-science
"  If accreting white dwarfs (WD) in binary systems are to produce type Ia
supernovae (SNIa), they must grow to nearly the Chandrasekhar mass and ignite
carbon burning. Proving conclusively that a WD has grown substantially since
its birth is a challenging task. Slow accretion of hydrogen inevitably leads to
the erosion, rather than the growth of WDs. Rapid hydrogen accretion does lead
to growth of a helium layer, due to both decreased degeneracy and the
inhibition of mixing of the accreted hydrogen with the underlying WD. However,
until recently, simulations of helium-accreting WDs all claimed to show the
explosive ejection of a helium envelope once it exceeded $\sim 10^{-1}\, \rm
M_{\odot}$. Because CO WDs cannot be born with masses in excess of $\sim 1.1\,
\rm M_{\odot}$, any such object, in excess of $\sim 1.2\, \rm M_{\odot}$, must
have grown substantially. We demonstrate that the WD in the symbiotic nova RS
Oph is in the mass range 1.2-1.4\,M$_{\odot}$. We compare UV spectra of RS Oph
with those of novae with ONe WDs, and with novae erupting on CO WDs. The RS Oph
WD is clearly made of CO, demonstrating that it has grown substantially since
birth. It is a prime candidate to eventually produce an SNIa.
",physics
"  Time Projection Chamber (TPC) has been chosen as the main tracking system in
several high-flux and high repetition rate experiments. These include on-going
experiments such as ALICE and future experiments such as PANDA at FAIR and ILC.
Different $\mathrm{R}\&\mathrm{D}$ activities were carried out on the adoption
of Gas Electron Multiplier (GEM) as the gas amplification stage of the
ALICE-TPC upgrade version. The requirement of low ion feedback has been
established through these activities. Low ion feedback minimizes distortions
due to space charge and maintains the necessary values of detector gain and
energy resolution. In the present work, Garfield simulation framework has been
used to study the related physical processes occurring within single, triple
and quadruple GEM detectors. Ion backflow and electron transmission of
quadruple GEMs, made up of foils with different hole pitch under different
electromagnetic field configurations (the projected solutions for the ALICE
TPC) have been studied. Finally a new triple GEM detector configuration with
low ion backflow fraction and good electron transmission properties has been
proposed as a simpler GEM-based alternative suitable for TPCs for future
collider experiments.
",physics
"  A presentation at the SciNeGHE conference of the past achievements, of the
present activities and of the perspectives for the future of the HARPO project,
the development of a time projection chamber as a high-performance gamma-ray
telescope and linear polarimeter in the e+e- pair creation regime.
",physics
"  Motivated by the question of whether the recently introduced Reduced Cutset
Coding (RCC) offers rate-complexity performance benefits over conventional
context-based conditional coding for sources with two-dimensional Markov
structure, this paper compares several row-centric coding strategies that vary
in the amount of conditioning as well as whether a model or an empirical table
is used in the encoding of blocks of rows. The conclusion is that, at least for
sources exhibiting low-order correlations, 1-sided model-based conditional
coding is superior to the method of RCC for a given constraint on complexity,
and conventional context-based conditional coding is nearly as good as the
1-sided model-based coding.
",computer-science
"  Recurrent neural networks (RNNs) serve as a fundamental building block for
many sequence tasks across natural language processing. Recent research has
focused on recurrent dropout techniques or custom RNN cells in order to improve
performance. Both of these can require substantial modifications to the machine
learning model or to the underlying RNN configurations. We revisit traditional
regularization techniques, specifically L2 regularization on RNN activations
and slowness regularization over successive hidden states, to improve the
performance of RNNs on the task of language modeling. Both of these techniques
require minimal modification to existing RNN architectures and result in
performance improvements comparable or superior to more complicated
regularization techniques or custom cell architectures. These regularization
techniques can be used without any modification on optimized LSTM
implementations such as the NVIDIA cuDNN LSTM.
",computer-science
"  Recent research has demonstrated the brittleness of machine learning systems
to adversarial perturbations. However, the studies have been mostly limited to
perturbations on images and more generally, classification that does not deal
with temporally varying inputs. In this paper we ask ""Are adversarial
perturbations possible in real-time video classification systems and if so,
what properties must they satisfy?"" Such systems find application in
surveillance applications, smart vehicles, and smart elderly care and thus,
misclassification could be particularly harmful (e.g., a mishap at an elderly
care facility may be missed). We show that accounting for temporal structure is
key to generating adversarial examples in such systems. We exploit recent
advances in generative adversarial network (GAN) architectures to account for
temporal correlations and generate adversarial samples that can cause
misclassification rates of over 80% for targeted activities. More importantly,
the samples also leave other activities largely unaffected making them
extremely stealthy. Finally, we also surprisingly find that in many scenarios,
the same perturbation can be applied to every frame in a video clip that makes
the adversary's ability to achieve misclassification relatively easy.
",statistics
"  Motivated by the description of Nurowski's conformal structure for maximally
symmetric homogeneous examples of bracket-generating rank 2 distributions in
dimension 5, aka $(2,3,5)$-distributions, we consider a rank $3$ Pfaffian
system in dimension 5 with $SU(2)$ symmetry. We find the conditions for which
this Pfaffian system has the maximal symmetry group (in the real case this is
the split real form of $G_2$), and give the associated Nurowski's conformal
classes. We also present a $SU(2)$ gauge-theoretic interpretation of the
results obtained.
",mathematics
"  Let $\mu$ be a measure in $\mathbb R^d$ with compact support and continuous
density, and let $$ R^s\mu(x)=\int\frac{y-x}{|y-x|^{s+1}}\,d\mu(y),\ \
x,y\in\mathbb R^d,\ \ 0<s<d. $$ We consider the following conjecture: $$
\sup_{x\in\mathbb R^d}|R^s\mu(x)|\le
C\sup_{x\in\text{supp}\,\mu}|R^s\mu(x)|,\quad C=C(d,s). $$ This relation was
known for $d-1\le s<d$, and is still an open problem in the general case. We
prove the maximum principle for $0< s<1$, and also for $0<s<d$ in the case of
radial measure. Moreover, we show that this conjecture is incorrect for
non-positive measures.
",mathematics
"  When a human drives a car along a road for the first time, they later
recognize where they are on the return journey typically without needing to
look in their rear-view mirror or turn around to look back, despite significant
viewpoint and appearance change. Such navigation capabilities are typically
attributed to our semantic visual understanding of the environment [1] beyond
geometry to recognizing the types of places we are passing through such as
""passing a shop on the left"" or ""moving through a forested area"". Humans are in
effect using place categorization [2] to perform specific place recognition
even when the viewpoint is 180 degrees reversed. Recent advances in deep neural
networks have enabled high-performance semantic understanding of visual places
and scenes, opening up the possibility of emulating what humans do. In this
work, we develop a novel methodology for using the semantics-aware higher-order
layers of deep neural networks for recognizing specific places from within a
reference database. To further improve the robustness to appearance change, we
develop a descriptor normalization scheme that builds on the success of
normalization schemes for pure appearance-based techniques such as SeqSLAM [3].
Using two different datasets - one road-based, one pedestrian-based, we
evaluate the performance of the system in performing place recognition on
reverse traversals of a route with a limited field of view camera and no
turn-back-and-look behaviours, and compare to existing state-of-the-art
techniques and vanilla off-the-shelf features. The results demonstrate
significant improvements over the existing state of the art, especially for
extreme perceptual challenges that involve both great viewpoint change and
environmental appearance change. We also provide experimental analyses of the
contributions of the various system components.
",computer-science
"  In this paper, we propose a new algorithm based on radial symmetry center
method to track colloidal particles close to contact, where the optical images
of the particles start to overlap in digital video microscopy. This overlapping
effect is important to observe the pair interaction potential in colloidal
studies and it appears as additional interaction in the measurement of the
interaction with conventional tracking analysis. The proposed algorithm in this
work is simple, fast and applicable for not only two particles but also three
and more particles without any modification. The algorithm uses gradient
vectors of the particle intensity distribution, which allows us to use a part
of the symmetric intensity distribution in the calculation of the actual
particle position. In this study, simulations are performed to see the
performance of the proposed algorithm for two and three particles, where the
simulation images are generated by using fitted curve to experimental particle
image for different sized particles. As a result, the algorithm yields the
maximum error smaller than 2 nm for 5.53 {\mu}m silica particles in contact
condition.
",physics
"  The advent of microcontrollers with enough CPU power and with analog and
digital peripherals makes possible to design a complete particle detector with
relative acquisition system around one microcontroller chip. The existence of a
world wide data infrastructure as internet allows for devising a distributed
network of cheap detectors capable to elaborate and send data or respond to
settings commands. The internet infrastructure enables to distribute the
absolute time (with precision of few milliseconds), to the simple devices far
apart, with few milliseconds precision, from a few meters to thousands of
kilometres. So it is possible to create a crowdsourcing experiment of citizen
science that use small scintillation-based particle detectors to monitor the
high energetic cosmic ray and the radiation environment.
",physics
"  In the Internet of Things (IoT) community, Wireless Sensor Network (WSN) is a
key technique to enable ubiquitous sensing of environments and provide reliable
services to applications. WSN programs, typically interrupt-driven, implement
the functionalities via the collaboration of Interrupt Procedure Instances
(IPIs, namely executions of interrupt processing logic). However, due to the
complicated concurrency model of WSN programs, the IPIs are interleaved
intricately and the program behaviours are hard to predicate from the source
codes. Thus, to improve the software quality of WSN programs, it is significant
to disentangle the interleaved executions and develop various IPI-based program
analysis techniques, including offline and online ones. As the common
foundation of those techniques, a generic efficient and real-time algorithm to
identify IPIs is urgently desired. However, the existing
instance-identification approach cannot satisfy the desires. In this paper, we
first formally define the concept of IPI. Next, we propose a generic
IPI-identification algorithm, and prove its correctness, real-time and
efficiency. We also conduct comparison experiments to illustrate that our
algorithm is more efficient than the existing one in terms of both time and
space. As the theoretical analyses and empirical studies exhibit, our algorithm
provides the groundwork for IPI-based analyses of WSN programs in IoT
environment.
",computer-science
"  We propose a new approach to the topological recursion of Eynard-Orantin
based on the notion of Airy structure, which we introduce in the paper. We
explain why Airy structure is a more fundamental object than the one of the
spectral curve. We explain how the concept of quantization of Airy structure
leads naturally to the formulas of topological recursion as well as their
generalizations. The notion of spectral curve is also considered in a more
general framework of Poisson surfaces endowed with foliation. We explain how
the deformation theory of spectral curves is related to Airy structures. Few
other topics (e.g. the Holomorphic Anomaly Equation) are also discussed from
the general point of view of Airy structures.
",mathematics
"  We provide a full analysis of ghost free higher derivative field theories
with coupled degrees of freedom. Assuming the absence of gauge symmetries, we
derive the degeneracy conditions in order to evade the Ostrogradsky ghosts, and
analyze which (non)trivial classes of solutions this allows for. It is shown
explicitly how Lorentz invariance avoids the propagation of ""half"" degrees of
freedom. Moreover, for a large class of theories, we construct the field
redefinitions and/or (extended) contact transformations that put the theory in
a manifestly first order form. Finally, we identify which class of theories
cannot be brought to first order form by such transformations.
",physics
"  We provide new theoretical insights on why over-parametrization is effective
in learning neural networks. For a $k$ hidden node shallow network with
quadratic activation and $n$ training data points, we show as long as $ k \ge
\sqrt{2n}$, over-parametrization enables local search algorithms to find a
\emph{globally} optimal solution for general smooth and convex loss functions.
Further, despite that the number of parameters may exceed the sample size,
using theory of Rademacher complexity, we show with weight decay, the solution
also generalizes well if the data is sampled from a regular distribution such
as Gaussian. To prove when $k\ge \sqrt{2n}$, the loss function has benign
landscape properties, we adopt an idea from smoothed analysis, which may have
other applications in studying loss surfaces of neural networks.
",statistics
"  Henrik Bruus is professor of lab-chip systems and theoretical physics at the
Technical University of Denmark. In this contribution, he summarizes some of
the recent results within theory and simulation of microscale acoustofluidic
systems that he has obtained in collaboration with his students and
international colleagues. The main emphasis is on three dynamical effects
induced by external ultrasound fields acting on aqueous solutions and particle
suspensions: The acoustic radiation force acting on suspended micro- and
nanoparticles, the acoustic streaming appearing in the fluid, and the newly
discovered acoustic body force acting on inhomogeneous solutions.
",quantitative-biology
"  Timing attacks have been a continuous threat to users' privacy in modern
browsers. To mitigate such attacks, existing approaches, such as Tor Browser
and Fermata, add jitters to the browser clock so that an attacker cannot
accurately measure an event. However, such defenses only raise the bar for an
attacker but do not fundamentally mitigate timing attacks, i.e., it just takes
longer than previous to launch a timing attack. In this paper, we propose a
novel approach, called deterministic browser, which can provably prevent timing
attacks in modern browsers. Borrowing from Physics, we introduce several
concepts, such as an observer and a reference frame. Specifically, a snippet of
JavaScript, i.e., an observer in JavaScript reference frame, will always obtain
the same, fixed timing information so that timing attacks are prevented; at
contrast, a user, i.e., an oracle observer, will perceive the JavaScript
differently and do not experience the performance slowdown. We have implemented
a prototype called DeterFox and our evaluation shows that the prototype can
defend against browser-related timing attacks.
",computer-science
"  We study the attractors of a class of holomorphic systems with an
irrationally indifferent fixed point. We prove a trichotomy for the topology of
the attractor based on the arithmetic of the rotation number at the fixed
point. That is, the attractor is either a Jordan curve, a one-sided hairy
circle, or a Cantor bouquet. This has a number of remarkable corollaries on a
conjecture of M. Herman about the optimal arithmetic condition for the
existence of a critical point on the boundary of the Siegel disk, and a
conjecture of A. Douady on the topology of the boundary of Siegel disks.
Combined with earlier results on the topic, this completes the topological
description of the behaviors of typical orbits near such fixed points, when the
rotation number is of high type.
",mathematics
"  One of the most challenging problems in correlated topological systems is a
realization of the reduction of topological classification, but very few
experimental platforms have been proposed so far. We here demonstrate that
ultracold dipolar fermions (e.g., $^{167}$Er, $^{161}$Dy, and $^{53}$Cr) loaded
in an optical lattice of two-leg ladder geometry can be the first promising
testbed for the reduction $\mathbb{Z}\to\mathbb{Z}_4$, where solid evidence for
the reduction is available thanks to their high controllability. We further
give a detailed account of how to experimentally access this phenomenon; around
the edges, the destruction of one-particle gapless excitations can be observed
by the local radio frequency spectroscopy, while that of gapless spin
excitations can be observed by a time-dependent spin expectation value of a
superposed state of the ground state and the first excited state. We clarify
that even when the reduction occurs, a gapless edge mode is recovered around a
dislocation, which can be another piece of evidence for the reduction.
",physics
"  This paper presents our approach to the quantitative modeling and analysis of
highly (re)configurable systems, such as software product lines. Different
combinations of the optional features of such a system give rise to
combinatorially many individual system variants. We use a formal modeling
language that allows us to model systems with probabilistic behavior, possibly
subject to quantitative feature constraints, and able to dynamically install,
remove or replace features. More precisely, our models are defined in the
probabilistic feature-oriented language QFLAN, a rich domain specific language
(DSL) for systems with variability defined in terms of features. QFLAN
specifications are automatically encoded in terms of a process algebra whose
operational behavior interacts with a store of constraints, and hence allows to
separate system configuration from system behavior. The resulting probabilistic
configurations and behavior converge seamlessly in a semantics based on
discrete-time Markov chains, thus enabling quantitative analysis. Our analysis
is based on statistical model checking techniques, which allow us to scale to
larger models with respect to precise probabilistic analysis techniques. The
analyses we can conduct range from the likelihood of specific behavior to the
expected average cost, in terms of feature attributes, of specific system
variants. Our approach is supported by a novel Eclipse-based tool which
includes state-of-the-art DSL utilities for QFLAN based on the Xtext framework
as well as analysis plug-ins to seamlessly run statistical model checking
analyses. We provide a number of case studies that have driven and validated
the development of our framework.
",computer-science
"  In this expository work we discuss the asymptotic behaviour of the solutions
of the classical heat equation posed in the whole Euclidean space.
After an introductory review of the main facts on the existence and
properties of solutions, we proceed with the proofs of convergence to the
Gaussian fundamental solution, a result that holds for all integrable
solutions, and represents in the PDE setting the Central Limit Theorem of
probability. We present several methods of proof: first, the scaling method.
Then several versions of the representation method. This is followed by the
functional analysis approach that leads to the famous related equations,
Fokker-Planck and Ornstein-Uhlenbeck. The analysis of this connection is also
given in rather complete form here. Finally, we present the Boltzmann entropy
method, coming from kinetic equations.
The different methods are interesting because of the possible extension to
prove the asymptotic behaviour or stabilization analysis for more general
equations, linear or nonlinear. It all depends a lot on the particular
features, and only one or some of the methods work in each case.Other settings
of the Heat Equation are briefly discussed in Section 9 and a longer mention of
results for different equations is done in Section 10.
",mathematics
"  Traffic speed is a key indicator for the efficiency of an urban
transportation system. Accurate modeling of the spatiotemporally varying
traffic speed thus plays a crucial role in urban planning and development. This
paper addresses the problem of efficient fine-grained traffic speed prediction
using big traffic data obtained from static sensors. Gaussian processes (GPs)
have been previously used to model various traffic phenomena, including flow
and speed. However, GPs do not scale with big traffic data due to their cubic
time complexity. In this work, we address their efficiency issues by proposing
local GPs to learn from and make predictions for correlated subsets of data.
The main idea is to quickly group speed variables in both spatial and temporal
dimensions into a finite number of clusters, so that future and unobserved
traffic speed queries can be heuristically mapped to one of such clusters. A
local GP corresponding to that cluster can then be trained on the fly to make
predictions in real-time. We call this method localization. We use non-negative
matrix factorization for localization and propose simple heuristics for cluster
mapping. We additionally leverage on the expressiveness of GP kernel functions
to model road network topology and incorporate side information. Extensive
experiments using real-world traffic data collected in the two U.S. cities of
Pittsburgh and Washington, D.C., show that our proposed local GPs significantly
improve both runtime performances and prediction accuracies compared to the
baseline global and local GPs.
",computer-science
"  Machine learning techniques have been used in the past using Monte Carlo
samples to construct predictors of the dynamic stability of power systems. In
this paper we move beyond the task of prediction and propose a comprehensive
approach to use predictors, such as Decision Trees (DT), within a standard
optimization framework for pre- and post-fault control purposes. In particular,
we present a generalizable method for embedding rules derived from DTs in an
operation decision-making model. We begin by pointing out the specific
challenges entailed when moving from a prediction to a control framework. We
proceed with introducing the solution strategy based on generalized disjunctive
programming (GDP) as well as a two-step search method for identifying optimal
hyper-parameters for balancing cost and control accuracy. We showcase how the
proposed approach constructs security proxies that cover multiple contingencies
while facing high-dimensional uncertainty with respect to operating conditions
with the use of a case study on the IEEE 39-bus system. The method is shown to
achieve efficient system control at a marginal increase in system price
compared to an oracle model.
",statistics
"  In topological semimetals the Dirac points can form zero-dimensional and
one-dimensional manifolds, as predicted for Dirac/Weyl semimetals and
topological nodal line semimetals, respectively. Here, based on
first-principles calculations, we predict a topological nodal line semimetal
phase in the two-dimensional compounds $X_2Y$ ($X$=Ca, Sr, and Ba; $Y$=As, Sb,
and Bi) in the absence of spin-orbit coupling (SOC) with a band inversion at
the M point. The mirror symmetry as well as the electrostatic interaction, that
can be engineered via strain, are responsible for the nontrivial phase. In
addition, we demonstrate that the exotic edge states can be also obtained
without and with SOC although a tiny gap appears at the nodal line for the bulk
states when SOC is included.
",physics
"  We consider the problem of estimating a regression function in the common
situation where the number of features is small, where interpretability of the
model is a high priority, and where simple linear or additive models fail to
provide adequate performance. To address this problem, we present Maximum
Variance Total Variation denoising (MVTV), an approach that is conceptually
related both to CART and to the more recent CRISP algorithm, a state-of-the-art
alternative method for interpretable nonlinear regression. MVTV divides the
feature space into blocks of constant value and fits the value of all blocks
jointly via a convex optimization routine. Our method is fully data-adaptive,
in that it incorporates highly robust routines for tuning all hyperparameters
automatically. We compare our approach against CART and CRISP via both a
complexity-accuracy tradeoff metric and a human study, demonstrating that that
MVTV is a more powerful and interpretable method.
",statistics
"  Exoplanet host star activity, in the form of unocculted star spots or
faculae, alters the observed transmission and emission spectra of the
exoplanet. This effect can be exacerbated when combining data from different
epochs if the stellar photosphere varies between observations due to activity.
redHere we present a method to characterize and correct for relative changes
due to stellar activity by exploiting multi-epoch ($\ge$2 visits/transits)
observations to place them in a consistent reference frame. Using measurements
from portions of the planet's orbit where negligible planet transmission or
emission can be assumed, we determine changes to the stellar spectral
amplitude. With the analytical methods described here, we predict the impact of
stellar variability on transit observations. Supplementing these forecasts with
Kepler-measured stellar variabilities for F-, G-, K-, and M-dwarfs, and
predicted transit precisions by JWST's NIRISS, NIRCam, and MIRI, we conclude
that stellar activity does not impact infrared transiting exoplanet
observations of most presently-known or predicted TESS targets by current or
near-future platforms, such as JWST.
",physics
"  In this paper, a hybrid measurement- and model-based method is proposed which
can estimate the dynamic state Jacobian matrix in near real-time. The proposed
method is computationally efficient and robust to the variation of network
topology. A numerical example is given to show that the proposed method is able
to provide good estimation for the dynamic state Jacobian matrix and is
superior to the model-based method under undetectable network topology change.
The proposed method may also help identify big discrepancy in the assumed
network model.
",computer-science
"  In this paper, we present a real-time robust multi-view pedestrian detection
and tracking system for video surveillance using neural networks which can be
used in dynamic environments. The proposed system consists of two phases:
multi-view pedestrian detection and tracking. First, pedestrian detection
utilizes background subtraction to segment the foreground blob. An adaptive
background subtraction method where each of the pixel of input image models as
a mixture of Gaussians and uses an on-line approximation to update the model
applies to extract the foreground region. The Gaussian distributions are then
evaluated to determine which are most likely to result from a background
process. This method produces a steady, real-time tracker in outdoor
environment that consistently deals with changes of lighting condition, and
long-term scene change. Second, the Tracking is performed at two phases:
pedestrian classification and tracking the individual subject. A sliding window
is applied on foreground binary image to select an input window which is used
for selecting the input image patches from actually input frame. The neural
networks is used for classification with PHOG features. Finally, a Kalman
filter is applied to calculate the subsequent step for tracking that aims at
finding the exact position of pedestrians in an input image. The experimental
result shows that the proposed approach yields promising performance on
multi-view pedestrian detection and tracking on different benchmark datasets.
",computer-science
"  We present experimental data and simulations on the effects of in-plane
tension on nanoindentation hardness and pop-in noise. Nanoindentation
experiments using a Berkovich tip are performed on bulk polycrystaline Al
samples, under tension in a custom 4pt-bending fixture. The hardness displays a
transition, for indentation depths smaller than 10nm, as function of the
in-plane stress at a value consistent with the bulk tensile yield stress.
Displacement bursts appear insensitive to in-plane tension and this transition
disappears for larger indentation depths. Two dimensional discrete dislocation
dynamics simulations confirm that a regime exists where hardness is sensitive
to tension-induced pre-existing dislocations.
",physics
"  A finite-dimensional algebra $A$ over an algebraically closed field $K$ is
called periodic if it is periodic under the action of the syzygy operator in
the category of $A-A-$ bimodules. The periodic algebras are self-injective and
occur naturally in the study of tame blocks of group algebras, actions of
finite groups on spheres, hypersurface singularities of finite Cohen-Macaulay
type, and Jacobian algebras of quivers with potentials. Recently, the tame
periodic algebras of polynomial growth have been classified and it is natural
to attempt to classify all tame periodic algebras. We introduce the weighted
surface algebras of triangulated surfaces with arbitrarily oriented triangles
and describe their basic properties. In particular, we prove that all these
algebras, except the singular tetrahedral algebras, are symmetric tame periodic
algebras of period $4$. Moreover, we describe the socle deformations of the
weighted surface algebras and prove that all these algebras are symmetric tame
periodic algebras of period $4$. The main results of the paper form an
important step towards a classification of all periodic symmetric tame algebras
of non-polynomial growth, and lead to a complete description of all algebras of
generalized quaternion type. Further, the orbit closures of the weighted
surface algebras (and their socle deformations) in the affine varieties of
associative $K$-algebra structures contain wide classes of tame symmetric
algebras related to algebras of dihedral and semidihedral types, which occur in
the study of blocks of group algebras with dihedral and semidihedral defect
groups.
",mathematics
"  A new definition of continuous-time equilibrium controls is introduced. As
opposed to the standard definition, which involves a derivative-type operation,
the new definition parallels how a discrete-time equilibrium is defined, and
allows for unambiguous economic interpretation. The terms ""strong equilibria""
and ""weak equilibria"" are coined for controls under the new and the standard
definitions, respectively. When the state process is a time-homogeneous
continuous-time Markov chain, a careful asymptotic analysis gives complete
characterizations of weak and strong equilibria. Thanks to Kakutani-Fan's
fixed-point theorem, general existence of weak and strong equilibria is also
established, under additional compactness assumption. Our theoretic results are
applied to a two-state model under non-exponential discounting. In particular,
we demonstrate explicitly that there can be incentive to deviate from a weak
equilibrium, which justifies the need for strong equilibria. Our analysis also
provides new results for the existence and characterization of discrete-time
equilibria under infinite horizon.
",quantitative-finance
"  Purpose: Basic surgical skills of suturing and knot tying are an essential
part of medical training. Having an automated system for surgical skills
assessment could help save experts time and improve training efficiency. There
have been some recent attempts at automated surgical skills assessment using
either video analysis or acceleration data. In this paper, we present a novel
approach for automated assessment of OSATS based surgical skills and provide an
analysis of different features on multi-modal data (video and accelerometer
data). Methods: We conduct the largest study, to the best of our knowledge, for
basic surgical skills assessment on a dataset that contained video and
accelerometer data for suturing and knot-tying tasks. We introduce ""entropy
based"" features - Approximate Entropy (ApEn) and Cross-Approximate Entropy
(XApEn), which quantify the amount of predictability and regularity of
fluctuations in time-series data. The proposed features are compared to
existing methods of Sequential Motion Texture (SMT), Discrete Cosine Transform
(DCT) and Discrete Fourier Transform (DFT), for surgical skills assessment.
Results: We report average performance of different features across all
applicable OSATS criteria for suturing and knot tying tasks. Our analysis shows
that the proposed entropy based features out-perform previous state-of-the-art
methods using video data. For accelerometer data, our method performs better
for suturing only. We also show that fusion of video and acceleration features
can improve overall performance with the proposed entropy features achieving
highest accuracy. Conclusions: Automated surgical skills assessment can be
achieved with high accuracy using the proposed entropy features. Such a system
can significantly improve the efficiency of surgical training in medical
schools and teaching hospitals.
",computer-science
"  In this work, we derive relations between generating functions of double
stuffle relations and double shuffle relations to express the alternating
double Euler sums $\zeta\left(\overline{r}, s\right)$, $\zeta\left(r,
\overline{s}\right)$ and $\zeta\left(\overline{r}, \overline{s}\right)$ with
$r+s$ odd in terms of zeta values. We also give a direct proof of a
hypergeometric identity which is a limiting case of a basic hypergeometric
identity of Andrews. Finally, we gave another proof for the formula of Zagier
on the multiple zeta values $\zeta(2,\ldots,2,3,2,\ldots,2)$.
",mathematics
"  Hand-built verb clusters such as the widely used Levin classes (Levin, 1993)
have proved useful, but have limited coverage. Verb classes automatically
induced from corpus data such as those from VerbKB (Wijaya, 2016), on the other
hand, can give clusters with much larger coverage, and can be adapted to
specific corpora such as Twitter. We present a method for clustering the
outputs of VerbKB: verbs with their multiple argument types, e.g.
""marry(person, person)"", ""feel(person, emotion)."" We make use of a novel
low-dimensional embedding of verbs and their arguments to produce high quality
clusters in which the same verb can be in different clusters depending on its
argument type. The resulting verb clusters do a better job than hand-built
clusters of predicting sarcasm, sentiment, and locus of control in tweets.
",computer-science
"  Typically, AI researchers and roboticists try to realize intelligent behavior
in machines by tuning parameters of a predefined structure (body plan and/or
neural network architecture) using evolutionary or learning algorithms. Another
but not unrelated longstanding property of these systems is their brittleness
to slight aberrations, as highlighted by the growing deep learning literature
on adversarial examples. Here we show robustness can be achieved by evolving
the geometry of soft robots, their control systems, and how their material
properties develop in response to one particular interoceptive stimulus
(engineering stress) during their lifetimes. By doing so we realized robots
that were equally fit but more robust to extreme material defects (such as
might occur during fabrication or by damage thereafter) than robots that did
not develop during their lifetimes, or developed in response to a different
interoceptive stimulus (pressure). This suggests that the interplay between
changes in the containing systems of agents (body plan and/or neural
architecture) at different temporal scales (evolutionary and developmental)
along different modalities (geometry, material properties, synaptic weights)
and in response to different signals (interoceptive and external perception)
all dictate those agents' abilities to evolve or learn capable and robust
strategies.
",computer-science
"  New results are added to the paper [4] about q-closed and solvable
sesquilinear forms. The structure of the Banach space
$\mathcal{D}[||\cdot||_\Omega]$ defined on the domain $\mathcal{D}$ of a
q-closed sesquilinear form $\Omega$ is unique up to isomorphism, and the
adjoint of a sesquilinear form has the same property of q-closure or of
solvability. The operator associated to a solvable sesquilinear form is the
greatest which represents the form and it is self-adjoint if, and only if, the
form is symmetric. We give more criteria of solvability for q-closed
sesquilinear forms. Some of these criteria are related to the numerical range,
and we analyse in particular the forms which are solvable with respect to inner
products. The theory of solvable sesquilinear forms generalises those of many
known sesquilinear forms in literature.
",mathematics
"  With the availability of more powerful computers, iterative reconstruction
algorithms are the subject of an ongoing work in the design of more efficient
reconstruction algorithms for X-ray computed tomography. In this work, we show
how two analytical reconstruction algorithms can be improved by correcting the
corresponding reconstructions using a randomized iterative reconstruction
algorithm. The combined analytical reconstruction followed by randomized
iterative reconstruction can also be viewed as a reconstruction algorithm
which, in the experiments we have conducted, uses up to $35\%$ less projection
angles as compared to the analytical reconstruction algorithms and produces the
same results in terms of quality of reconstruction, without increasing the
execution time significantly.
",computer-science
"  Recently, the educational initiative TED-Ed has published a popular brain
teaser coined the 'frog riddle', which illustrates non-intuitive implications
of conditional probabilities. In its intended form, the frog riddle is a
reformulation of the classic boy-girl paradox. However, the authors alter the
narrative of the riddle in a form, that subtly changes the way information is
conveyed. The presented solution, unfortunately, does not take this point into
full account, and as a consequence, lacks consistency in the sense that
different parts of the problem are treated on unequal footing. We here review,
how the mechanism of receiving information matters, and why this is exactly the
reason that such kind of problems challenge intuitive thinking. Subsequently,
we present a generalized solution, that accounts for the above difficulties,
and preserves full logical consistency. Eventually, the relation to the
boy-girl paradox is discussed.
",physics
"  The Least Significant Bit (LSB) substitution is an old and simple data hiding
method that could almost effortlessly be implemented in spatial or transform
domain over any digital media. This method can be attacked by several
steganalysis methods, because it detectably changes statistical and perceptual
characteristics of the cover signal. A typical method for steganalysis of the
LSB substitution is the histogram attack that attempts to diagnose anomalies in
the cover image's histogram. A well-known method to stand the histogram attack
is the LSB+ steganography that intentionally embeds some extra bits to make the
histogram look natural. However, the LSB+ method still affects the perceptual
and statistical characteristics of the cover signal. In this paper, we propose
a new method for image steganography, called LSB++, which improves over the
LSB+ image steganography by decreasing the amount of changes made to the
perceptual and statistical attributes of the cover image. We identify some
sensitive pixels affecting the signal characteristics, and then lock and keep
them from the extra bit embedding process of the LSB+ method, by introducing a
new embedding key. Evaluation results show that, without reducing the embedding
capacity, our method can decrease potentially detectable changes caused by the
embedding process.
",computer-science
"  We extend existing methods for using cross-correlations to derive redshift
distributions for photometric galaxies, without using photometric redshifts.
The model presented in this paper simultaneously yields highly accurate and
unbiased redshift distributions and, for the first time, redshift-dependent
luminosity functions, using only clustering information and the apparent
magnitudes of the galaxies as input. In contrast to many existing techniques
for recovering unbiased redshift distributions, the output of our method is not
degenerate with the galaxy bias b(z), which is achieved by modelling the shape
of the luminosity bias. We successfully apply our method to a mock galaxy
survey and discuss improvements to be made before applying our model to real
data.
",physics
"  We canonically quantize $O(D+2)$ nonlinear sigma models (NLSMs) with theta
term on arbitrary smooth, closed, connected, oriented $D$-dimensional spatial
manifolds $\mathcal{M}$, with the goal of proving the suitability of these
models for describing symmetry-protected topological (SPT) phases of bosons in
$D$ spatial dimensions. We show that in the disordered phase of the NLSM, and
when the coefficient $\theta$ of the theta term is an integer multiple of
$2\pi$, the theory on $\mathcal{M}$ has a unique ground state and a finite
energy gap to all excitations. We also construct the ground state wave
functional of the NLSM in this parameter regime, and we show that it is
independent of the metric on $\mathcal{M}$ and given by the exponential of a
Wess-Zumino term for the NLSM field, in agreement with previous results on flat
space. Our results show that the NLSM in the disordered phase and at
$\theta=2\pi k$, $k\in\mathbb{Z}$, has a symmetry-preserving ground state but
no topological order (i.e., no topology-dependent ground state degeneracy),
making it an ideal model for describing SPT phases of bosons. Thus, our work
places previous results on SPT phases derived using NLSMs on solid theoretical
ground. To canonically quantize the NLSM on $\mathcal{M}$ we use Dirac's method
for the quantization of systems with second class constraints, suitably
modified to account for the curvature of space. In a series of four appendices
we provide the technical background needed to follow the discussion in the main
sections of the paper.
",physics
"  Meta learning of optimal classifier error rates allows an experimenter to
empirically estimate the intrinsic ability of any estimator to discriminate
between two populations, circumventing the difficult problem of estimating the
optimal Bayes classifier. To this end we propose a weighted nearest neighbor
(WNN) graph estimator for a tight bound on the Bayes classification error; the
Henze-Penrose (HP) divergence. Similar to recently proposed HP estimators
[berisha2016], the proposed estimator is non-parametric and does not require
density estimation. However, unlike previous approaches the proposed estimator
is rate-optimal, i.e., its mean squared estimation error (MSEE) decays to zero
at the fastest possible rate of $O(1/M+1/N)$ where $M,N$ are the sample sizes
of the respective populations. We illustrate the proposed WNN meta estimator
for several simulated and real data sets.
",statistics
"  Hamiltonian Monte Carlo (HMC) is a powerful Markov chain Monte Carlo (MCMC)
method for performing approximate inference in complex probabilistic models of
continuous variables. In common with many MCMC methods, however, the standard
HMC approach performs poorly in distributions with multiple isolated modes. We
present a method for augmenting the Hamiltonian system with an extra continuous
temperature control variable which allows the dynamic to bridge between
sampling a complex target distribution and a simpler unimodal base
distribution. This augmentation both helps improve mixing in multimodal targets
and allows the normalisation constant of the target distribution to be
estimated. The method is simple to implement within existing HMC code,
requiring only a standard leapfrog integrator. We demonstrate experimentally
that the method is competitive with annealed importance sampling and simulating
tempering methods at sampling from challenging multimodal distributions and
estimating their normalising constants.
",statistics
"  We present a new method for numerical hydrodynamics which uses a
multidimensional generalisation of the Roe solver and operates on an
unstructured triangular mesh. The main advantage over traditional methods based
on Riemann solvers, which commonly use one-dimensional flux estimates as
building blocks for a multidimensional integration, is its inherently
multidimensional nature, and as a consequence its ability to recognise
multidimensional stationary states that are not hydrostatic. A second novelty
is the focus on Graphics Processing Units (GPUs). By tailoring the algorithms
specifically to GPUs we are able to get speedups of 100-250 compared to a
desktop machine. We compare the multidimensional upwind scheme to a
traditional, dimensionally split implementation of the Roe solver on several
test problems, and we find that the new method significantly outperforms the
Roe solver in almost all cases. This comes with increased computational costs
per time step, which makes the new method approximately a factor of 2 slower
than a dimensionally split scheme acting on a structured grid.
",physics
"  The object of study in the present dissertation are some topics in
differential geometry of smooth manifolds with additional tensor structures and
metrics of Norden type. There are considered four cases depending on the
dimension of the manifold: 2n, 2n + 1, 4n and 4n + 3. The studied tensor
structures, which are counterparts in the different related dimensions, are the
almost complex/contact/hypercomplex structure and the almost contact
3-structure. The considered metric on the 2n-dimensional case is the Norden
metric, and the metrics in the other three cases are generated by it. The
purpose of the dissertation is to carry out the following: 1. Further
investigations of almost complex manifolds with Norden metric including
studying of natural connections with conditions for their torsion and invariant
tensors under the twin interchange of Norden metrics. 2. Further investigations
of almost contact manifolds with B-metric including studying of natural
connections with conditions for their torsion and associated Schouten-van
Kampen connections as well as a classification of affine connections. 3.
Introducing and studying of Sasaki-like almost contact complex Riemannian
manifolds. 4. Further investigations of almost hypercomplex manifolds with
Hermitian-Norden metrics including studying of integrable structures of the
considered type on 4-dimensional Lie algebra and tangent bundles with the
complete lift of the base metric; introducing of associated Nijenhuis tensors
in relation with natural connections having totally skew-symmetric torsion as
well as quaternionic Kähler manifolds with Hermitian-Norden metrics. 5.
Introducing and studying of manifolds with almost contact 3-structures and
metrics of Hermitian-Norden type and, in particular, associated Nijenhuis
tensors and their relationship with natural connections having totally
skew-symmetric torsion.
",mathematics
"  We present a brief review of discrete structures in a finite Hilbert space,
relevant for the theory of quantum information. Unitary operator bases,
mutually unbiased bases, Clifford group and stabilizer states, discrete Wigner
function, symmetric informationally complete measurements, projective and
unitary t--designs are discussed. Some recent results in the field are covered
and several important open questions are formulated. We advocate a geometric
approach to the subject and emphasize numerous links to various mathematical
problems.
",mathematics
"  Smillie (1984) proved an interesting result on the stability of nonlinear,
time-invariant, strongly cooperative, and tridiagonal dynamical systems. This
result has found many applications in models from various fields including
biology, ecology, and chemistry. Smith (1991) has extended Smillie's result and
proved entrainment in the case where the vector field is time-varying and
periodic. We use the theory of linear totally nonnegative differential systems
developed by Schwarz (1970) to give a generalization of these two results. This
is based on weakening the requirement for strong cooperativity to
cooperativity, and adding an additional observability-type condition.
",computer-science
"  Let $f$ be a continuous real function defined in a subset of the real line.
The standard definition of continuity at a point $x$ allow us to correlate any
given epsilon with a (possibly depending of $x$) delta value. This pairing is
known as the epsilon--delta relation of $f$. In this work, we demonstrate the
existence of a privileged choice of delta in the sense that it is continuous,
invertible, maximal and it is the solution of a simple functional equation. We
also introduce an algorithm that can be used to numerically calculate this map
in polylogarithm time, proving the computability of the epsilon--delta
relation. Finally, some examples are analyzed in order to showcase the accuracy
and effectiveness of these methods, even when the explicit formula for the
aforementioned privileged function is unknown due to the lack of analytical
tools for solving the functional equation.
",mathematics
"  The magnetic field induced rearrangement of the cycloidal spin structure in
ferroelectric mono-domain single crystals of the room-temperature multiferroic
BiFeO$_3$ is studied using small-angle neutron scattering (SANS). The cycloid
propagation vectors are observed to rotate when magnetic fields applied
perpendicular to the rhombohedral (polar) axis exceed a pinning threshold value
of $\sim$5\,T. In light of these experimental results, a phenomenological model
is proposed that captures the rearrangement of the cycloidal domains, and we
revisit the microscopic origin of the magnetoelectric effect. A new coupling
between the magnetic anisotropy and the polarization is proposed that explains
the recently discovered magnetoelectric polarization to the rhombohedral axis.
",physics
"  Spatially resolving the immediate surroundings of young stars is a key
challenge for the planet formation community. SPHERE on the VLT represents an
important step forward by increasing the opportunities offered by optical or
near-infrared imaging instruments to image protoplanetary discs. The Guaranteed
Time Observation Disc team has concentrated much of its efforts on polarimetric
differential imaging, a technique that enables the efficient removal of stellar
light and thus facilitates the detection of light scattered by the disc within
a few au from the central star. These images reveal intriguing complex disc
structures and diverse morphological features that are possibly caused by
ongoing planet formation in the disc. An overview of the recent advances
enabled by SPHERE is presented.
",physics
"  We prove for any positive integer $n$ there exist boundary-sum irreducible
${\mathbb Z}_n$-corks with Stein structure. Here `boundary-sum irreducible'
means the manifold is indecomposable with respect to boundary-sum. We also
verify that some of the finite order corks admit hyperbolic boundary by HIKMOT.
",mathematics
"  A high order wavelet integral collocation method (WICM) is developed for
general nonlinear boundary value problems in physics. This method is
established based on Coiflet approximation of multiple integrals of interval
bounded functions combined with an accurate and adjustable boundary extension
technique. The convergence order of this approximation has been proven to be N
as long as the Coiflet with N-1 vanishing moment is adopted, which can be any
positive even integers. Before the conventional collocation method is applied
to the general problems, the original differential equation is changed into its
equivalent form by denoting derivatives of the unknown function as new
functions and constructing relations between the low and high order
derivatives. For the linear cases, error analysis has proven that the proposed
WICM is order N, and condition numbers of relevant matrices are almost
independent of the number of collocation points. Numerical examples of a wide
range of nonlinear differential equations in physics demonstrate that accuracy
of the proposed WICM is even greater than N, and most interestingly, such
accuracy is independent of the order of the differential equation to be solved.
Comparison to existing numerical methods further justifies the accuracy and
efficiency of the proposed method.
",mathematics
"  Due to the growth of geo-tagged images, recent web and mobile applications
provide search capabilities for images that are similar to a given query image
and simultaneously within a given geographical area. In this paper, we focus on
designing index structures to expedite these spatial-visual searches. We start
by baseline indexes that are straightforward extensions of the current popular
spatial (R*-tree) and visual (LSH) index structures. Subsequently, we propose
hybrid index structures that evaluate both spatial and visual features in
tandem. The unique challenge of this type of query is that there are
inaccuracies in both spatial and visual features. Therefore, different
traversals of the index structures may produce different images as output, some
of which more relevant to the query than the others. We compare our hybrid
structures with a set of baseline indexes in both performance and result
accuracy using three real world datasets from Flickr, Google Street View, and
GeoUGV.
",computer-science
"  Given an elliptic curve $E$ over a finite field $\mathbb{F}_q$ we study the
finite extensions $\mathbb{F}_{q^n}$ of $\mathbb{F}_q$ such that the number of
$\mathbb{F}_{q^n}$-rational points on $E$ attains the Hasse upper bound. We
obtain an upper bound on the degree $n$ for $E$ ordinary using an estimate for
linear forms in logarithms, which allows us to compute the pairs of isogeny
classes of such curves and degree $n$ for small $q$. Using a consequence of
Schmidt's Subspace Theorem, we improve the upper bound to $n\leq 11$ for
sufficiently large $q$. We also show that there are infinitely many isogeny
classes of ordinary elliptic curves with $n=3$.
",mathematics
"  Sr$_2$RuO$_4$ is the best candidate for spin-triplet superconductivity, an
unusual and elusive superconducting state of fundamental importance. In the
last three decades Sr$_2$RuO$_4$ has been very carefully studied and despite
its apparent simplicity when compared with strongly correlated high-$T_{c}$
cuprates, for which the pairing symmetry is understood, there is no scenario
that can explain all the major experimental observations, a conundrum that has
generated tremendous interest. Here we present a density-functional based
analysis of magnetic interactions in Sr$_{2}$RuO$_{4}$ and discuss the role of
magnetic anisotropy in its unconventional superconductivity. Our goal is
twofold. First, we access the possibility of the superconducting order
parameter rotation in an external magnetic field of 200 Oe, and conclude that
the spin-orbit interaction in this material is several orders of magnitude too
strong to be consistent with this hypothesis. Thus, the observed invariance of
the Knight shift across $T_{c}$ has no plausible explanation, and casts doubt
on using the Knight shift as an ultimate litmus paper for the pairing symmetry.
Second, we propose a quantitative double-exchange-like model for combining
itinerant fermions with an anisotropic Heisenberg magnetic Hamiltonian. This
model is complementary to the Hubbard-model-based calculations published so
far, and forms an alternative framework for exploring superconducting symmetry
in Sr$_{2}$RuO$_{4}.$ As an example, we use this model to analyze the
degeneracy between various $p-$triplet states in the simplest mean-field
approximation, and show that it splits into a single and two doublets with the
ground state defined by the competition between the ""Ising"" and ""compass""
anisotropic terms.
",physics
"  We present a systematic study of core-shell Au/Fe_3O_4 nanoparticles produced
by thermal decomposition under mild conditions. The morphology and crystal
structure of the nanoparticles revealed the presence of Au core of <d> =
(6.9\pm 1.0) nm surrounded by Fe_3O_4 shell with a thickness of ~3.5 nm,
epitaxially grown onto the Au core surface. The Au/Fe_3O_4 core-shell structure
was demonstrated by high angle annular dark field scanning transmission
electron microscopy analysis. The magnetite shell grown on top of the Au
nanoparticle displayed a thermal blocking state at temperatures below T_B = 59
K and a relaxed state well above T_B. Remarkably, an exchange bias effect was
observed when cooling down the samples below room temperature under an external
magnetic field. Moreover, the exchange bias field (H_{EX}) started to appear at
T~40 K and its value increased by decreasing the temperature. This effect has
been assigned to the interaction of spins located in the magnetically
disordered regions (in the inner and outer surface of the Fe_3O_4 shell) and
spins located in the ordered region of the Fe_3O_4 shell.
",physics
"  We show that $K(2)$-locally, the smash product of the string bordism spectrum
and the spectrum $T_2$ splits into copies of Morava $E$-theories. Here, $T_2$
is related to the Thom spectrum of the canonical bundle over $\Omega SU(4)$.
",mathematics
"  We report on tunnel-injected deep ultraviolet light emitting diodes (UV LEDs)
configured with a polarization engineered Al0.75Ga0.25N/ In0.2Ga0.8N tunnel
junction structure. Tunnel-injected UV LED structure enables n-type contacts
for both bottom and top contact layers. However, achieving Ohmic contact to
wide bandgap n-AlGaN layers is challenging and typically requires high
temperature contact metal annealing. In this work, we adopted a compositionally
graded top contact layer for non-alloyed metal contact, and obtained a low
contact resistance of Rc=4.8x10-5 Ohm cm2 on n-Al0.75Ga0.25N. We also observed
a significant reduction in the forward operation voltage from 30.9 V to 19.2 V
at 1 kA/cm2 by increasing the Mg doping concentration from 6.2x1018 cm-3 to
1.5x1019 cm-3. Non-equilibrium hole injection into wide bandgap Al0.75Ga0.25N
with Eg>5.2 eV was confirmed by light emission at 257 nm. This work
demonstrates the feasibility of tunneling hole injection into deep UV LEDs, and
provides a novel structural design towards high power deep-UV emitters.
",physics
"  Many households in developing countries lack formal financial histories,
making it difficult for banks to extend loans, and for potential borrowers to
receive them. However, many of these households have mobile phones, which
generate rich data about behavior. This paper shows that behavioral signatures
in mobile phone data predict loan default, using call records matched to loan
outcomes. In a middle income South American country, individuals in the highest
quintile of risk by our measure are 2.8 times more likely to default than those
in the lowest quintile. On our sample of individuals with (thin) financial
histories, our method outperforms models using credit bureau information, both
within time and when tested on a different time period. The method forms the
basis for new forms of lending that reach the unbanked.
",computer-science
"  In this paper we introduce new characterizations of spectral fractional
Laplacian to incorporate nonhomogeneous Dirichlet and Neumann boundary
conditions. The classical cases with homogeneous boundary conditions arise as a
special case. We apply our definition to fractional elliptic equations of order
$s \in (0,1)$ with nonzero Dirichlet and Neumann boundary condition. Here the
domain $\Omega$ is assumed to be a bounded, quasi-convex Lipschitz domain. To
impose the nonzero boundary conditions, we construct fractional harmonic
extensions of the boundary data. It is shown that solving for the fractional
harmonic extension is equivalent to solving for the standard harmonic extension
in the very-weak form. The latter result is of independent interest as well.
The remaining fractional elliptic problem (with homogeneous boundary data) can
be realized using the existing techniques. We introduce finite element
discretizations and derive discretization error estimates in natural norms,
which are confirmed by numerical experiments. We also apply our
characterizations to Dirichlet and Neumann boundary optimal control problems
with fractional elliptic equation as constraints.
",mathematics
"  We are reporting that the Lugiato-Lefever equation describing the frequency
comb generation in ring resonators with the localized pump and loss terms also
describes the simultaneous nonlinear resonances leading to the multistability
of nonlinear modes and coexisting solitons that are associated with the
spectrally distinct frequency combs.
",physics
"  This paper aims at solving a one-dimensional backward stochastic differential
equation (BSDE for short) with only integrable parameters. We first establish
the existence of a minimal $L^1$ solution for the BSDE when the generator $g$
is stronger continuous in $(y,z)$ and monotonic in $y$ as well as it has a
general growth in $y$ and a sublinear growth in $z$. Particularly, the $g$ may
be not uniformly continuous in $z$. Then, we put forward and prove a comparison
theorem and a Levi type theorem on the minimal $L^1$ solutions. A Lebesgue type
theorem on $L^1$ solutions is also obtained. Furthermore, we investigate the
same problem in the case that $g$ may be discontinuous in $y$. Finally, we
prove a general comparison theorem on $L^1$ solutions when $g$ is weakly
monotonic in $y$ and uniformly continuous in $z$ as well as it has a stronger
sublinear growth in $z$. As a byproduct, we also obtain a general existence and
unique theorem on $L^1$ solutions. Our results extend some known works.
",mathematics
"  We present a form of Schwarz's lemma for holomorphic maps between convex
domains $D_1$ and $D_2$. This result provides a lower bound on the distance
between the images of relatively compact subsets of $D_1$ and the boundary of
$D_2$. This is a natural improvement of an old estimate by Bernal-González
that takes into account the geometry of $\partial{D_1}$. We also provide a new
estimate for the Kobayashi metric on bounded convex domains.
",mathematics
"  The tree inclusion problem is, given two node-labeled trees $P$ and $T$ (the
""pattern tree"" and the ""text tree""), to locate every minimal subtree in $T$ (if
any) that can be obtained by applying a sequence of node insertion operations
to $P$. The ordered tree inclusion problem is known to be solvable in
polynomial time while the unordered tree inclusion problem is NP-hard. The
currently fastest algorithm for the latter is from 1995 and runs in
$O(poly(m,n) \cdot 2^{2d}) = O^{\ast}(4^{d})$ time, where $m$ and $n$ are the
sizes of the pattern and text trees, respectively, and $d$ is the degree of the
pattern tree. Here, we develop a new algorithm that improves the exponent $2d$
to $d$ by considering a particular type of ancestor-descendant relationships
and applying dynamic programming, thus reducing the time complexity to
$O^{\ast}(2^{d})$. We then study restricted variants of the unordered tree
inclusion problem where the number of occurrences of different node labels
and/or the input trees' heights are bounded and show that although the problem
remains NP-hard in many such cases, if the leaves of $P$ are distinctly labeled
and each label occurs at most $c$ times in $T$ then it can be solved in
polynomial time for $c = 2$ and in $O^{\ast}(1.8^d)$ time for $c = 3$.
",computer-science
"  Tangles of quantized vortex line of initial density ${\cal L}(0) \sim 6\times
10^3$\,cm$^{-2}$ and variable amplitude of fluctuations of flow velocity $U(0)$
at the largest length scale were generated in superfluid $^4$He at $T=0.17$\,K,
and their free decay ${\cal L}(t)$ was measured. If $U(0)$ is small, the excess
random component of vortex line length firstly decays as ${\cal L} \propto
t^{-1}$ until it becomes comparable with the structured component responsible
for the classical velocity field, and the decay changes to ${\cal L} \propto
t^{-3/2}$. The latter regime always ultimately prevails, provided the classical
description of $U$ holds. A quantitative model of coexisting cascades of
quantum and classical energies describes all regimes of the decay.
",physics
"  Gradient-based optimization is the foundation of deep learning and
reinforcement learning. Even when the mechanism being optimized is unknown or
not differentiable, optimization using high-variance or biased gradient
estimates is still often the best strategy. We introduce a general framework
for learning low-variance, unbiased gradient estimators for black-box functions
of random variables. Our method uses gradients of a neural network trained
jointly with model parameters or policies, and is applicable in both discrete
and continuous settings. We demonstrate this framework for training discrete
latent-variable models. We also give an unbiased, action-conditional extension
of the advantage actor-critic reinforcement learning algorithm.
",computer-science
"  This work reports an electronic and micro-structural study of an appealing
system for optoelectronics: tungsten disulphide WS$_2$ on epitaxial graphene
(EG) on SiC(0001). The WS$_2$ is grown via chemical vapor deposition (CVD) onto
the EG. Low-energy electron diffraction (LEED) measurements assign the
zero-degree orientation as the preferential azimuthal alignment for WS$_2$/EG.
The valence-band (VB) structure emerging from this alignment is investigated by
means of photoelectron spectroscopy measurements, with both high space and
energy resolution. We find that the spin-orbit splitting of monolayer WS$_2$ on
graphene is of 462 meV, larger than what is reported to date for other
substrates. We determine the value of the work function for the WS$_2$/EG to be
4.5$\pm$0.1 eV. A large shift of the WS$_2$ VB maximum is observed as well ,
due to the lowering of the WS$_2$ work function caused by the donor-like
interfacial states of EG. Density functional theory (DFT) calculations carried
out on a coincidence supercell confirm the experimental band structure to an
excellent degree. X-ray photoemission electron microscopy (XPEEM) measurements
performed on single WS$_2$ crystals confirm the van der Waals nature of the
interface coupling between the two layers. In virtue of its band alignment and
large spin-orbit splitting, this system gains strong appeal for optical
spin-injection experiments and opto-spintronic applications in general.
",physics
"  We consider the Bogolubov-de Gennes equations giving an equivalent
formulation of the BCS theory of superconductivity. We are interested in the
case when the magnetic field is present. We (a) discuss their general features,
(b) isolate key physical classes of solutions (normal, vortex and vortex
lattice states) and (c) prove existence of the normal, vortex and vortex
lattice states and stability/instability of the normal states for large/small
temperature or/and magnetic fields.
",mathematics
"  We show that for any singular dominant integral weight $\lambda$ of a complex
semisimple Lie algebra $\mathfrak{g}$, the endomorphism algebra $B$ of any
projective-injective module of the parabolic BGG category
$\mathcal{O}_\lambda^{\mathfrak{p}}$ is a symmetric algebra (as conjectured by
Khovanov) extending the results of Mazorchuk and Stroppel for the regular
dominant integral weight. Moreover, the endomorphism algebra $B$ is equipped
with a homogeneous (non-degenerate) symmetrizing form. In the appendix, there
is a short proof due to K. Coulembier and V. Mazorchuk showing that the
endomorphism algebra $B_\lambda^{\mathfrak{p}}$ of the basic
projective-injective module of $\mathcal{O}_\lambda^{\mathfrak{p}}$ is a
symmetric algebra.
",mathematics
"  We present models for embedding words in the context of surrounding words.
Such models, which we refer to as token embeddings, represent the
characteristics of a word that are specific to a given context, such as word
sense, syntactic category, and semantic role. We explore simple, efficient
token embedding models based on standard neural network architectures. We learn
token embeddings on a large amount of unannotated text and evaluate them as
features for part-of-speech taggers and dependency parsers trained on much
smaller amounts of annotated data. We find that predictors endowed with token
embeddings consistently outperform baseline predictors across a range of
context window and training set sizes.
",computer-science
"  We apply a method that combines the tight-binding approximation and the
Lowdin down-folding procedure to evaluate the electronic band structure of the
newly discovered pressure-induced superconductor CrAs. By integrating out all
low-lying arsenic degrees of freedom, we derive an effective Hamiltonian model
describing the Cr d bands near the Fermi level. We calculate and make
predictions for the energy spectra, the Fermi surface, the density of states
and transport and magnetic properties of this compound. Our results are
consistent with local-density approximation calculations as well as they show
good agreement with available experimental data for resistivity and Cr magnetic
moment.
",physics
"  The rise in life expectancy is one of the great achievements of the twentieth
century. This phenomenon originates a still increasing interest in Ambient
Assisted Living (AAL) technological solutions that may support people in their
daily routines allowing an independent and safe lifestyle as long as possible.
AAL systems generally acquire data from the field and reason on them and the
context to accomplish their tasks. Very often, AAL systems are vertical
solutions, thus making hard their reuse and adaptation to different domains
with respect to the ones for which they have been developed. In this paper we
propose an architectural solution that allows the acquisition level of an ALL
system to be easily built, configured, and extended without affecting the
reasoning level of the system. We experienced our proposal in a fall detection
system.
",computer-science
"  We report measurements of the de Haas-van Alphen effect in the layered
heavy-fermion compound CePt$_2$In$_7$ in high magnetic fields up to 35 T. Above
an angle-dependent threshold field, we observed several de Haas-van Alphen
frequencies originating from almost ideally two-dimensional Fermi surfaces. The
frequencies are similar to those previously observed to develop only above a
much higher field of 45 T, where a clear anomaly was detected and proposed to
originate from a change in the electronic structure [M. M. Altarawneh et al.,
Phys. Rev. B 83, 081103 (2011)]. Our experimental results are compared with
band structure calculations performed for both CePt$_2$In$_7$ and
LaPt$_2$In$_7$, and the comparison suggests localized $f$ electrons in
CePt$_2$In$_7$. This conclusion is further supported by comparing
experimentally observed Fermi surfaces in CePt$_2$In$_7$ and PrPt$_2$In$_7$,
which are found to be almost identical. The measured effective masses in
CePt$_2$In$_7$ are only moderately enhanced above the bare electron mass $m_0$,
from 2$m_0$ to 6$m_0$.
",physics
"  The Advanced Virgo detector uses two monolithic optical cavities at its
output port to suppress higher order modes and radio frequency sidebands from
the carrier light used for gravitational wave detection. These two cavities in
series form the output mode cleaner. We present a measured upper limit on the
length noise of these cavities that is consistent with the thermo-refractive
noise prediction of $8 \times 10^{-16}\,\textrm{m/Hz}^{1/2}$ at 15 Hz. The
cavity length is controlled using Peltier cells and piezo-electric actuators to
maintain resonance on the incoming light. A length lock precision of $3.5
\times 10^{-13}\,\textrm{m}$ is achieved. These two results are combined to
demonstrate that the broadband length noise of the output mode cleaner in the
10-60 Hz band is at least a factor 10 below other expected noise sources in the
Advanced Virgo detector design configuration.
",physics
"  Unmanned aerial vehicles (UAVs) have gained a lot of popularity in diverse
wireless communication fields. They can act as high-altitude flying relays to
support communications between ground nodes due to their ability to provide
line-of-sight links. With the flourishing Internet of Things, several types of
new applications are emerging. In this paper, we focus on bandwidth hungry and
delay-tolerant applications where multiple pairs of transceivers require the
support of UAVs to complete their transmissions. To do so, the UAVs have the
possibility to employ two different bands namely the typical microwave and the
high-rate millimeter wave bands. In this paper, we develop a generic framework
to assign UAVs to supported transceivers and optimize their trajectories such
that a weighted function of the total service time is minimized. Taking into
account both the communication time needed to relay the message and the flying
time of the UAVs, a mixed non-linear programming problem aiming at finding the
stops at which the UAVs hover to forward the data to the receivers is
formulated. An iterative approach is then developed to solve the problem.
First, a mixed linear programming problem is optimally solved to determine the
path of each available UAV. Then, a hierarchical iterative search is executed
to enhance the UAV stops' locations and reduce the service time. The behavior
of the UAVs and the benefits of the proposed framework are showcased for
selected scenarios.
",computer-science
"  Double-stranded DNA may contain mismatched base pairs beyond the Watson-Crick
pairs guanine-cytosine and adenine-thymine. Such mismatches bear adverse
consequences for human health. We utilize molecular dynamics and metadynamics
computer simulations to study the equilibrium structure and dynamics for both
matched and mismatched base pairs. We discover significant differences between
matched and mismatched pairs in structure, hydrogen bonding, and base flip work
profiles. Mismatched pairs shift further in the plane normal to the DNA strand
and are more likely to exhibit non-canonical structures, including the e-motif.
We discuss potential implications on mismatch repair enzymes' detection of DNA
mismatches.
",quantitative-biology
"  Microblogging sites are the direct platform for the users to express their
views. It has been observed from previous studies that people are viable to
flaunt their emotions for events (eg. natural catastrophes, sports, academics
etc.), for persons (actor/actress, sports person, scientist) and for the places
they visit. In this study we focused on a sport event, particularly the cricket
tournament and collected the emotions of the fans for their favorite players
using their tweets. Further, we acquired the stock market performance of the
brands which are either endorsing the players or sponsoring the match in the
tournament. It has been observed that performance of the player triggers the
users to flourish their emotions over social media therefore, we observed
correlation between players performance and fans' emotions. Therefore, we found
the direct connection between player's performance with brand's behavior on
stock market.
",computer-science
"  In the derivation of the generating function of the Gaudin Hamiltonians with
boundary terms, we follow the same approach used previously in the rational
case, which in turn was based on Sklyanin's method in the periodic case. Our
derivation is centered on the quasi-classical expansion of the linear
combination of the transfer matrix of the XXZ Heisenberg spin chain and the
central element, the so-called Sklyanin determinant. The corresponding Gaudin
Hamiltonians with boundary terms are obtained as the residues of the generating
function. By defining the appropriate Bethe vectors which yield strikingly
simple off-shell action of the generating function, we fully implement the
algebraic Bethe ansatz, obtaining the spectrum of the generating function and
the corresponding Bethe equations.
",physics
"  We present the results of very long baseline interferometry (VLBI)
observations of gamma-ray bright blazar S5 0716+714 using the Korean VLBI
Network (KVN) at the 22, 43, 86, and 129 GHz bands, as part of the
Interferometric Monitoring of Gamma-ray Bright AGNs (iMOGABA) KVN key science
program. Observations were conducted in 29 sessions from January 16, 2013 to
March 1, 2016, with the source being detected and imaged at all available
frequencies. In all epochs, the source was compact on the milliarcsecond (mas)
scale, yielding a compact VLBI core dominating the synchrotron emission on
these scales. Based on the multi-wavelength data between 15 GHz (Owens Valley
Radio Observatory) and 230 GHz (Submillimeter Array), we found that the source
shows multiple prominent enhancements of the flux density at the centimeter
(cm) and millimeter (mm) wavelengths, with mm enhancements leading cm
enhancements by -16$\pm$8 days. The turnover frequency was found to vary
between 21 to 69GHz during our observations. By assuming a synchrotron
self-absorption model for the relativistic jet emission in S5 0716+714, we
found the magnetic field strength in the mas emission region to be $\le$5 mG
during the observing period, yielding a weighted mean of 1.0$\pm$0.6 mG for
higher turnover frequencies (e.g., >45 GHz).
",physics
"  What happens when a new social convention replaces an old one? While the
possible forces favoring norm change - such as institutions or committed
activists - have been identified since a long time, little is known about how a
population adopts a new convention, due to the difficulties of finding
representative data. Here we address this issue by looking at changes occurred
to 2,541 orthographic and lexical norms in English and Spanish through the
analysis of a large corpora of books published between the years 1800 and 2008.
We detect three markedly distinct patterns in the data, depending on whether
the behavioral change results from the action of a formal institution, an
informal authority or a spontaneous process of unregulated evolution. We
propose a simple evolutionary model able to capture all the observed behaviors
and we show that it reproduces quantitatively the empirical data. This work
identifies general mechanisms of norm change and we anticipate that it will be
of interest to researchers investigating the cultural evolution of language
and, more broadly, human collective behavior.
",quantitative-biology
"  We present development of a genetic algorithm for fitting potential energy
curves of diatomic molecules to experimental data. Our approach does not
involve any functional form for fitting, which makes it a general fitting
procedure. In particular, it takes in a guess potential, perhaps from an $ab \
initio$ calculation, along with experimental measurements of vibrational
binding energies, rotational constants, and their experimental uncertainties.
The fitting procedure is able to modify the guess potential until it converges
to better than 1% uncertainty, as measured by $\bar{\chi}^2$. We present the
details of this technique along with a comparison of potentials calculated by
our genetic algorithm and the state of the art fitting techniques based on
inverted perturbation approach for the $X \ ^1\Sigma^+$ and $C \ ^1\Sigma^+$
potentials of lithium-rubidium.
",physics
"  We study the multiclass online learning problem where a forecaster makes a
sequence of predictions using the advice of $n$ experts. Our main contribution
is to analyze the regime where the best expert makes at most $b$ mistakes and
to show that when $b = o(\log_4{n})$, the expected number of mistakes made by
the optimal forecaster is at most $\log_4{n} + o(\log_4{n})$. We also describe
an adversary strategy showing that this bound is tight and that the worst case
is attained for binary prediction.
",statistics
"  Existing visual reasoning datasets such as Visual Question Answering (VQA),
often suffer from biases conditioned on the question, image or answer
distributions. The recently proposed CLEVR dataset addresses these limitations
and requires fine-grained reasoning but the dataset is synthetic and consists
of similar objects and sentence structures across the dataset.
In this paper, we introduce a new inference task, Visual Entailment (VE) -
consisting of image-sentence pairs whereby a premise is defined by an image,
rather than a natural language sentence as in traditional Textual Entailment
tasks. The goal of a trained VE model is to predict whether the image
semantically entails the text. To realize this task, we build a dataset SNLI-VE
based on the Stanford Natural Language Inference corpus and Flickr30k dataset.
We evaluate various existing VQA baselines and build a model called Explainable
Visual Entailment (EVE) system to address the VE task. EVE achieves up to 71%
accuracy and outperforms several other state-of-the-art VQA based models.
Finally, we demonstrate the explainability of EVE through cross-modal attention
visualizations. The SNLI-VE dataset is publicly available at
this https URL necla-ml/SNLI-VE.
",computer-science
"  We define a map $f\colon X\to Y$ to be a phantom map relative to a map
$\varphi\colon B\to Y$ if the restriction of $f$ to any finite dimensional
skeleton of $X$ lifts to $B$ through $\varphi$, up to homotopy. There are two
kinds of maps which are obviously relative phantom maps: (1) the composite of a
map $X\to B$ with $\varphi$; (2) a usual phantom map $X\to Y$. A relative
phantom map of type (1) is called trivial, and a relative phantom map out of a
suspension which is a sum of (1) and (2) is called relatively trivial. We study
the (relative) triviality of relative phantom maps from a suspension, and in
particular, we give rational homotopy conditions for the (relative) triviality.
We also give a rational homotopy condition for the triviality of relative
phantom maps from a non-suspension to a finite Postnikov section.
",mathematics
"  Empirical researchers often trim observations with small denominator A when
they estimate moments of the form E[B/A]. Large trimming is a common practice
to mitigate variance, but it incurs large trimming bias. This paper provides a
novel method of correcting large trimming bias. If a researcher is willing to
assume that the joint distribution between A and B is smooth, then a large
trimming bias may be estimated well. With the bias correction, we also develop
a valid and robust inference result for E[B/A].
",statistics
"  Couder and Fort discovered that droplets walking on a vibrating bath possess
certain features previously thought to be exclusive to quantum systems. These
millimetric droplets synchronize with their Faraday wavefield, creating a
macroscopic pilot-wave system. In this paper we exploit the fact that the waves
generated are nearly monochromatic and propose a hydrodynamic model capable of
quantitatively capturing the interaction between bouncing drops and a variable
topography. We show that our reduced model is able to reproduce some important
experiments involving the drop-topography interaction, such as non-specular
reflection and single-slit diffraction.
",physics
"  We propose Batch-Expansion Training (BET), a framework for running a batch
optimizer on a gradually expanding dataset. As opposed to stochastic
approaches, batches do not need to be resampled i.i.d. at every iteration, thus
making BET more resource efficient in a distributed setting, and when
disk-access is constrained. Moreover, BET can be easily paired with most batch
optimizers, does not require any parameter-tuning, and compares favorably to
existing stochastic and batch methods. We show that when the batch size grows
exponentially with the number of outer iterations, BET achieves optimal
$O(1/\epsilon)$ data-access convergence rate for strongly convex objectives.
Experiments in parallel and distributed settings show that BET performs better
than standard batch and stochastic approaches.
",computer-science
"  Acid solutions exhibit a variety of complex structural and dynamical features
arising from the presence of multiple interacting reactive proton defects and
counterions. However, disentangling the transient structural motifs of proton
defects in the water hydrogen bond network and the mechanisms for their
interconversion remains a formidable challenge. Here, we use simulations
treating the quantum nature of both the electrons and nuclei to show how the
experimentally observed spectroscopic features and relaxation timescales can be
elucidated using a physically transparent coordinate that encodes the overall
asymmetry of the solvation environment of the proton defect. We demonstrate
that this coordinate can be used both to discriminate the extremities of the
features observed in the linear vibrational spectrum and to explain the
molecular motions that give rise to the interconversion timescales observed in
recent nonlinear experiments. This analysis provides a unified condensed-phase
picture of proton structure and dynamics that, at its extrema, encompasses
proton sharing and spectroscopic features resembling the limiting Eigen
[H$_{3}$O(H$_{2}$O)$_{3}$]$^{+}$ and Zundel [H(H$_{2}$O)$_{2}$]$^{+}$ gas-phase
structures, while also describing the rich variety of interconverting
environments in the liquid phase.
",physics
"  The new cyber attack pattern of advanced persistent threat (APT) has posed a
serious threat to modern society. This paper addresses the APT defense problem,
i.e., the problem of how to effectively defend against an APT campaign. Based
on a novel APT attack-defense model, the effectiveness of an APT defense
strategy is quantified. Thereby, the APT defense problem is modeled as an
optimal control problem, in which an optimal control stands for a most
effective APT defense strategy. The existence of an optimal control is proved,
and an optimality system is derived. Consequently, an optimal control can be
figured out by solving the optimality system. Some examples of the optimal
control are given. Finally, the influence of some factors on the effectiveness
of an optimal control is examined through computer experiments. These findings
help organizations to work out policies of defending against APTs.
",computer-science
"  We consider the problem of dynamic spectrum access for network utility
maximization in multichannel wireless networks. The shared bandwidth is divided
into K orthogonal channels. In the beginning of each time slot, each user
selects a channel and transmits a packet with a certain transmission
probability. After each time slot, each user that has transmitted a packet
receives a local observation indicating whether its packet was successfully
delivered or not (i.e., ACK signal). The objective is a multi-user strategy for
accessing the spectrum that maximizes a certain network utility in a
distributed manner without online coordination or message exchanges between
users. Obtaining an optimal solution for the spectrum access problem is
computationally expensive in general due to the large state space and partial
observability of the states. To tackle this problem, we develop a novel
distributed dynamic spectrum access algorithm based on deep multi-user
reinforcement leaning. Specifically, at each time slot, each user maps its
current state to spectrum access actions based on a trained deep-Q network used
to maximize the objective function. Game theoretic analysis of the system
dynamics is developed for establishing design principles for the implementation
of the algorithm. Experimental results demonstrate strong performance of the
algorithm.
",computer-science
"  This paper studies effective separability for subgroups of finitely generated
nilpotent groups and more broadly effective subgroup separability of finitely
generated nilpotent groups. We provide upper and lower bounds that are
polynomial with respect to the logarithm of the word length for infinite index
subgroups of nilpotent groups. In the case of normal subgroups, we provide an
exact computation generalizing work of the second author. We introduce a
function that quantifies subgroup separability, and we provide polynomial upper
and lower bounds. We finish by demonstrating that our results extend to
virtually nilpotent groups.
",mathematics
"  Let $(X,\omega)$ be a compact Hermitian manifold of complex dimension $n$. In
this article, we first survey recent progress towards Grauert-Riemenschneider
type criterions. Secondly, we give a simplified proof of Boucksom's conjecture
given by the author under the assumption that the Hermitian metric $\omega$
satisfies $\partial\overline{\partial}\omega^l=$ for all $l$, i.e., if $T$ is a
closed positive current on $X$ such that $\int_XT_{ac}^n>0$, then the class
$\{T\}$ is big and $X$ is Kähler. Finally, as an easy observation, we point
out that Nguyen's result can be generalized as follows: if
$\partial\overline{\partial}\omega=0$, and $T$ is a closed positive current
with analytic singularities, such that $\int_XT^n_{ac}>0$, then the class
$\{T\}$ is big and $X$ is Kähler.
",mathematics
"  In partially observed environments, it can be useful for a human to provide
the robot with declarative information that represents probabilistic relational
constraints on properties of objects in the world, augmenting the robot's
sensory observations. For instance, a robot tasked with a search-and-rescue
mission may be informed by the human that two victims are probably in the same
room. An important question arises: how should we represent the robot's
internal knowledge so that this information is correctly processed and combined
with raw sensory information? In this paper, we provide an efficient belief
state representation that dynamically selects an appropriate factoring,
combining aspects of the belief when they are correlated through information
and separating them when they are not. This strategy works in open domains, in
which the set of possible objects is not known in advance, and provides
significant improvements in inference time over a static factoring, leading to
more efficient planning for complex partially observed tasks. We validate our
approach experimentally in two open-domain planning problems: a 2D discrete
gridworld task and a 3D continuous cooking task. A supplementary video can be
found at this http URL.
",computer-science
"  We present a monitoring approach for verifying systems at runtime. Our
approach targets systems whose components communicate with the monitors over
unreliable channels, where messages can be delayed or lost. In contrast to
prior works, whose property specification languages are limited to
propositional temporal logics, our approach handles an extension of the
real-time logic MTL with freeze quantifiers for reasoning about data values. We
present its underlying theory based on a new three-valued semantics that is
well suited to soundly and completely reason online about event streams in the
presence of message delay or loss. We also evaluate our approach
experimentally. Our prototype implementation processes hundreds of events per
second in settings where messages are received out of order.
",computer-science
"  In the Bak-Sneppen model, the lowest fitness particle and its two nearest
neighbors are renewed at each temporal step with a uniform (0,1) fitness
distribution. The model presents a critical value that depends on the
interaction criteria (two nearest neighbors) and on the update procedure
(uniform). Here we calculate the critical value for models where one or both
properties are changed. We study models with non-uniform updates, models with
random neighbors and models with binary fitness and obtain exact results for
the average fitness and for $p_c$.
",physics
"  Let (P) denote the problem of existence of a point in the plane of a given
triangle T, that is at rational distance from all the vertices of T. In this
article, we provide a complete solution to (P) for all equilateral triangles.
",mathematics
"  This paper addresses the problem of automatic speech recognition (ASR) of a
target speaker in background speech. The novelty of our approach is that we
focus on a wakeup keyword, which is usually used for activating ASR systems
like smart speakers. The proposed method firstly utilizes a DNN-based mask
estimator to separate the mixture signal into the keyword signal uttered by the
target speaker and the remaining background speech. Then the separated signals
are used for calculating a beamforming filter to enhance the subsequent
utterances from the target speaker. Experimental evaluations show that the
trained DNN-based mask can selectively separate the keyword and background
speech from the mixture signal. The effectiveness of the proposed method is
also verified with Japanese ASR experiments, and we confirm that the character
error rates are significantly improved by the proposed method for both
simulated and real recorded test sets.
",computer-science
"  The detection of software vulnerabilities (or vulnerabilities for short) is
an important problem that has yet to be tackled, as manifested by many
vulnerabilities reported on a daily basis. This calls for machine learning
methods to automate vulnerability detection. Deep learning is attractive for
this purpose because it does not require human experts to manually define
features. Despite the tremendous success of deep learning in other domains, its
applicability to vulnerability detection is not systematically understood. In
order to fill this void, we propose the first systematic framework for using
deep learning to detect vulnerabilities. The framework, dubbed Syntax-based,
Semantics-based, and Vector Representations (SySeVR), focuses on obtaining
program representations that can accommodate syntax and semantic information
pertinent to vulnerabilities. Our experiments with 4 software products
demonstrate the usefulness of the framework: we detect 15 vulnerabilities that
are not reported in the National Vulnerability Database. Among these 15
vulnerabilities, 7 are unknown and have been reported to the vendors, and the
other 8 have been ""silently"" patched by the vendors when releasing newer
versions of the products.
",statistics
"  Polariton lasing is the coherent emission arising from a macroscopic
polariton condensate first proposed in 1996. Over the past two decades,
polariton lasing has been demonstrated in a few inorganic and organic
semiconductors in both low and room temperatures. Polariton lasing in inorganic
materials significantly relies on sophisticated epitaxial growth of crystalline
gain medium layers sandwiched by two distributed Bragg reflectors in which
combating the built-in strain and mismatched thermal properties is nontrivial.
On the other hand, organic active media usually suffer from large threshold
density and weak nonlinearity due to the Frenkel exciton nature. Further
development of polariton lasing towards technologically significant
applications demand more accessible materials, ease of device fabrication and
broadly tunable emission at room temperature. Herein, we report the
experimental realization of room-temperature polariton lasing based on an
epitaxy-free all-inorganic cesium lead chloride perovskite microcavity.
Polariton lasing is unambiguously evidenced by a superlinear power dependence,
macroscopic ground state occupation, blueshift of ground state emission,
narrowing of the linewidth and the build-up of long-range spatial coherence.
Our work suggests considerable promise of lead halide perovskites towards
large-area, low-cost, high performance room temperature polariton devices and
coherent light sources extending from the ultraviolet to near infrared range.
",physics
"  This paper is concerned with the problem of stochastic control of gene
regulatory networks (GRNs) observed indirectly through noisy measurements and
with uncertainty in the intervention inputs. The partial observability of the
gene states and uncertainty in the intervention process are accounted for by
modeling GRNs using the partially-observed Boolean dynamical system (POBDS)
signal model with noisy gene expression measurements. Obtaining the optimal
infinite-horizon control strategy for this problem is not attainable in
general, and we apply reinforcement learning and Gaussian process techniques to
find a near-optimal solution. The POBDS is first transformed to a
directly-observed Markov Decision Process in a continuous belief space, and the
Gaussian process is used for modeling the cost function over the belief and
intervention spaces. Reinforcement learning then is used to learn the cost
function from the available gene expression data. In addition, we employ
sparsification, which enables the control of large partially-observed GRNs. The
performance of the resulting algorithm is studied through a comprehensive set
of numerical experiments using synthetic gene expression data generated from a
melanoma gene regulatory network.
",statistics
"  We show that the zeroth coefficient of the cables of the HOMFLY polynomial
(colored HOMFLY polynomials) does not distinguish mutants. This makes a sharp
contrast with the total HOMFLY polynomial whose 3-cables can distinguish
mutants.
",mathematics
"  The X-ray transform on the periodic slab $[0,1]\times\mathbb T^n$, $n\geq0$,
has a non-trivial kernel due to the symmetry of the manifold and presence of
trapped geodesics. For tensor fields gauge freedom increases the kernel
further, and the X-ray transform is not solenoidally injective unless $n=0$. We
characterize the kernel of the geodesic X-ray transform for $L^2$-regular
$m$-tensors for any $m\geq0$. The characterization extends to more general
manifolds, twisted slabs, including the Möbius strip as the simplest example.
",mathematics
"  The main object of this article is to present an extension of the
zero-inflated Poisson-Lindley distribution, called of zero-modified
Poisson-Lindley. The additional parameter $\pi$ of the zero-modified
Poisson-Lindley has a natural interpretation in terms of either
zero-deflated/inflated proportion. Inference is dealt with by using the
likelihood approach. In particular the maximum likelihood estimators of the
distribution's parameter are compared in small and large samples. We also
consider an alternative bias-correction mechanism based on Efron's bootstrap
resampling. The model is applied to real data sets and found to perform better
than other competing models.
",statistics
"  We present gravitational lens models of the multiply imaged quasar DES
J0408-5354, recently discovered in the Dark Energy Survey (DES) footprint, with
the aim of interpreting its remarkable quad-like configuration. We first model
the DES single-epoch $grizY$ images as a superposition of a lens galaxy and
four point-like objects, obtaining spectral energy distributions (SEDs) and
relative positions for the objects. Three of the point sources (A,B,D) have
SEDs compatible with the discovery quasar spectra, while the faintest
point-like image (G2/C) shows significant reddening and a `grey' dimming of
$\approx0.8$mag. In order to understand the lens configuration, we fit
different models to the relative positions of A,B,D. Models with just a single
deflector predict a fourth image at the location of G2/C but considerably
brighter and bluer. The addition of a small satellite galaxy ($R_{\rm
E}\approx0.2$"") in the lens plane near the position of G2/C suppresses the flux
of the fourth image and can explain both the reddening and grey dimming. All
models predict a main deflector with Einstein radius between $1.7""$ and $2.0"",$
velocity dispersion $267-280$km/s and enclosed mass $\approx
6\times10^{11}M_{\odot},$ even though higher resolution imaging data are needed
to break residual degeneracies in model parameters. The longest time-delay
(B-A) is estimated as $\approx 85$ (resp. $\approx125$) days by models with
(resp. without) a perturber near G2/C. The configuration and predicted
time-delays of J0408-5354 make it an excellent target for follow-up aimed at
understanding the source quasar host galaxy and substructure in the lens, and
measuring cosmological parameters. We also discuss some lessons learnt from
J0408-5354 on lensed quasar finding strategies, due to its chromaticity and
morphology.
",physics
"  Topological metrics of graphs provide a natural way to describe the prominent
features of various types of networks. Graph metrics describe the structure and
interplay of graph edges and have found applications in many scientific fields.
In this work, graph metrics are used in network estimation by developing
optimisation methods that incorporate prior knowledge of a network's topology.
The derivatives of graph metrics are used in gradient descent schemes for
weighted undirected network denoising, network completion, and network
decomposition. The successful performance of our methodology is shown in a
number of toy examples and real-world datasets. Most notably, our work
establishes a new link between graph theory, network science and optimisation.
",computer-science
"  We present an algorithm for rapidly learning controllers for robotics
systems. The algorithm follows the model-based reinforcement learning paradigm,
and improves upon existing algorithms; namely Probabilistic learning in Control
(PILCO) and a sample-based version of PILCO with neural network dynamics
(Deep-PILCO). We propose training a neural network dynamics model using
variational dropout with truncated Log-Normal noise. This allows us to obtain a
dynamics model with calibrated uncertainty, which can be used to simulate
controller executions via rollouts. We also describe set of techniques,
inspired by viewing PILCO as a recurrent neural network model, that are crucial
to improve the convergence of the method. We test our method on a variety of
benchmark tasks, demonstrating data-efficiency that is competitive with PILCO,
while being able to optimize complex neural network controllers. Finally, we
assess the performance of the algorithm for learning motor controllers for a
six legged autonomous underwater vehicle. This demonstrates the potential of
the algorithm for scaling up the dimensionality and dataset sizes, in more
complex control tasks.
",computer-science
"  This paper presents a new multi-objective deep reinforcement learning (MODRL)
framework based on deep Q-networks. We propose the use of linear and non-linear
methods to develop the MODRL framework that includes both single-policy and
multi-policy strategies. The experimental results on two benchmark problems
including the two-objective deep sea treasure environment and the
three-objective mountain car problem indicate that the proposed framework is
able to converge to the optimal Pareto solutions effectively. The proposed
framework is generic, which allows implementation of different deep
reinforcement learning algorithms in different complex environments. This
therefore overcomes many difficulties involved with standard multi-objective
reinforcement learning (MORL) methods existing in the current literature. The
framework creates a platform as a testbed environment to develop methods for
solving various problems associated with the current MORL. Details of the
framework implementation can be referred to
this http URL.
",statistics
"  Software has long been established as an essential aspect of the scientific
process in mathematics and other disciplines. However, reliably referencing
software in scientific publications is still challenging for various reasons. A
crucial factor is that software dynamics with temporal versions or states are
difficult to capture over time. We propose to archive and reference surrogates
instead, which can be found on the Web and reflect the actual software to a
remarkable extent. Our study shows that about a half of the webpages of
software are already archived with almost all of them including some kind of
documentation.
",computer-science
"  The collective behavior of active semiflexible filaments is studied with a
model of tangentially driven self-propelled worm-like chains. The combination
of excluded-volume interactions and self-propulsion leads to several distinct
dynamic phases as a function of bending rigidity, activity, and aspect ratio of
individual filaments. We consider first the case of intermediate filament
density. For high-aspect-ratio filaments, we identify a transition with
increasing propulsion from a state of free-swimming filaments to a state of
spiraled filaments with nearly frozen translational motion. For lower aspect
ratios, this gas-of-spirals phase is suppressed with growing density due to
filament collisions; instead, filaments form clusters similar to self-propelled
rods, as activity increases. Finite bending rigidity strongly effects the
dynamics and phase behavior. Flexible filaments form small and transient
clusters, while stiffer filaments organize into giant clusters, similarly as
self-propelled rods, but with a reentrant phase behavior from giant to smaller
clusters as activity becomes large enough to bend the filaments. For high
filament densities, we identify a nearly frozen jamming state at low
activities, a nematic laning state at intermediate activities, and an
active-turbulence state at high activities. The latter state is characterized
by a power-law decay of the energy spectrum as a function of wave number. The
resulting phase diagrams encapsulate tunable non-equilibrium steady states that
can be used in the organization of living matter.
",quantitative-biology
"  Jacobsthal's function was recently generalised for the case of paired
progressions. It was proven that a specific bound of this function is
sufficient for the truth of Goldbach's conjecture and of the prime pairs
conjecture as well. We extended and adapted algorithms described for the
computation of the common Jacobsthal function, and computed respective function
values of the paired Jacobsthal function for primorial numbers for primes up to
73. All these values fulfil the conjectured specific bound. In addition to this
note, we provide a detailed review of the algorithmic approaches and the
complete computational results in ancillary files.
",mathematics
"  What role do asymptomatically infected individuals play in the transmission
dynamics? There are many diseases, such as norovirus and influenza, where some
infected hosts show symptoms of the disease while others are asymptomatically
infected, i.e. do not show any symptoms. The current paper considers a class of
epidemic models following an SEIR (Susceptible $\to$ Exposed $\to$ Infectious
$\to$ Recovered) structure that allows for both symptomatic and asymptomatic
cases. The following question is addressed: what fraction $\rho$ of those
individuals getting infected are infected by symptomatic (asymptomatic) cases?
This is a more complicated question than the related question for the beginning
of the epidemic: what fraction of the expected number of secondary cases of a
typical newly infected individual, i.e. what fraction of the basic reproduction
number $R_0$, is caused by symptomatic individuals? The latter fraction only
depends on the type-specific reproduction numbers, while the former fraction
$\rho$ also depends on timing and hence on the probabilistic distributions of
latent and infectious periods of the two types (not only their means). Bounds
on $\rho$ are derived for the situation where these distributions (and even
their means) are unknown. Special attention is given to the class of Markov
models and the class of continuous-time Reed-Frost models as two classes of
distribution functions. We show how these two classes of models can exhibit
very different behaviour.
",quantitative-biology
"  This chapter provides an introduction to the modeling and control of power
generation from wind turbine systems. In modeling, the focus is on the
electrical components: electrical machine (e.g. permanent-magnet synchronous
generators), back-to-back converter (consisting of machine-side and grid-side
converter sharing a common DC-link), mains filters and ideal (balanced) power
grid. The aerodynamics and the torque generation of the wind turbine are
explained in simplified terms using a so-called power coefficient. The overall
control system is considered. In particular, the phase-locked loop system for
grid-side voltage orientation, the nonlinear speed control system for the
generator (and turbine), and the non-minimum phase DC-link voltage control
system are discussed in detail; based on a brief derivation of the underlying
machine-side and grid-side current control systems. With the help of the power
balance of the wind turbine, the operation management and the control of the
power flow are explained. Concluding simulation results illustrate the overall
system behavior of a controlled wind turbine with a permanent-magnet
synchronous generator.
",computer-science
"  Heuristic tools from statistical physics have been used in the past to locate
the phase transitions and compute the optimal learning and generalization
errors in the teacher-student scenario in multi-layer neural networks. In this
contribution, we provide a rigorous justification of these approaches for a
two-layers neural network model called the committee machine. We also introduce
a version of the approximate message passing (AMP) algorithm for the committee
machine that allows to perform optimal learning in polynomial time for a large
set of parameters. We find that there are regimes in which a low generalization
error is information-theoretically achievable while the AMP algorithm fails to
deliver it, strongly suggesting that no efficient algorithm exists for those
cases, and unveiling a large computational gap.
",statistics
"  In the cryptographic currency Bitcoin, all transactions are recorded in the
blockchain - a public, global, and immutable ledger. Because transactions are
public, Bitcoin and its users employ obfuscation to maintain a degree of
financial privacy. Critically, and in contrast to typical uses of obfuscation,
in Bitcoin obfuscation is not aimed against the system designer but is instead
enabled by design. We map sixteen proposed privacy-preserving techniques for
Bitcoin on an obfuscation-vs.-cryptography axis, and find that those that are
used in practice tend toward obfuscation. We argue that this has led to a
balance between privacy and regulatory acceptance.
",computer-science
"  Fragmentation of filaments into dense cores is thought to be an important
step in forming stars. The bar-mode instability of spherically collapsing cores
found in previous linear analysis invokes a possibility of re-fragmentation of
the cores due to their ellipsoidal (prolate or oblate) deformation. To
investigate this possibility, here we perform three-dimensional
self-gravitational hydrodynamics simulations that follow all the way from
filament fragmentation to subsequent core collapse. We assume the gas is
polytropic with index \gamma, which determines the stability of the bar-mode.
For the case that the fragmentation of isolated hydrostatic filaments is
triggered by the most unstable fragmentation mode, we find the bar mode grows
as collapse proceeds if \gamma < 1.1, in agreement with the linear analysis.
However, it takes more than ten orders-of-magnitude increase in the central
density for the distortion to become non-linear. In addition to this fiducial
case, we also study non-fiducial ones such as the fragmentation is triggered by
a fragmentation mode with a longer wavelength and it occurs during radial
collapse of filaments and find the distortion rapidly grows. In most of
astrophysical applications, the effective polytropic index of collapsing gas
exceeds 1.1 before ten orders-of-magnitude increase in the central density.
Thus, supposing the fiducial case of filament fragmentation, re-fragmentation
of dense cores would not be likely and their final mass would be determined
when the filaments fragment.
",physics
"  We investigate fine Selmer groups for elliptic curves and for Galois
representations over a number field. More specifically, we discuss Conjecture
A, which states that the fine Selmer group of an elliptic curve over the
cyclotomic extension is a finitely generated $\mathbb{Z}_p$-module. The
relationship between this conjecture and Iwasawa's classical $\mu=0$ conjecture
is clarified. We also present some partial results towards the question whether
Conjecture A is invariant under isogenies.
",mathematics
"  In this paper we introduce, the FlashText algorithm for replacing keywords or
finding keywords in a given text. FlashText can search or replace keywords in
one pass over a document. The time complexity of this algorithm is not
dependent on the number of terms being searched or replaced. For a document of
size N (characters) and a dictionary of M keywords, the time complexity will be
O(N). This algorithm is much faster than Regex, because regex time complexity
is O(MxN). It is also different from Aho Corasick Algorithm, as it doesn't
match substrings. FlashText is designed to only match complete words (words
with boundary characters on both sides). For an input dictionary of {Apple},
this algorithm won't match it to 'I like Pineapple'. This algorithm is also
designed to go for the longest match first. For an input dictionary {Machine,
Learning, Machine learning} on a string 'I like Machine learning', it will only
consider the longest match, which is Machine Learning. We have made python
implementation of this algorithm available as open-source on GitHub, released
under the permissive MIT License.
",computer-science
"  Recently, the introduction of the generative adversarial network (GAN) and
its variants has enabled the generation of realistic synthetic samples, which
has been used for enlarging training sets. Previous work primarily focused on
data augmentation for semi-supervised and supervised tasks. In this paper, we
instead focus on unsupervised anomaly detection and propose a novel generative
data augmentation framework optimized for this task. In particular, we propose
to oversample infrequent normal samples - normal samples that occur with small
probability, e.g., rare normal events. We show that these samples are
responsible for false positives in anomaly detection. However, oversampling of
infrequent normal samples is challenging for real-world high-dimensional data
with multimodal distributions. To address this challenge, we propose to use a
GAN variant known as the adversarial autoencoder (AAE) to transform the
high-dimensional multimodal data distributions into low-dimensional unimodal
latent distributions with well-defined tail probability. Then, we
systematically oversample at the `edge' of the latent distributions to increase
the density of infrequent normal samples. We show that our oversampling
pipeline is a unified one: it is generally applicable to datasets with
different complex data distributions. To the best of our knowledge, our method
is the first data augmentation technique focused on improving performance in
unsupervised anomaly detection. We validate our method by demonstrating
consistent improvements across several real-world datasets.
",statistics
"  With the rapid growth of social media, massive misinformation is also
spreading widely on social media, such as microblog, and bring negative effects
to human life. Nowadays, automatic misinformation identification has drawn
attention from academic and industrial communities. For an event on social
media usually consists of multiple microblogs, current methods are mainly based
on global statistical features. However, information on social media is full of
noisy and outliers, which should be alleviated. Moreover, most of microblogs
about an event have little contribution to the identification of
misinformation, where useful information can be easily overwhelmed by useless
information. Thus, it is important to mine significant microblogs for a
reliable misinformation identification method. In this paper, we propose an
Attention-based approach for Identification of Misinformation (AIM). Based on
the attention mechanism, AIM can select microblogs with largest attention
values for misinformation identification. The attention mechanism in AIM
contains two parts: content attention and dynamic attention. Content attention
is calculated based textual features of each microblog. Dynamic attention is
related to the time interval between the posting time of a microblog and the
beginning of the event. To evaluate AIM, we conduct a series of experiments on
the Weibo dataset and the Twitter dataset, and the experimental results show
that the proposed AIM model outperforms the state-of-the-art methods.
",computer-science
"  Recent years have witnessed great success of convolutional neural network
(CNN) for various problems both in low and high level visions. Especially
noteworthy is the residual network which was originally proposed to handle
high-level vision problems and enjoys several merits. This paper aims to extend
the merits of residual network, such as skip connection induced fast training,
for a typical low-level vision problem, i.e., single image super-resolution. In
general, the two main challenges of existing deep CNN for supper-resolution lie
in the gradient exploding/vanishing problem and large numbers of parameters or
computational cost as CNN goes deeper. Correspondingly, the skip connections or
identity mapping shortcuts are utilized to avoid gradient exploding/vanishing
problem. In addition, the skip connections have naturally centered the
activation which led to better performance. To tackle with the second problem,
a lightweight CNN architecture which has carefully designed width, depth and
skip connections was proposed. In particular, a strategy of gradually varying
the shape of network has been proposed for residual network. Different residual
architectures for image super-resolution have also been compared. Experimental
results have demonstrated that the proposed CNN model can not only achieve
state-of-the-art PSNR and SSIM results for single image super-resolution but
also produce visually pleasant results. This paper has extended the mmm 2017
oral conference paper with a considerable new analyses and more experiments
especially from the perspective of centering activations and ensemble behaviors
of residual network.
",computer-science
"  As modern precision cosmological measurements continue to show agreement with
the broad features of the standard $\Lambda$-Cold Dark Matter ($\Lambda$CDM)
cosmological model, we are increasingly motivated to look for small departures
from the standard model's predictions which might not be detected with standard
approaches. While searches for extensions and modifications of $\Lambda$CDM
have to date turned up no convincing evidence of beyond-the-standard-model
cosmology, the list of models compared against $\Lambda$CDM is by no means
complete and is often governed by readily-coded modifications to standard
Boltzmann codes. Also, standard goodness-of-fit methods such as a naive
$\chi^2$ test fail to put strong pressure on the null $\Lambda$CDM hypothesis,
since modern datasets have orders of magnitudes more degrees of freedom than
$\Lambda$CDM. Here we present a method of tuning goodness-of-fit tests to
detect potential sub-dominant extra-$\Lambda$CDM signals present in the data
through compressing observations in a way that maximizes extra-$\Lambda$CDM
signal variation over noise and $\Lambda$CDM variation. This method, based on a
Karhunen-Loève transformation of the data, is tuned to be maximally
sensitive to particular types of variations characteristic of the tuning model;
but, unlike direct model comparison, the test is also sensitive to features
that only partially mimic the tuning model. As an example of its use, we apply
this method in the context of a nonstandard primordial power spectrum compared
against the $2015$ $Planck$ CMB temperature and polarization power spectrum. We
find weak evidence of extra-$\Lambda$CDM physics, conceivably due to known
systematics in the 2015 Planck polarization release.
",physics
"  Understanding how delayed information impacts queueing systems is an
important area of research. However, much of the current literature neglects
one important feature of many queueing systems, namely non-stationary arrivals.
Non-stationary arrivals model the fact that customers tend to access services
during certain times of the day and not at a constant rate. In this paper, we
analyze two two-dimensional deterministic fluid models that incorporate
customer choice behavior based on delayed queue length information with time
varying arrivals. In the first model, customers receive queue length
information that is delayed by a constant Delta. In the second model, customers
receive information about the queue length through a moving average of the
queue length where the moving average window is Delta. We analyze the impact of
the time varying arrival rate and show using asymptotic analysis that the time
varying arrival rate does not impact the critical delay unless the frequency of
the time varying arrival rate is twice that of the critical delay. When the
frequency of the arrival rate is twice that of the critical delay, then the
stability is enlarged by a wedge that is determined by the model parameters. As
a result, this problem allows us to combine the theory of nonlinear dynamics,
parametric excitation, delays, and time varying queues together to provide
insight on the impact of information in queueing systems.
",mathematics
"  Fracton order is a new kind of quantum order characterized by topological
excitations that exhibit remarkable mobility restrictions and a robust ground
state degeneracy (GSD) which can increase exponentially with system size. In
this paper, we present a generic lattice construction (in three dimensions) for
a generalized X-cube model of fracton order, where the mobility restrictions of
the subdimensional particles inherit the geometry of the lattice. This helps
explain a previous result that lattice curvature can produce a robust GSD, even
on a manifold with trivial topology. We provide explicit examples to show that
the (zero temperature) phase of matter is sensitive to the lattice geometry. In
one example, the lattice geometry confines the dimension-1 particles to small
loops, which allows the fractons to be fully mobile charges, and the resulting
phase is equivalent to (3+1)-dimensional toric code. However, the phase is
sensitive to more than just lattice curvature; different lattices without
curvature (e.g. cubic or stacked kagome lattices) also result in different
phases of matter, which are separated by phase transitions. Unintuitively
however, according to a previous definition of phase [Chen, Gu, Wen 2010], even
just a rotated or rescaled cubic lattice results in different phases of matter,
which motivates us to propose a new and coarser definition of phase for gapped
ground states and fracton order. The new equivalence relation between ground
states is given by the composition of a local unitary transformation and a
quasi-isometry (which can rotate and rescale the lattice); equivalently, ground
states are in the same phase if they can be adiabatically connected by varying
both the Hamiltonian and the positions of the degrees of freedom (via a
quasi-isometry). In light of the importance of geometry, we further propose
that fracton orders should be regarded as a geometric order.
",physics
"  Optic flow is two dimensional, but no special qualities are attached to one
or other of these dimensions. For binocular disparity, on the other hand, the
terms 'horizontal' and 'vertical' disparities are commonly used. This is odd,
since binocular disparity and optic flow describe essentially the same thing.
The difference is that, generally, people tend to fixate relatively close to
the direction of heading as they move, meaning that fixation is close to the
optic flow epipole, whereas, for binocular vision, fixation is close to the
head-centric midline, i.e. approximately 90 degrees from the binocular epipole.
For fixating animals, some separations of flow may lead to simple algorithms
for the judgement of surface structure and the control of action. We consider
the following canonical flow patterns that sum to produce overall flow: (i)
'towards' flow, the component of translational flow produced by approaching (or
retreating from) the fixated object, which produces pure radial flow on the
retina; (ii) 'sideways' flow, the remaining component of translational flow,
which is produced by translation of the optic centre orthogonal to the
cyclopean line of sight and (iii) 'vergence' flow, rotational flow produced by
a counter-rotation of the eye in order to maintain fixation. A general flow
pattern could also include (iv) 'cyclovergence' flow, produced by rotation of
one eye relative to the other about the line of sight. We consider some
practical advantages of dividing up flow in this way when an observer fixates
as they move. As in some previous treatments, we suggest that there are certain
tasks for which it is sensible to consider 'towards' flow as one component and
'sideways' + 'vergence' flow as another.
",quantitative-biology
"  In this paper we use detailed Monte Carlo simulations to demonstrate that
liquid xenon (LXe) can be used to build a Cherenkov-based TOF-PET, with an
intrinsic coincidence resolving time (CRT) in the vicinity of 10 ps. This
extraordinary performance is due to three facts: a) the abundant emission of
Cherenkov photons by liquid xenon; b) the fact that LXe is transparent to
Cherenkov light; and c) the fact that the fastest photons in LXe have
wavelengths higher than 300 nm, therefore making it possible to separate the
detection of scintillation and Cherenkov light. The CRT in a Cherenkov LXe
TOF-PET detector is, therefore, dominated by the resolution (time jitter)
introduced by the photosensors and the electronics. However, we show that for
sufficiently fast photosensors (e.g, an overall 40 ps jitter, which can be
achieved by current micro-channel plate photomultipliers) the overall CRT
varies between 30 and 55 ps, depending of the detection efficiency. This is
still one order of magnitude better than commercial CRT devices and improves by
a factor 3 the best CRT obtained with small laboratory prototypes.
",physics
"  We introduce a Unified Disentanglement Network (UFDN) trained on The Cancer
Genome Atlas (TCGA). We demonstrate that the UFDN learns a biologically
relevant latent space of gene expression data by applying our network to two
classification tasks of cancer status and cancer type. Our UFDN specific
algorithms perform comparably to random forest methods. The UFDN allows for
continuous, partial interpolation between distinct cancer types. Furthermore,
we perform an analysis of differentially expressed genes between skin cutaneous
melanoma(SKCM) samples and the same samples interpolated into glioblastoma
(GBM). We demonstrate that our interpolations learn relevant metagenes that
recapitulate known glioblastoma mechanisms and suggest possible starting points
for investigations into the metastasis of SKCM into GBM.
",quantitative-biology
"  We develop a theory based on the formalism of quasiclassical Green's
functions to study the spin dynamics in superfluid $^3$He. First, we derive
kinetic equations for the spin-dependent distribution function in the bulk
superfluid reproducing the results obtained earlier without quasiclassical
approximation. Then we consider a spin dynamics near the surface of fully
gapped $^3$He-B phase taking into account spin relaxation due to the
transitions in the spectrum of localized fermionic states. The lifetime of
longitudinal and transverse spin waves is calculate taking into account the
Fermi-liquid corrections which lead to the crucial modification of fermionic
spectrum and spin responses.
",physics
"  In this paper, we propose a novel approach to obtaining a reliable and simple
mathematical model of a dielectrophoretic force for model-based feedback
micromanipulation. Any such model is expected to sufficiently accurately relate
the voltages (electric potentials) applied to the electrodes to the resulting
forces exerted on microparticles at given locations in the workspace. This
model also has to be computationally simple enough to be used in real time as
required by model-based feedback control. Most existing models involve solving
two- or three-dimensional mixed boundary value problems. As such, they are
usually analytically intractable and have to be solved numerically instead. A
numerical solution is, however, infeasible in real time, hence such models are
not suitable for feedback control. We present a novel approximation of the
boundary value data for which a closed-form analytical solution is feasible; we
solve a mixed boundary value problem numerically off-line only once, and based
on this solution we approximate the mixed boundary conditions by Dirichlet
boundary conditions. This way we get an approximated boundary value problem
allowing the application of the analytical framework of Green's functions. Thus
obtained closed-form analytical solution is amenable to real-time use and
closely matches the numerical solution of the original exact problem.
",physics
"  A regular $t$-balanced Cayley map (RBCM$_t$ for short) on a group $\Gamma$ is
an embedding of a Cayley graph on $\Gamma$ into a surface with some special
symmetric properties. We propose a reduction method to study RBCM$_t$'s, and as
a first practice, we completely classify RBCM$_t$'s for a class of split
metacyclic 2-groups.
",mathematics
"  The reduction by restricting the spectral parameters $k$ and $k'$ on a
generic algebraic curve of degree $\mathcal{N}$ is performed for the discrete
AKP, BKP and CKP equations, respectively. A variety of two-dimensional discrete
integrable systems possessing a more general solution structure arise from the
reduction, and in each case a unified formula for generic positive integer
$\mathcal{N}\geq 2$ is given to express the corresponding reduced integrable
lattice equations. The obtained extended two-dimensional lattice models give
rise to many important integrable partial difference equations as special
degenerations. Some new integrable lattice models such as the discrete
Sawada--Kotera, Kaup--Kupershmidt and Hirota--Satsuma equations in extended
form are given as examples within the framework.
",physics
"  We consider the problem of the annual mean temperature prediction. The years
taken into account and the corresponding annual mean temperatures are denoted
by $0,\ldots, n$ and $t_0$, $\ldots$, $t_n$, respectively. We propose to
predict the temperature $t_{n+1}$ using the data $t_0$, $\ldots$, $t_n$. For
each $0\leq l\leq n$ and each parametrization $\Theta^{(l)}$ of the Euclidean
space $\mathbb{R}^{l+1}$ we construct a list of weights for the data
$\{t_0,\ldots, t_l\}$ based on the rows of $\Theta^{(l)}$ which are correlated
with the constant trend. Using these weights we define a list of predictors of
$t_{l+1}$ from the data $t_0$, $\ldots$, $t_l$. We analyse how the
parametrization affects the prediction, and provide three optimality criteria
for the selection of weights and parametrization. We illustrate our results for
the annual mean temperature of France and Morocco.
",statistics
"  In this work we first examine transverse and longitudinal fluxes in a $\cal
PT$-symmetric photonic dimer using a coupled-mode theory. Several surprising
understandings are obtained from this perspective: The longitudinal flux shows
that the $\cal PT$ transition in a dimer can be regarded as a classical effect,
despite its analogy to $\cal PT$-symmetric quantum mechanics. The longitudinal
flux also indicates that the so-called giant amplification in the $\cal
PT$-symmetric phase is a sub-exponential behavior and does not outperform a
single gain waveguide. The transverse flux, on the other hand, reveals that the
apparent power oscillations between the gain and loss waveguides in the $\cal
PT$-symmetric phase can be deceiving in certain cases, where the transverse
power transfer is in fact unidirectional. We also show that this power transfer
cannot be arbitrarily fast even when the exceptional point is approached.
Finally, we go beyond the coupled-mode theory by using the paraxial wave
equation and also extend our discussions to a $\cal PT$ diamond and a
one-dimensional periodic lattice.
",physics
"  We use a secular model to describe the non-resonant dynamics of
trans-Neptunian objects in the presence of an external ten-earth-mass
perturber. The secular dynamics is analogous to an ""eccentric Kozai mechanism""
but with both an inner component (the four giant planets) and an outer one (the
eccentric distant perturber). By the means of Poincaré sections, the cases of
a non-inclined or inclined outer planet are successively studied, making the
connection with previous works. In the inclined case, the problem is reduced to
two degrees of freedom by assuming a non-precessing argument of perihelion for
the perturbing body.
The size of the perturbation is typically ruled by the semi-major axis of the
small body: we show that the classic integrable picture is still valid below
about 70 AU, but it is progressively destroyed when we get closer to the
external perturber. In particular, for a>150 AU, large-amplitude orbital flips
become possible, and for a>200 AU, the Kozai libration islands are totally
submerged by the chaotic sea. Numerous resonance relations are highlighted. The
most large and persistent ones are associated to apsidal alignments or
anti-alignments with the orbit of the distant perturber.
",physics
"  The Deep Impact spacecraft fly-by of comet 103P/Hartley 2 occurred on 2010
November 4, one week after perihelion with a closest approach (CA) distance of
about 700 km. We used narrowband images obtained by the Medium Resolution
Imager (MRI) onboard the spacecraft to study the gas and dust in the innermost
coma. We derived an overall dust reddening of 15\%/100 nm between 345 and 749
nm and identified a blue enhancement in the dust coma in the sunward direction
within 5 km from the nucleus, which we interpret as a localized enrichment in
water ice. OH column density maps show an anti-sunward enhancement throughout
the encounter except for the highest resolution images, acquired at CA, where a
radial jet becomes visible in the innermost coma, extending up to 12 km from
the nucleus. The OH distribution in the inner coma is very different from that
expected for a fragment species. Instead, it correlates well with the water
vapor map derived by the HRI-IR instrument onboard Deep Impact
\citep{AHearn2011}. Radial profiles of the OH column density and derived water
production rates show an excess of OH emission during CA that cannot be
explained with pure fluorescence. We attribute this excess to a prompt emission
process where photodissociation of H$_2$O directly produces excited
OH*($A^2\it{\Sigma}^+$) radicals. Our observations provide the first direct
imaging of Near-UV prompt emission of OH. We therefore suggest the use of a
dedicated filter centered at 318.8 nm to directly trace the water in the coma
of comets.
",physics
"  In this work, we present a method to compute the Kantorovich distance, that
is, the Wasserstein distance of order one, between a pair of two-dimensional
histograms. Recent works in Computer Vision and Machine Learning have shown the
benefits of measuring Wasserstein distances of order one between histograms
with $N$ bins, by solving a classical transportation problem on (very large)
complete bipartite graphs with $N$ nodes and $N^2$ edges. The main contribution
of our work is to approximate the original transportation problem by an
uncapacitated min cost flow problem on a reduced flow network of size $O(N)$.
More precisely, when the distance among the bin centers is measured with the
1-norm or the $\infty$-norm, our approach provides an optimal solution. When
the distance amongst bins is measured with the 2-norm: (i) we derive a
quantitative estimate on the error between optimal and approximate solution;
(ii) given the error, we construct a reduced flow network of size $O(N)$. We
numerically show the benefits of our approach by computing Wasserstein
distances of order one on a set of grey scale images used as benchmarks in the
literature. We show how our approach scales with the size of the images with
1-norm, 2-norm and $\infty$-norm ground distances.
",statistics
"  Vanadium pentoxide (V2O5), the most stable member of vanadium oxide family,
exhibits interesting semiconductor to metal transition in the temperature range
of 530-560 K. The metallic behavior originates because of the reduction of V2O5
through oxygen vacancies. In the present report, V2O5 nanorods in the
orthorhombic phase with crystal orientation of (001) are grown using vapor
transport process. Among three nonequivalent oxygen atoms in a VO5 pyramidal
formula unit in V2O5 structure, the role of terminal vanadyl oxygen (OI) in the
formation of metallic phase above the transition temperature is established
from the temperature-dependent Raman spectroscopic studies. The origin of the
metallic behavior of V2O5 is also understood due to the breakdown of pdpi bond
between OI and nearest V atom instigated by the formation of vanadyl OI
vacancy, confirmed from the downward shift of the bottom most split-off
conduction bands in the material with increasing temperature.
",physics
"  We consider the problem of variable selection in high-dimensional statistical
models where the goal is to report a set of variables, out of many predictors
$X_1, \dotsc, X_p$, that are relevant to a response of interest. For linear
high-dimensional model, where the number of parameters exceeds the number of
samples $(p>n)$, we propose a procedure for variables selection and prove that
it controls the \emph{directional} false discovery rate (FDR) below a
pre-assigned significance level $q\in [0,1]$. We further analyze the
statistical power of our framework and show that for designs with subgaussian
rows and a common precision matrix $\Omega\in\mathbb{R}^{p\times p}$, if the
minimum nonzero parameter $\theta_{\min}$ satisfies $$\sqrt{n} \theta_{\min} -
\sigma \sqrt{2(\max_{i\in [p]}\Omega_{ii})\log\left(\frac{2p}{qs_0}\right)} \to
\infty\,,$$ then this procedure achieves asymptotic power one.
Our framework is built upon the debiasing approach and assumes the standard
condition $s_0 = o(\sqrt{n}/(\log p)^2)$, where $s_0$ indicates the number of
true positives among the $p$ features. Notably, this framework achieves exact
directional FDR control without any assumption on the amplitude of unknown
regression parameters, and does not require any knowledge of the distribution
of covariates or the noise level. We test our method in synthetic and real data
experiments to asses its performance and to corroborate our theoretical
results.
",statistics
"  We show that any $d$-colored set of points in general position in
$\mathbb{R}^d$ can be partitioned into $n$ subsets with disjoint convex hulls
such that the set of points and all color classes are partitioned as evenly as
possible. This extends results by Holmsen, Kynčl & Valculescu (2017) and
establishes a special case of their general conjecture. Our proof utilizes a
result obtained independently by Soberón and by Karasev in 2010, on
simultaneous equipartitions of $d$ continuous measures in $\mathbb{R}^d$ by $n$
convex regions. This gives a convex partition of $\mathbb{R}^d$ with the
desired properties, except that points may lie on the boundaries of the
regions. In order to resolve the ambiguous assignment of these points, we set
up a network flow problem. The equipartition of the continuous measures gives a
fractional flow. The existence of an integer flow then yields the desired
partition of the point set.
",mathematics
"  A multi-user multi-armed bandit (MAB) framework is used to develop algorithms
for uncoordinated spectrum access. The number of users is assumed to be unknown
to each user. A stochastic setting is first considered, where the rewards on a
channel are the same for each user. In contrast to prior work, it is assumed
that the number of users can possibly exceed the number of channels, and that
rewards can be non-zero even under collisions. The proposed algorithm consists
of an estimation phase and an allocation phase. It is shown that if every user
adopts the algorithm, the system wide regret is constant with time with high
probability. The regret guarantees hold for any number of users and channels,
in particular, even when the number of users is less than the number of
channels. Next, an adversarial multi-user MAB framework is considered, where
the rewards on the channels are user-dependent. It is assumed that the number
of users is less than the number of channels, and that the users receive zero
reward on collision. The proposed algorithm combines the Exp3.P algorithm
developed in prior work for single user adversarial bandits with a collision
resolution mechanism to achieve sub-linear regret. It is shown that if every
user employs the proposed algorithm, the system wide regret is of the order
$O(T^\frac{3}{4})$ over a horizon of time $T$. The algorithms in both
stochastic and adversarial scenarios are extended to the dynamic case where the
number of users in the system evolves over time and are shown to lead to
sub-linear regret.
",statistics
"  This paper presents a multi-pose face recognition approach using hybrid face
features descriptors (HFFD). The HFFD is a face descriptor containing of rich
discriminant information that is created by fusing some frequency-based
features extracted using both wavelet and DCT analysis of several different
poses of 2D face images. The main aim of this method is to represent the
multi-pose face images using a dominant frequency component with still having
reasonable achievement compared to the recent multi-pose face recognition
methods. The HFFD based face recognition tends to achieve better performance
than that of the recent 2D-based face recognition method. In addition, the
HFFD-based face recognition also is sufficiently to handle large face
variability due to face pose variations .
",computer-science
"  Magnetic anisotropies of ferromagnetic thin films are induced by epitaxial
strain from the substrate via strain-induced anisotropy in the orbital magnetic
moment and that in the spatial distribution of spin-polarized electrons.
However, the preferential orbital occupation in ferromagnetic metallic
La$_{1-x}$Sr$_x$MnO$_3$ (LSMO) thin films studied by x-ray linear dichroism
(XLD) has always been found out-of-plane for both tensile and compressive
epitaxial strain and hence irrespective of the magnetic anisotropy. In order to
resolve this mystery, we directly probed the preferential orbital occupation of
spin-polarized electrons in LSMO thin films under strain by angle-dependent
x-ray magnetic circular dichroism (XMCD). Anisotropy of the spin-density
distribution was found to be in-plane for the tensile strain and out-of-plane
for the compressive strain, consistent with the observed magnetic anisotropy.
The ubiquitous out-of-plane preferential orbital occupation seen by XLD is
attributed to the occupation of both spin-up and spin-down out-of-plane
orbitals in the surface magnetic dead layer.
",physics
"  Consider an ample and globally generated line bundle $L$ on a smooth
projective variety $X$ of dimension $N\geq 2$ over $\mathbb{C}$. Let $D$ be a
smooth divisor in the complete linear system of $L$. We construct reflexive
sheaves on $X$ by an elementary transformation of a trivial bundle on $X$ along
certain globally generated torsion-free sheaves on $D$. The dual reflexive
sheaves are called the Lazarsfeld-Mukai reflexive sheaves. We prove the
$\mu_L$-(semi)stability of such reflexive sheaves under certain conditions.
",mathematics
"  The Tabu Search (TS) metaheuristic has been proposed for K-Means clustering
as an alternative to Lloyd's algorithm, which for all its ease of
implementation and fast runtime, has the major drawback of being trapped at
local optima. While the TS approach can yield superior performance, it involves
a high computational complexity. Moreover, the difficulty in parameter
selection in the existing TS approach does not make it any more attractive.
This paper presents an alternative, low-complexity formulation of the TS
optimization procedure for K-Means clustering. This approach does not require
many parameter settings. We initially constrain the centers to points in the
dataset. We then aim at evolving these centers using a unique neighborhood
structure that makes use of gradient information of the objective function.
This results in an efficient exploration of the search space, after which the
means are refined. The proposed scheme is implemented in MATLAB and tested on
four real-world datasets, and it achieves a significant improvement over the
existing TS approach in terms of the intra cluster sum of squares and
computational time.
",computer-science
"  The regularization approach for variable selection was well developed for a
completely observed data set in the past two decades. In the presence of
missing values, this approach needs to be tailored to different missing data
mechanisms. In this paper, we focus on a flexible and generally applicable
missing data mechanism, which contains both ignorable and nonignorable missing
data mechanism assumptions. We show how the regularization approach for
variable selection can be adapted to the situation under this missing data
mechanism. The computational and theoretical properties for variable selection
consistency are established. The proposed method is further illustrated by
comprehensive simulation studies and real data analyses, for both low and high
dimensional settings.
",statistics
"  In rapid release development processes, patches that fix critical issues, or
implement high-value features are often promoted directly from the development
channel to a stabilization channel, potentially skipping one or more
stabilization channels. This practice is called patch uplift. Patch uplift is
risky, because patches that are rushed through the stabilization phase can end
up introducing regressions in the code. This paper examines patch uplift
operations at Mozilla, with the aim to identify the characteristics of uplifted
patches that introduce regressions. Through statistical and manual analyses, we
quantitatively and qualitatively investigate the reasons behind patch uplift
decisions and the characteristics of uplifted patches that introduced
regressions. Additionally, we interviewed three Mozilla release managers to
understand organizational factors that affect patch uplift decisions and
outcomes. Results show that most patches are uplifted because of a wrong
functionality or a crash. Uplifted patches that lead to faults tend to have
larger patch size, and most of the faults are due to semantic or memory errors
in the patches. Also, release managers are more inclined to accept patch uplift
requests that concern certain specific components, and-or that are submitted by
certain specific developers.
",computer-science
"  To each weighted Dirichlet space $\mathcal{D}_p$, $0<p<1$, we associate a
family of Morrey-type spaces ${\mathcal{D}}_p^{\lambda}$, $0< \lambda < 1$,
constructed by imposing growth conditions on the norm of hyperbolic translates
of functions. We indicate some of the properties of these spaces, mention the
characterization in terms of boundary values, and study integration and
multiplication operators on them.
",mathematics
"  In this paper, we deal with the problem of inferring causal directions when
the data is on discrete domain. By considering the distribution of the cause
$P(X)$ and the conditional distribution mapping cause to effect $P(Y|X)$ as
independent random variables, we propose to infer the causal direction via
comparing the distance correlation between $P(X)$ and $P(Y|X)$ with the
distance correlation between $P(Y)$ and $P(X|Y)$. We infer ""$X$ causes $Y$"" if
the dependence coefficient between $P(X)$ and $P(Y|X)$ is smaller. Experiments
are performed to show the performance of the proposed method.
",statistics
"  We present the Vortex Image Processing (VIP) library, a python package
dedicated to astronomical high-contrast imaging. Our package relies on the
extensive python stack of scientific libraries and aims to provide a flexible
framework for high-contrast data and image processing. In this paper, we
describe the capabilities of VIP related to processing image sequences acquired
using the angular differential imaging (ADI) observing technique. VIP
implements functionalities for building high-contrast data processing
pipelines, encompass- ing pre- and post-processing algorithms, potential
sources position and flux estimation, and sensitivity curves generation. Among
the reference point-spread function subtraction techniques for ADI
post-processing, VIP includes several flavors of principal component analysis
(PCA) based algorithms, such as annular PCA and incremental PCA algorithm
capable of processing big datacubes (of several gigabytes) on a computer with
limited memory. Also, we present a novel ADI algorithm based on non-negative
matrix factorization (NMF), which comes from the same family of low-rank matrix
approximations as PCA and provides fairly similar results. We showcase the ADI
capabilities of the VIP library using a deep sequence on HR8799 taken with the
LBTI/LMIRCam and its recently commissioned L-band vortex coronagraph. Using VIP
we investigated the presence of additional companions around HR8799 and did not
find any significant additional point source beyond the four known planets. VIP
is available at this http URL and is accompanied with
Jupyter notebook tutorials illustrating the main functionalities of the
library.
",physics
"  This paper is concerned with minimization of a fourth-order linearized
Canham-Helfrich energy subject to Dirichlet boundary conditions on curves
inside the domain. Such problems arise in the modeling of the mechanical
interaction of biomembranes with embedded particles. There, the curve
conditions result from the imposed particle--membrane coupling. We prove
almost-$H^{\frac{5}{2}}$ regularity of the solution and then consider two
possible penalty formulations. For the combination of these penalty
formulations with a Bogner-Fox-Schmit finite element discretization we prove
discretization error estimates which are optimal in view of the solution's
reduced regularity. The error estimates are based on a general estimate for
linear penalty problems in Hilbert spaces. Finally, we illustrate the
theoretical results by numerical computations. An important feature of the
presented discretization is that it does not require to resolve the particle
boundary. This is crucial in order to avoid re-meshing if the presented problem
arises as subproblem in a model where particles are allowed to move or rotate.
",mathematics
"  In this paper we study sectional curvature bounds for Riemannian manifolds
with density from the perspective of a weighted torsion free connection
introduced recently by the last two authors. We develop two new tools for
studying weighted sectional curvature bounds: a new weighted Rauch comparison
theorem and a modified notion of convexity for distance functions. As
applications we prove generalizations of theorems of Preissman and Byers for
negative curvature, the (homeomorphic) quarter-pinched sphere theorem, and
Cheeger's finiteness theorem. We also improve results of the first two authors
for spaces of positive weighted sectional curvature and symmetry.
",mathematics
"  This paper develops and compares two motion planning algorithms for a
tethered UAV with and without the possibility of the tether contacting the
confined and cluttered environment. Tethered aerial vehicles have been studied
due to their advantages such as power duration, stability, and safety. However,
the disadvantages brought in by the extra tether have not been well
investigated by the robotic locomotion community, especially when the tethered
agent is locomoting in a non-free space occupied with obstacles. In this work,
we propose two motion planning frameworks that (1) reduce the reachable
configuration space by taking into account the tether and (2) deliberately plan
(and relax) the contact point(s) of the tether with the environment and enable
an equivalent reachable configuration space as the non-tethered counterpart
would have. Both methods are tested on a physical robot, Fotokite Pro. With our
approaches, tethered aerial vehicles could find their applications in confined
and cluttered environments with obstacles as opposed to ideal free space, while
still maintaining the advantages from the usage of a tether. The motion
planning strategies are particularly suitable for marsupial heterogeneous
robotic teams, such as visual servoing/assisting for another mobile,
tele-operated primary robot.
",computer-science
"  Recent results of Grepstad and Lev are used to show that weighted
cut-and-project sets with one-dimensional physical space and one-dimensional
internal space are bounded distance equivalent to some lattice if the weight
function $h$ is continuous on the internal space, and if $h$ is either
piecewise linear, or twice differentiable with bounded curvature.
",mathematics
"  This paper studies dynamic complexity under definable change operations in
the DynFO framework by Patnaik and Immerman. It is shown that for changes
definable by parameter-free first-order formulas, all (uniform) $AC^1$ queries
can be maintained by first-order dynamic programs. Furthermore, many
maintenance results for single-tuple changes are extended to more powerful
change operations: (1) The reachability query for undirected graphs is
first-order maintainable under single tuple changes and first-order defined
insertions, likewise the reachability query for directed acyclic graphs under
quantifier-free insertions. (2) Context-free languages are first-order
maintainable under $\Sigma_1$-defined changes. These results are complemented
by several inexpressibility results, for example, that the reachability query
cannot be maintained by quantifier-free programs under definable,
quantifier-free deletions.
",computer-science
"  In this paper, market values of the football players in the forward positions
are estimated using multiple linear regression by including the physical and
performance factors in 2017-2018 season. Players from 4 major leagues of Europe
are examined, and by applying the test for homoscedasticity, a reasonable
regression model within 0.10 significance level is built, and the most and the
least affecting factors are explained in detail.
",statistics
"  A synoptic view on the long-established theory of light propagation in
crystalline dielectrics is presented, providing a new exact solution for the
microscopic local electromagnetic field thus disclosing the role of the
divergence-free (transversal) and curl-free (longitudinal) parts of the
electromagnetic field inside a material as a function of the density of
polarizable atoms. Our results enable fast and efficient calculation of the
photonic bandstructure and also the (non-local) dielectric tensor, solely with
the crystalline symmetry and atom-individual polarizabilities as input.
",physics
"  In most realistic models for quantum chaotic systems, the Hamiltonian
matrices in unperturbed bases have a sparse structure. We study correlations in
eigenfunctions of such systems and derive explicit expressions for some of the
correlation functions with respect to energy. The analytical results are tested
in several models by numerical simulations. An application is given for a
relation between transition probabilities.
",physics
"  When conducting large scale inference, such as genome-wide association
studies or image analysis, nominal $p$-values are often adjusted to improve
control over the family-wise error rate (FWER). When the majority of tests are
null, procedures controlling the False discovery rate (Fdr) can be improved by
replacing the theoretical global null with its empirical estimate. However,
these other adjustment procedures remain sensitive to the working model
assumption. Here we propose two key ideas to improve inference in this space.
First, we propose $p$-values that are standardized to the empirical null
distribution (instead of the theoretical null). Second, we propose model
averaging $p$-values by bootstrap aggregation (Bagging) to account for model
uncertainty and selection procedures. The combination of these two key ideas
yields bagged empirical null $p$-values (BEN $p$-values) that often
dramatically alter the rank ordering of significant findings. Moreover, we find
that a multidimensional selection criteria based on BEN $p$-values and bagged
model fit statistics is more likely to yield reproducible findings. A
re-analysis of the famous Golub Leukemia data is presented to illustrate these
ideas. We uncovered new findings in these data, not detected previously, that
are backed by published bench work pre-dating the Gloub experiment. A
pseudo-simulation using the leukemia data is also presented to explore the
stability of this approach under broader conditions, and illustrates the
superiority of the BEN $p$-values compared to the other approaches.
",statistics
"  In this paper we study sharp generalizations of $\dot{F}_p^{0,q}$ multiplier
theorem of Mikhlin-Hörmander type. The class of multipliers that we consider
involves Herz spaces $K_u^{s,t}$. Plancherel's theorem proves
$\widehat{L_s^2}=K_2^{s,2}$ and we study the optimal triple $(u,t,s)$ for which
$\sup_{k\in\mathbb{Z}}{\big\Vert \big(
m(2^k\cdot)\varphi\big)^{\vee}\big\Vert_{K_u^{s,t}}}<\infty$ implies
$\dot{F}_p^{0,q}$ boundedness of multiplier operator $T_m$ where $\varphi$ is a
cutoff function. Our result also covers the $BMO$-type space
$\dot{F}_{\infty}^{0,q}$.
",mathematics
"  Open bisimilarity is the original notion of bisimilarity to be introduced for
the pi-calculus that is a congruence. In open bisimilarity, free names in
processes are treated as variables that may be instantiated lazily; in contrast
to early and late bisimilarity where free names are constants. We build on the
established line of work, due to Milner, Parrow, and Walker, on classical modal
logics characterising early and late bisimilarity for the $\pi$-calculus. The
important insight is, to characterise open bisimilarity, we move to the setting
of intuitionistic modal logics. The intuitionistic modal logic introduced,
called OM, is such that modalities are closed under (respectful) substitutions,
inducing a property known as intuitionistic hereditary. Intuitionistic
hereditary reflects the lazy instantiation of names in open bisimilarity. The
soundness proof for open bisimilarity with respect to the modal logic is
mechanised in Abella. The constructive content of the completeness proof
provides an algorithm for generating distinguishing formulae, where such
formulae are useful as a certificate explaining why two processes are not open
bisimilar. We draw attention to the fact that open bisimilarity is not the only
notion of bisimilarity that is a congruence: for name-passing calculi there is
a classical/intuitionistic spectrum of bisimilarities.
",computer-science
"  Although block compressive sensing (BCS) makes it tractable to sense
large-sized images and video, its recovery performance has yet to be
significantly improved because its recovered images or video usually suffer
from blurred edges, loss of details, and high-frequency oscillatory artifacts,
especially at a low subrate. This paper addresses these problems by designing a
modified total variation technique that employs multi-block gradient
processing, a denoised Lagrangian multiplier, and patch-based sparse
representation. In the case of video, the proposed recovery method is able to
exploit both spatial and temporal similarities. Simulation results confirm the
improved performance of the proposed method for compressive sensing of images
and video in terms of both objective and subjective qualities.
",computer-science
"  Several recent papers investigate Active Learning (AL) for mitigating the
data dependence of deep learning for natural language processing. However, the
applicability of AL to real-world problems remains an open question. While in
supervised learning, practitioners can try many different methods, evaluating
each against a validation set before selecting a model, AL affords no such
luxury. Over the course of one AL run, an agent annotates its dataset
exhausting its labeling budget. Thus, given a new task, an active learner has
no opportunity to compare models and acquisition functions. This paper provides
a large scale empirical study of deep active learning, addressing multiple
tasks and, for each, multiple datasets, multiple models, and a full suite of
acquisition functions. We find that across all settings, Bayesian active
learning by disagreement, using uncertainty estimates provided either by
Dropout or Bayes-by Backprop significantly improves over i.i.d. baselines and
usually outperforms classic uncertainty sampling.
",statistics
"  In this paper, we aim at the completion problem of high order tensor data
with missing entries. The existing tensor factorization and completion methods
suffer from the curse of dimensionality when the order of tensor N>>3. To
overcome this problem, we propose an efficient algorithm called TT-WOPT
(Tensor-train Weighted OPTimization) to find the latent core tensors of tensor
data and recover the missing entries. Tensor-train decomposition, which has the
powerful representation ability with linear scalability to tensor order, is
employed in our algorithm. The experimental results on synthetic data and
natural image completion demonstrate that our method significantly outperforms
the other related methods. Especially when the missing rate of data is very
high, e.g., 85% to 99%, our algorithm can achieve much better performance than
other state-of-the-art algorithms.
",computer-science
"  The layered cuprate Bi$_{2}$CuO$_{4}$ is investigated using magnetic,
dielectric and pyroelectric measurements. This system is observed to be an
improper multiferroic, with a robust ferroelectric state being established near
the magnetic transition. Magnetic and dielectric measurements indicate the
presence of a region above the antiferromagnetic Neel temperature with
concomitant polar and magnetic short range order. Bi$_{2}$CuO$_{4}$ is also
seen to exhibit colossal dielectric constants at higher temperatures with
clearly distinguishable grain and grain boundary contributions, both of which
exhibit non-Debye relaxation.
",physics
"  Research on numerical stability of difference equations has been quite
intensive in the past century. The choice of difference schemes for the
derivative terms in these equations contributes to a wide range of the
stability analysis issues - one of which is how a chosen scheme may directly or
indirectly contribute to such stability. In the present paper, how far the
forward difference scheme for the time derivative in the wave equation
influences the stability of the equation numerical solution, is particularly
investigated. The stability analysis of the corresponding difference equation
involving four schemes, namely Lax's, central, forward, and rearward
differences, were carried out, and the resulting stability criteria were
compared. The results indicate that the instability of the solution of wave
equation is not always due to the forward difference scheme for the time
derivative. Rather, it is shown in this paper that the stability criterion is
still possible when the spatial derivative is represented by an appropriate
difference scheme. This sheds light on the degree of applicability of a
difference scheme for a hyperbolic equation.
",computer-science
"  This paper is devoted to the investigation of the following function $$ f:
x=\Delta^{3}_{\alpha_{1}\alpha_{2}...\alpha_{n}...}{\rightarrow}
\Delta^{3}_{\varphi(\alpha_{1})\varphi(\alpha_{2})...\varphi(\alpha_{n})...}=f(x)=y,
$$ where $\varphi(i)=\frac{-3i^{2}+7i}{2}$, $ i \in N^{0}_{2}=\{0,1,2\}$, and
$\Delta^{3}_{\alpha_{1}\alpha_{2}...\alpha_{n}...}$ is the ternary
representation of $x \in [0;1]$. That is values of this function are obtained
from the ternary representation of the argument by the following change of
digits: 0 by 0, 1 by 2, and 2 by 1. This function preserves the ternary digit
$0$.
Main mapping properties and differential, integral, fractal properties of the
function are studied. Equivalent representations by additionally defined
auxiliary functions of this function are proved.
This paper is the paper translated from Ukrainian (the Ukrainian variant
available at this https URL). In 2012, the
Ukrainian variant of this paper was represented by the author in the
International Scientific Conference ""Asymptotic Methods in the Theory of
Differential Equations"" dedicated to 80th anniversary of M. I. Shkil (the
conference paper available at
this https URL). In 2013, the
investigations of the present article were generalized by the author in the
paper ""One one class of functions with complicated local structure""
(this https URL) and in the several conference papers
(available at: this https URL,
this https URL).
",mathematics
"  We estimate the spin distribution of primordial black holes based on the
recent study of the critical phenomena in the gravitational collapse of a
rotating radiation fluid. We find that primordial black holes are mostly slowly
rotating.
",physics
"  We formulate a type B extended nilHecke algebra, following the type A
construction of Naisse and Vaz. We describe an action of this algebra on
extended polynomials and describe some results on the structure on the extended
symmetric polynomials. Finally, following Appel, Egilmez, Hogancamp, and Lauda,
we prove a result analogous to a classical theorem of Solomon connecting the
extended symmetric polynomial ring to a ring of usual symmetric polynomials and
their differentials.
",mathematics
"  Knowledge graphs are large, useful, but incomplete knowledge repositories.
They encode knowledge through entities and relations which define each other
through the connective structure of the graph. This has inspired methods for
the joint embedding of entities and relations in continuous low-dimensional
vector spaces, that can be used to induce new edges in the graph, i.e., link
prediction in knowledge graphs. Learning these representations relies on
contrasting positive instances with negative ones. Knowledge graphs include
only positive relation instances, leaving the door open for a variety of
methods for selecting negative examples. In this paper we present an empirical
study on the impact of negative sampling on the learned embeddings, assessed
through the task of link prediction. We use state-of-the-art knowledge graph
embeddings -- \rescal , TransE, DistMult and ComplEX -- and evaluate on
benchmark datasets -- FB15k and WN18. We compare well known methods for
negative sampling and additionally propose embedding based sampling methods. We
note a marked difference in the impact of these sampling methods on the two
datasets, with the ""traditional"" corrupting positives method leading to best
results on WN18, while embedding based methods benefiting the task on FB15k.
",computer-science
"  This paper proposes a novel non-oscillatory pattern (NOP) learning scheme for
several oscillatory data analysis problems including signal decomposition,
super-resolution, and signal sub-sampling. To the best of our knowledge, the
proposed NOP is the first algorithm for these problems with fully
non-stationary oscillatory data with close and crossover frequencies, and
general oscillatory patterns. NOP is capable of handling complicated situations
while existing algorithms fail; even in simple cases, e.g., stationary cases
with trigonometric patterns, numerical examples show that NOP admits
competitive or better performance in terms of accuracy and robustness than
several state-of-the-art algorithms.
",statistics
"  Accurately predicting and detecting interstitial lung disease (ILD) patterns
given any computed tomography (CT) slice without any pre-processing
prerequisites, such as manually delineated regions of interest (ROIs), is a
clinically desirable, yet challenging goal. The majority of existing work
relies on manually-provided ILD ROIs to extract sampled 2D image patches from
CT slices and, from there, performs patch-based ILD categorization. Acquiring
manual ROIs is labor intensive and serves as a bottleneck towards
fully-automated CT imaging ILD screening over large-scale populations.
Furthermore, despite the considerable high frequency of more than one ILD
pattern on a single CT slice, previous works are only designed to detect one
ILD pattern per slice or patch.
To tackle these two critical challenges, we present multi-label deep
convolutional neural networks (CNNs) for detecting ILDs from holistic CT slices
(instead of ROIs or sub-images). Conventional single-labeled CNN models can be
augmented to cope with the possible presence of multiple ILD pattern labels,
via 1) continuous-valued deep regression based robust norm loss functions or 2)
a categorical objective as the sum of element-wise binary logistic losses. Our
methods are evaluated and validated using a publicly available database of 658
patient CT scans under five-fold cross-validation, achieving promising
performance on detecting four major ILD patterns: Ground Glass, Reticular,
Honeycomb, and Emphysema. We also investigate the effectiveness of a CNN
activation-based deep-feature encoding scheme using Fisher vector encoding,
which treats ILD detection as spatially-unordered deep texture classification.
",computer-science
"  Until recently almost nothing was known about the evolution of magnetic
fields found in upper main sequence Ap/Bp stars during their long main sequence
lifetime. We are thus studying magnetic Ap/Bp stars in open clusters in order
to obtain observational evidence of how the properties of Ap/Bp magnetic stars,
such as field strength and structure, evolve with age during the main sequence.
One important aspect of this study is to search for the very rare examples of
hot magnetic stars in short-period binary systems among magnetic cluster
members. In this paper we characterize the object BD-19~5044L, which is both a
member of the open cluster IC 4725 = M~25, and a short-period SB2 system
containing a magnetic primary star. We have obtained a series of intensity and
circular polarisation spectra distributed through the orbital and rotation
cycles of BD-19 5044L with the ESPaDOnS spectropolarimeter at CFHT. We find
that the orbit of BD-19 5044L AB is quite eccentric (e = 0.477), with a period
of 17.63 d. The primary is a magnetic Bp star with a variable longitudinal
magnetic field, a polar field strength of ~1400 G and a low obliquity, while
the secondary is probably a hot Am star and does not appear to be magnetic. The
rotation period of the primary (5.04 d) is not synchronised with the orbit, but
the rotation angular velocity is close to being synchronised with the orbital
angular velocity of the secondary at periastron, perhaps as a result of tidal
interactions. The periastron separation is small enough (about 12 times the
radius of the primary star) that BD-19 5044L may be one of the very rare known
cases of a tidally interacting SB2 binary system containing a magnetic Ap/Bp
star.
",physics
"  We prove, by a computer aided proof, the existence of noise induced order in
the model of chaotic chemical reactions where it was first discovered
numerically by Matsumoto and Tsuda in 1983. We prove that in this random
dynamical system the increase in amplitude of the noise causes the Lyapunov
exponent to decrease from positive to negative, stabilizing the system. The
method used is based on a certified approximation of the stationary measure in
the $L^{1}$ norm. This is done by an efficient algorithm which is general
enough to be adapted to any piecewise differentiable dynamical system on the
interval with additive noise. We also prove that the stationary measure of the
system and its Lyapunov exponent have a Lipschitz stability under several kinds
of perturbation of the noise and of the system itself. The Lipschitz constants
of this stability result are also estimated explicitly.
",mathematics
"  In this paper we study different questions concerning automorphisms of
quandles. For a conjugation quandle $Q={\rm Conj}(G)$ of a group $G$ we
determine several subgroups of ${\rm Aut}(Q)$ and find necessary and sufficient
conditions when these subgroups coincide with the whole group ${\rm Aut}(Q)$.
In particular, we prove that ${\rm Aut}({\rm Conj}(G))={\rm Z}(G)\rtimes {\rm
Aut}(G)$ if and only if either ${\rm Z}(G)=1$ or $G$ is one of the groups
$\mathbb{Z}_2$, $\mathbb{Z}_2^2$ or $\mathbb{Z}_3$. For a big list of Takasaki
quandles $T(G)$ of an abelian group $G$ with $2$-torsion we prove that the
group of inner automorphisms ${\rm Inn}(T(G))$ is a Coxeter group. We study
automorphisms of certain extensions of quandles and determine some interesting
subgroups of the automorphism groups of these quandles. Also we classify finite
quandles $Q$ with $3\leq k$-transitive action of ${\rm Aut}(Q)$.
",mathematics
"  According to the DeGroot-Friedkin model of a social network, an individual's
social power evolves as the network discusses individual opinions over a
sequence of issues. Under mild assumptions on the connectivity of the network,
the social power of every individual converges to a constant strictly positive
value as the number of issues discussed increases. If the network has a special
topology, termed ""star topology"", then all social power accumulates with the
individual at the centre of the star. This paper studies the strategic
introduction of new individuals and/or interpersonal relationships into a
social network with star topology to reduce the social power of the centre
individual. In fact, several strategies are proposed. For each strategy, we
derive necessary and sufficient conditions on the strength of the new
interpersonal relationships, based on local information, which ensures that the
centre individual no longer has the greatest social power within the social
network. Interpretations of these conditions show that the strategies are
remarkably intuitive and that certain strategies are favourable compared to
others, all of which is sociologically expected.
",computer-science
"  We study empirical statistical and gap distributions of several important
tilings of the plane. In particular, we consider the slope distributions, the
angle distributions, pair correlation, squared-distance pair correlation, angle
gap distributions, and slope gap distributions for the Ammann Chair tiling, the
recently discovered fifteenth pentagonal tiling, and a few pertinent tilings
related to these famous examples. We also consider the spatial statistics of
generalized Ulam sets in two dimensions. Additionally, we carefully prove a
tight asymptotic formula for the time steps in which Ulam set points at certain
prescribed geometric positions in their plots in the plane formally enter the
recursively-defined sets.
The software we have developed to these generate numerical approximations to
the distributions for the tilings we consider here is written in Python under
the Sage environment and is released as open-source software which is available
freely on our websites. In addition to the small subset of tilings and other
point sets in the plane we study within the article, our program supports many
other tiling variants and is easily extended for researchers to explore related
tilings and iterative sets.
",mathematics
"  The goal of semantic parsing is to map natural language to a machine
interpretable meaning representation language (MRL). One of the constraints
that limits full exploration of deep learning technologies for semantic parsing
is the lack of sufficient annotation training data. In this paper, we propose
using sequence-to-sequence in a multi-task setup for semantic parsing with a
focus on transfer learning. We explore three multi-task architectures for
sequence-to-sequence modeling and compare their performance with an
independently trained model. Our experiments show that the multi-task setup
aids transfer learning from an auxiliary task with large labeled data to a
target task with smaller labeled data. We see absolute accuracy gains ranging
from 1.0% to 4.4% in our in- house data set, and we also see good gains ranging
from 2.5% to 7.0% on the ATIS semantic parsing tasks with syntactic and
semantic auxiliary tasks.
",computer-science
"  Donoho's JCGS (in press) paper is a spirited call to action for
statisticians, who he points out are losing ground in the field of data science
by refusing to accept that data science is its own domain. (Or, at least, a
domain that is becoming distinctly defined.) He calls on writings by John
Tukey, Bill Cleveland, and Leo Breiman, among others, to remind us that
statisticians have been dealing with data science for years, and encourages
acceptance of the direction of the field while also ensuring that statistics is
tightly integrated.
As faculty at baccalaureate institutions (where the growth of undergraduate
statistics programs has been dramatic), we are keen to ensure statistics has a
place in data science and data science education. In his paper, Donoho is
primarily focused on graduate education. At our undergraduate institutions, we
are considering many of the same questions.
",statistics
"  Synchronizations of processing elements (PEs) in massively parallel
simulations, which arise due to communication or load imbalances between PEs,
significantly affect the scalability of scientific applications. We have
recently proposed a method based on finite-difference schemes to solve partial
differential equations in an asynchronous fashion -- synchronization between
PEs is relaxed at a mathematical level. While standard schemes can maintain
their stability in the presence of asynchrony, their accuracy is drastically
affected. In this work, we present a general methodology to derive
asynchrony-tolerant (AT) finite difference schemes of arbitrary order of
accuracy, which can maintain their accuracy when synchronizations are relaxed.
We show that there are several choices available in selecting a stencil to
derive these schemes and discuss their effect on numerical and computational
performance. We provide a simple classification of schemes based on the stencil
and derive schemes that are representative of different classes. Their
numerical error is rigorously analyzed within a statistical framework to obtain
the overall accuracy of the solution. Results from numerical experiments are
used to validate the performance of the schemes.
",physics
"  Diffusion magnetic resonance imaging (dMRI) is currently the only tool for
noninvasively imaging the brain's white matter tracts. The fiber orientation
(FO) is a key feature computed from dMRI for fiber tract reconstruction.
Because the number of FOs in a voxel is usually small, dictionary-based sparse
reconstruction has been used to estimate FOs with a relatively small number of
diffusion gradients. However, accurate FO estimation in regions with complex FO
configurations in the presence of noise can still be challenging. In this work
we explore the use of a deep network for FO estimation in a dictionary-based
framework and propose an algorithm named Fiber Orientation Reconstruction
guided by a Deep Network (FORDN). FORDN consists of two steps. First, we use a
smaller dictionary encoding coarse basis FOs to represent the diffusion
signals. To estimate the mixture fractions of the dictionary atoms (and thus
coarse FOs), a deep network is designed specifically for solving the sparse
reconstruction problem. Here, the smaller dictionary is used to reduce the
computational cost of training. Second, the coarse FOs inform the final FO
estimation, where a larger dictionary encoding dense basis FOs is used and a
weighted l1-norm regularized least squares problem is solved to encourage FOs
that are consistent with the network output. FORDN was evaluated and compared
with state-of-the-art algorithms that estimate FOs using sparse reconstruction
on simulated and real dMRI data, and the results demonstrate the benefit of
using a deep network for FO estimation.
",computer-science
"  This paper develops systematically the output feedback exponential
stabilization for a one-dimensional unstable/anti-stable wave equation where
the control boundary suffers from both internal nonlinear uncertainty and
external disturbance. Using only two displacement signals, we propose a
disturbance estimator that not only can estimate successfully the disturbance
in the sense that the error is in $L^2(0,\infty)$ but also is free high-gain.
With the estimated disturbance, we design a state observer that is
exponentially convergent to the state of original system. An observer-based
output feedback stabilizing control law is proposed. The disturbance is then
canceled in the feedback loop by its approximated value. The closed-loop system
is shown to be exponentially stable and it can be guaranteed that all internal
signals are uniformly bounded.
",mathematics
"  Self Organizing Networks (SONs) are considered as vital deployments towards
upcoming dense cellular networks. From a mobile carrier point of view,
continuous coverage optimization is critical for better user perceptions. The
majority of SON contributions introduce novel algorithms that optimize specific
performance metrics. However, they require extensive processing delays and
advanced knowledge of network statistics that may not be available. In this
work, a progressive Autonomous Coverage Optimization (ACO) method combined with
adaptive cell dimensioning is proposed. The proposed method emphasizes the fact
that the effective cell coverage is a variant on actual user distributions. ACO
algorithm builds a generic Space-Time virtual coverage map per cell to detect
coverage holes in addition to limited or extended coverage conditions.
Progressive levels of optimization are followed to timely resolve coverage
issues with maintaining optimization stability. Proposed ACO is verified under
both simulations and practical deployment in a pilot cluster for a worldwide
mobile carrier. Key Performance Indicators show that proposed ACO method
significantly enhances system coverage and performance.
",computer-science
"  The low-energy quasiparticles of Weyl semimetals are a condensed-matter
realization of the Weyl fermions introduced in relativistic field theory.
Chiral anomaly, the nonconservation of the chiral charge under parallel
electric and magnetic fields, is arguably the most important phenomenon of Weyl
semimetals and has been explained as an imbalance between the occupancies of
the gapless, zeroth Landau levels with opposite chiralities. This widely
accepted picture has served as the basis for subsequent studies. Here we report
the breakdown of the chiral anomaly in Weyl semimetals in a strong magnetic
field based on ab initio calculations. A sizable energy gap that depends
sensitively on the direction of the magnetic field may open up due to the
mixing of the zeroth Landau levels associated with the opposite-chirality Weyl
points that are away from each other in the Brillouin zone. Our study provides
a theoretical framework for understanding a wide range of phenomena closely
related to the chiral anomaly in topological semimetals, such as
magnetotransport, thermoelectric responses, and plasmons, to name a few.
",physics
"  Magnetic skyrmions are topological spin structures having immense potential
for energy efficient spintronic devices. However, observations of skyrmions at
room temperature are limited to patterned nanostructures. Here, we report the
observation of stable skyrmions in unpatterned Ta/Co2FeAl(CFA)/MgO thin film
heterostructures at room temperature and in zero external magnetic field
employing magnetic force microscopy. The skyrmions are observed in a trilayer
structure comprised of heavy metal (HM)/ferromagnet (FM)/Oxide interfaces which
result in strong interfacial Dzyaloshinskii-Moriya interaction (i-DMI) as
evidenced by Brillouin light scattering measurements, in agreement with the
results of micromagnetic simulations. We also emphasize on room temperature
observation of multiple skyrmions which can be stabilized for suitable choices
of CFA layer thickness, perpendicular magnetic anisotropy, and i-DMI. These
results open up a new paradigm for designing room temperature spintronic
devices based on skyrmions in FM continuous thin films.
",physics
"  We consider the problem of automated assignment of papers to reviewers in
conference peer review, with a focus on fairness and statistical accuracy. Our
fairness objective is to maximize the review quality of the most disadvantaged
paper, in contrast to the commonly used objective of maximizing the total
quality over all papers. We design an assignment algorithm based on an
incremental max-flow procedure that we prove is near-optimally fair. Our
statistical accuracy objective is to ensure correct recovery of the papers that
should be accepted. We provide a sharp minimax analysis of the accuracy of the
peer-review process for a popular objective-score model as well as for a novel
subjective-score model that we propose in the paper. Our analysis proves that
our proposed assignment algorithm also leads to a near-optimal statistical
accuracy. Finally, we design a novel experiment that allows for an objective
comparison of various assignment algorithms, and overcomes the inherent
difficulty posed by the absence of a ground truth in experiments on
peer-review. The results of this experiment corroborate the theoretical
guarantees of our algorithm.
",statistics
"  Let $\mathfrak{g}$ be a hyperbolic Kac-Moody algebra of rank $2$, and set
$\lambda: = \Lambda_1 - \Lambda_2$, where $\Lambda_1, \Lambda_2$ are the
fundamental weights for $\mathfrak{g}$; note that $\lambda$ is neither dominant
nor antidominant. Let $\mathbb{B}(\lambda)$ be the crystal of all
Lakshmibai-Seshadri paths of shape $\lambda$. We prove that (the crystal graph
of) $\mathbb{B}(\lambda)$ is connected. Furthermore, we give an explicit
description of Lakshmibai-Seshadri paths of shape $\lambda$.
",mathematics
"  Deep neural networks have achieved increasingly accurate results on a wide
variety of complex tasks. However, much of this improvement is due to the
growing use and availability of computational resources (e.g use of GPUs, more
layers, more parameters, etc). Most state-of-the-art deep networks, despite
performing well, over-parameterize approximate functions and take a significant
amount of time to train. With increased focus on deploying deep neural networks
on resource constrained devices like smart phones, there has been a push to
evaluate why these models are so resource hungry and how they can be made more
efficient. This work evaluates and compares three distinct methods for deep
model compression and acceleration: weight pruning, low rank factorization, and
knowledge distillation. Comparisons on VGG nets trained on CIFAR10 show that
each of the models on their own are effective, but that the true power lies in
combining them. We show that by combining pruning and knowledge distillation
methods we can create a compressed network 85 times smaller than the original,
all while retaining 96% of the original model's accuracy.
",statistics
"  Tumor stromal interactions have been shown to be the driving force behind the
poor prognosis associated with aggressive breast tumors. These interactions,
specifically between tumor and the surrounding ECM, and tumor and vascular
endothelium, promote tumor formation, angiogenesis, and metastasis. In this
study, we develop an in vitro vascularized tumor platform that allows for
investigation of tumor-stromal interactions in three breast tumor derived cell
lines of varying aggressiveness: MDA-IBC3, SUM149, and MDA-MB-231. The platform
recreates key features of breast tumors, including increased vascular
permeability, vessel sprouting, and ECM remodeling. Morphological and
quantitative analysis reveals differential effects from each tumor cell type on
endothelial coverage, permeability, expression of VEGF, and collagen
remodeling. Triple negative tumors, SUM149 and MDA-MB-321, resulted in a
significantly (p<0.05) higher endothelial permeability and decreased
endothelial coverage compared to the control TIME only platform. SUM149/TIME
platforms were 1.3 fold lower (p<0.05), and MDA-MB-231/TIME platforms were 1.5
fold lower (p<0.01) in endothelial coverage compared to the control TIME only
platform. HER2+ MDA-IBC3 tumor cells expressed high levels of VEGF (p<0.01) and
induced vessel sprouting. Vessels sprouting was tracked for 3 weeks and with
increasing time exhibited formation of multiple vessel sprouts that invaded
into the ECM and surrounded clusters of MDA-IBC3 cells. Both IBC cell lines,
SUM149 and MDA-IBC3, resulted in a collagen ECM with significantly greater
porosity with 1.6 and 1.1 fold higher compared to control, p<0.01. The breast
cancer in vitro vascularized platforms introduced in this paper are an
adaptable, high throughout tool for unearthing tumor-stromal mechanisms and
dynamics behind tumor progression and may prove essential in developing
effective targeted therapeutics.
",quantitative-biology
"  The Sinc approximation has shown high efficiency for numerical methods in
many fields. Conformal maps play an important role in the success, i.e.,
appropriate conformal map must be employed to elicit high performance of the
Sinc approximation. Appropriate conformal maps have been proposed for typical
cases; however, such maps may not be optimal. Thus, the performance of the Sinc
approximation may be improved by using another conformal map rather than an
existing map. In this paper, we propose a new conformal map for the case where
functions are defined over the semi-infinite interval and decay exponentially.
Then, we demonstrate in both theoretical and numerical ways that the
convergence rate is improved by replacing the existing conformal map with the
proposed map.
",computer-science
"  We present an implementation of the relativistic quantum-chemical density
matrix renormalization group (DMRG) approach based on a matrix-product
formalism. Our approach allows us to optimize matrix product state (MPS) wave
functions including a variational description of scalar-relativistic effects
and spin-orbit coupling from which we can calculate, for example, first-order
electric and magnetic properties in a relativistic framework. While
complementing our pilot implementation (S. Knecht et al., J. Chem. Phys., 140,
041101 (2014)) this work exploits all features provided by its underlying
non-relativistic DMRG implementation based on an matrix product state and
operator formalism. We illustrate the capabilities of our relativistic DMRG
approach by studying the ground-state magnetization as well as current density
of a paramagnetic $f^9$ dysprosium complex as a function of the active orbital
space employed in the MPS wave function optimization.
",physics
"  This paper presents a new method for 3D action recognition with skeleton
sequences (i.e., 3D trajectories of human skeleton joints). The proposed method
first transforms each skeleton sequence into three clips each consisting of
several frames for spatial temporal feature learning using deep neural
networks. Each clip is generated from one channel of the cylindrical
coordinates of the skeleton sequence. Each frame of the generated clips
represents the temporal information of the entire skeleton sequence, and
incorporates one particular spatial relationship between the joints. The entire
clips include multiple frames with different spatial relationships, which
provide useful spatial structural information of the human skeleton. We propose
to use deep convolutional neural networks to learn long-term temporal
information of the skeleton sequence from the frames of the generated clips,
and then use a Multi-Task Learning Network (MTLN) to jointly process all frames
of the generated clips in parallel to incorporate spatial structural
information for action recognition. Experimental results clearly show the
effectiveness of the proposed new representation and feature learning method
for 3D action recognition.
",computer-science
"  This work is concerned with a unique combination of high order local
absorbing boundary conditions (ABC) with a general curvilinear Finite Element
Method (FEM) and its implementation in Isogeometric Analysis (IGA) for
time-harmonic acoustic waves. The ABC employed were recently devised by
Villamizar, Acosta and Dastrup [J. Comput. Phys. 333 (2017) 331] . They are
derived from exact Farfield Expansions representations of the outgoing waves in
the exterior of the regions enclosed by the artificial boundary. As a
consequence, the error due to the ABC on the artificial boundary can be reduced
conveniently such that the dominant error comes from the volume discretization
method used in the interior of the computational domain. Reciprocally, the
error in the interior can be made as small as the error at the artificial
boundary by appropriate implementation of {\it p-} and {\it h}- refinement. We
apply this novel method to cylindrical, spherical and arbitrary shape
scatterers including a prototype submarine. Our numerical results exhibits
spectral-like approximation and high order convergence rate. Additionally, they
show that the proposed method can reduce both the pollution and artificial
boundary errors to negligible levels even in very low- and high- frequency
regimes with rather coarse discretization densities in the IGA. As a result, we
have developed a highly accurate computational platform to numerically solve
time-harmonic acoustic wave scattering in two- and three-dimensions.
",computer-science
"  The hexatic phase predicted by the theories of two-dimensional melting is
characterised by the power law decay of the orientational correlations whereas
the in-layer bond orientational order in all the hexatic smectic phases
observed so far was found to be long-range. We report a hexatic smectic phase
where the in-layer bond orientational correlations decay as $\propto r^{-1/4}$,
in quantitative agreement with the hexatic ordering predicted by the theory for
two dimensions. The phase was formed in a molecular dynamics simulation of a
one-component system of particles interacting via a spherically symmetric
potential. This is the first observation of the theoretically predicted
two-dimensional hexatic order in a three-dimensional system.
",physics
"  Even- and odd-frequency superconductivity coexist due to broken time-reversal
symmetry under magnetic field. In order to describe this mixing, we extend the
linearized Eliashberg equation for the spin and charge fluctuation mechanism in
strongly correlated electron systems. We apply this extended Eliashberg
equation to the odd-frequency superconductivity on a quasi-one-dimensional
isosceles triangular lattice under in-plane magnetic field and examine the
effect of the even-frequency component.
",physics
"  We performed simulations for solid molecular hydrogen at high pressures
(250GPa$\leq$P$\leq$500GPa) along two isotherms at T=200 K (phases III and VI)
and at T=414 K (phase IV). At T=200K we considered likely candidates for phase
III, the C2c and Cmca12 structures, while at T=414K in phase IV we studied the
Pc48 structure. We employed both Coupled Electron-Ion Monte Carlo (CEIMC) and
Path Integral Molecular Dynamics (PIMD) based on Density Functional Theory
(DFT) using the vdW-DF approximation. The comparison between the two methods
allows us to address the question of the accuracy of the xc approximation of
DFT for thermal and quantum protons without recurring to perturbation theories.
In general, we find that atomic and molecular fluctuations in PIMD are larger
than in CEIMC which suggests that the potential energy surface from vdW-DF is
less structured than the one from Quantum Monte Carlo. We find qualitatively
different behaviors for systems prepared in the C2c structure for increasing
pressure. Within PIMD the C2c structure is dynamically partially stable for
P$\leq$250GPa only: it retains the symmetry of the molecular centers but not
the molecular orientation; at intermediate pressures it develops layered
structures like Pbcn or Ibam and transforms to the metallic Cmca-4 structure at
P$\geq$450GPa. Instead, within CEIMC, the C2c structure is found to be
dynamically stable at least up to 450GPa; at increasing pressure the molecular
bond length increases and the nuclear correlation decreases. For the other two
structures the two methods are in qualitative agreement although quantitative
differences remain. We discuss various structural properties and the electrical
conductivity. We find these structures become conducting around 350GPa but the
metallic Drude-like behavior is reached only at around 500GPa, consistent with
recent experimental claims.
",physics
"  In this note we present a fast algorithm that finds for any $r$ the number
$N_r$ of $\mathbb{F}_{q^r}$ rational points on a smooth absolutely irreducible
curve $C$ defined over $\mathbb{F}_{q}$ assuming that we know $N_1,\cdots,N_g$,
where $g$ is the genus of $C$. The proof of its validity is given in detail and
its working are illustrated with several examples. In an Appendix we list the
Python function in which we have implemented the algorithm together with other
routines used in the examples.
",mathematics
"  A unified viewpoint on the van Vleck and Herman-Kluk propagators in Hilbert
space and their recently developed counterparts in Wigner representation is
presented. It is shown that the numerical protocol for the Herman-Kluk
propagator, which contains the van Vleck one as a particular case, coincides in
both representations. The flexibility of the Wigner version in choosing the
Gaussians' width for the underlying coherent states, being not bound to minimal
uncertainty, is investigated numerically on prototypical potentials. Exploiting
this flexibility provides neither qualitative nor quantitative improvements.
Thus, the well-established Herman-Kluk propagator in Hilbert space remains the
best choice to date given the large number of semiclassical developments and
applications based on it.
",physics
"  Given a constant vector field $Z$ in Minkowski space, a timelike surface is
said to have a canonical null direction with respect to $Z$ if the projection
of $Z$ on the tangent space of the surface gives a lightlike vector field. In
this paper we describe these surfaces in the ruled case. For example when the
Minkowski space has three dimensions then a surface with a canonical null
direction is minimal and flat. On the other hand, we describe several
properties in the non ruled case and we partially describe these surfaces in
four-dimensional Minkowski space. We give different ways for building these
surfaces in four-dimensional Minkowski space and we finally use the Gauss map
for describe another properties of these surfaces.
",mathematics
"  We describe the results of a qualitative study on journalists' information
seeking behavior on social media. Based on interviews with eleven journalists
along with a study of a set of university level journalism modules, we
determined the categories of information need types that lead journalists to
social media. We also determined the ways that social media is exploited as a
tool to satisfy information needs and to define influential factors, which
impacted on journalists' information seeking behavior. We find that not only is
social media used as an information source, but it can also be a supplier of
stories found serendipitously. We find seven information need types that expand
the types found in previous work. We also find five categories of influential
factors that affect the way journalists seek information.
",computer-science
"  We compare the results of the semi-classical (SC) and quantum-mechanical (QM)
formalisms for angular-momentum changing transitions in Rydberg atom collisions
given by Vrinceanu & Flannery, J. Phys. B 34, L1 (2001), and Vrinceanu, Onofrio
& Sadeghpour, ApJ 747, 56 (2012), with those of the SC formalism using a
modified Monte Carlo realization. We find that this revised SC formalism agrees
well with the QM results. This provides further evidence that the rates derived
from the QM treatment are appropriate to be used when modelling recombination
through Rydberg cascades, an important process in understanding the state of
material in the early universe. The rates for $\Delta\ell=\pm1$ derived from
the QM formalism diverge when integrated to sufficiently large impact
parameter, $b$. Further to the empirical limits to the $b$ integration
suggested by Pengelly & Seaton, MNRAS 127, 165 (1964), we suggest that the
fundamental issue causing this divergence in the theory is that it does not
fully cater for the finite time taken for such distant collisions to complete.
",physics
"  We give some examples of the existence of solutions of geometric PDEs (Yamabe
equation, Prescribed Scalar Curvature Equation, Gaussian curvature).We also
give some remarks on second order PDE and Green functions and on the maximum
principles.
",mathematics
"  In processing human produced text using natural language processing (NLP)
techniques, two fundamental subtasks that arise are (i) segmentation of the
plain text into meaningful subunits (e.g., entities), and (ii) dependency
parsing, to establish relations between subunits. In this paper, we develop a
relatively simple and effective neural joint model that performs both
segmentation and dependency parsing together, instead of one after the other as
in most state-of-the-art works. We will focus in particular on the real estate
ad setting, aiming to convert an ad to a structured description, which we name
property tree, comprising the tasks of (1) identifying important entities of a
property (e.g., rooms) from classifieds and (2) structuring them into a tree
format. In this work, we propose a new joint model that is able to tackle the
two tasks simultaneously and construct the property tree by (i) avoiding the
error propagation that would arise from the subtasks one after the other in a
pipelined fashion, and (ii) exploiting the interactions between the subtasks.
For this purpose, we perform an extensive comparative study of the pipeline
methods and the new proposed joint model, reporting an improvement of over
three percentage points in the overall edge F1 score of the property tree.
Also, we propose attention methods, to encourage our model to focus on salient
tokens during the construction of the property tree. Thus we experimentally
demonstrate the usefulness of attentive neural architectures for the proposed
joint model, showcasing a further improvement of two percentage points in edge
F1 score for our application.
",computer-science
"  Sample efficiency is critical in solving real-world reinforcement learning
problems, where agent-environment interactions can be costly. Imitation
learning from expert advice has proved to be an effective strategy for reducing
the number of interactions required to train a policy. Online imitation
learning, which interleaves policy evaluation and policy optimization, is a
particularly effective technique with provable performance guarantees. In this
work, we seek to further accelerate the convergence rate of online imitation
learning, thereby making it more sample efficient. We propose two model-based
algorithms inspired by Follow-the-Leader (FTL) with prediction: MoBIL-VI based
on solving variational inequalities and MoBIL-Prox based on stochastic
first-order updates. These two methods leverage a model to predict future
gradients to speed up policy learning. When the model oracle is learned online,
these algorithms can provably accelerate the best known convergence rate up to
an order. Our algorithms can be viewed as a generalization of stochastic
Mirror-Prox (Juditsky et al., 2011), and admit a simple constructive FTL-style
analysis of performance.
",statistics
"  Compressive sensing (CS) combines data acquisition with compression coding to
reduce the number of measurements required to reconstruct a sparse signal. In
optics, this usually takes the form of projecting the field onto sequences of
random spatial patterns that are selected from an appropriate random ensemble.
We show here that CS can be exploited in `native' optics hardware without
introducing added components. Specifically, we show that random sub-Nyquist
sampling of an interferogram helps reconstruct the field modal structure. The
distribution of reduced sensing matrices corresponding to random measurements
is provably incoherent and isotropic, which helps us carry out CS successfully.
",physics
"  Texture characterization is a key problem in image understanding and pattern
recognition. In this paper, we present a flexible shape-based texture
representation using shape co-occurrence patterns. More precisely, texture
images are first represented by tree of shapes, each of which is associated
with several geometrical and radiometric attributes. Then four typical kinds of
shape co-occurrence patterns based on the hierarchical relationship of the
shapes in the tree are learned as codewords. Three different coding methods are
investigated to learn the codewords, with which, any given texture image can be
encoded into a descriptive vector. In contrast with existing works, the
proposed method not only inherits the strong ability to depict geometrical
aspects of textures and the high robustness to variations of imaging conditions
from the shape-based method, but also provides a flexible way to consider shape
relationships and to compute high-order statistics on the tree. To our
knowledge, this is the first time to use co-occurrence patterns of explicit
shapes as a tool for texture analysis. Experiments on various texture datasets
and scene datasets demonstrate the efficiency of the proposed method.
",computer-science
"  Over the years data has become increasingly higher dimensional, which has
prompted an increased need for dimension reduction techniques. This is perhaps
especially true for clustering (unsupervised classification) as well as
semi-supervised and supervised classification. Although dimension reduction in
the area of clustering for multivariate data has been quite thoroughly
discussed within the literature, there is relatively little work in the area of
three-way, or matrix variate, data. Herein, we develop a mixture of matrix
variate bilinear factor analyzers (MMVBFA) model for use in clustering
high-dimensional matrix variate data. This work can be considered both the
first matrix variate bilinear factor analysis model as well as the first MMVBFA
model. Parameter estimation is discussed, and the MMVBFA model is illustrated
using simulated and real data.
",statistics
"  Recently, Prakash et. al. have discovered bulk superconductivity in single
crystals of bismuth, which is a semi metal with extremely low carrier density.
At such low density, we argue that conventional electron-phonon coupling is too
weak to be responsible for the binding of electrons into Cooper pairs. We study
a dynamically screened Coulomb interaction with effective attraction generated
on the scale of the collective plasma modes. We model the electronic states in
bismuth to include three Dirac pockets with high velocity and one hole pocket
with a significantly smaller velocity. We find a weak coupling instability,
which is greatly enhanced by the presence of the hole pocket. Therefore, we
argue that bismuth is the first material to exhibit superconductivity driven by
retardation effects of Coulomb repulsion alone. By using realistic parameters
for bismuth we find that the acoustic plasma mode does not play the central
role in pairing. We also discuss a matrix element effect, resulting from the
Dirac nature of the conduction band, which may affect $T_c$ in the $s$-wave
channel without breaking time-reversal symmetry.
",physics
"  Existing black-box attacks on deep neural networks (DNNs) so far have largely
focused on transferability, where an adversarial instance generated for a
locally trained model can ""transfer"" to attack other learning models. In this
paper, we propose novel Gradient Estimation black-box attacks for adversaries
with query access to the target model's class probabilities, which do not rely
on transferability. We also propose strategies to decouple the number of
queries required to generate each adversarial sample from the dimensionality of
the input. An iterative variant of our attack achieves close to 100%
adversarial success rates for both targeted and untargeted attacks on DNNs. We
carry out extensive experiments for a thorough comparative evaluation of
black-box attacks and show that the proposed Gradient Estimation attacks
outperform all transferability based black-box attacks we tested on both MNIST
and CIFAR-10 datasets, achieving adversarial success rates similar to well
known, state-of-the-art white-box attacks. We also apply the Gradient
Estimation attacks successfully against a real-world Content Moderation
classifier hosted by Clarifai. Furthermore, we evaluate black-box attacks
against state-of-the-art defenses. We show that the Gradient Estimation attacks
are very effective even against these defenses.
",computer-science
"  Completeness of a dynamic priority scheduling scheme is of fundamental
importance for the optimal control of queues in areas as diverse as computer
communications, communication networks, supply chains and manufacturing
systems. Our first main contribution is to identify the mean waiting time
completeness as a unifying aspect for four different dynamic priority
scheduling schemes by proving their completeness and equivalence in 2-class
M/G/1 queue. These dynamic priority schemes are earliest due date based, head
of line priority jump, relative priority, and probabilistic priority.
In our second main contribution, we characterize the optimal scheduling
policies for the case studies in different domains by exploiting the
completeness of above dynamic priority schemes. The major theme of second main
contribution is resource allocation/optimal control in revenue management
problems for contemporary systems such as cloud computing, high-performance
computing, etc., where congestion is inherent. Using completeness and
theoretically tractable nature of relative priority policy, we study the impact
of approximation in a fairly generic data network utility framework. We
introduce the notion of min-max fairness in multi-class queues and show that a
simple global FCFS policy is min-max fair. Next, we re-derive the celebrated
$c/\rho$ rule for 2-class M/G/1 queues by an elegant argument and also simplify
a complex joint pricing and scheduling problem for a wider class of scheduling
policies.
",computer-science
"  200 nm thick SiO2 layers grown on Si substrates and Ge ions of 150 keV energy
were implanted into SiO2 matrix with Different fluences. The implanted samples
were annealed at 950 C for 30 minutes in Ar ambience. Topographical studies of
implanted as well as annealed samples were captured by the atomic force
microscopy (AFM). Two dimension (2D) multifractal detrended fluctuation
analysis (MFDFA) based on the partition function approach has been used to
study the surfaces of ion implanted and annealed samples. The partition
function is used to calculate generalized Hurst exponent with the segment size.
Moreover, it is seen that the generalized Hurst exponents vary nonlinearly with
the moment, thereby exhibiting the multifractal nature. The multifractality of
surface is pronounced after annealing for the surface implanted with fluence
7.5X1016 ions/cm^2.
",physics
"  The multi-agent path-finding (MAPF) problem has recently received a lot of
attention. However, it does not capture important characteristics of many
real-world domains, such as automated warehouses, where agents are constantly
engaged with new tasks. In this paper, we therefore study a lifelong version of
the MAPF problem, called the multi-agent pickup and delivery (MAPD) problem. In
the MAPD problem, agents have to attend to a stream of delivery tasks in an
online setting. One agent has to be assigned to each delivery task. This agent
has to first move to a given pickup location and then to a given delivery
location while avoiding collisions with other agents. We present two decoupled
MAPD algorithms, Token Passing (TP) and Token Passing with Task Swaps (TPTS).
Theoretically, we show that they solve all well-formed MAPD instances, a
realistic subclass of MAPD instances. Experimentally, we compare them against a
centralized strawman MAPD algorithm without this guarantee in a simulated
warehouse system. TP can easily be extended to a fully distributed MAPD
algorithm and is the best choice when real-time computation is of primary
concern since it remains efficient for MAPD instances with hundreds of agents
and tasks. TPTS requires limited communication among agents and balances well
between TP and the centralized MAPD algorithm.
",computer-science
"  Accounting fraud is a global concern representing a significant threat to the
financial system stability due to the resulting diminishing of the market
confidence and trust of regulatory authorities. Several tricks can be used to
commit accounting fraud, hence the need for non-static regulatory interventions
that take into account different fraudulent patterns. Accordingly, this study
aims to improve the detection of accounting fraud via the implementation of
several machine learning methods to better differentiate between fraud and
non-fraud companies, and to further assist the task of examination within the
riskier firms by evaluating relevant financial indicators. Out-of-sample
results suggest there is a great potential in detecting falsified financial
statements through statistical modelling and analysis of publicly available
accounting information. The proposed methodology can be of assistance to public
auditors and regulatory agencies as it facilitates auditing processes, and
supports more targeted and effective examinations of accounting reports.
",statistics
"  We consider the spatially homogeneous Boltzmann equation for hard potentials
with angular cutoff. This equation has a unique conservative weak solution
$(f_t)_{t\geq 0}$, once the initial condition $f_0$ with finite mass and energy
is fixed. Taking advantage of the energy conservation, we propose a recursive
algorithm that produces a $(0,\infty)\times\mathbb{R}^3$ random variable
$(M_t,V_t)$ such that $E[M_t {\bf 1}_{\{V_t \in \cdot\}}]=f_t$. We also write
down a series expansion of $f_t$. Although both the algorithm and the series
expansion might be theoretically interesting in that they explicitly express
$f_t$ in terms of $f_0$, we believe that the algorithm is not very efficient in
practice and that the series expansion is rather intractable. This is a tedious
extension to non-Maxwellian molecules of Wild's sum and of its interpretation
by McKean.
",mathematics
"  The OPERA experiment was designed to search for $\nu_{\mu} \rightarrow
\nu_{\tau}$ oscillations in appearance mode through the direct observation of
tau neutrinos in the CNGS neutrino beam. In this paper, we report a study of
the multiplicity of charged particles produced in charged-current neutrino
interactions in lead. We present charged hadron average multiplicities, their
dispersion and investigate the KNO scaling in different kinematical regions.
The results are presented in detail in the form of tables that can be used in
the validation of Monte Carlo generators of neutrino-lead interactions.
",physics
"  It is well-established by cognitive neuroscience that human perception of
objects constitutes a complex process, where object appearance information is
combined with evidence about the so-called object ""affordances"", namely the
types of actions that humans typically perform when interacting with them. This
fact has recently motivated the ""sensorimotor"" approach to the challenging task
of automatic object recognition, where both information sources are fused to
improve robustness. In this work, the aforementioned paradigm is adopted,
surpassing current limitations of sensorimotor object recognition research.
Specifically, the deep learning paradigm is introduced to the problem for the
first time, developing a number of novel neuro-biologically and
neuro-physiologically inspired architectures that utilize state-of-the-art
neural networks for fusing the available information sources in multiple ways.
The proposed methods are evaluated using a large RGB-D corpus, which is
specifically collected for the task of sensorimotor object recognition and is
made publicly available. Experimental results demonstrate the utility of
affordance information to object recognition, achieving an up to 29% relative
error reduction by its inclusion.
",computer-science
"  The physics of active systems of self-propelled particles, in the regime of a
dense liquid state, is an open puzzle of great current interest, both for
statistical physics and because such systems appear in many biological
contexts. We develop a nonequilibrium mode-coupling theory (MCT) for such
systems, where activity is included as a colored noise with the particles
having a self-propulsion foce $f_0$ and persistence time $\tau_p$. Using the
extended MCT and a generalized fluctuation-dissipation theorem, we calculate
the effective temperature $T_{eff}$ of the active fluid. The nonequilibrium
nature of the systems is manifested through a time-dependent $T_{eff}$ that
approaches a constant in the long-time limit, which depends on the activity
parameters $f_0$ and $\tau_p$. We find, phenomenologically, that this long-time
limit is captured by the potential energy of a single, trapped active particle
(STAP). Through a scaling analysis close to the MCT glass transition point, we
show that $\tau_\alpha$, the $\alpha$-relaxation time, behaves as
$\tau_\alpha\sim f_0^{-2\gamma}$, where $\gamma=1.74$ is the MCT exponent for
the passive system. $\tau_\alpha$ may increase or decrease as a function of
$\tau_p$ depending on the type of active force correlations, but the behavior
is always governed by the same value of the exponent $\gamma$. Comparison with
numerical solution of the nonequilibrium MCT as well as simulation results give
excellent agreement with the scaling analysis.
",physics
"  We consider linear groups which do not contain unipotent elements of infinite
order, which includes all linear groups in positive characteristic, and show
that this class of groups has good properties which resemble those held by
groups of non positive curvature and which do not hold for arbitrary
characteristic zero linear groups. In particular if such a linear group is
finitely generated then centralisers virtually split and all finitely generated
abelian subgroups are undistorted. If further the group is virtually torsion
free (which always holds in characteristic zero) then we have a strong property
on small subgroups: any subgroup either contains a non abelian free group or is
finitely generated and virtually abelian, hence also undistorted. We present
applications, including that the mapping class group of a surface having genus
at least 3 has no faithful linear representation which is complex unitary or
over any field of positive characteristic.
",mathematics
"  In this article we study the behavior as $p \nearrow+\infty$ of the Fucik
spectrum for $p$-Laplace operator with zero Dirichlet boundary conditions in a
bounded domain $\Omega\subset \mathbb{R}^n$. We characterize the limit
equation, and we provide a description of the limit spectrum. Furthermore, we
show some explicit computations of the spectrum for certain configurations of
the domain.
",mathematics
"  We propose an end-to-end approach to the natural language object retrieval
task, which localizes an object within an image according to a natural language
description, i.e., referring expression. Previous works divide this problem
into two independent stages: first, compute region proposals from the image
without the exploration of the language description; second, score the object
proposals with regard to the referring expression and choose the top-ranked
proposals. The object proposals are generated independently from the referring
expression, which makes the proposal generation redundant and even irrelevant
to the referred object. In this work, we train an agent with deep reinforcement
learning, which learns to move and reshape a bounding box to localize the
object according to the referring expression. We incorporate both the spatial
and temporal context information into the training procedure. By simultaneously
exploiting local visual information, the spatial and temporal context and the
referring language a priori, the agent selects an appropriate action to take at
each time. A special action is defined to indicate when the agent finds the
referred object, and terminate the procedure. We evaluate our model on various
datasets, and our algorithm significantly outperforms the compared algorithms.
Notably, the accuracy improvement of our method over the recent method GroundeR
and SCRC on the ReferItGame dataset are 7.67% and 18.25%, respectively.
",computer-science
"  We give an extension of Rubio de Francia's extrapolation theorem for
functions taking values in UMD Banach function spaces to the multilinear
limited range setting. In particular we show how boundedness of an
$m$-(sub)linear operator \[T:L^{p_1}(w_1^{p_1})\times\cdots\times
L^{p_m}(w_m^{p_m})\to L^p(w^p) \] for a certain class of Muckenhoupt weights
yields an extension of the operator to Bochner spaces $L^{p}(w^p;X)$ for a wide
class of Banach function spaces $X$, which includes certain Lebesgue, Lorentz
and Orlicz spaces.
We apply the extrapolation result to various operators, which yields new
vector-valued bounds. Our examples include the bilinear Hilbert transform,
certain Fourier multipliers and various operators satisfying sparse domination
results.
",mathematics
"  We present a sample of $\sim 1000$ emission line galaxies at $z=0.4-4.7$ from
the $\sim0.7$deg$^2$ High-$z$ Emission Line Survey (HiZELS) in the Boötes
field identified with a suite of six narrow-band filters at $\approx 0.4-2.1$
$\mu$m. These galaxies have been selected on their Ly$\alpha$ (73), {\sc [Oii]}
(285), H$\beta$/{\sc [Oiii]} (387) or H$\alpha$ (362) emission-line, and have
been classified with optical to near-infrared colours. A subsample of 98
sources have reliable redshifts from multiple narrow-band (e.g. [O{\sc
ii}]-H$\alpha$) detections and/or spectroscopy. In this survey paper, we
present the observations, selection and catalogs of emitters. We measure number
densities of Ly$\alpha$, [O{\sc ii}], H$\beta$/{\sc [Oiii]} and H$\alpha$ and
confirm strong luminosity evolution in star-forming galaxies from $z\sim0.4$ to
$\sim 5$, in agreement with previous results. To demonstrate the usefulness of
dual-line emitters, we use the sample of dual [O{\sc ii}]-H$\alpha$ emitters to
measure the observed [O{\sc ii}]/H$\alpha$ ratio at $z=1.47$. The observed
[O{\sc ii}]/H$\alpha$ ratio increases significantly from 0.40$\pm0.01$ at
$z=0.1$ to 0.52$\pm0.05$ at $z=1.47$, which we attribute to either decreasing
dust attenuation with redshift, or due to a bias in the (typically)
fiber-measurements in the local Universe which only measure the central kpc
regions. At the bright end, we find that both the H$\alpha$ and Ly$\alpha$
number densities at $z\approx2.2$ deviate significantly from a Schechter form,
following a power-law. We show that this is driven entirely by an increasing
X-ray/AGN fraction with line-luminosity, which reaches $\approx 100$ \% at
line-luminosities $L\gtrsim3\times10^{44}$ erg s$^{-1}$.
",physics
"  We developed general approach to the calculation of power-law infrared
asymptotics of spin-spin correlation functions in the Kitaev honeycomb model
with different types of perturbations. We have shown that in order to find
these correlation functions, one can perform averaging of some bilinear forms
composed out of free Majorana fermions, and we presented the method for
explicit calculation of these fermionic densities. We demonstrated how to
derive an effective Hamiltonian for the Majorana fermions, including the
effects of perturbations. For specific application of the general theory, we
have studied the effect of the Dzyaloshinskii-Moriya anisotropic spin-spin
interaction; we demonstrated that it leads, already in the second order over
its relative magnitude $D/K$, to a power-law spin correlation functions, and
calculated dynamical spin structure factor of the system. We have shown that an
external magnetic field $h$ in presence of the DM interaction, opens a gap in
the excitation spectrum of magnitude $\Delta \propto D h$.
",physics
"  We propose a new multi-frame method for efficiently computing scene flow
(dense depth and optical flow) and camera ego-motion for a dynamic scene
observed from a moving stereo camera rig. Our technique also segments out
moving objects from the rigid scene. In our method, we first estimate the
disparity map and the 6-DOF camera motion using stereo matching and visual
odometry. We then identify regions inconsistent with the estimated camera
motion and compute per-pixel optical flow only at these regions. This flow
proposal is fused with the camera motion-based flow proposal using fusion moves
to obtain the final optical flow and motion segmentation. This unified
framework benefits all four tasks - stereo, optical flow, visual odometry and
motion segmentation leading to overall higher accuracy and efficiency. Our
method is currently ranked third on the KITTI 2015 scene flow benchmark.
Furthermore, our CPU implementation runs in 2-3 seconds per frame which is 1-3
orders of magnitude faster than the top six methods. We also report a thorough
evaluation on challenging Sintel sequences with fast camera and object motion,
where our method consistently outperforms OSF [Menze and Geiger, 2015], which
is currently ranked second on the KITTI benchmark.
",computer-science
"  We review and modify the active set algorithm by Duembgen et al. (2011) for
nonparametric maximum-likelihood estimation of a log-concave density. This
particular estimation problem is embedded into a more general framework
including also the estimation of a log-convex tail inflation function as
proposed by McCullagh and Polson (2012).
",statistics
"  Melamed, Harrell, and Simpson have recently reported on an experiment which
appears to show that cooperation can arise in a dynamic network without
reputational knowledge, i.e., purely via dynamics [1]. We believe that their
experimental design is actually not testing this, in so far as players do know
the last action of their current partners before making a choice on their own
next action and subsequently deciding which link to cut. Had the authors given
no information at all, the result would be a decline in cooperation as shown in
[2].
",computer-science
"  The field of structural bioinformatics has seen significant advances with the
use of Molecular Dynamics (MD) simulations of biological systems. The MD
methodology has allowed to explain and discover molecular mechanisms in a wide
range of natural processes. There is an impending need to readily share the
ever-increasing amount of MD data, which has been hindered by the lack of
specialized tools in the past. To solve this problem, we present HTMoL, a
state-of-the-art plug-in-free hardware-accelerated web application specially
designed to efficiently transfer and visualize raw MD trajectory files on a web
browser. Now, individual research labs can publish MD data on the Internet, or
use HTMoL to profoundly improve scientific reports by including supplemental MD
data in a journal publication. HTMoL can also be used as a visualization
interface to access MD trajectories generated on a high-performance computer
center directly.
Availability: HTMoL is available free of charge for academic use. All major
browsers are supported. A complete online documentation including instructions
for download, installation, configuration, and examples is available at the
HTMoL website this http URL. Supplementary data are available
online. Corresponding author: mauricio.carrillo@cinvestav.mx
",computer-science
"  Batygin and Brown (2016) have suggested the existence of a new Solar System
planet supposed to be responsible for the perturbation of eccentric orbits of
small outer bodies. The main challenge is now to detect and characterize this
putative body. Here we investigate the principles of the determination of its
physical parameters, mainly its mass and radius. For that purpose we
concentrate on two methods, stellar occultations and gravitational microlensing
effects (amplification, deflection and time delay). We estimate the main
characteristics of a possible occultation or gravitational effects: flux
variation of a background star, duration and probability of occurence. We
investigate also additional benefits of direct imaging and of an occultation.
",physics
"  We remark that sparse and Carleson coefficients are equivalent for every
countable collection of Borel sets and hence, in particular, for dyadic
rectangles, the case relevant to the theory of bi-parameter singular integrals.
The key observation is that a dual refomulation by I. E. Verbitsky for
Carleson coefficients over dyadic cubes holds also for Carleson coefficients
over general sets. We give a simple proof for this reformulation.
",mathematics
"  Non-reversible Markov chain Monte Carlo schemes based on piecewise
deterministic Markov processes have been recently introduced in applied
probability, automatic control, physics and statistics. Although these
algorithms demonstrate experimentally good performance and are accordingly
increasingly used in a wide range of applications, geometric ergodicity results
for such schemes have only been established so far under very restrictive
assumptions. We give here verifiable conditions on the target distribution
under which the Bouncy Particle Sampler algorithm introduced in \cite{P_dW_12}
is geometrically ergodic. This holds whenever the target satisfies a curvature
condition and has tails decaying at least as fast as an exponential and at most
as fast as a Gaussian distribution. This allows us to provide a central limit
theorem for the associated ergodic averages. When the target has tails thinner
than a Gaussian distribution, we propose an original modification of this
scheme that is geometrically ergodic. For thick-tailed target distributions,
such as $t$-distributions, we extend the idea pioneered in \cite{J_G_12} in a
random walk Metropolis context. We apply a change of variable to obtain a
transformed target satisfying the tail conditions for geometric ergodicity. By
sampling the transformed target using the Bouncy Particle Sampler and mapping
back the Markov process to the original parameterization, we obtain a
geometrically ergodic algorithm.
",statistics
"  We disentangle all the individual degrees of freedom in the quantum impurity
problem to deconstruct the Kondo singlet, both in real and energy space, by
studying the contribution of each individual free electron eigenstate. This is
a problem of two spins coupled to a bath, where the bath is formed by the
remaining conduction electrons. Being a mixed state, we resort to the
""concurrence"" to quantify entanglement. We identify ""projected natural
orbitals"" that allow us to individualize a single-particle electronic wave
function that is responsible of more than $90\%$ of the impurity screening. In
the weak coupling regime, the impurity is entangled to an electron at the Fermi
level, while in the strong coupling regime, the impurity counterintuitively
entangles mostly with the high energy electrons and disentangles completely
from the low-energy states carving a ""hole"" around the Fermi level. This
enables one to use concurrence as a pseudo order parameter to compute the
characteristic ""size"" of the Kondo cloud, beyond which electrons are are weakly
correlated to the impurity and are dominated by the physics of the boundary.
",physics
"  We improve the best known upper bound on the length of the shortest reset
words of synchronizing automata. The new bound is slightly better than $114 n^3
/ 685 + O(n^2)$. The Černý conjecture states that $(n-1)^2$ is an upper
bound. So far, the best general upper bound was $(n^3-n)/6-1$ obtained by
J.-E.~Pin and P.~Frankl in 1982. Despite a number of efforts, it remained
unchanged for about 35 years.
To obtain the new upper bound we utilize avoiding words. A word is avoiding
for a state $q$ if after reading the word the automaton cannot be in $q$. We
obtain upper bounds on the length of the shortest avoiding words, and using the
approach of Trahtman from 2011 combined with the well known Frankl theorem from
1982, we improve the general upper bound on the length of the shortest reset
words. For all the bounds, there exist polynomial algorithms finding a word of
length not exceeding the bound.
",computer-science
"  In this paper we deal with Seifert fibre spaces, which are compact
3-manifolds admitting a foliation by circles. We give a combinatorial
description for these manifolds in all the possible cases: orientable,
non-orientable, closed, with boundary. Moreover, we compute a potentially sharp
upper bound for their complexity in terms of the invariants of the
combinatorial description, extending to the non-orientable case results by
Fominykh and Wiest for the orientable case with boundary and by Martelli and
Petronio for the closed orientable case.
",mathematics
"  Heterogeneous wireless networks (HWNs) composed of densely deployed base
stations of different types with various radio access technologies have become
a prevailing trend to accommodate ever-increasing traffic demand in enormous
volume. Nowadays, users rely heavily on HWNs for ubiquitous network access that
contains valuable and critical information such as financial transactions,
e-health, and public safety. Cyber risks, representing one of the most
significant threats to network security and reliability, are increasing in
severity. To address this problem, this article introduces the concept of cyber
insurance to transfer the cyber risk (i.e., service outage, as a consequence of
cyber risks in HWNs) to a third party insurer. Firstly, a review of the
enabling technologies for HWNs and their vulnerabilities to cyber risks is
presented. Then, the fundamentals of cyber insurance are introduced, and
subsequently, a cyber insurance framework for HWNs is presented. Finally, open
issues are discussed and the challenges are highlighted for integrating cyber
insurance as a service of next generation HWNs.
",computer-science
"  Measuring and analyzing the performance of software has reached a high
complexity, caused by more advanced processor designs and the intricate
interaction between user programs, the operating system, and the processor's
microarchitecture. In this report, we summarize our experience about how
performance characteristics of software should be measured when running on a
Linux operating system and a modern processor. In particular, (1) We provide a
general overview about hardware and operating system features that may have a
significant impact on timing and how they interact, (2) we identify sources of
errors that need to be controlled in order to obtain unbiased measurement
results, and (3) we propose a measurement setup for Linux to minimize errors.
Although not the focus of this report, we describe the measurement process
using hardware performance counters, which can faithfully reflect the real
bottlenecks on a given processor. Our experiments confirm that our measurement
setup has a large impact on the results. More surprisingly, however, they also
suggest that the setup can be negligible for certain analysis methods.
Furthermore, we found that our setup maintains significantly better performance
under background load conditions, which means it can be used to improve
software in high-performance applications.
",computer-science
"  Information and Communication Technology (ICT) has been playing a pivotal
role since the last decade in developing countries that brings citizen services
to the doorsteps and connecting people. With this aspiration ICT has introduced
several technologies of citizen services towards all categories of people. The
purpose of this study is to examine the Governance technology perspectives for
political party, emphasizing on the basic critical steps through which it could
be operationalized. We call it P-Governance. P-Governance shows technologies to
ensure governance, management, interaction communication in a political party
by improving decision making processes using big data. P-Governance challenges
the competence perspective to apply itself more assiduously to
operationalization, including the need to choose and give definition to one or
more units of analysis (of which the routine is a promising candidate). This
paper is to focus on research challenges posed by competence to which
P-Governance can and should respond include different strategy issues faced by
particular sections. Both the qualitative as well as quantitative research
approaches were conducted. The standard of citizen services, choice &
consultation, courtesy & consultation, entrance & information, and value for
money have found the positive relation with citizen's satisfaction. This study
results how can be technology make important roles on political movements in
developing countries using big data.
",computer-science
"  Near-future electric distribution grids operation will have to rely on
demand-side flexibility, both by implementation of demand response strategies
and by taking advantage of the intelligent management of increasingly common
small-scale energy storage. The Home energy management system (HEMS), installed
at low voltage residential clients, will play a crucial role on the flexibility
provision to both system operators and market players like aggregators.
Modeling and forecasting multi-period flexibility from residential prosumers,
such as battery storage and electric water heater, while complying with
internal constraints (comfort levels, data privacy) and uncertainty is a
complex task. This papers describes a computational method that is capable of
efficiently learn and define the feasibility flexibility space from
controllable resources connected to a HEMS. An Evolutionary Particle Swarm
Optimization (EPSO) algorithm is adopted and reshaped to derive a set of
feasible temporal trajectories for the residential net-load, considering
storage, flexible appliances, and predefined costumer preferences, as well as
load and photovoltaic (PV) forecast uncertainty. A support vector data
description (SVDD) algorithm is used to build models capable of classifying
feasible and non-feasible HEMS operating trajectories upon request from an
optimization/control algorithm operated by a DSO or market player.
",computer-science
"  Type II Weyl semimetal, a three dimensional gapless topological phase, has
drawn enormous interest recently. These topological semimetals enjoy overtilted
dispersion and Weyl nodes that separate the particle and hole pocket. Using
perturbation renormalization group, we identify possible renormalization of the
interaction vertices, which show a tendency toward instability. We further
adopt a self-consistent mean-field approach to study possible instability of
the type II Weyl semimetals under short-range electron-electron interaction. It
is found that the instabilities are much easier to form in type II Weyl
semimetals than the type I case. Eight different mean-field orders are
identified, among which we further show that the polar charge density wave
(CDW) phase exhibits the lowest energy. This CDW order is originated from the
nesting of the Fermi surfaces and could be a possible ground state in
interacting type II Weyl semimetals.
",physics
"  Training robots for operation in the real world is a complex, time consuming
and potentially expensive task. Despite significant success of reinforcement
learning in games and simulations, research in real robot applications has not
been able to match similar progress. While sample complexity can be reduced by
training policies in simulation, such policies can perform sub-optimally on the
real platform given imperfect calibration of model dynamics. We present an
approach -- supplemental to fine tuning on the real robot -- to further benefit
from parallel access to a simulator during training and reduce sample
requirements on the real robot. The developed approach harnesses auxiliary
rewards to guide the exploration for the real world agent based on the
proficiency of the agent in simulation and vice versa. In this context, we
demonstrate empirically that the reciprocal alignment for both agents provides
further benefit as the agent in simulation can adjust to optimize its behaviour
for states commonly visited by the real-world agent.
",computer-science
"  Characterization of the primary events involved in the $cis-trans$
photoisomerization of the rhodopsin retinal chromophore was approximated by a
minimum one-dimensional quantum-classical model. The developed mathematical
model is identical to that obtained using conventional quantum-classical
approaches, and multiparametric quantum-chemical or molecular dynamics (MD)
computations were not required. The quantum subsystem of the model includes
three electronic states for rhodopsin: (i) the ground state, (ii) the excited
state, and (iii) the primary photoproduct in the ground state. The resultant
model is in perfect agreement with experimental data in terms of the quantum
yield, the time required to reach the conical intersection and to complete the
quantum evolution, the range of the characteristic low frequencies active
within the primary events of the $11-cis$ retinal isomerization, and the
coherent character of the photoreaction. An effective redistribution of excess
energy between the vibration modes of rhodopsin was revealed by analysis of the
dissipation process. The results confirm the validity of the minimal model,
despite its one-dimensional character. The fundamental nature of the
photoreaction was therefore demonstrated using a minimum mathematical model for
the first time.
",physics
"  Given a combinatorial design $\mathcal{D}$ with block set $\mathcal{B}$, the
block-intersection graph (BIG) of $\mathcal{D}$ is the graph that has
$\mathcal{B}$ as its vertex set, where two vertices $B_{1} \in \mathcal{B}$ and
$B_{2} \in \mathcal{B} $ are adjacent if and only if $|B_{1} \cap B_{2}| > 0$.
The $i$-block-intersection graph ($i$-BIG) of $\mathcal{D}$ is the graph that
has $\mathcal{B}$ as its vertex set, where two vertices $B_{1} \in \mathcal{B}$
and $B_{2} \in \mathcal{B}$ are adjacent if and only if $|B_{1} \cap B_{2}| =
i$. In this paper several constructions are obtained that start with twofold
triple systems (TTSs) with Hamiltonian $2$-BIGs and result in larger TTSs that
also have Hamiltonian $2$-BIGs. These constructions collectively enable us to
determine the complete spectrum of TTSs with Hamiltonian $2$-BIGs (equivalently
TTSs with cyclic $2$-intersecting Gray codes) as well as the complete spectrum
for TTSs with $2$-BIGs that have Hamilton paths (i.e., for TTSs with
$2$-intersecting Gray codes).
In order to prove these spectrum results, we sometimes require ingredient
TTSs that have large partial parallel classes; we prove lower bounds on the
sizes of partial parallel clasess in arbitrary TTSs, and then construct larger
TTSs with both cyclic $2$-intersecting Gray codes and parallel classes.
",mathematics
"  We study a pumping lemma for the word/tree languages generated by
higher-order grammars. Pumping lemmas are known up to order-2 word languages
(i.e., for regular/context-free/indexed languages), and have been used to show
that a given language does not belong to the classes of
regular/context-free/indexed languages. We prove a pumping lemma for word/tree
languages of arbitrary orders, modulo a conjecture that a higher-order version
of Kruskal's tree theorem holds. We also show that the conjecture indeed holds
for the order-2 case, which yields a pumping lemma for order-2 tree languages
and order-3 word languages.
",computer-science
"  Consumers often react expressively to products such as food samples, perfume,
jewelry, sunglasses, and clothing accessories. This research discusses a
multimodal affect recognition system developed to classify whether a consumer
likes or dislikes a product tested at a counter or kiosk, by analyzing the
consumer's facial expression, body posture, hand gestures, and voice after
testing the product. A depth-capable camera and microphone system - Kinect for
Windows - is utilized. An emotion identification engine has been developed to
analyze the images and voice to determine affective state of the customer. The
image is segmented using skin color and adaptive threshold. Face, body and
hands are detected using the Haar cascade classifier. Canny edges are
identified and the lip, body and hand contours are extracted using spatial
filtering. Edge count and orientation around the mouth, cheeks, eyes,
shoulders, fingers and the location of the edges are used as features.
Classification is done by an emotion template mapping algorithm and training a
classifier using support vector machines. The real-time performance, accuracy
and feasibility for multimodal affect recognition in feedback assessment are
evaluated.
",computer-science
"  Planetary rings produce a distinct shape distortion in transit lightcurves.
However, to accurately model such lightcurves the observations need to cover
the entire transit, especially ingress and egress, as well as an out-of-transit
baseline. Such observations can be challenging for long period planets, where
the transits may last for over a day. Planetary rings will also impact the
shape of absorption lines in the stellar spectrum, as the planet and rings
cover different parts of the rotating star (the Rossiter-McLaughlin effect).
These line-profile distortions depend on the size, structure, opacity,
obliquity and sky projected angle of the ring system. For slow rotating stars,
this mainly impacts the amplitude of the induced velocity shift, however, for
fast rotating stars the large velocity gradient across the star allows the line
distortion to be resolved, enabling direct determination of the ring
parameters. We demonstrate that by modeling these distortions we can recover
ring system parameters (sky-projected angle, obliquity and size) using only a
small part of the transit. Substructure in the rings, e.g. gaps, can be
recovered if the width of the features ($\delta W$) relative to the size of the
star is similar to the intrinsic velocity resolution (set by the width of the
local stellar profile, $\gamma$) relative to the stellar rotation velocity ($v$
sin$i$, i.e. $\delta W / R_* \gtrsim v$sin$i$/$\gamma$). This opens up a new
way to study the ring systems around planets with long orbital periods, where
observations of the full transit, covering the ingress and egress, are not
always feasible.
",physics
"  How is reliable physiological function maintained in cells despite
considerable variability in the values of key parameters of multiple
interacting processes that govern that function? Here we use the classic
Hodgkin-Huxley formulation of the squid giant axon action potential to propose
a possible approach to this problem. Although the full Hodgkin-Huxley model is
very sensitive to fluctuations that independently occur in its many parameters,
the outcome is in fact determined by simple combinations of these parameters
along two physiological dimensions: Structural and Kinetic (denoted $S$ and
$K$). Structural parameters describe the properties of the cell, including its
capacitance and the densities of its ion channels. Kinetic parameters are those
that describe the opening and closing of the voltage-dependent conductances.
The impacts of parametric fluctuations on the dynamics of the system, seemingly
complex in the high dimensional representation of the Hodgkin-Huxley model, are
tractable when examined within the $S-K$ plane. We demonstrate that slow
inactivation, a ubiquitous activity-dependent feature of ionic channels, is a
powerful local homeostatic control mechanism that stabilizes excitability amid
changes in structural and kinetic parameters.
",quantitative-biology
"  Given the importance of crystal symmetry for the emergence of topological
quantum states, we have studied, as exemplified in NbNiTe2, the interplay of
crystal symmetry, atomic displacements (lattice vibration), band degeneracy,
and band topology. For NbNiTe2 structure in space group 53 (Pmna) - having an
inversion center arising from two glide planes and one mirror plane with a
2-fold rotation and screw axis - a full gap opening exists between two band
manifolds near the Fermi energy. Upon atomic displacements by optical phonons,
the symmetry lowers to space group 28 (Pma2), eliminating one glide plane along
c, the associated rotation and screw axis, and the inversion center. As a
result, twenty Weyl points emerge, including four type-II Weyl points in the
G-X direction at the boundary between a pair of adjacent electron and hole
bands. Thus, optical phonons may offer control of the transition to a Weyl
fermion state.
",physics
"  Recent studies show interest in materials with asymmetric friction forces. We
investigate terminal motion of a solid body with circular contact area. We
assume that friction forces are asymmetric orthotropic. Two cases of pressure
distribution are analyzed: Hertz and Boussinesq laws. Equations for friction
force and moment are formulated and solved for these cases. Numer- ical results
show significant impact of the asymmetry of friction on the motion. Our results
can be used for more accurate prediction of contact behavior of bodies made
from new materials with asymmetric surface textures.
",physics
"  A method for inverse design of horizontal axis wind turbines (HAWTs) is
presented in this paper. The direct solver for aerodynamic analysis solves the
Reynolds Averaged Navier Stokes (RANS) equations, where the effect of the
turbine rotor is modeled as momentum sources using the actuator disk model
(ADM); this approach is referred to as RANS/ADM. The inverse problem is posed
as follows: for a given selection of airfoils, the objective is to find the
blade geometry (described as blade twist and chord distributions) which
realizes the desired turbine aerodynamic performance at the design point; the
desired performance is prescribed as angle of attack ($\alpha$) and axial
induction factor ($a$) distributions along the blade. An iterative approach is
used. An initial estimate of blade geometry is used with the direct solver
(RANS/ADM) to obtain $\alpha$ and $a$. The differences between the calculated
and desired values of $\alpha$ and $a$ are computed and a new estimate for the
blade geometry (chord and twist) is obtained via nonlinear least squares
regression using the Trust-Region-Reflective (TRF) method. This procedure is
continued until the difference between the calculated and the desired values is
within acceptable tolerance. The method is demonstrated for conventional,
single-rotor HAWTs and then extended to multi-rotor, specifically dual-rotor
wind turbines. The TRF method is also compared with the multi-dimensional
Newton iteration method and found to provide better convergence when
constraints are imposed in blade design, although faster convergence is
obtained with the Newton method for unconstrained optimization.
",physics
"  We study the ideal structure of reduced crossed product of topological
dynamical systems of a countable discrete group. More concretely, for a compact
Hausdorff space $X$ with an action of a countable discrete group $\Gamma$, we
consider the absence of a non-zero ideals in the reduced crossed product $C(X)
\rtimes_r \Gamma$ which has a zero intersection with $C(X)$. We characterize
this condition by a property for amenable subgroups of the stabilizer subgroups
of $X$ in terms of the Chabauty space of $\Gamma$. This generalizes Kennedy's
algebraic characterization of the simplicity for a reduced group
$\mathrm{C}^{*}$-algebra of a countable discrete group.
",mathematics
"  We consider the chemotaxis problem for a one-dimensional system. To analyze
the interaction of bacteria and attractant we use a modified Keller-Segel model
which accounts attractant absorption. To describe the system we use the
chemotaxis sensitivity function, which characterizes nonuniformity of bacteria
distribution. In particular, we investigate how the chemotaxis sensitivity
function depends on the concentration of attractant at the boundary of the
system. It is known that in the system without absorption the chemotaxis
sensitivity function has a bell shape maximum. Here we show that attractant
absorption and special boundary conditions for bacteria can cause the
appearance of an additional maximum in the chemotaxis sensitivity function. The
value of this maximum is determined by the intensity of absorption.
",physics
"  Recent 60Fe results have suggested that the estimated distances of supernovae
in the last few million years should be reduced from 100 pc to 50 pc. Two
events or series of events are suggested, one about 2.7 million years to 1.7
million years ago, and another may at 6.5 to 8.7 million years ago. We ask what
effects such supernovae are expected to have on the terrestrial atmosphere and
biota. Assuming that the Local Bubble was formed before the event being
considered, and that the supernova and the Earth were both inside a weak,
disordered magnetic field at that time, TeV-PeV cosmic rays at Earth will
increase by a factor of a few hundred. Tropospheric ionization will increase
proportionately, and the overall muon radiation load on terrestrial organisms
will increase by a factor of 150. All return to pre-burst levels within 10kyr.
In the case of an ordered magnetic field, effects depend strongly on the field
orientation. The upper bound in this case is with a largely coherent field
aligned along the line of sight to the supernova, in which case TeV-PeV cosmic
ray flux increases are 10^4; in the case of a transverse field they are below
current levels. We suggest a substantial increase in the extended effects of
supernovae on Earth and in the lethal distance estimate; more work is
needed.This paper is an explicit followup to Thomas et al. (2016). We also here
provide more detail on the computational procedures used in both works.
",physics
"  We report the design, fabrication and characterization of ultralight highly
emissive metaphotonic structures with record-low mass/area that emit thermal
radiation efficiently over a broad spectral (2 to 35 microns) and angular (0-60
degrees) range. The structures comprise one to three pairs of alternating
nanometer-scale metallic and dielectric layers, and have measured effective 300
K hemispherical emissivities of 0.7 to 0.9. To our knowledge, these structures,
which are all subwavelength in thickness are the lightest reported metasurfaces
with comparable infrared emissivity. The superior optical properties, together
with their mechanical flexibility, low outgassing, and low areal mass, suggest
that these metasurfaces are candidates for thermal management in applications
demanding of ultralight flexible structures, including aerospace applications,
ultralight photovoltaics, lightweight flexible electronics, and textiles for
thermal insulation.
",physics
"  In this paper, we give some low-dimensional examples of local cocycle 3-Lie
bialgebras and double construction 3-Lie bialgebras which were introduced in
the study of the classical Yang-Baxter equation and Manin triples for 3-Lie
algebras. We give an explicit and practical formula to compute the
skew-symmetric solutions of the 3-Lie classical Yang-Baxter equation (CYBE). As
an illustration, we obtain all skew-symmetric solutions of the 3-Lie CYBE in
complex 3-Lie algebras of dimension 3 and 4 and then the induced local cocycle
3-Lie bialgebras. On the other hand, we classify the double construction 3-Lie
bialgebras for complex 3-Lie algebras in dimensions 3 and 4 and then give the
corresponding 8-dimensional pseudo-metric 3-Lie algebras.
",mathematics
"  While optimizing convex objective (loss) functions has been a powerhouse for
machine learning for at least two decades, non-convex loss functions have
attracted fast growing interests recently, due to many desirable properties
such as superior robustness and classification accuracy, compared with their
convex counterparts. The main obstacle for non-convex estimators is that it is
in general intractable to find the optimal solution. In this paper, we study
the computational issues for some non-convex M-estimators. In particular, we
show that the stochastic variance reduction methods converge to the global
optimal with linear rate, by exploiting the statistical property of the
population loss. En route, we improve the convergence analysis for the batch
gradient method in \cite{mei2016landscape}.
",statistics
"  In the context of dynamic emission tomography, the conventional processing
pipeline consists of independent image reconstruction of single time frames,
followed by the application of a suitable kinetic model to time activity curves
(TACs) at the voxel or region-of-interest level. The relatively new field of 4D
PET direct reconstruction, by contrast, seeks to move beyond this scheme and
incorporate information from multiple time frames within the reconstruction
task. Existing 4D direct models are based on a deterministic description of
voxels' TACs, captured by the chosen kinetic model, considering the photon
counting process the only source of uncertainty. In this work, we introduce a
new probabilistic modeling strategy based on the key assumption that activity
time course would be subject to uncertainty even if the parameters of the
underlying dynamic process were known. This leads to a hierarchical Bayesian
model, which we formulate using the formalism of Probabilistic Graphical
Modeling (PGM). The inference of the joint probability density function arising
from PGM is addressed using a new gradient-based iterative algorithm, which
presents several advantages compared to existing direct methods: it is flexible
to an arbitrary choice of linear and nonlinear kinetic model; it enables the
inclusion of arbitrary (sub)differentiable priors for parametric maps; it is
simpler to implement and suitable to integration in computing frameworks for
machine learning. Computer simulations and an application to real patient scan
showed how the proposed approach allows us to weight the importance of the
kinetic model, providing a bridge between indirect and deterministic direct
methods.
",statistics
"  Coexistence of a new-type antiferromagnetic (AFM) state, the so-called
hedgehog spin-vortex crystal (SVC), and superconductivity (SC) is evidenced by
$^{75}$As nuclear magnetic resonance study on single-crystalline
CaK(Fe$_{0.951}$Ni$_{0.049}$)$_4$As$_4$. The hedgehog SVC order is clearly
demonstrated by the direct observation of the internal magnetic induction along
the $c$ axis at the As1 site (close to K) and a zero net internal magnetic
induction at the As2 site (close to Ca) below an AFM ordering temperature
$T_{\rm N}$ $\sim$ 52 K. The nuclear spin-lattice relaxation rate 1/$T_1$ shows
a distinct decrease below $T_{\rm c}$ $\sim$ 10 K, providing also unambiguous
evidence for the microscopic coexistence. Furthermore, based on the analysis of
the 1/$T_1$ data, the hedgehog SVC-type spin correlations are found to be
enhanced below $T$ $\sim$ 150 K in the paramagnetic state. These results
indicate the hedgehog SVC-type spin correlations play an important role for the
appearance of SC in the new magnetic superconductor.
",physics
"  We consider relative error low rank approximation of $tensors$ with respect
to the Frobenius norm: given an order-$q$ tensor $A \in
\mathbb{R}^{\prod_{i=1}^q n_i}$, output a rank-$k$ tensor $B$ for which
$\|A-B\|_F^2 \leq (1+\epsilon)$OPT, where OPT $= \inf_{\textrm{rank-}k~A'}
\|A-A'\|_F^2$. Despite the success on obtaining relative error low rank
approximations for matrices, no such results were known for tensors. One
structural issue is that there may be no rank-$k$ tensor $A_k$ achieving the
above infinum. Another, computational issue, is that an efficient relative
error low rank approximation algorithm for tensors would allow one to compute
the rank of a tensor, which is NP-hard. We bypass these issues via (1)
bicriteria and (2) parameterized complexity solutions:
(1) We give an algorithm which outputs a rank $k' = O((k/\epsilon)^{q-1})$
tensor $B$ for which $\|A-B\|_F^2 \leq (1+\epsilon)$OPT in $nnz(A) + n \cdot
\textrm{poly}(k/\epsilon)$ time in the real RAM model. Here $nnz(A)$ is the
number of non-zero entries in $A$.
(2) We give an algorithm for any $\delta >0$ which outputs a rank $k$ tensor
$B$ for which $\|A-B\|_F^2 \leq (1+\epsilon)$OPT and runs in $ ( nnz(A) + n
\cdot \textrm{poly}(k/\epsilon) + \exp(k^2/\epsilon) ) \cdot n^\delta$ time in
the unit cost RAM model.
For outputting a rank-$k$ tensor, or even a bicriteria solution with
rank-$Ck$ for a certain constant $C > 1$, we show a $2^{\Omega(k^{1-o(1)})}$
time lower bound under the Exponential Time Hypothesis.
Our results give the first relative error low rank approximations for tensors
for a large number of robust error measures for which nothing was known, as
well as column row and tube subset selection. We also obtain new results for
matrices, such as $nnz(A)$-time CUR decompositions, improving previous
$nnz(A)\log n$-time algorithms, which may be of independent interest.
",computer-science
"  The metriplectic formalism couples Poisson brackets of the Hamiltonian
description with metric brackets for describing systems with both Hamiltonian
and dissipative components. The construction builds in asymptotic convergence
to a preselected equilibrium state. Phenomena such as friction, electric
resistivity, thermal conductivity and collisions in kinetic theories are well
represented in this framework. In this paper we present an application of the
metriplectic formalism of interest for the theory of control: a suitable torque
is applied to a free rigid body, which is expressed through a metriplectic
extension of its ""natural"" Poisson algebra. On practical grounds, the effect is
to drive the body to align its angular velocity to rotation about a stable
principal axis of inertia, while conserving its kinetic energy in the process.
On theoretical grounds, this example shows how the non-Hamiltonian part of a
metriplectic system may include convergence to a limit cycle, the first example
of a non-zero dimensional attractor in this formalism. The method suggests a
way to extend metriplectic dynamics to systems with general attractors, e.g.
chaotic ones, with the hope of representing bio-physical, geophysical and
ecological models.
",physics
"  This paper considers mean field games in a multi-agent Markov decision
process (MDP) framework. Each player has a continuum state and binary action.
By active control, a player can bring its state to a resetting point. All
players are coupled through their cost functions. The structural property of
the individual strategies is characterized in terms of threshold policies when
the mean field game admits a solution. We further introduce a stationary
equation system of the mean field game and analyze uniqueness of its solution
under positive externalities.
",mathematics
"  We provide an algorithm that computes a set of generators for any complete
ideal in a smooth complex surface. More interestingly, these generators admit a
presentation as monomials in a set of maximal contact elements associated to
the minimal log-resolution of the ideal. Furthermore, the monomial expression
given by our method is an equisingularity invariant of the ideal. As an
outcome, we provide a geometric method to compute the integral closure of a
planar ideal and we apply our algorithm to some families of complete ideals.
",mathematics
"  Inverse Compton scattering (ICS) is a unique mechanism for producing fast
pulses - picosecond and below - of bright X- to gamma-rays. These nominally
narrow spectral bandwidth electromagnetic radiation pulses are efficiently
produced in the interaction between intense, well-focused electron and laser
beams. The spectral characteristics of such sources are affected by many
experimental parameters, such as the bandwidth of the laser, and the angles of
both the electrons and laser photons at collision. The laser field amplitude
induces harmonic generation and importantly, for the present work, nonlinear
red shifting, both of which dilute the spectral brightness of the radiation. As
the applications enabled by this source often depend sensitively on its
spectra, it is critical to resolve the details of the wavelength and angular
distribution obtained from ICS collisions. With this motivation, we present
here an experimental study that greatly improves on previous spectral
measurement methods based on X-ray K-edge filters, by implementing a
multi-layer bent-crystal X-ray spectrometer. In tandem with a collimating slit,
this method reveals a projection of the double-differential angular-wavelength
spectrum of the ICS radiation in a single shot. The measurements enabled by
this diagnostic illustrate the combined off-axis and nonlinear-field-induced
red shifting in the ICS emission process. They reveal in detail the strength of
the normalized laser vector potential, and provide a non-destructive measure of
the temporal and spatial electron-laser beam overlap.
",physics
"  The revival structures for the X_m exceptional orthogonal polynomials of the
Scarf I potential endowed with position-dependent effective mass is studied in
the context of the generalized Gazeau-Klauder coherent states. It is shown that
in the case of the constant mass, the deduced coherent states mimic full and
fractional revivals phenomena. However in the case of position-dependent
effective mass, although full revivals take place during their time evolution,
there is no fractional revivals as defined in the common sense. These
properties are illustrated numerically by means of some specific profile mass
functions, with and without singularities. We have also observed a close
connection between the coherence time {\tau}_coh^m? and the mass parameter ?.
",mathematics
"  Betweenness centrality is an important index widely used in different domains
such as social networks, traffic networks and the world wide web. However, even
for mid-size networks that have only a few hundreds thousands vertices, it is
computationally expensive to compute exact betweenness scores. Therefore in
recent years, several approximate algorithms have been developed. In this
paper, first given a network $G$ and a vertex $r \in V(G)$, we propose a
Metropolis-Hastings MCMC algorithm that samples from the space $V(G)$ and
estimates betweenness score of $r$. The stationary distribution of our MCMC
sampler is the optimal sampling proposed for betweenness centrality estimation.
We show that our MCMC sampler provides an $(\epsilon,\delta)$-approximation,
where the number of required samples depends on the position of $r$ in $G$ and
in many cases, it is a constant. Then, given a network $G$ and a set $R \subset
V(G)$, we present a Metropolis-Hastings MCMC sampler that samples from the
joint space $R$ and $V(G)$ and estimates relative betweenness scores of the
vertices in $R$. We show that for any pair $r_i, r_j \in R$, the ratio of the
expected values of the estimated relative betweenness scores of $r_i$ and $r_j$
respect to each other is equal to the ratio of their betweenness scores. We
also show that our joint-space MCMC sampler provides an
$(\epsilon,\delta)$-approximation of the relative betweenness score of $r_i$
respect to $r_j$, where the number of required samples depends on the position
of $r_j$ in $G$ and in many cases, it is a constant.
",computer-science
"  In this paper we study a non strictly systems of conservation law by
stochastic perturbation. We show the existence and uniqueness of the solution.
We do not assume that $BV$-regularity for the initial conditions. The proofs
are based on the concept of entropy solution and in the characteristics method
(in the influence of noise). This is the first result on the regularization by
noise in hyperbolic systems of conservation law.
",mathematics
"  We consider the problem of enabling robust range estimation of eigenvalue
decomposition (EVD) algorithm for a reliable fixed-point design. The simplicity
of fixed-point circuitry has always been so tempting to implement EVD algo-
rithms in fixed-point arithmetic. Working towards an effective fixed-point
design, integer bit-width allocation is a significant step which has a crucial
impact on accuracy and hardware efficiency. This paper investigates the
shortcomings of the existing range estimation methods while deriving bounds for
the variables of the EVD algorithm. In light of the circumstances, we introduce
a range estimation approach based on vector and matrix norm properties together
with a scaling procedure that maintains all the assets of an analytical method.
The method could derive robust and tight bounds for the variables of EVD
algorithm. The bounds derived using the proposed approach remain same for any
input matrix and are also independent of the number of iterations or size of
the problem. Some benchmark hyperspectral data sets have been used to evaluate
the efficiency of the proposed technique. It was found that by the proposed
range estimation approach, all the variables generated during the computation
of Jacobi EVD is bounded within $\pm1$.
",computer-science
"  High temperature (high-Tc) superconductors like cuprates have superior
critical current properties in magnetic fields over other superconductors.
However, superconducting wires for high-field-magnet applications are still
dominated by low-Tc Nb3Sn due probably to cost and processing issues. The
recent discovery of a second class of high-Tc materials, Fe-based
superconductors, may provide another option for high-field-magnet wires. In
particular, AEFe2As2 (AE: Alkali earth elements, AE-122) is one of the best
candidates for high-field-magnet applications because of its high upper
critical field, Hc2, moderate Hc2 anisotropy, and intermediate Tc. Here we
report on in-field transport properties of P-doped BaFe2As2 (Ba-122) thin films
grown on technical substrates (i.e., biaxially textured oxides templates on
metal tapes) by pulsed laser deposition. The P-doped Ba-122 coated conductor
sample exceeds a transport Jc of 10^5 A/cm^2 at 15 T for both major
crystallographic directions of the applied magnetic field, which is favourable
for practical applications. Our P-doped Ba-122 coated conductors show a
superior in-field Jc over MgB2 and NbTi, and a comparable level to Nb3Sn above
20 T. By analysing the E-J curves for determining Jc, a non-Ohmic linear
differential signature is observed at low field due to flux flow along the
grain boundaries. However, grain boundaries work as flux pinning centres as
demonstrated by the pinning force analysis.
",physics
"  Existence of steady states in elastic media at small strains with diffusion
of a solvent or fluid due to Fick's or Darcy's laws is proved by combining
usage of variational methods inspired from static situations with Schauder's
fixed-point arguments. In the plain variant, the problem consists in the force
equilibrium coupled with the continuity equation, and the underlying operator
is non-potential and non-pseudomonotone so that conventional methods are not
applicable. In advanced variants, electrically-charged multi-component flows
through an electrically charged elastic solid are treated, employing critical
points of the saddle-point type. Eventually, anisothermal variants involving
heat-transfer equation are treated, too.
",mathematics
"  We prove regularity estimates for entropy solutions to scalar conservation
laws with a force. Based on the kinetic form of a scalar conservation law, a
new decomposition of entropy solutions is introduced, by means of a
decomposition in the velocity variable, adapted to the non-degeneracy
properties of the flux function. This allows a finer control of the degeneracy
behavior of the flux. In addition, this decomposition allows to make use of the
fact that the entropy dissipation measure has locally finite singular moments.
Based on these observations, improved regularity estimates for entropy
solutions to (forced) scalar conservation laws are obtained.
",mathematics
"  We present a strongly interacting quadruple system associated with the K2
target EPIC 220204960. The K2 target itself is a Kp = 12.7 magnitude star at
Teff ~ 6100 K which we designate as ""B-N"" (blue northerly image). The host of
the quadruple system, however, is a Kp = 17 magnitude star with a composite
M-star spectrum, which we designate as ""R-S"" (red southerly image). With a 3.2""
separation and similar radial velocities and photometric distances, 'B-N' is
likely physically associated with 'R-S', making this a quintuple system, but
that is incidental to our main claim of a strongly interacting quadruple system
in 'R-S'. The two binaries in 'R-S' have orbital periods of 13.27 d and 14.41
d, respectively, and each has an inclination angle of >89 degrees. From our
analysis of radial velocity measurements, and of the photometric lightcurve, we
conclude that all four stars are very similar with masses close to 0.4 Msun.
Both of the binaries exhibit significant ETVs where those of the primary and
secondary eclipses 'diverge' by 0.05 days over the course of the 80-day
observations. Via a systematic set of numerical simulations of quadruple
systems consisting of two interacting binaries, we conclude that the outer
orbital period is very likely to be between 300 and 500 days. If sufficient
time is devoted to RV studies of this faint target, the outer orbit should be
measurable within a year.
",physics
"  A regular ordered semigroup $S$ is called right inverse if every principal
left ideal of $S$ is generated by an $\mathcal{R}$-unique ordered idempotent.
Here we explore the theory of right inverse ordered semigroups. We show that a
regular ordered semigroup is right inverse if and only if any two right
inverses of an element $a\in S$ are $\mathcal{R}$-related. Furthermore,
different characterizations of right Clifford, right group-like, group like
ordered semigroups are done by right inverse ordered semigroups. Thus a
foundation of right inverse semigroups has been developed.
",mathematics
"  In this study, we address the question whether (and to what extent,
respectively) altmetrics are related to the scientific quality of papers (as
measured by peer assessments). Only a few studies have previously investigated
the relationship between altmetrics and assessments by peers. In the first
step, we analyse the underlying dimensions of measurement for traditional
metrics (citation counts) and altmetrics - by using principal component
analysis (PCA) and factor analysis (FA). In the second step, we test the
relationship between the dimensions and quality of papers (as measured by the
post-publication peer-review system of F1000Prime assessments) - using
regression analysis. The results of the PCA and FA show that altmetrics operate
along different dimensions, whereas Mendeley counts are related to citation
counts, and tweets form a separate dimension. The results of the regression
analysis indicate that citation-based metrics and readership counts are
significantly more related to quality, than tweets. This result on the one hand
questions the use of Twitter counts for research evaluation purposes and on the
other hand indicates potential use of Mendeley reader counts.
",computer-science
"  The sample matrix inversion (SMI) beamformer implements Capon's minimum
variance distortionless (MVDR) beamforming using the sample covariance matrix
(SCM). In a snapshot limited environment, the SCM is poorly conditioned
resulting in a suboptimal performance from the SMI beamformer. Imposing
structural constraints on the SCM estimate to satisfy known theoretical
properties of the ensemble MVDR beamformer mitigates the impact of limited
snapshots on the SMI beamformer performance. Toeplitz rectification and
bounding the norm of weight vector are common approaches for such constrains.
This paper proposes the unit circle rectification technique which constraints
the SMI beamformer to satisfy a property of the ensemble MVDR beamformer: for
narrowband planewave beamforming on a uniform linear array, the zeros of the
MVDR weight array polynomial must fall on the unit circle. Numerical
simulations show that the resulting unit circle MVDR (UC MVDR) beamformer
frequently improves the suppression of both discrete interferers and white
background noise compared to the classic SMI beamformer. Moreover, the UC MVDR
beamformer is shown to suppress discrete interferers better than the MVDR
beamformer diagonally loaded to maximize the SINR.
",computer-science
"  Data sharing among partners---users, organizations, companies---is crucial
for the advancement of data analytics in many domains. Sharing through secure
computation and differential privacy allows these partners to perform private
computations on their sensitive data in controlled ways. However, in reality,
there exist complex relationships among members. Politics, regulations,
interest, trust, data demands and needs are one of the many reasons. Thus,
there is a need for a mechanism to meet these conflicting relationships on data
sharing. This paper presents Curie, an approach to exchange data among members
whose membership has complex relationships. The CPL policy language that allows
members to define the specifications of data exchange requirements is
introduced. Members (partners) assert who and what to exchange through their
local policies and negotiate a global sharing agreement. The agreement is
implemented in a multi-party computation that guarantees sharing among members
will comply with the policy as negotiated. The use of Curie is validated
through an example of a health care application built on recently introduced
secure multi-party computation and differential privacy frameworks, and policy
and performance trade-offs are explored.
",computer-science
"  We introduce a method for using Fizeau interferometry to measure the
intrinsic resolving power of a diffraction grating. This method is more
accurate than traditional techniques based on a long-trace profiler (LTP),
since it is sensitive to long-distance phase errors not revealed by a d-spacing
map. We demonstrate 50,400 resolving power for a mechanically ruled XUV grating
from Inprentus, Inc.
",physics
"  Listed as No. 53 among the one hundred famous unsolved problems in [J. A.
Bondy, U. S. R. Murty, Graph Theory, Springer, Berlin, 2008] is Steinberg's
conjecture, which states that every planar graph without 4- and 5-cycles is
3-colorable. In this paper, we show that plane graphs without 4- and 5-cycles
are 3-colorable if they have no ext-triangular 7-cycles. This implies that (1)
planar graphs without 4-, 5-, 7-cycles are 3-colorable, and (2) planar graphs
without 4-, 5-, 8-cycles are 3-colorable, which cover a number of known results
in the literature motivated by Steinberg's conjecture.
",mathematics
"  A novel matching based heuristic algorithm designed to detect specially
formulated infeasible zero-one IPs is presented. The algorithm input is a set
of nested doubly stochastic subsystems and a set E of instance defining
variables set at zero level. The algorithm deduces additional variables at zero
level until either a constraint is violated (the IP is infeasible), or no more
variables can be deduced zero (the IP is undecided). All feasible IPs, and all
infeasible IPs not detected infeasible are undecided. We successfully apply the
algorithm to a small set of specially formulated infeasible zero-one IP
instances of the Hamilton cycle decision problem. We show how to model both the
graph and subgraph isomorphism decision problems for input to the algorithm.
Increased levels of nested doubly stochastic subsystems can be implemented
dynamically. The algorithm is designed for parallel processing, and for
inclusion of techniques in addition to matching.
",computer-science
"  Runtime Monitoring is a lightweight and dynamic verification technique that
involves observing the internal operations of a software system and/or its
interactions with other external entities, with the aim of determining whether
the system satisfies or violates a correctness specification. Compilation
techniques employed in Runtime Monitoring tools allow monitors to be
automatically derived from high-level correctness specifications (aka.
properties). This allows the same property to be converted into different types
of monitors, which may apply different instrumentation techniques for checking
whether the property was satisfied or not. In this paper we compare and
contrast the various types of monitoring methodologies found in the current
literature, and classify them into a spectrum of monitoring instrumentation
techniques, ranging from completely asynchronous monitoring on the one end and
completely synchronous monitoring on the other.
",computer-science
"  We derive some Positivstellensatzë for noncommutative rational expressions
from the Positivstellensatzë for noncommutative polynomials. Specifically, we
show that if a noncommutative rational expression is positive on a polynomially
convex set, then there is an algebraic certificate witnessing that fact. As in
the case of noncommutative polynomials, our results are nicer when we
additionally assume positivity on a convex set-- that is, we obtain a so-called
""perfect Positivstellensatz"" on convex sets.
",mathematics
"  One of the most important features of mobile rescue robots is the ability to
autonomously detect casualties, i.e. human bodies, which are usually lying on
the ground. This paper proposes a novel method for autonomously detecting
casualties lying on the ground using obtained 3D point-cloud data from an
on-board sensor, such as an RGB-D camera or a 3D LIDAR, on a mobile rescue
robot. In this method, the obtained 3D point-cloud data is projected onto the
detected ground plane, i.e. floor, within the point cloud. Then, this projected
point cloud is converted into a grid-map that is used afterwards as an input
for the algorithm to detect human body shapes. The proposed method is evaluated
by performing detection of a human dummy, placed in different random positions
and orientations, using an on-board RGB-D camera on a mobile rescue robot
called ResQbot. To evaluate the robustness of the casualty detection method to
different camera angles, the orientation of the camera is set to different
angles. The experimental results show that using the point-cloud data from the
on-board RGB-D camera, the proposed method successfully detects the casualty in
all tested body positions and orientations relative to the on-board camera, as
well as in all tested camera angles.
",computer-science
"  In this paper, we investigate the umbral representation of the Fubini
polynomials $F_{x}^{n}:=F_{n}(x)$ to derive some properties involving these
polynomials. For any prime number $p$ and any polynomial $f$ with integer
coefficients, we show $(f(F_{x}))^{p}\equiv f(F_{x})$ and we give other curious
congruences.
",mathematics
"  Compact and portable in-situ NMR spectrometers which can be dipped in the
liquid to be measured, and are easily maintained, with affordable coil
constructions and electronics, together with an apparatus to recover depleted
magnets are presented, that provide a new real-time processing method for NMR
spectrum acquisition, that remains stable despite magnetic field fluctuations.
",physics
"  With a plethora of available classification performance measures, choosing
the right metric for the right task requires careful thought. To make this
decision in an informed manner, one should study and compare general properties
of candidate measures. However, analysing measures with respect to complete
ranges of their domain values is a difficult and challenging task. In this
study, we attempt to support such analyses with a specialized visualization
technique, which operates in a barycentric coordinate system using a 3D
tetrahedron. Additionally, we adapt this technique to the context of imbalanced
data and put forward a set of properties which should be taken into account
when selecting a classification performance measure. As a result, we compare 22
popular measures and show important differences in their behaviour. Moreover,
for parametric measures such as the F$_{\beta}$ and IBA$_\alpha$(G-mean), we
analytically derive parameter thresholds that change measure properties.
Finally, we provide an online visualization tool that can aid the analysis of
complete domain ranges of performance measures.
",computer-science
"  Model-based clustering is a popular approach for clustering multivariate data
which has seen applications in numerous fields. Nowadays, high-dimensional data
are more and more common and the model-based clustering approach has adapted to
deal with the increasing dimensionality. In particular, the development of
variable selection techniques has received a lot of attention and research
effort in recent years. Even for small size problems, variable selection has
been advocated to facilitate the interpretation of the clustering results. This
review provides a summary of the methods developed for variable selection in
model-based clustering. Existing R packages implementing the different methods
are indicated and illustrated in application to two data analysis examples.
",statistics
"  We present a new compressed representation of free trajectories of moving
objects. It combines a partial-sums-based structure that retrieves in constant
time the position of the object at any instant, with a hierarchical
minimum-bounding-boxes representation that allows determining if the object is
seen in a certain rectangular area during a time period. Combined with spatial
snapshots at regular intervals, the representation is shown to outperform
classical ones by orders of magnitude in space, and also to outperform previous
compressed representations in time performance, when using the same amount of
space.
",computer-science
"  JavaBIP allows the coordination of software components by clearly separating
the functional and coordination aspects of the system behavior. JavaBIP
implements the principles of the BIP component framework rooted in rigorous
operational semantics. Recent work both on BIP and JavaBIP allows the
coordination of static components defined prior to system deployment, i.e., the
architecture of the coordinated system is fixed in terms of its component
instances. Nevertheless, modern systems, often make use of components that can
register and deregister dynamically during system execution. In this paper, we
present an extension of JavaBIP that can handle this type of dynamicity. We use
first-order interaction logic to define synchronization constraints based on
component types. Additionally, we use directed graphs with edge coloring to
model dependencies among components that determine the validity of an online
system. We present the software architecture of our implementation, provide and
discuss performance evaluation results.
",computer-science
"  It is known that gas bubbles on the surface bounding a fluid flow can change
the coefficient of friction and affect the parameters of the boundary layer. In
this paper, we propose a method that allows us to create, in the near-wall
region, a thin layer of liquid filled with bubbles. It will be shown that if
there is an oscillating piezoelectric plate on the surface bounding a liquid,
then, under certain conditions, cavitation develops in the boundary layer. The
relationship between the parameters of cavitation and the characteristics of
the piezoelectric plate oscillations is obtained. Possible applications are
discussed.
",physics
"  In critical applications of anomaly detection including computer security and
fraud prevention, the anomaly detector must be configurable by the analyst to
minimize the effort on false positives. One important way to configure the
anomaly detector is by providing true labels for a few instances. We study the
problem of label-efficient active learning to automatically tune anomaly
detection ensembles and make four main contributions. First, we present an
important insight into how anomaly detector ensembles are naturally suited for
active learning. This insight allows us to relate the greedy querying strategy
to uncertainty sampling, with implications for label-efficiency. Second, we
present a novel formalism called compact description to describe the discovered
anomalies and show that it can also be employed to improve the diversity of the
instances presented to the analyst without loss in the anomaly discovery rate.
Third, we present a novel data drift detection algorithm that not only detects
the drift robustly, but also allows us to take corrective actions to adapt the
detector in a principled manner. Fourth, we present extensive experiments to
evaluate our insights and algorithms in both batch and streaming settings. Our
results show that in addition to discovering significantly more anomalies than
state-of-the-art unsupervised baselines, our active learning algorithms under
the streaming-data setup are competitive with the batch setup.
",statistics
"  Assume that $f:(\mathbb{C}^n,0) \to (\mathbb{C},0)$ is an analytic function
germ at the origin with only isolated singularity. Let $\mu$ and $\tau$ be the
corresponding Milnor and Tjurina numbers. We show that $\dfrac{\mu}{\tau} \leq
n$. As an application, we give a lower bound for the Tjurina number in terms of
$n$ and the multiplicity of $f$ at the origin.
",mathematics
"  We study the multi-armed bandit (MAB) problem where the agent receives a
vectorial feedback that encodes many possibly competing objectives to be
optimized. The goal of the agent is to find a policy, which can optimize these
objectives simultaneously in a fair way. This multi-objective online
optimization problem is formalized by using the Generalized Gini Index (GGI)
aggregation function. We propose an online gradient descent algorithm which
exploits the convexity of the GGI aggregation function, and controls the
exploration in a careful way achieving a distribution-free regret
$\tilde{\bigO} (T^{-1/2} )$ with high probability. We test our algorithm on
synthetic data as well as on an electric battery control problem where the goal
is to trade off the use of the different cells of a battery in order to balance
their respective degradation rates.
",computer-science
"  We introduce a new ferromagnetic model capable of reproducing one of the most
intriguing properties of collective behaviour in starling flocks, namely the
fact that strong collective order of the system coexists with scale-free
correlations of the modulus of the microscopic degrees of freedom, that is the
birds' speeds. The key idea of the new theory is that the single-particle
potential needed to bound the modulus of the microscopic degrees of freedom
around a finite value, is marginal, that is has zero curvature. We study the
model by using mean-field approximation and Monte Carlo simulations in three
dimensions, complemented by finite-size scaling analysis. While at the standard
critical temperature, $T_c$, the properties of the marginal model are exactly
the same as a normal ferromagnet with continuous symmetry-breaking, our results
show that a novel zero-temperature critical point emerges, so that in its
deeply ordered phase the marginal model develops divergent susceptibility and
correlation length of the modulus of the microscopic degrees of freedom, in
complete analogy with experimental data on natural flocks of starlings.
",quantitative-biology
"  We develop numerical tools for Diagrammatic Monte-Carlo simulations of
non-Abelian lattice field theories in the t'Hooft large-N limit based on the
weak-coupling expansion. First we note that the path integral measure of such
theories contributes a bare mass term in the effective action which is
proportional to the bare coupling constant. This mass term renders the
perturbative expansion infrared-finite and allows to study it directly in the
large-N and infinite-volume limits using the Diagrammatic Monte-Carlo approach.
On the exactly solvable example of a large-N O(N) sigma model in D=2 dimensions
we show that this infrared-finite weak-coupling expansion contains, in addition
to powers of bare coupling, also powers of its logarithm, reminiscent of
re-summed perturbation theory in thermal field theory and resurgent
trans-series without exponential terms. We numerically demonstrate the
convergence of these double series to the manifestly non-perturbative dynamical
mass gap. We then develop a Diagrammatic Monte-Carlo algorithm for sampling
planar diagrams in the large-N matrix field theory, and apply it to study this
infrared-finite weak-coupling expansion for large-N U(N)xU(N) nonlinear sigma
model (principal chiral model) in D=2. We sample up to 12 leading orders of the
weak-coupling expansion, which is the practical limit set by the increasingly
strong sign problem at high orders. Comparing Diagrammatic Monte-Carlo with
conventional Monte-Carlo simulations extrapolated to infinite N, we find a good
agreement for the energy density as well as for the critical temperature of the
""deconfinement"" transition. Finally, we comment on the applicability of our
approach to planar QCD at zero and finite density.
",physics
"  Unwanted variation can be highly problematic and so its detection is often
crucial. Relative log expression (RLE) plots are a powerful tool for
visualising such variation in high dimensional data. We provide a detailed
examination of these plots, with the aid of examples and simulation, explaining
what they are and what they can reveal. RLE plots are particularly useful for
assessing whether a procedure aimed at removing unwanted variation, i.e. a
normalisation procedure, has been successful. These plots, while originally
devised for gene expression data from microarrays, can also be used to reveal
unwanted variation in many other kinds of high dimensional data, where such
variation can be problematic.
",statistics
"  We introduce Casper, a proof of stake-based finality system which overlays an
existing proof of work blockchain. Casper is a partial consensus mechanism
combining proof of stake algorithm research and Byzantine fault tolerant
consensus theory. We introduce our system, prove some desirable features, and
show defenses against long range revisions and catastrophic crashes. The Casper
overlay provides almost any proof of work chain with additional protections
against block reversions.
",computer-science
"  A meticulous assessment of the risk of extreme environmental events is of
great necessity for populations, civil authorities as well as the
insurance/reinsurance industry. Koch (2017, 2018) introduced a concept of
spatial risk measure and a related set of axioms which are well-suited to
analyse and quantify the risk due to events having a spatial extent, precisely
such as natural disasters. In this paper, we first carry out a detailed study
of the correlation (and covariance) structure of powers of the Smith and
Brown-Resnick max-stable random fields. Then, using the latter results, we
thoroughly investigate spatial risk measures associated with variance and
induced by powers of max-stable random fields. In addition, we show that
spatial risk measures associated with several classical risk measures and
induced by such cost fields satisfy (at least) part of the previously mentioned
axioms under appropriate conditions on the max-stable fields. Considering such
cost fields is particularly relevant when studying the impact of extreme wind
speeds on buildings and infrastructure.
",quantitative-finance
"  We identify [Se III] 1.0994 micron in the planetary nebula (PN) NGC 5315 and
[Kr VI] 1.2330 micron in three PNe, from spectra obtained with the FIRE
spectrometer on the 6.5-m Baade Telescope. Se and Kr are the two most
widely-detected neutron-capture elements in astrophysical nebulae, and can be
enriched by s-process nucleosynthesis in PN progenitor stars. The detection of
[Se III] 1.0994 micron is particularly valuable when paired with observations
of [Se IV] 2.2858 micron, as it can be used to improve the accuracy of nebular
Se abundance determinations, and allows Se ionization correction factor (ICF)
schemes to be empirically tested for the first time. We present new effective
collision strength calculations for Se^{2+} and Kr^{5+}, which we use to
compute ionic abundances. In NGC 5315, we find that the Se abundance computed
from Se^{3+}/H^+ is lower than that determined with ICFs that incorporate
Se^{2+}/H^+. We compute new Kr ICFs that take Kr^{5+}/H^+ into account, by
fitting correlations found in grids of Cloudy models between Kr ionic fractions
and those of more abundant elements, and use these to derive Kr abundances in
four PNe. Observations of [Se III] and [Kr VI] in a larger sample of PNe, with
a range of excitation levels, are needed to rigorously test the ICF
prescriptions for Se and our new Kr ICFs.
",physics
"  Monocular 3D facial shape reconstruction from a single 2D facial image has
been an active research area due to its wide applications. Inspired by the
success of deep neural networks (DNN), we propose a DNN-based approach for
End-to-End 3D FAce Reconstruction (UH-E2FAR) from a single 2D image. Different
from recent works that reconstruct and refine the 3D face in an iterative
manner using both an RGB image and an initial 3D facial shape rendering, our
DNN model is end-to-end, and thus the complicated 3D rendering process can be
avoided. Moreover, we integrate in the DNN architecture two components, namely
a multi-task loss function and a fusion convolutional neural network (CNN) to
improve facial expression reconstruction. With the multi-task loss function, 3D
face reconstruction is divided into neutral 3D facial shape reconstruction and
expressive 3D facial shape reconstruction. The neutral 3D facial shape is
class-specific. Therefore, higher layer features are useful. In comparison, the
expressive 3D facial shape favors lower or intermediate layer features. With
the fusion-CNN, features from different intermediate layers are fused and
transformed for predicting the 3D expressive facial shape. Through extensive
experiments, we demonstrate the superiority of our end-to-end framework in
improving the accuracy of 3D face reconstruction.
",computer-science
"  We extend the work of Fouvry, Kowalski and Michel on correlation between
Hecke eigenvalues of modular forms and algebraic trace functions in order to
establish an asymptotic formula for a generalized cubic moment of modular
L-functions at the central point s = 1/2 and for prime moduli q. As an
application, we exploit our recent result on the mollification of the fourth
moment of Dirichlet L-functions to derive that for any pair
$(\omega_1,\omega_2)$ of multiplicative characters modulo q, there is a
positive proportion of $\chi$ (mod q) such that $L(\chi, 1/2 ), L(\chi\omega_1,
1/2 )$ and $L(\chi\omega_2, 1/2)$ are simultaneously not too small.
",mathematics
"  In this paper we establish the best constant of an anisotropic
Gagliardo-Nirenberg-type inequality related to the
Benjamin-Ono-Zakharov-Kuznetsov equation. As an application of our results, we
prove the uniform bound of solutions for such a equation in the energy space.
",mathematics
"  A new initiative from the International Swaps and Derivatives Association
(ISDA) aims to establish a ""Common Domain Model"" (ISDA CDM): a new standard for
data and process representation across the full range of derivatives
instruments. Design of the ISDA CDM is at an early stage and the draft
definition contains considerable complexity. This paper contributes by offering
insight, analysis and discussion relating to key topics in the design space
such as data lineage, timestamps, consistency, operations, events, state and
state transitions.
",computer-science
"  In multiferroic BiFeO$_3$ a cycloidal antiferromagnetic structure is coupled
to a large electric polarization at room temperature, giving rise to
magnetoelectric functionality that may be exploited in novel multiferroic-based
devices. In this paper, we demonstrate that by substituting samarium for 10% of
the bismuth ions the periodicity of the room temperature cycloid is increased,
and by cooling below $\sim15$ K the magnetic structure tends towards a simple
G-type antiferromagnet, which is fully established at 1.5 K. We show that this
transition results from $f-d$ exchange coupling, which induces a local
anisotropy on the iron magnetic moments that destroys the cycloidal order - a
result of general significance regarding the stability of non-collinear
magnetic structures in the presence of multiple magnetic sublattices.
",physics
"  We experimentally demonstrate a ring geometry all-fiber cavity system for
cavity quantum electrodynamics with an ensemble of cold atoms. The fiber cavity
contains a nanofiber section which mediates atom-light interactions through an
evanescent field. We observe well-resolved, vacuum Rabi splitting of the cavity
transmission spectrum in the weak driving limit due to a collective enhancement
of the coupling rate by the ensemble of atoms within the evanescent field, and
we present a simple theoretical model to describe this. In addition, we
demonstrate a method to control and stabilize the resonant frequency of the
cavity by utilizing the thermal properties of the nanofiber.
",physics
"  Cyclic data structures, such as cyclic lists, in functional programming are
tricky to handle because of their cyclicity. This paper presents an
investigation of categorical, algebraic, and computational foundations of
cyclic datatypes. Our framework of cyclic datatypes is based on second-order
algebraic theories of Fiore et al., which give a uniform setting for syntax,
types, and computation rules for describing and reasoning about cyclic
datatypes. We extract the ""fold"" computation rules from the categorical
semantics based on iteration categories of Bloom and Esik. Thereby, the rules
are correct by construction. We prove strong normalisation using the General
Schema criterion for second-order computation rules. Rather than the fixed
point law, we particularly choose Bekic law for computation, which is a key to
obtaining strong normalisation. We also prove the property of ""Church-Rosser
modulo bisimulation"" for the computation rules. Combining these results, we
have a remarkable decidability result of the equational theory of cyclic data
and fold.
",computer-science
"  Woodin has shown that if there is a measurable Woodin cardinal then there is,
in an appropriate sense, a sharp for the Chang model. We produce, in a weaker
sense, a sharp for the Chang model using only the existence of a cardinal
$\kappa$ having an extender of length $\kappa^{+\omega_1}$.
",mathematics
"  We discuss the distributed matching scheme in accelerators where control of
transverse beam phase space, oscillation, and transport is accomplished by
flexible distribution of focusing elements beyond dedicated matching sections.
Besides freeing accelerator design from fixed matching sections, such a scheme
has many operational advantages, and enables fluid optics manipulation not
possible in conventional schemes. Combined with an interpolation scheme this
can bring about a new paradigm for efficient, flexible, and robust optics
control. A rigorous and deterministic algorithm is developed for its
realization. The algorithm is a matching tool in its own right with unique
characteristics in robustness and determinism. The beam phase space dynamics is
naturally integrated into the algorithm, instead of being treated as generic
numerical parameters as in traditional schemes. It is applicable to a wider
range of problems, such as trading-off between competing options for desired
machine states.
",physics
"  Carmesin, Federici, and Georgakopoulos [arXiv:1603.06712] constructed a
transient hyperbolic graph that has no transient subtrees and that has the
Liouville property for harmonic functions. We modify their construction to get
a unimodular random graph with the same properties.
",mathematics
"  Parsing Expression Grammars (PEGs) are a formalism used to describe top-down
parsers with backtracking. As PEGs do not provide a good error recovery
mechanism, PEG-based parsers usually do not recover from syntax errors in the
input, or recover from syntax errors using ad-hoc, implementation-specific
features. The lack of proper error recovery makes PEG parsers unsuitable for
using with Integrated Development Environments (IDEs), which need to build
syntactic trees even for incomplete, syntactically invalid programs.
We propose a conservative extension, based on PEGs with labeled failures,
that adds a syntax error recovery mechanism for PEGs. This extension associates
recovery expressions to labels, where a label now not only reports a syntax
error but also uses this recovery expression to reach a synchronization point
in the input and resume parsing. We give an operational semantics of PEGs with
this recovery mechanism, and use an implementation based on such semantics to
build a robust parser for the Lua language. We evaluate the effectiveness of
this parser, alone and in comparison with a Lua parser with automatic error
recovery generated by ANTLR, a popular parser generator.
",computer-science
"  Experiments are reported on the performance of a pitching and heaving
two-dimensional foil in a water channel in either continuous or intermittent
motion. We find that the thrust and power are independent of the mean
freestream velocity for two-fold changes in the mean velocity (four-fold in the
dynamic pressure), and for oscillations in the velocity up to 38\% of the mean,
where the oscillations are intended to mimic those of freely swimming motions
where the thrust varies during the flapping cycle. We demonstrate that the
correct velocity scale is not the flow velocity but the mean velocity of the
trailing edge. We also find little or no impact of streamwise velocity change
on the wake characteristics such as vortex organization, vortex strength, and
time-averaged velocity profile development---the wake is both qualitatively and
quantitatively unchanged. Our results suggest that constant velocity studies
can be used to make robust conclusions about swimming performance without a
need to explore the free-swimming condition.
",physics
"  We propose SoaAlloc, a dynamic object allocator for Single-Method
Multiple-Objects applications in CUDA. SoaAlloc is the first allocator for GPUs
that (a) arranges allocations in a SIMD-friendly Structure of Arrays (SOA) data
layout, (b) provides a do-all operation for maximizing the benefit of SOA, and
(c) is on par with state-of-the-art memory allocators for raw (de)allocation
time. Our benchmarks show that the SOA layout leads to significantly better
memory bandwidth utilization, resulting in a 2x speedup of application code.
",computer-science
"  Understanding patterns of demand is fundamental for fleet management of bike
sharing systems. In this paper we analyze data from the Divvy system of the
city of Chicago. We show that the demand of bicycles can be modeled as a
multivariate temporal point process, with each dimension corresponding to a
bike station in the network. The availability of daily replications of the
process allows nonparametric estimation of the intensity functions, even for
stations with low daily counts, and straightforward estimation of pairwise
correlations between stations. These correlations are then used for clustering,
revealing different patterns of bike usage.
",statistics
"  We propose a machine-learning method for evaluating the potential barrier
governing atomic transport based on the preferential selection of dominant
points for the atomic transport. The proposed method generates numerous random
samples of the entire potential energy surface (PES) from a probabilistic
Gaussian process model of the PES, which enables defining the likelihood of the
dominant points. The robustness and efficiency of the method are demonstrated
on a dozen model cases for proton diffusion in oxides, in comparison with a
conventional nudge elastic band method.
",physics
"  This is the second in a series of papers where we construct an invariant of a
four-dimensional piecewise linear manifold $M$ with a given middle cohomology
class $h\in H^2(M,\mathbb C)$. This invariant is the square root of the torsion
of unusual chain complex introduced in Part I (arXiv:1605.06498) of our work,
multiplied by a correcting factor. Here we find this factor by studying the
behavior of our construction under all four-dimensional Pachner moves, and show
that it can be represented in a multiplicative form: a product of same-type
multipliers over all 2-faces, multiplied by a product of same-type multipliers
over all pentachora.
",mathematics
"  We investigate prime character degree graphs of solvable groups that have six
vertices. There are one hundred twelve non-isomorphic connected graphs with six
vertices, of which all except nine are classified in this paper. We also
completely classify the disconnected graphs with six vertices.
",mathematics
"  Spatio-temporal data and processes are prevalent across a wide variety of
scientific disciplines. These processes are often characterized by nonlinear
time dynamics that include interactions across multiple scales of spatial and
temporal variability. The data sets associated with many of these processes are
increasing in size due to advances in automated data measurement, management,
and numerical simulator output. Non- linear spatio-temporal models have only
recently seen interest in statistics, but there are many classes of such models
in the engineering and geophysical sciences. Tradi- tionally, these models are
more heuristic than those that have been presented in the statistics
literature, but are often intuitive and quite efficient computationally. We
show here that with fairly simple, but important, enhancements, the echo state
net- work (ESN) machine learning approach can be used to generate long-lead
forecasts of nonlinear spatio-temporal processes, with reasonable uncertainty
quantification, and at only a fraction of the computational expense of a
traditional parametric nonlinear spatio-temporal models.
",statistics
"  We show that Variational Autoencoders consistently fail to learn marginal
distributions in latent and visible space. We ask whether this is a consequence
of matching conditional distributions, or a limitation of explicit model and
posterior distributions. We explore alternatives provided by marginal
distribution matching and implicit distributions through the use of Generative
Adversarial Networks in variational inference. We perform a large-scale
evaluation of several VAE-GAN hybrids and explore the implications of class
probability estimation for learning distributions. We conclude that at present
VAE-GAN hybrids have limited applicability: they are harder to scale, evaluate,
and use for inference compared to VAEs; and they do not improve over the
generation quality of GANs.
",statistics
"  We present a study on the impact of Mn$^{3+}$ substitution in the
geometrically frustrated Ising garnet Ho$_3$Ga$_5$O$_{12}$ using bulk magnetic
measurements and low temperature powder neutron diffraction. We find that the
transition temperature, $T_N$ = 5.8 K, for Ho$_3$MnGa$_4$O$_{12}$ is raised by
almost 20 when compared to Ho$_3$Ga$_5$O$_{12}$. Powder neutron diffraction on
Ho$_3$Mn$_x$Ga$_{5-x}$O$_{12}$ ($x$ = 0.5, 1) below $T_N$ shows the formation
of a long range ordered ordered state with $\mathbf{k}$ = (0,0,0). Ho$^{3+}$
spins are aligned antiferromagnetically along the six crystallographic axes
with no resultant moment while the Mn$^{3+}$ spins are oriented along the body
diagonals, such that there is a net moment along [111]. The magnetic structure
can be visualised as ten-membered rings of corner-sharing triangles of
Ho$^{3+}$ spins with the Mn$^{3+}$ spins ferromagnetically coupled to each
individual Ho$^{3+}$ spin in the triangle. Substitution of Mn$^{3+}$ completely
relieves the magnetic frustration with $f = \theta_{CW}/T_N \approx 1.1$ for
Ho$_3$MnGa$_4$O$_{12}$.
",physics
"  We consider multi-time correlators for output signals from linear detectors,
continuously measuring several qubit observables at the same time. Using the
quantum Bayesian formalism, we show that for unital (symmetric) evolution in
the absence of phase backaction, an $N$-time correlator can be expressed as a
product of two-time correlators when $N$ is even. For odd $N$, there is a
similar factorization, which also includes a single-time average. Theoretical
predictions agree well with experimental results for two detectors, which
simultaneously measure non-commuting qubit observables.
",physics
"  We introduce a metric of mutual energy for adelic measures associated to the
Arakelov-Zhang pairing. Using this metric and potential theoretic techniques
involving discrete approximations to energy integrals, we prove an effective
bound on a problem of Baker and DeMarco on unlikely intersections of dynamical
systems, specifically, for the set of complex parameters $c$ for which $z=0$
and $1$ are both preperiodic under iteration of $f_c(z)=z^2 + c$.
",mathematics
"  This work is motivated by the problem of testing for differences in the mean
electricity prices before and after Germany's abrupt nuclear phaseout after the
nuclear disaster in Fukushima Daiichi, Japan, in mid-March 2011. Taking into
account the nature of the data and the auction design of the electricity
market, we approach this problem using a Local Linear Kernel (LLK) estimator
for the nonparametric mean function of sparse covariate-adjusted functional
data. We build upon recent theoretical work on the LLK estimator and propose a
two-sample test statistics using a finite sample correction to avoid size
distortions. Our nonparametric test results on the price differences point to a
Simpson's paradox explaining an unexpected result recently reported in the
literature.
",statistics
"  Deep convolutional neural networks (CNN) based solutions are the current
state- of-the-art for computer vision tasks. Due to the large size of these
models, they are typically run on clusters of CPUs or GPUs. However, power
requirements and cost budgets can be a major hindrance in adoption of CNN for
IoT applications. Recent research highlights that CNN contain significant
redundancy in their structure and can be quantized to lower bit-width
parameters and activations, while maintaining acceptable accuracy. Low
bit-width and especially single bit-width (binary) CNN are particularly
suitable for mobile applications based on FPGA implementation, due to the
bitwise logic operations involved in binarized CNN. Moreover, the transition to
lower bit-widths opens new avenues for performance optimizations and model
improvement. In this paper, we present an automatic flow from trained
TensorFlow models to FPGA system on chip implementation of binarized CNN. This
flow involves quantization of model parameters and activations, generation of
network and model in embedded-C, followed by automatic generation of the FPGA
accelerator for binary convolutions. The automated flow is demonstrated through
implementation of binarized ""YOLOV2"" on the low cost, low power Cyclone- V FPGA
device. Experiments on object detection using binarized YOLOV2 demonstrate
significant performance benefit in terms of model size and inference speed on
FPGA as compared to CPU and mobile CPU platforms. Furthermore, the entire
automated flow from trained models to FPGA synthesis can be completed within
one hour.
",computer-science
"  Elucidating the interaction between magnetic moments and itinerant carriers
is an important step to spintronic applications. Here, we investigate magnetic
and transport properties in d0 ferromagnetic SiC single crystals prepared by
postimplantation pulsed laser annealing. Magnetic moments are contributed by
the p states of carbon atoms, but their magnetic circular dichroism is
different from that in semi-insulating SiC samples. The anomalous Hall effect
and negative magnetoresistance indicate the influence of d0 spin order on free
carriers. The ferromagnetism is relatively weak in N-implanted SiC compared
with that in Al-implanted SiC after annealing. The results suggest that d0
magnetic moments and itinerant carriers can interact with each other, which
will facilitate the development of SiC spintronic devices with d0
ferromagnetism.
",physics
"  We introduce recurrent additive networks (RANs), a new gated RNN which is
distinguished by the use of purely additive latent state updates. At every time
step, the new state is computed as a gated component-wise sum of the input and
the previous state, without any of the non-linearities commonly used in RNN
transition dynamics. We formally show that RAN states are weighted sums of the
input vectors, and that the gates only contribute to computing the weights of
these sums. Despite this relatively simple functional form, experiments
demonstrate that RANs perform on par with LSTMs on benchmark language modeling
problems. This result shows that many of the non-linear computations in LSTMs
and related networks are not essential, at least for the problems we consider,
and suggests that the gates are doing more of the computational work than
previously understood.
",computer-science
"  Capabilities of detecting temporal relations between two events can benefit
many applications. Most of existing temporal relation classifiers were trained
in a supervised manner. Instead, we explore the observation that regular event
pairs show a consistent temporal relation despite of their various contexts,
and these rich contexts can be used to train a contextual temporal relation
classifier, which can further recognize new temporal relation contexts and
identify new regular event pairs. We focus on detecting after and before
temporal relations and design a weakly supervised learning approach that
extracts thousands of regular event pairs and learns a contextual temporal
relation classifier simultaneously. Evaluation shows that the acquired regular
event pairs are of high quality and contain rich commonsense knowledge and
domain specific knowledge. In addition, the weakly supervised trained temporal
relation classifier achieves comparable performance with the state-of-the-art
supervised systems.
",computer-science
"  The photoelectric effect established by Einstein is well known, which
indicates that electrons on lower energy levels can jump up to higher levels by
absorbing photons, or jump down from higher levels to lower levels and give out
photons1-3. However, how do photons act on electrons and further on atoms have
kept unknown up to now. Here we show the results that photons collide on
electrons with energy-transmission in semiconductors and pass their momenta to
electrons, which make the electrons jump up from lower energy levels to higher
levels. We found that (i) photons have rest mass of 7.287exp(-38) kg and
2.886exp(-35) kg, in vacuum and silicon respectively; (ii) excited by photons
with energy of 1.12eV, electrons in silicon may jump up from the top of valance
band to the bottom of conduction band with initial speed of 2.543exp(3) m/s and
taking time of 4.977exp(-17) s; (iii) acted by photons with energy of 4.6eV,
the atoms who lose electrons may be catapulted out of the semiconductors by the
extruded neighbor atoms, and taking time of 2.224exp(-15) s. These results make
reasonable explanation to rapid thermal annealing, laser ablation and laser
cutting.
",physics
"  We propose a simple and generic layer formulation that extends the properties
of convolutional layers to any domain that can be described by a graph. Namely,
we use the support of its adjacency matrix to design learnable weight sharing
filters able to exploit the underlying structure of signals in the same fashion
as for images. The proposed formulation makes it possible to learn the weights
of the filter as well as a scheme that controls how they are shared across the
graph. We perform validation experiments with image datasets and show that
these filters offer performances comparable with convolutional ones.
",computer-science
"  Models in econophysics, i.e., the emerging field of statistical physics that
applies the main concepts of traditional physics to economics, typically
consist of large systems of economic agents who are characterized by the amount
of money they have. In the simplest model, at each time step, one agent gives
one dollar to another agent, with both agents being chosen independently and
uniformly at random from the system. Numerical simulations of this model
suggest that, at least when the number of agents and the average amount of
money per agent are large, the distribution of money converges to an
exponential distribution reminiscent of the Boltzmann-Gibbs distribution of
energy in physics. The main objective of this paper is to give a rigorous proof
of this result and show that the convergence to the exponential distribution is
universal in the sense that it holds more generally when the economic agents
are located on the vertices of a connected graph and interact locally with
their neighbors rather than globally with all the other agents. We also study a
closely related model where, at each time step, agents buy with a probability
proportional to the amount of money they have, and prove that in this case the
limiting distribution of money is Poissonian.
",mathematics
"  A variety of energy resources has been identified as being flexible in their
electric energy consumption or generation. This energetic flexibility can be
used for various purposes such as minimizing energy procurement costs or
providing ancillary services to power grids. To fully leverage the flexibility
available from distributed small-scale resources, their flexibility must be
quantified and aggregated.
This paper introduces a generic and scalable approach for flexible energy
systems to quantitatively describe and price their flexibility based on
zonotopic sets. The description proposed allows aggregators to efficiently pool
the flexibility of large numbers of systems and to make control and market
decisions on the aggregate level. In addition, an algorithm is presented that
distributes aggregate-level control decisions among the individual systems of
the pool in an economically fair and computationally efficient way. Finally, it
is shown how the zonotopic description of flexibility enables an efficient
computation of aggregate regulation power bid-curves.
",computer-science
"  In the setting of the pi-calculus with binary sessions, we aim at relaxing
the notion of duality of session types by the concept of retractable compliance
developed in contract theory. This leads to extending session types with a new
type operator of ""speculative selection"" including choices not necessarily
offered by a compliant partner. We address the problem of selecting successful
communicating branches by means of an operational semantics based on
orchestrators, which has been shown to be equivalent to the retractable
semantics of contracts, but clearly more feasible. A type system, sound with
respect to such a semantics, is hence provided.
",computer-science
"  Confluence of a nondeterministic program ensures a functional input-output
relation, freeing the programmer from considering the actual scheduling
strategy, and allowing optimized and perhaps parallel implementations. The more
general property of confluence modulo equivalence ensures that equivalent
inputs are related to equivalent outputs, that need not be identical.
Confluence under invariants is also considered. Constraint Handling Rules (CHR)
is an important example of a rewrite based logic programming language, and we
aim at a mechanizable method for proving confluence modulo equivalence of
terminating programs. While earlier approaches to confluence for CHR programs
concern an idealized logic subset, we refer to a semantics compatible with
standard Prolog-based implementations. We specify a meta-level constraint
language in which invariants and equivalences can be expressed and manipulated,
extending our previous theoretical results towards a practical implementation.
",computer-science
"  In this paper we introduce a new framework to detect elephant flows at very
high speed rates and under uncertainty. The framework provides exact
mathematical formulas to compute the detection likelihood and introduces a new
flow reconstruction lemma under partial information. These theoretical results
lead to the design of BubbleCache, a new elephant flow detection algorithm
designed to operate near the optimal tradeoff between computational scalability
and accuracy by dynamically tracking the traffic's natural cutoff sampling
rate. We demonstrate on a real world 100 Gbps network that the BubbleCache
algorithm helps reduce the computational cost by a factor of 1000 and the
memory requirements by a factor of 100 while detecting the top flows on the
network with very high probability.
",computer-science
"  This note contains a reformulation of the Hodge index theorem within the
framework of Atiyah's $L^2$-index theory. More precisely, given a compact
Kähler manifold $(M,h)$ of even complex dimension $2m$, we prove that
$$\sigma(M)=\sum_{p,q=0}^{2m}(-1)^ph_{(2),\Gamma}^{p,q}(M)$$ where $\sigma(M)$
is the signature of $M$ and $h_{(2),\Gamma}^{p,q}(M)$ are the $L^2$-Hodge
numbers of $M$ with respect to a Galois covering having $\Gamma$ as group of
Deck transformations. Likewise we also prove an $L^2$-version of the
Frölicher index theorem. Afterwards we give some applications of these two
theorems and finally we conclude this paper by collecting other properties of
the $L^2$-Hodge numbers.
",mathematics
"  An additive fast Fourier transform over a finite field of characteristic two
efficiently evaluates polynomials at every element of an $\mathbb{F}_2$-linear
subspace of the field. We view these transforms as performing a change of basis
from the monomial basis to the associated Lagrange basis, and consider the
problem of performing the various conversions between these two bases, the
associated Newton basis, and the '' novel '' basis of Lin, Chung and Han (FOCS
2014). Existing algorithms are divided between two families, those designed for
arbitrary subspaces and more efficient algorithms designed for specially
constructed subspaces of fields with degree equal to a power of two. We
generalise techniques from both families to provide new conversion algorithms
that may be applied to arbitrary subspaces, but which benefit equally from the
specially constructed subspaces. We then construct subspaces of fields with
smooth degree for which our algorithms provide better performance than existing
algorithms.
",computer-science
"  As part of a large investigation with Hubble Space Telescope to study the
faintest stars within the globular cluster Omega Centauri, in this work we
present early results on the multiplicity of its main sequence (MS) stars,
based on deep optical and near-infrared observations. By using appropriate
color-magnitude diagrams we have identified, for the first time, the two main
stellar populations I, and II along the entire MS, from the turn-off towards
the hydrogen-burning limit. We have compared the observations with suitable
synthetic spectra of MS stars and conclude that the two MSs are consistent with
stellar populations with different metallicity, helium, and light-element
abundance. Specifically, MS-I corresponds to a metal-poor stellar population
([Fe/H]~-1.7) with Y~ 0.25 and [O/Fe]~0.30. The MS-II hosts helium-rich
(Y~0.37-0.40) stars with metallicity ranging from [Fe/H]~-1.7 to -1.4. Below
the MS knee (mF160W~19.5, our photometry reveals that each of the two main MSs
hosts stellar subpopulations with different oxygen abundances, with very O-poor
stars ([O/Fe]~-0.5) populating the MS-II. Such a complexity has never been
observed in previous studies of M-dwarfs in globular clusters. A few months
before the lunch of the James Webb Space Telescope, these results demonstrate
the power of optical and near-infrared photometry in the study of multiple
stellar populations in globular clusters.
",physics
"  In this paper, we propose a new approach to Cwikel estimates both for the
Euclidean space and for the noncommutative Euclidean space.
",mathematics
"  We revisit the question of reducing online learning to approximate
optimization of the offline problem. In this setting, we give two algorithms
with near-optimal performance in the full information setting: they guarantee
optimal regret and require only poly-logarithmically many calls to the
approximation oracle per iteration. Furthermore, these algorithms apply to the
more general improper learning problems. In the bandit setting, our algorithm
also significantly improves the best previously known oracle complexity while
maintaining the same regret.
",statistics
"  Recent studies show that the fast growing expansion of wind power generation
may lead to extremely high levels of price volatility in wholesale electricity
markets. Storage technologies, regardless of their specific forms e.g.
pump-storage hydro, large-scale or distributed batteries, are capable of
alleviating the extreme price volatility levels due to their energy usage time
shifting, fast-ramping and price arbitrage capabilities. In this paper, we
propose a stochastic bi-level optimization model to find the optimal nodal
storage capacities required to achieve a certain price volatility level in a
highly volatile electricity market. The decision on storage capacities is made
in the upper level problem and the operation of strategic/regulated generation,
storage and transmission players is modeled at the lower level problem using an
extended Cournot-based stochastic game. The South Australia (SA) electricity
market, which has recently experienced high levels of price volatility, is
considered as the case study for the proposed storage allocation framework. Our
numerical results indicate that 80% price volatility reduction in SA
electricity market can be achieved by installing either 340 MWh regulated
storage or 420 MWh strategic storage. In other words, regulated storage firms
are more efficient in reducing the price volatility than strategic storage
firms.
",mathematics
"  The $\mathbb{Z}_2$ topological phase in the quantum dimer model on the
Kagomé-lattice is a candidate for the description of the low-energy physics
of the anti-ferromagnetic Heisenberg model on the same lattice. We study the
extend of the topological phase by interpolating between the exactly solvable
parent Hamiltonian of the topological phase and an effective low-energy
description of the Heisenberg model in terms of a quantum-dimer Hamiltonian.
Therefore, we perform a perturbative treatment of the low-energy excitations in
the topological phase including free and interacting quasi-particles. We find a
phase transition out of the topological phase far from the Heisenberg point.
The resulting phase is characterized by a spontaneously broken rotational
symmetry and a unit cell involving six sites.
",physics
"  We analyze the charge- and spin response functions of rare-earth nickelates
RNiO3 and their heterostructures using random-phase approximation in a two-band
Hubbard model. The inter-orbital charge fluctuation is found to be the driving
mechanism for the rock-salt type bond order in bulk RNiO3, and good agreement
of the ordering temperature with experimental values is achieved for all RNiO3
using realistic crystal structures and interaction parameters. We further show
that magnetic ordering in bulk is not driven by the spin fluctuation and should
be instead explained as ordering of localized moments. This picture changes for
low-dimensional heterostructures, where the charge fluctuation is suppressed
and overtaken by the enhanced spin instability, which results in a
spin-density-wave ground state observed in recent experiments. Predictions for
spectroscopy allow for further experimental testing of our claims.
",physics
"  Two dimensional (2D) materials provide a unique platform for spintronics and
valleytronics due to the ability to combine vastly different functionalities
into one vertically-stacked heterostructure, where the strengths of each of the
constituent materials can compensate for the weaknesses of the others. Graphene
has been demonstrated to be an exceptional material for spin transport at room
temperature, however it lacks a coupling of the spin and optical degrees of
freedom. In contrast, spin/valley polarization can be efficiently generated in
monolayer transition metal dichalcogenides (TMD) such as MoS2 via absorption of
circularly-polarized photons, but lateral spin or valley transport has not been
realized at room temperature. In this letter, we fabricate monolayer
MoS2/few-layer graphene hybrid spin valves and demonstrate, for the first time,
the opto-valleytronic spin injection across a TMD/graphene interface. We
observe that the magnitude and direction of spin polarization is controlled by
both helicity and photon energy. In addition, Hanle spin precession
measurements confirm optical spin injection, spin transport, and electrical
detection up to room temperature. Finally, analysis by a one-dimensional
drift-diffusion model quantifies the optically injected spin current and the
spin transport parameters. Our results demonstrate a 2D spintronic/valleytronic
system that achieves optical spin injection and lateral spin transport at room
temperature in a single device, which paves the way for multifunctional 2D
spintronic devices for memory and logic applications.
",physics
"  Let $G$ be a reductive algebraic group over a $p$-adic field or number field
$K$, and let $V$ be a $K$-linear faithful representation of $G$. A lattice
$\Lambda$ in the vector space $V$ defines a model $\hat{G}_{\Lambda}$ of $G$
over $\mathscr{O}_K$. One may wonder to what extent $\Lambda$ is determined by
the group scheme $\hat{G}_{\Lambda}$. In this paper we prove that up to a
natural equivalence relation on the set of lattices there are only finitely
many $\Lambda$ corresponding to one model $\hat{G}_{\Lambda}$. Furthermore, we
relate this fact to moduli spaces of abelian varieties as follows: let
$\mathscr{A}_{g,n}$ be the moduli space of principally polarised abelian
varieties of dimension $g$ with level $n$ structure. We prove that there are at
most finitely many special subvarieties of $\mathscr{A}_{g,n}$ with a given
integral generic Mumford-Tate group.
",mathematics
"  We present a simple model for the development of shear layers between
parallel flows in confining channels. Such flows are important across a wide
range of topics from diffusers, nozzles and ducts to urban air flow and
geophysical fluid dynamics. The model approximates the flow in the shear layer
as a linear profile separating uniform-velocity streams. Both the channel
geometry and wall drag affect the development of the flow. The model shows good
agreement with both particle-image-velocimetry experiments and computational
turbulence modelling. The low computational cost of the model allows it to be
used for design purposes, which we demonstrate by investigating optimal
pressure recovery in diffusers with non-uniform inflow.
",physics
"  Recent literature in the robotics community has focused on learning robot
behaviors that abstract out lower-level details of robot control. To fully
leverage the efficacy of such behaviors, it is necessary to select and sequence
them to achieve a given task. In this paper, we present an approach to both
learn and sequence robot behaviors, applied to the problem of visual navigation
of mobile robots. We construct a layered representation of control policies
composed of low- level behaviors and a meta-level policy. The low-level
behaviors enable the robot to locomote in a particular environment while
avoiding obstacles, and the meta-level policy actively selects the low-level
behavior most appropriate for the current situation based purely on visual
feedback. We demonstrate the effectiveness of our method on three simulated
robot navigation tasks: a legged hexapod robot which must successfully traverse
varying terrain, a wheeled robot which must navigate a maze-like course while
avoiding obstacles, and finally a wheeled robot navigating in the presence of
dynamic obstacles. We show that by learning control policies in a layered
manner, we gain the ability to successfully traverse new compound environments
composed of distinct sub-environments, and outperform both the low-level
behaviors in their respective sub-environments, as well as a hand-crafted
selection of low-level policies on these compound environments.
",computer-science
"  We show that the patterns in the Abelian sandpile are stable. The proof
combines the structure theory for the patterns with the regularity machinery
for non-divergence form elliptic equations. The stability results allows one to
improve weak-* convergence of the Abelian sandpile to pattern convergence for
certain classes of solutions.
",mathematics
"  Objective: To evaluate unsupervised clustering methods for identifying
individual-level behavioral-clinical phenotypes that relate personal biomarkers
and behavioral traits in type 2 diabetes (T2DM) self-monitoring data. Materials
and Methods: We used hierarchical clustering (HC) to identify groups of meals
with similar nutrition and glycemic impact for 6 individuals with T2DM who
collected self-monitoring data. We evaluated clusters on: 1) correspondence to
gold standards generated by certified diabetes educators (CDEs) for 3
participants; 2) face validity, rated by CDEs, and 3) impact on CDEs' ability
to identify patterns for another 3 participants. Results: Gold standard (GS)
included 9 patterns across 3 participants. Of these, all 9 were re-discovered
using HC: 4 GS patterns were consistent with patterns identified by HC (over
50% of meals in a cluster followed the pattern); another 5 were included as
sub-groups in broader clusers. 50% (9/18) of clusters were rated over 3 on
5-point Likert scale for validity, significance, and being actionable. After
reviewing clusters, CDEs identified patterns that were more consistent with
data (70% reduction in contradictions between patterns and participants'
records). Discussion: Hierarchical clustering of blood glucose and
macronutrient consumption appears suitable for discovering behavioral-clinical
phenotypes in T2DM. Most clusters corresponded to gold standard and were rated
positively by CDEs for face validity. Cluster visualizations helped CDEs
identify more robust patterns in nutrition and glycemic impact, creating new
possibilities for visual analytic solutions. Conclusion: Machine learning
methods can use diabetes self-monitoring data to create personalized
behavioral-clinical phenotypes, which may prove useful for delivering
personalized medicine.
",statistics
"  The transverse momentum ($p_T$) spectra from heavy-ion collisions at
intermediate momenta are described by non-extensive statistical models.
Assuming a fixed relative variance of the temperature fluctuating event by
event or alternatively a fixed mean multiplicity in a negative binomial
distribution (NBD), two different linear relations emerge between the
temperature, $T$, and the Tsallis parameter $q-1$. Our results qualitatively
agree with that of G.~Wilk. Furthermore we revisit the ""Soft+Hard"" model,
proposed recently by G.~G.~Barnaföldi \textit{et.al.}, by a $T$-independent
average $p_T^2$ assumption. Finally we compare results with those predicted by
another deformed distribution, using Kaniadakis' $\kappa$ parametrization.
",physics
"  Latent Dirichlet Allocation (LDA) models trained without stopword removal
often produce topics with high posterior probabilities on uninformative words,
obscuring the underlying corpus content. Even when canonical stopwords are
manually removed, uninformative words common in that corpus will still dominate
the most probable words in a topic. In this work, we first show how the
standard topic quality measures of coherence and pointwise mutual information
act counter-intuitively in the presence of common but irrelevant words, making
it difficult to even quantitatively identify situations in which topics may be
dominated by stopwords. We propose an additional topic quality metric that
targets the stopword problem, and show that it, unlike the standard measures,
correctly correlates with human judgements of quality. We also propose a
simple-to-implement strategy for generating topics that are evaluated to be of
much higher quality by both human assessment and our new metric. This approach,
a collection of informative priors easily introduced into most LDA-style
inference methods, automatically promotes terms with domain relevance and
demotes domain-specific stop words. We demonstrate this approach's
effectiveness in three very different domains: Department of Labor accident
reports, online health forum posts, and NIPS abstracts. Overall we find that
current practices thought to solve this problem do not do so adequately, and
that our proposal offers a substantial improvement for those interested in
interpreting their topics as objects in their own right.
",computer-science
"  Applying a many mode Floquet formalism for magnetically trapped atoms
interacting with a polychromatic rf-field, we predict a large two photon
transition probability in the atomic system of cold $^{87}Rb$ atoms. The
physical origin of this enormous increase in the two photon transition
probability is due to the formation of avoided crossings between eigen-energy
levels originating from different Floquet sub-manifolds and redistribution of
population in the resonant intermediate levels to give rise to the resonance
enhancement effect. Other exquisite features of the studied atom-field
composite system include the splitting of the generated avoided crossings at
the strong field strength limit and a periodic variation of the single and two
photon transition probabilities with the mode separation frequency of the
polychromatic rf-field. This work can find applications to characterize
properties of cold atom clouds in the magnetic traps using rf-spectroscopy
techniques.
",physics
"  We prove that indecomposable $\Sigma$-pure-injective modules for a string
algebra are string or band modules. The key step in our proof is a splitting
result for infinite-dimensional linear relations.
",mathematics
"  Learning to optimize - the idea that we can learn from data algorithms that
optimize a numerical criterion - has recently been at the heart of a growing
number of research efforts. One of the most challenging issues within this
approach is to learn a policy that is able to optimize over classes of
functions that are fairly different from the ones that it was trained on. We
propose a novel way of framing learning to optimize as a problem of learning a
good navigation policy on a partially observable loss surface. To this end, we
develop Rover Descent, a solution that allows us to learn a fairly broad
optimization policy from training on a small set of prototypical
two-dimensional surfaces that encompasses the classically hard cases such as
valleys, plateaus, cliffs and saddles and by using strictly zero-order
information. We show that, without having access to gradient or curvature
information, we achieve state-of-the-art convergence speed on optimization
problems not presented at training time such as the Rosenbrock function and
other hard cases in two dimensions. We extend our framework to optimize over
high dimensional landscapes, while still handling only two-dimensional local
landscape information and show good preliminary results.
",statistics
"  Dust devils are likely the dominant source of dust for the martian
atmosphere, but the amount and frequency of dust-lifting depend on the
statistical distribution of dust devil parameters. Dust devils exhibit pressure
perturbations and, if they pass near a barometric sensor, they may register as
a discernible dip in a pressure time-series. Leveraging this fact, several
surveys using barometric sensors on landed spacecraft have revealed dust devil
structures and occurrence rates. However powerful they are, though, such
surveys suffer from non-trivial biases that skew the inferred dust devil
properties. For example, such surveys are most sensitive to dust devils with
the widest and deepest pressure profiles, but the recovered profiles will be
distorted, broader and shallow than the actual profiles. In addition, such
surveys often do not provide wind speed measurements alongside the pressure
time series, and so the durations of the dust devil signals in the time series
cannot be directly converted to profile widths. Fortunately, simple statistical
and geometric considerations can de-bias these surveys, allowing conversion of
the duration of dust devil signals into physical widths, given only a
distribution of likely translation velocities, and the recovery of the
underlying distributions of physical parameters. In this study, we develop a
scheme for de-biasing such surveys. Applying our model to an in-situ survey
using data from the Phoenix lander suggests a larger dust flux and a dust devil
occurrence rate about ten times larger than previously inferred. Comparing our
results to dust devil track surveys suggests only about one in five
low-pressure cells lifts sufficient dust to leave a visible track.
",physics
"  In this paper we study the cubic fractional nonlinear Schrodinger equation
(NLS) on the torus and on the real line. Combining the normal form and the
restricted norm methods we prove that the nonlinear part of the solution is
smoother than the initial data. Our method applies to both focusing and
defocusing nonlinearities. In the case of full dispersion (NLS) and on the
torus, the gain is a full derivative, while on the real line we get a
derivative smoothing with an $\epsilon$ loss. Our result lowers the regularity
requirement of a recent theorem of Kappeler et al. on the periodic defocusing
cubic NLS, and extends it to the focusing case and to the real line. We also
obtain estimates on the higher order Sobolev norms of the global smooth
solutions in the defocusing case.
",mathematics
"  This paper studies the numerical approximation of solution of the Dirichlet
problem for the fully nonlinear Monge-Ampere equation. In this approach, we
take the advantage of reformulation the Monge-Ampere problem as an optimization
problem, to which we associate a well defined functional whose minimum provides
us with the solution to the Monge-Ampere problem after resolving a Poisson
problem by the finite element Galerkin method. We present some numerical
examples, for which a good approximation is obtained in 68 iterations.
",mathematics
"  The timed pattern matching problem is an actively studied topic because of
its relevance in monitoring of real-time systems. There one is given a log $w$
and a specification $\mathcal{A}$ (given by a timed word and a timed automaton
in this paper), and one wishes to return the set of intervals for which the log
$w$, when restricted to the interval, satisfies the specification
$\mathcal{A}$. In our previous work we presented an efficient timed pattern
matching algorithm: it adopts a skipping mechanism inspired by the classic
Boyer--Moore (BM) string matching algorithm. In this work we tackle the problem
of online timed pattern matching, towards embedded applications where it is
vital to process a vast amount of incoming data in a timely manner.
Specifically, we start with the Franek-Jennings-Smyth (FJS) string matching
algorithm---a recent variant of the BM algorithm---and extend it to timed
pattern matching. Our experiments indicate the efficiency of our FJS-type
algorithm in online and offline timed pattern matching.
",computer-science
"  Machine learning models benefit from large and diverse datasets. Using such
datasets, however, often requires trusting a centralized data aggregator. For
sensitive applications like healthcare and finance this is undesirable as it
could compromise patient privacy or divulge trade secrets. Recent advances in
secure and privacy-preserving computation, including trusted hardware enclaves
and differential privacy, offer a way for mutually distrusting parties to
efficiently train a machine learning model without revealing the training data.
In this work, we introduce Myelin, a deep learning framework which combines
these privacy-preservation primitives, and use it to establish a baseline level
of performance for fully private machine learning.
",statistics
"  In this paper, we propose an unsupervised face clustering algorithm called
""Proximity-Aware Hierarchical Clustering"" (PAHC) that exploits the local
structure of deep representations. In the proposed method, a similarity measure
between deep features is computed by evaluating linear SVM margins. SVMs are
trained using nearest neighbors of sample data, and thus do not require any
external training data. Clusters are then formed by thresholding the similarity
scores. We evaluate the clustering performance using three challenging
unconstrained face datasets, including Celebrity in Frontal-Profile (CFP),
IARPA JANUS Benchmark A (IJB-A), and JANUS Challenge Set 3 (JANUS CS3)
datasets. Experimental results demonstrate that the proposed approach can
achieve significant improvements over state-of-the-art methods. Moreover, we
also show that the proposed clustering algorithm can be applied to curate a set
of large-scale and noisy training dataset while maintaining sufficient amount
of images and their variations due to nuisance factors. The face verification
performance on JANUS CS3 improves significantly by finetuning a DCNN model with
the curated MS-Celeb-1M dataset which contains over three million face images.
",computer-science
"  CONTEXT. It is theoretically possible for rings to have formed around
extrasolar planets in a similar way to that in which they formed around the
giant planets in our solar system. However, no such rings have been detected to
date.
AIMS: We aim to test the possibility of detecting rings around exoplanets by
investigating the photometric and spectroscopic ring signatures in
high-precision transit signals.
METHODS: The photometric and spectroscopic transit signals of a ringed planet
is expected to show deviations from that of a spherical planet. We used these
deviations to quantify the detectability of rings. We present SOAP3.0 which is
a numerical tool to simulate ringed planet transits and measure ring
detectability based on amplitudes of the residuals between the ringed planet
signal and best fit ringless model.
RESULTS: We find that it is possible to detect the photometric and
spectroscopic signature of near edge-on rings especially around planets with
high impact parameter. Time resolution $\leq$ 7 mins is required for the
photometric detection, while 15 mins is sufficient for the spectroscopic
detection. We also show that future instruments like CHEOPS and ESPRESSO, with
precisions that allow ring signatures to be well above their noise-level,
present good prospects for detecting rings.
",physics
"  We report experimental studies of the influence of symmetric dual-loop
optical feedback on the RF linewidth and timing jitter of self-mode-locked
two-section quantum dash lasers emitting at 1550 nm. Various feedback schemes
were investigated and optimum levels determined for narrowest RF linewidth and
low timing jitter, for single-loop and symmetric dual-loop feedback. Two
symmetric dual-loop configurations, with balanced and unbalanced feedback
ratios, were studied. We demonstrate that unbalanced symmetric dual loop
feedback, with the inner cavity resonant and fine delay tuning of the outer
loop, gives narrowest RF linewidth and reduced timing jitter over a wide range
of delay, unlike single and balanced symmetric dual-loop configurations. This
configuration with feedback lengths 80 and 140 m narrows the RF linewidth by
4-67x and 10-100x, respectively, across the widest delay range, compared to
free-running. For symmetric dual-loop feedback, the influence of different
power split ratios through the feedback loops was determined. Our results show
that symmetric dual-loop feedback is markedly more effective than single-loop
feedback in reducing RF linewidth and timing jitter, and is much less sensitive
to delay phase, making this technique ideal for applications where robustness
and alignment tolerance are essential.
",physics
"  In cavity-based axion dark matter search experiments exploring high mass
regions, multiple-cavity design is considered to increase the detection volume
within a given magnet bore. We introduce a new idea, referred to as
multiple-cell cavity, which provides various benefits including a larger
detection volume, simpler experimental setup, and easier phase-matching
mechanism. We present the characteristics of this concept and demonstrate the
experimental feasibility with an example of a double-cell cavity.
",physics
"  High-signal to noise observations of the Ly$\alpha$ forest transmissivity in
the z = 7.085 QSO ULAS J1120+0641 show seven narrow transmission spikes
followed by a long 240 cMpc/h trough. Here we use radiative transfer
simulations of cosmic reionization previously calibrated to match a wider range
of Ly$\alpha$ forest data to show that the occurrence of seven transmission
spikes in the narrow redshift range z = 5.85 - 6.1 is very sensitive to the
exact timing of reionization. Occurrence of the spikes requires the most under
dense regions of the IGM to be already fully ionised. The rapid onset of a long
trough at z = 6.12 requires a strong decrease of the photo-ionisation rate at
z$\sim$6.1 in this line-of-sight, consistent with the end of percolation at
this redshift. The narrow range of reionisation histories that we previously
found to be consistent with a wider range of Ly$\alpha$ forest data have a
reasonable probability of showing seven spikes and the mock absorption spectra
provide an excellent match to the spikes and the trough in the observed
spectrum of ULAS J1120+0641. Despite the large overall opacity of Ly$\alpha$ at
z > 5.8, larger samples of high signal-to-noise observations of rare
transmission spikes should therefore provide important further insights into
the exact timing of the percolation of HII bubbles at the tail-end of
reionization
",physics
"  We give a parametrization of the simple Bernstein components of inner forms
of a general linear group over a local field by invariants constructed from
type theory, and explicitly describe its behaviour under the Jacquet-Langlands
correspondence. Along the way, we prove a conjecture of Broussous, Sécherre
and Stevens on preservation of endo-classes.
",mathematics
"  The dependence of the mass accretion rate on the stellar properties is a key
constraint for star formation and disk evolution studies. Here we present a
study of a sample of stars in the Chamaeleon I star forming region carried out
using the VLT/X-Shooter spectrograph. The sample is nearly complete down to
M~0.1Msun for the young stars still harboring a disk in this region. We derive
the stellar and accretion parameters using a self-consistent method to fit the
broad-band flux-calibrated medium resolution spectrum. The correlation between
the accretion luminosity to the stellar luminosity, and of the mass accretion
rate to the stellar mass in the logarithmic plane yields slopes of 1.9 and 2.3,
respectively. These slopes and the accretion rates are consistent with previous
results in various star forming regions and with different theoretical
frameworks. However, we find that a broken power-law fit, with a steeper slope
for stellar luminosity smaller than ~0.45 Lsun and for stellar masses smaller
than ~ 0.3 Msun, is slightly preferred according to different statistical
tests, but the single power-law model is not excluded. The steeper relation for
lower mass stars can be interpreted as a faster evolution in the past for
accretion in disks around these objects, or as different accretion regimes in
different stellar mass ranges. Finally, we find two regions on the mass
accretion versus stellar mass plane empty of objects. One at high mass
accretion rates and low stellar masses, which is related to the steeper
dependence of the two parameters we derived. The second one is just above the
observational limits imposed by chromospheric emission. This empty region is
located at M~0.3-0.4Msun, typical masses where photoevaporation is known to be
effective, and at mass accretion rates ~10^-10 Msun/yr, a value compatible with
the one expected for photoevaporation to rapidly dissipate the inner disk.
",physics
"  Modern tracking technology has made the collection of large numbers of
densely sampled trajectories of moving objects widely available. We consider a
fundamental problem encountered when analysing such data: Given $n$ polygonal
curves $S$ in $\mathbb{R}^d$, preprocess $S$ into a data structure that answers
queries with a query curve $q$ and radius $\rho$ for the curves of $S$ that
have \Frechet distance at most $\rho$ to $q$.
We initiate a comprehensive analysis of the space/query-time trade-off for
this data structuring problem. Our lower bounds imply that any data structure
in the pointer model model that achieves $Q(n) + O(k)$ query time, where $k$ is
the output size, has to use roughly $\Omega\left((n/Q(n))^2\right)$ space in
the worst case, even if queries are mere points (for the discrete \Frechet
distance) or line segments (for the continuous \Frechet distance). More
importantly, we show that more complex queries and input curves lead to
additional logarithmic factors in the lower bound. Roughly speaking, the number
of logarithmic factors added is linear in the number of edges added to the
query and input curve complexity. This means that the space/query time
trade-off worsens by an exponential factor of input and query complexity. This
behaviour addresses an open question in the range searching literature: whether
it is possible to avoid the additional logarithmic factors in the space and
query time of a multilevel partition tree. We answer this question negatively.
On the positive side, we show we can build data structures for the \Frechet
distance by using semialgebraic range searching. Our solution for the discrete
\Frechet distance is in line with the lower bound, as the number of levels in
the data structure is $O(t)$, where $t$ denotes the maximal number of vertices
of a curve. For the continuous \Frechet distance, the number of levels
increases to $O(t^2)$.
",computer-science
"  For $n\geq 4$ we show that generic closed Riemannian $n$-manifolds have no
nontrivial totally geodesic submanifolds, answering a question of Spivak. An
immediate consequence is a severe restriction on the isometry group of a
generic Riemannian metric. Both results are widely believed to be true, but we
are not aware of any proofs in the literature.
",mathematics
"  This paper presents the recently published Cerema AWP (Adverse Weather
Pedestrian) dataset for various machine learning tasks and its exports in
machine learning friendly format. We explain why this dataset can be
interesting (mainly because it is a greatly controlled and fully annotated
image dataset) and present baseline results for various tasks. Moreover, we
decided to follow the very recent suggestions of datasheets for dataset, trying
to standardize all the available information of the dataset, with a
transparency objective.
",statistics
"  Swelling media (e.g. gels, tumors) are usually described by mechanical
constitutive laws (e.g. Hooke or Darcy laws). However, constitutive relations
of real swelling media are not well known. Here, we take an opposite route and
consider a simple packing heuristics, i.e. the particles can't overlap. We
deduce a formula for the equilibrium density under a confining potential. We
then consider its evolution when the average particle volume and confining
potential depend on time under two additional heuristics: (i) any two particles
can't swap their position; (ii) motion should obey some energy minimization
principle. These heuristics determine the medium velocity consistently with the
continuity equation. In the direction normal to the potential level sets the
velocity is related with that of the level sets while in the parallel
direction, it is determined by a Laplace-Beltrami operator on these sets. This
complex geometrical feature cannot be recovered using a simple Darcy law.
",mathematics
"  In a single winner election with several candidates and ranked choice or
rating scale ballots, a Condorcet winner is one who wins all their two way
races by majority rule or MR. A voting system has Condorcet consistency or CC
if it names any Condorcet winner the winner. Many voting systems lack CC, but a
three step line of reasoning is used here to show why it is necessary. In step
1 we show that we can dismiss all the electoral criteria which conflict with
CC. In step 2 we point out that CC follows almost automatically if we can agree
that MR is the only acceptable system for elections with two candidates. In
step 3 we make that argument for MR. This argument itself has three parts.
First, in races with two candidates, the only well known alternatives to MR can
sometimes name as winner a candidate who is preferred over their opponent by
only one voter, with all others preferring the opponent. That is unacceptable.
Second, those same systems are also extremely susceptible to strategic
insincere voting. Third, in simulation studies using spatial models with two
candidates, the best known alternative to MR picks the best or most centrist
candidate significantly less often than MR does.
",statistics
"  Our start point is a 3D piecewise smooth vector field defined in two zones
and presenting a shared fold curve for the two smooth vector fields considered.
Moreover, these smooth vector fields are symmetric relative to the fold curve,
giving raise to a continuum of nested topological cylinders such that each
orthogonal section of these cylinders is filled by centers. First we prove that
the normal form considered represents a whole class of piecewise smooth vector
fields. After we perturb the initial model in order to obtain exactly
$\mathcal{L}$ invariant planes containing centers. A second perturbation of the
initial model also is considered in order to obtain exactly $k$ isolated
cylinders filled by periodic orbits. Finally, joining the two previous
bifurcations we are able to exhibit a model, preserving the symmetry relative
to the fold curve, and having exactly $k.\mathcal{L}$ limit cycles.
",mathematics
"  In many scenarios of a language identification task, the user will specify a
small set of languages which he/she can speak instead of a large set of all
possible languages. We want to model such prior knowledge into the way we train
our neural networks, by replacing the commonly used softmax loss function with
a novel loss function named tuplemax loss. As a matter of fact, a typical
language identification system launched in North America has about 95% users
who could speak no more than two languages. Using the tuplemax loss, our system
achieved a 2.33% error rate, which is a relative 39.4% improvement over the
3.85% error rate of standard softmax loss method.
",computer-science
"  In this paper, we propose a novel scheme for data hiding in the fingerprint
minutiae template, which is the most popular in fingerprint recognition
systems. Various strategies are proposed in data embedding in order to maintain
the accuracy of fingerprint recognition as well as the undetectability of data
hiding. In bits replacement based data embedding, we replace the last few bits
of each element of the original minutiae template with the data to be hidden.
This strategy can be further improved using an optimized bits replacement based
data embedding, which is able to minimize the impact of data hiding on the
performance of fingerprint recognition. The third strategy is an order
preserving mechanism which is proposed to reduce the detectability of data
hiding. By using such a mechanism, it would be difficult for the attacker to
differentiate the minutiae template with hidden data from the original minutiae
templates. The experimental results show that the proposed data hiding scheme
achieves sufficient capacity for hiding common personal data, where the
accuracy of fingerprint recognition is acceptable after the data hiding.
",computer-science
"  Within the standard framework of quasi-steady flight, this paper derives a
speed that realizes the maximal obtainable range per unit of fuel. If this
speed is chosen at each instant of a flight plan $h(x)$ giving altitude $h$ as
a function of distance $x$, a variational problem for finding an optimal $h(x)$
can be formulated and solved. It yields flight plans with maximal range, and
these turn out to consist of mainly three phases using the optimal speed:
starting with a climb at maximal continuous admissible thrust, ending with a
continuous descent at idle thrust, and in between with a transition based on a
solution of the Euler-Lagrange equation for the variational problem. A similar
variational problem is derived and solved for speed-restricted flights, e.g. at
250 KIAS below 10000 ft. In contrast to the literature, the approach of this
paper does not need more than standard ordinary differential equations solving
variational problems to derive range-optimal trajectories. Various numerical
examplesbased on a Standard Business Jet are added for illustration.
",mathematics
"  We study the parameterized complexity of several positional games. Our main
result is that Short Generalized Hex is W[1]-complete parameterized by the
number of moves. This solves an open problem from Downey and Fellows'
influential list of open problems from 1999. Previously, the problem was
thought of as a natural candidate for AW[*]-completeness. Our main tool is a
new fragment of first-order logic where universally quantified variables only
occur in inequalities. We show that model-checking on arbitrary relational
structures for a formula in this fragment is W[1]-complete when parameterized
by formula size. We also consider a general framework where a positional game
is represented as a hypergraph and two players alternately pick vertices. In a
Maker-Maker game, the first player to have picked all the vertices of some
hyperedge wins the game. In a Maker-Breaker game, the first player wins if she
picks all the vertices of some hyperedge, and the second player wins otherwise.
In an Enforcer-Avoider game, the first player wins if the second player picks
all the vertices of some hyperedge, and the second player wins otherwise. Short
Maker-Maker is AW[*]-complete, whereas Short Maker-Breaker is W[1]-complete and
Short Enforcer-Avoider co-W[1]-complete parameterized by the number of moves.
This suggests a rough parameterized complexity categorization into positional
games that are complete for the first level of the W-hierarchy when the winning
configurations only depend on which vertices one player has been able to pick,
but AW[*]-completeness when the winning condition depends on which vertices
both players have picked. However, some positional games where the board and
the winning configurations are highly structured are fixed-parameter tractable.
We give another example of such a game, Short k-Connect, which is
fixed-parameter tractable when parameterized by the number of moves.
",computer-science
"  On Kickstarter only 36% of crowdfunding campaigns successfully raise
sufficient funds for their projects. In this paper, we explore the possibility
of redistribution of crowdfunding donations to increase the chances of success.
We define several intuitive redistribution policies and, using data from a real
crowdfunding platform, LaunchGood, we assess the potential improvement in
campaign fundraising success rates. We find that an aggressive redistribution
scheme can boost campaign success rates from 37% to 79%, but such
choice-agnostic redistribution schemes come at the cost of disregarding donor
preferences. Taking inspiration from offline giving societies and donor clubs,
we build a case for choice preserving redistribution schemes that strike a
balance between increasing the number of successful campaigns and respecting
giving preference. We find that choice-preserving redistribution can easily
achieve campaign success rates of 48%. Finally, we discuss the implications of
these different redistribution schemes for the various stakeholders in the
crowdfunding ecosystem.
",computer-science
"  We investigate two arithmetic functions naturally occurring in the study of
the Euler and Carmichael quotients. The functions are related to the frequency
of vanishing of the Euler and Carmichael quotients. We obtain several results
concerning the relations between these functions as well as their typical and
extreme values.
",mathematics
"  Complex contagion models have been developed to understand a wide range of
social phenomena such as adoption of cultural fads, the diffusion of belief,
norms, and innovations in social networks, and the rise of collective action to
join a riot. Most existing works focus on contagions where individuals' states
are represented by {\em binary} variables, and propagation takes place over a
single isolated network. However, characterization of an individual's standing
on a given matter as a binary state might be overly simplistic as most of our
opinions, feelings, and perceptions vary over more than two states. Also, most
real-world contagions take place over multiple networks (e.g., Twitter and
Facebook) or involve {\em multiplex} networks where individuals engage in
different {\em types} of relationships (e.g., acquaintance, co-worker, family,
etc.). To this end, this paper studies {\em multi-stage} complex contagions
that take place over multi-layer or multiplex networks. Under a linear
threshold based contagion model, we give analytic results for the probability
and expected size of \textit{global} cascades, i.e., cases where a randomly
chosen node can initiate a propagation that eventually reaches a {\em positive}
fraction of the whole population. Analytic results are also confirmed and
supported by an extensive numerical study. In particular, we demonstrate how
the dynamics of complex contagions is affected by the extra weight exerted by
\textit{hyper-active} nodes and by the structural properties of the networks
involved. Among other things, we reveal an interesting connection between the
assortativity of a network and the impact of \textit{hyper-active} nodes on the
cascade size.
",computer-science
"  Human populations exhibit complex behaviors---characterized by long-range
correlations and surges in activity---across a range of social, political, and
technological contexts. Yet it remains unclear where these collective behaviors
come from, or if there even exists a set of unifying principles. Indeed,
existing explanations typically rely on context-specific mechanisms, such as
traffic jams driven by work schedules or spikes in online traffic induced by
significant events. However, analogies with statistical mechanics suggest a
more general mechanism: that collective patterns can emerge organically from
fine-scale interactions within a population. Here, across four different modes
of human activity, we show that the simplest correlations in a
population---those between pairs of individuals---can yield accurate
quantitative predictions for the large-scale behavior of the entire population.
To quantify the minimal consequences of pairwise correlations, we employ the
principle of maximum entropy, making our description equivalent to an Ising
model whose interactions and external fields are notably calculated from past
observations of population activity. In addition to providing accurate
quantitative predictions, we show that the topology of learned Ising
interactions resembles the network of inter-human communication within a
population. Together, these results demonstrate that fine-scale correlations
can be used to predict large-scale social behaviors, a perspective that has
critical implications for modeling and resource allocation in human
populations.
",computer-science
"  Graph drawings are useful tools for exploring the structure and dynamics of
data that can be represented by pair-wise relationships among a set of objects.
Typical real-world social, biological or technological networks exhibit high
complexity resulting from a large number and broad heterogeneity of objects and
relationships. Thus, mapping these networks into a low-dimensional space to
visualize the dynamics of network-driven processes is a challenging task. Often
we want to analyze how a single node is influenced by or is influencing its
local network as the source of a spreading process. Here I present a network
layout algorithm for graphs with millions of nodes that visualizes spreading
phenomena from the perspective of a single node. The algorithm consists of
three stages to allow for an interactive graph exploration: First, a global
solution for the network layout is found in spherical space that minimizes
distance errors between all nodes. Second, a focal node is interactively
selected, and distances to this node are further optimized. Third, node
coordinates are mapped to a circular representation and drawn with additional
features to represent the network-driven phenomenon. The effectiveness and
scalability of this method are shown for a large collaboration network of
scientists, where we are interested in the citation dynamics around a focal
author.
",computer-science
"  Protoplanetary disks undergo substantial mass-loss by photoevaporation, a
mechanism which is crucial to their dynamical evolution. However, the processes
regulating the gas energetics have not been well constrained by observations so
far. We aim at studying the processes involved in disk photoevaporation when it
is driven by far-UV photons. We present a unique Herschel survey and new ALMA
observations of four externally-illuminated photoevaporating disks (a.k.a.
proplyds). For the analysis of these data, we developed a 1D model of the
photodissociation region (PDR) of a proplyd, based on the Meudon PDR code and
computed the far infrared line emission. We successfully reproduce most of the
observations and derive key physical parameters, i.e. densities at the disk
surface of about $10^{6}$ cm$^{-3}$ and local gas temperatures of about 1000 K.
Our modelling suggests that all studied disks are found in a transitional
regime resulting from the interplay between several heating and cooling
processes that we identify. These differ from those dominating in classical
PDRs, i.e. grain photo-electric effect and cooling by [OI] and [CII] FIR lines.
This energetic regime is associated to an equilibrium dynamical point of the
photoevaporation flow: the mass-loss rate is self-regulated to set the envelope
column density at a value that maintains the temperature at the disk surface
around 1000 K. From our best-fit models, we estimate mass-loss rates - of the
order of $10^{-7}$ $\mathrm{M}_\odot$/yr - that are in agreement with earlier
spectroscopic observation of ionised gas tracers. This holds only if we assume
an evaporation flow launched from the disk surface at sound speed
(supercritical regime). We have identified the energetic regime regulating
FUV-photoevaporation in proplyds. This regime could be implemented into models
of the dynamical evolution of protoplanetary disks.
",physics
"  In Compressed Sensing, a real-valued sparse vector has to be recovered from
an underdetermined system of linear equations. In many applications, however,
the elements of the sparse vector are drawn from a finite set. Adapted
algorithms incorporating this additional knowledge are required for the
discrete-valued setup. In this paper, turbo-based algorithms for both cases are
elucidated and analyzed from a communications engineering perspective, leading
to a deeper understanding of the algorithm. In particular, we gain the
intriguing insight that the calculation of extrinsic values is equal to the
unbiasing of a biased estimate and present an improved algorithm.
",computer-science
"  Community detection in networks is a very actual and important field of
research with applications in many areas. But, given that the amount of
processed data increases more and more, existing algorithms need to be adapted
for very large graphs. The objective of this project was to parallelise the
Synchronised Louvain Method, a community detection algorithm developed by
Arnaud Browet, in order to improve its performances in terms of computation
time and thus be able to faster detect communities in very large graphs. To
reach this goal, we used the API OpenMP to parallelise the algorithm and then
carried out performance tests. We studied the computation time and speedup of
the parallelised algorithm and were able to bring out some qualitative trends.
We obtained a great speedup, compared with the theoretical prediction of Amdahl
law. To conclude, using the parallel implementation of the algorithm of Browet
on large graphs seems to give good results, both in terms of computation time
and speedup. Further tests should be carried out in order to obtain more
quantitative results.
",computer-science
"  Guided by critical systems found in nature we develop a novel mechanism
consisting of inhomogeneous polynomial regularisation via which we can induce
scale invariance in deep learning systems. Technically, we map our deep
learning (DL) setup to a genuine field theory, on which we act with the
Renormalisation Group (RG) in momentum space and produce the flow equations of
the couplings; those are translated to constraints and consequently interpreted
as ""critical regularisation"" conditions in the optimiser; the resulting
equations hence prove to be sufficient conditions for - and serve as an elegant
and simple mechanism to induce scale invariance in any deep learning setup.
",computer-science
"  Despite the outstanding achievements of modern cosmology, the classical
dispute on the precise value of $H_0$, which is the first ever parameter of
modern cosmology and one of the prime parameters in the field, still goes on
and on after over half a century of measurements. Recently the dispute came to
the spotlight with renewed strength owing to the significant tension (at
$>3\sigma$ c.l.) between the latest Planck determination obtained from the CMB
anisotropies and the local (distance ladder) measurement from the Hubble Space
Telescope (HST), based on Cepheids. In this work, we investigate the impact of
the running vacuum model (RVM) and related models on such a controversy. For
the RVM, the vacuum energy density $\rho_{\Lambda}$ carries a mild dependence
on the cosmic expansion rate, i.e. $\rho_{\Lambda}(H)$, which allows to
ameliorate the fit quality to the overall $SNIa+BAO+H(z)+LSS+CMB$ cosmological
observations as compared to the concordance $\Lambda$CDM model. By letting the
RVM to deviate from the vacuum option, the equation of state $w=-1$ continues
to be favored by the overall fit. Vacuum dynamics also predicts the following:
i) the CMB range of values for $H_0$ is more favored than the local ones, and
ii) smaller values for $\sigma_8(0)$. As a result, a better account for the LSS
structure formation data is achieved as compared to the $\Lambda$CDM, which is
based on a rigid (i.e. non-dynamical) $\Lambda$ term.
",physics
"  We study the formation of massive black holes in the first star clusters. We
first locate star-forming gas clouds in proto-galactic haloes of $\gtrsim
\!10^7\,{\rm M}_{\odot}$ in cosmological hydrodynamics simulations and use them
to generate the initial conditions for star clusters with masses of $\sim
\!10^5\,{\rm M}_{\odot}$. We then perform a series of direct-tree hybrid
$N$-body simulations to follow runaway stellar collisions in the dense star
clusters. In all the cluster models except one, runaway collisions occur within
a few million years, and the mass of the central, most massive star reaches
$\sim \!400-1900\,{\rm M}_{\odot}$. Such very massive stars collapse to leave
intermediate-mass black holes (IMBHs). The diversity of the final masses may be
attributed to the differences in a few basic properties of the host haloes such
as mass, central gas velocity dispersion, and mean gas density of the central
core. Finally, we derive the IMBH mass to cluster mass ratios, and compare them
with the observed black hole to bulge mass ratios in the present-day Universe.
",physics
"  For a Riemannian covering $M_1\to M_0$ of complete Riemannian manifolds with
boundary (possibly empty) and respective fundamental groups
$\Gamma_1\subseteq\Gamma_0$, we show that the bottoms of the spectra of $M_0$
and $M_1$ coincide if the right action of $\Gamma_0$ on
$\Gamma_1\backslash\Gamma_0$ is amenable.
",mathematics
"  Field-aligned currents in the Earth's magnetotail are traditionally
associated with transient plasma flows and strong plasma pressure gradients in
the near-Earth side. In this paper we demonstrate a new field-aligned current
system present at the lunar orbit tail. Using magnetotail current sheet
observations by two ARTEMIS probes at $\sim60 R_E$, we analyze statistically
the current sheet structure and current density distribution closest to the
neutral sheet. For about half of our 130 current sheet crossings, the
equatorial magnetic field component across-the tail (along the main, cross-tail
current) contributes significantly to the vertical pressure balance. This
magnetic field component peaks at the equator, near the cross-tail current
maximum. For those cases, a significant part of the tail current, having an
intensity in the range 1-10nA/m$^2$, flows along the magnetic field lines (it
is both field-aligned and cross-tail). We suggest that this current system
develops in order to compensate the thermal pressure by particles that on its
own is insufficient to fend off the lobe magnetic pressure.
",physics
"  In higher category theory, we use fibrations to model presheaves. In this
paper we introduce a new method to build such fibrations. Concretely, for
suitable reflective subcategories of simplicial spaces, we build fibrations
that model presheaves valued in that subcategory. Using this we can build
Cartesian fibrations, but we can also model presheaves valued in Segal spaces.
Additionally, using this new approach, we define representable Cartesian
fibrations, generalizing representable presheaves valued in spaces, and show
they have similar properties.
",mathematics
"  We report $^{139}$La and $^{63}$Cu NMR investigation of the successive charge
order, spin order, and superconducting transitions in super-oxygenated
La$_2$CuO$_{4+y}$ single crystal with stage-4 excess oxygen order at
$T_{stage}\simeq 290$ K. We show that the stage-4 order induces tilting of
CuO$_6$ octahedra below $T_{stage}$, which in turn causes $^{139}$La NMR line
broadening. The structural distortion continues to develop far below
$T_{stage}$, and completes at $T_{charge}\simeq 60$ K, where charge order sets
in. This sequence is reminiscent of the the charge order transition in Nd
co-doped La$_{1.88}$Sr$_{0.12}$CuO$_4$ that sets in once the low temperature
tetragonal (LTT) phase is established. We also show that the paramagnetic
$^{63}$Cu NMR signals are progressively wiped out below $T_{charge}$ due to
enhanced low frequency spin fluctuations, but the residual $^{63}$Cu NMR
signals continue to exhibit the characteristics expected for optimally doped
superconducting CuO$_2$ planes. This indicates that charge order in
La$_2$CuO$_{4+y}$ does not take place uniformly in space. Low frequency Cu spin
fluctuations as probed by $^{139}$La nuclear spin-lattice relaxation rate are
mildly glassy, and do not exhibit critical divergence at $T_{spin}$($\simeq
T_{c}$)=42 K. These findings, including the spatially inhomogeneous nature of
the charge ordered state, are qualitatively similar to the case of
La$_{1.885}$Sr$_{0.115}$CuO$_4$ [T. Imai et al., Phys. Rev. B 96 (2017) 224508,
and A. Arsenault et al., Phys. Rev. B 97 (2018) 064511], but both charge and
spin order take place more sharply in the present case.
",physics
"  A semi-relativistic density-functional theory that includes spin-orbit
couplings and Zeeman fields on equal footing with the electromagnetic
potentials, is an appealing framework to develop a unified first-principles
computational approach for non-collinear magnetism, spintronics, orbitronics,
and topological states. The basic variables of this theory include the
paramagnetic current and the spin-current density, besides the particle and the
spin density, and the corresponding exchange-correlation (xc) energy functional
is invariant under local U(1)$\times$SU(2) gauge transformations. The xc-energy
functional must be approximated to enable practical applications, but, contrary
to the case of the standard density functional theory, finding simple
approximations suited to deal with realistic atomistic inhomogeneities has been
a long-standing challenge. Here, we propose a way out of this impasse by
showing that approximate gauge-invariant functionals can be easily generated
from existing approximate functionals of ordinary density-functional theory by
applying a simple {\it minimal substitution} on the kinetic energy density,
which controls the short-range behavior of the exchange hole. Our proposal
opens the way to the construction of approximate, yet non-empirical
functionals, which do not assume weak inhomogeneity and should therefore have a
wide range of applicability in atomic, molecular and condensed matter physics.
",physics
"  The main aim of this article is to give a necessary and sufficient condition
for a real Bott manifold to admit a spin structure and further give a
combinatorial characterization for the spin structure in terms of the
associated acyclic digraph.
",mathematics
"  As mentioned by Schwartz (1974) and Cokelet (1977), it was failed to gain
convergent results of limiting Stokes' waves in extremely shallow water by
means of perturbation methods even with the aid of extrapolation techniques
such as Padé approximant. Especially, it is extremely difficult for
traditional analytic/numerical approaches to present the wave profile of
limiting waves with a sharp crest of $120^\circ$ included angle first mentioned
by Stokes in 1880s. Thus, traditionally, different wave models are used for
waves in different water depths. In this paper, by means of the homotopy
analysis method (HAM), an analytic approximation method for highly nonlinear
equations, we successfully gain convergent results (and especially the wave
profiles) of the limiting Stokes' waves with this kind of sharp crest in
arbitrary water depth, even including solitary waves of extreme form in
extremely shallow water, without using any extrapolation techniques. Therefore,
in the frame of the HAM, the Stokes' wave can be used as a unified theory for
all kinds of waves, including periodic waves in deep and intermediate depth,
cnoidal waves in shallow water and solitary waves in extremely shallow water.
",physics
"  Depth-sensing is important for both navigation and scene understanding.
However, procuring RGB images with corresponding depth data for training deep
models is challenging; large-scale, varied, datasets with ground truth training
data are scarce. Consequently, several recent methods have proposed treating
the training of monocular color-to-depth estimation networks as an image
reconstruction problem, thus forgoing the need for ground truth depth.
There are multiple concepts and design decisions for these networks that seem
sensible, but give mixed or surprising results when tested. For example,
binocular stereo as the source of self-supervision seems cumbersome and hard to
scale, yet results are less blurry compared to training with monocular videos.
Such decisions also interplay with questions about architectures, loss
functions, image scales, and motion handling. In this paper, we propose a
simple yet effective model, with several general architectural and loss
innovations, that surpasses all other self-supervised depth estimation
approaches on KITTI.
",statistics
"  We consider explicit polar constructions of blocklength $n\rightarrow\infty$
for the two extreme cases of code rates $R\rightarrow1$ and $R\rightarrow0.$
For code rates $R\rightarrow1,$ we design codes with complexity order of $n\log
n$ in code construction, encoding, and decoding. These codes achieve the
vanishing output bit error rates on the binary symmetric channels with any
transition error probability $p\rightarrow 0$ and perform this task with a
substantially smaller redundancy $(1-R)n$ than do other known high-rate codes,
such as BCH codes or Reed-Muller (RM). We then extend our design to the
low-rate codes that achieve the vanishing output error rates with the same
complexity order of $n\log n$ and an asymptotically optimal code rate
$R\rightarrow0$ for the case of $p\rightarrow1/2.$
",computer-science
"  A new majority and minority voted redundancy (MMR) scheme is proposed that
can provide the same degree of fault tolerance as N-modular redundancy (NMR)
but with fewer function units and a less sophisticated voting logic. Example
NMR and MMR circuits were implemented using a 32/28nm CMOS process and
compared. The results show that MMR circuits dissipate less power, occupy less
area, and encounter less critical path delay than the corresponding NMR
circuits while providing the same degree of fault tolerance. Hence the MMR is a
promising alternative to the NMR to efficiently implement high levels of
redundancy in safety-critical applications.
",computer-science
"  Path planning is an important problem in robotics. One way to plan a path
between two points $x,y$ within a (not necessarily simply-connected) planar
domain $\Omega$, is to define a non-negative distance function $d(x,y)$ on
$\Omega\times\Omega$ such that following the (descending) gradient of this
distance function traces such a path. This presents two equally important
challenges: A mathematical challenge -- to define $d$ such that $d(x,y)$ has a
single minimum for any fixed $y$ (and this is when $x=y$), since a local
minimum is in effect a ""dead end"", A computational challenge -- to define $d$
such that it may be computed efficiently. In this paper, given a description of
$\Omega$, we show how to assign coordinates to each point of $\Omega$ and
define a family of distance functions between points using these coordinates,
such that both the mathematical and the computational challenges are met. This
is done using the concepts of \emph{harmonic measure} and
\emph{$f$-divergences}.
In practice, path planning is done on a discrete network defined on a finite
set of \emph{sites} sampled from $\Omega$, so any method that works well on the
continuous domain must be adapted so that it still works well on the discrete
domain. Given a set of sites sampled from $\Omega$, we show how to define a
network connecting these sites such that a \emph{greedy routing} algorithm
(which is the discrete equivalent of continuous gradient descent) based on the
distance function mentioned above is guaranteed to generate a path in the
network between any two such sites. In many cases, this network is close to a
(desirable) planar graph, especially if the set of sites is dense.
",computer-science
"  The exact law for fully developed homogeneous compressible
magnetohydrodynamics (CMHD) turbulence is derived. For an isothermal plasma,
without the assumption of isotropy, the exact law is expressed as a function of
the plasma velocity field, the compressible Alfvén velocity and the scalar
density, instead of the Elsässer variables used in previous works. The
theoretical results show four different types of terms that are involved in the
nonlinear cascade of the total energy in the inertial range. Each category is
examined in detail, in particular those that can be written either as source or
flux terms. Finally, the role of the background magnetic field $B_0$ is
highlighted and comparison with the incompressible MHD (IMHD) model is
discussed. This point is particularly important when testing the exact law on
numerical simulations and in situ observations in space plasmas.
",physics
"  Tetrachiral materials are characterized by a cellular microstructure made by
a periodic pattern of stiff rings and flexible ligaments. Their mechanical
behaviour can be described by a planar lattice of rigid massive bodies and
elastic massless beams. The periodic cell dynamics is governed by a monoatomic
structural model, conveniently reduced to the only active degrees-of-freedom.
The paper presents an explicit parametric description of the band structure
governing the free propagation of elastic waves. By virtue of multiparametric
perturbation techniques, sensitivity analyses are performed to achieve
analytical asymptotic approximation of the dispersion functions. The parametric
conditions for the existence of full band gaps in the low-frequency range are
established. Furthermore, the band gap amplitude is analytically assessed in
the admissible parameter range. In tetrachiral acoustic metamaterials, stop
bands can be opened by the introduction of intra-ring resonators. Perturbation
methods can efficiently deal with the consequent enlargement of the mechanical
parameter space. Indeed high-accuracy parametric approximations are achieved
for the band structure, enriched by the new optical branches related to the
resonator frequencies. In particular, target stop bands in the metamaterial
spectrum are analytically designed through the asymptotic solution of inverse
spectral problems.
",physics
"  We consider alternate formulations of recently proposed hierarchical Nearest
Neighbor Gaussian Process (NNGP) models (Datta et al., 2016a) for improved
convergence, faster computing time, and more robust and reproducible Bayesian
inference. Algorithms are defined that improve CPU memory management and
exploit existing high-performance numerical linear algebra libraries.
Computational and inferential benefits are assessed for alternate NNGP
specifications using simulated datasets and remotely sensed light detection and
ranging (LiDAR) data collected over the US Forest Service Tanana Inventory Unit
(TIU) in a remote portion of Interior Alaska. The resulting data product is the
first statistically robust map of forest canopy for the TIU.
",statistics
"  An effective approach to non-parallel voice conversion (VC) is to utilize
deep neural networks (DNNs), specifically variational auto encoders (VAEs), to
model the latent structure of speech in an unsupervised manner. A previous
study has confirmed the ef- fectiveness of VAE using the STRAIGHT spectra for
VC. How- ever, VAE using other types of spectral features such as mel- cepstral
coefficients (MCCs), which are related to human per- ception and have been
widely used in VC, have not been prop- erly investigated. Instead of using one
specific type of spectral feature, it is expected that VAE may benefit from
using multi- ple types of spectral features simultaneously, thereby improving
the capability of VAE for VC. To this end, we propose a novel VAE framework
(called cross-domain VAE, CDVAE) for VC. Specifically, the proposed framework
utilizes both STRAIGHT spectra and MCCs by explicitly regularizing multiple
objectives in order to constrain the behavior of the learned encoder and de-
coder. Experimental results demonstrate that the proposed CD- VAE framework
outperforms the conventional VAE framework in terms of subjective tests.
",computer-science
"  We study the two-dimensional geometric knapsack problem (2DK) in which we are
given a set of n axis-aligned rectangular items, each one with an associated
profit, and an axis-aligned square knapsack. The goal is to find a
(non-overlapping) packing of a maximum profit subset of items inside the
knapsack (without rotating items). The best-known polynomial-time approximation
factor for this problem (even just in the cardinality case) is (2 + \epsilon)
[Jansen and Zhang, SODA 2004].
In this paper, we break the 2 approximation barrier, achieving a
polynomial-time (17/9 + \epsilon) < 1.89 approximation, which improves to
(558/325 + \epsilon) < 1.72 in the cardinality case. Essentially all prior work
on 2DK approximation packs items inside a constant number of rectangular
containers, where items inside each container are packed using a simple greedy
strategy. We deviate for the first time from this setting: we show that there
exists a large profit solution where items are packed inside a constant number
of containers plus one L-shaped region at the boundary of the knapsack which
contains items that are high and narrow and items that are wide and thin. As a
second major and the main algorithmic contribution of this paper, we present a
PTAS for this case. We believe that this will turn out to be useful in future
work in geometric packing problems.
We also consider the variant of the problem with rotations (2DKR), where
items can be rotated by 90 degrees. Also, in this case, the best-known
polynomial-time approximation factor (even for the cardinality case) is (2 +
\epsilon) [Jansen and Zhang, SODA 2004]. Exploiting part of the machinery
developed for 2DK plus a few additional ideas, we obtain a polynomial-time (3/2
+ \epsilon)-approximation for 2DKR, which improves to (4/3 + \epsilon) in the
cardinality case.
",computer-science
"  As the effort to scale up existing quantum hardware proceeds, it becomes
necessary to schedule quantum gates in a way that minimizes the number of
operations. There are three constraints that have to be satisfied: the order or
dependency of the quantum gates in the specific algorithm, the fact that any
qubit may be involved in at most one gate at a time, and the restriction that
two-qubit gates are implementable only between connected qubits. The last
aspect implies that the compilation depends not only on the algorithm, but also
on hardware properties like connectivity. Here we suggest a two-step approach
in which logical gates are initially scheduled neglecting connectivity
considerations, while routing operations are added at a later step in a way
that minimizes their overhead. We rephrase the subtasks of gate scheduling in
terms of graph problems like edge-coloring and maximum subgraph isomorphism.
While this approach is general, we specialize to a one dimensional array of
qubits to propose a routing scheme that is minimal in the number of exchange
operations. As a practical application, we schedule the Quantum Approximate
Optimization Algorithm in a linear geometry and quantify the reduction in the
number of gates and circuit depth that results from increasing the efficacy of
the scheduling strategies.
",computer-science
"  To investigate the existence of sterile neutrino, we propose a new neutrino
production method using $^{13}$C beams and a $^{9}$Be target for short-baseline
electron antineutrino (${\bar{\nu}}_{e}$) disappearance study. The production
of secondary unstable isotopes which can emit neutrinos from the $^{13}$C +
$^{9}$Be reaction is calculated with three different nucleus-nucleus (AA)
reaction models. Different isotope yields are obtained using these models, but
the results of the neutrino flux are found to have unanimous similarities. This
feature gives an opportunity to study neutrino oscillation through shape
analysis. In this work, expected neutrino flux and event rates are discussed in
detail through intensive simulation of the light ion collision reaction and the
neutrino flux from the beta decay of unstable isotopes followed by this
collision. Together with the reactor and accelerator anomalies, the present
proposed ${\bar{\nu}}_{e}$ source is shown to be a practically alternative test
of the existence of the $\Delta m^{2}$ $\sim$ 1 eV$^{2}$ scale sterile
neutrino.
",physics
"  A new method is developed to deal with the problem that a complex
decentralized control system needs to keep centralized control performance. The
systematic procedure emphasizes quickly finding the decentralized
subcontrollers that matching the closed-loop performance and robustness
characteristics of the centralized controller, which is featured by the fact
that GA is used to optimize the design of centralized H-infinity controller
K(s) and decentralized engine subcontroller KT(s), and that only one interface
variable needs to satisfy decentralized control system requirement according to
the proposed selection principle. The optimization design is motivated by the
implementation issues where it is desirable to reduce the time in trial and
error process and accurately find the best decentralized subcontrollers. The
method is applied to decentralized control system design for a short takeoff
and landing fighter. By comparing the simulation results of the decentralized
control system with those of the centralized control system, the target of the
decentralized control attains the performance and robustness of centralized
control is validated.
",computer-science
"  In this paper we propose a general framework for modeling an insurance
claims' information flow in continuous time, by generalizing the reduced-form
framework for credit risk and life insurance. In particular, we assume a
nontrivial dependence structure between the reference filtration and the
insurance internal filtration. We apply these results for pricing non-life
insurance liabilities in hybrid financial and insurance markets, while taking
into account the role of inflation under the benchmark approach. This framework
offers at the same time a general and flexible structure, and explicit and
treatable pricing formula.
",quantitative-finance
"  Currently, we are in an environment where the fraction of automated vehicles
is negligibly small. We anticipate that this fraction will increase in coming
decades before if ever, we have a fully automated transportation system.
Motivated by this we address the problem of provable safety of mixed traffic
consisting of both intelligent vehicles (IVs) as well as human-driven vehicles
(HVs). An important issue that arises is that such mixed systems may well have
lesser throughput than all human traffic systems if the automated vehicles are
expected to remain provably safe with respect to human traffic. This
necessitates the consideration of strategies such as platooning of automated
vehicles in order to increase the throughput. In this paper, we address the
design of provably safe systems consisting of a mix of automated and
human-driven vehicles including the use of platooning by automated vehicles.
We design motion planing policies and coordination rules for participants in
this novel mixed system. HVs are considered as nearsighted and modeled with
relatively loose constraints, while IVs are considered as capable of following
much tighter constraints. HVs are expected to follow reasonable and simple
rules. IVs are designed to move under a model predictive control (MPC) based
motion plans and coordination protocols. Our contribution of this paper is in
showing how to integrate these two types of models safely into a mixed system.
System safety is proved in single lane scenarios, as well as in multi-lane
situations allowing lane changes.
",computer-science
"  CaFe2As2 exhibits collapsed tetragonal (cT) structure and varied exotic
behavior under pressure at low temperatures that led to debate on linking the
structural changes to its exceptional electronic properties like
superconductivity, magnetism, etc. Here, we investigate the electronic
structure of CaFe2As2 forming in different structures employing density
functional theory. The results indicate better stability of the cT phase with
enhancement in hybridization induced effects and shift of the energy bands
towards lower energies. The Fermi surface centered around $\Gamma$ point
gradually vanishes with the increase in pressure. Consequently, the nesting
between the hole and electron Fermi surfaces associated to the spin density
wave state disappears indicating a pathway to achieve the proximity to quantum
fluctuations. The magnetic moment at the Fe sites diminishes in the cT phase
consistent with the magnetic susceptibility results. Notably, the hybridization
of Ca 4s states (Ca-layer may be treated as a charge reservoir layer akin to
those in cuprate superconductors) is significantly enhanced in the cT phase
revealing its relevance in its interesting electronic properties.
",physics
"  The emergence of oscillations in models of the El-Niño effect is of utmost
relevance. Here we investigate a coupled nonlinear delay differential system
modeling theEl-Niño/ Southern Oscillation (ENSO) phenomenon, which arises
through the strong coupling of the ocean-atmosphere system. In particular, we
study the temporal patterns of the sea surface temperature anomaly of the two
sub-regions. For identical sub-regions we typically observe a co-existence of
amplitude and oscillator death behavior for low delays, and heterogeneous
oscillations for high delays, when inter-region coupling is weak. For moderate
inter-region coupling strengths one obtains homogeneous oscillations for
sufficiently large delays and amplitude death for small delays. When the
inter-region coupling strength is large, oscillations are suppressed
altogether, implying that strongly coupled sub-regions do not exhibit ENSO-like
oscillations. Further we observe that larger strengths of self-delay coupling
favours oscillations, while oscillations die out when the delayed coupling is
weak. This indicates again that delayed feedback, incorporating oceanic wave
transit effects, is the principal cause of oscillatory behaviour. So the effect
of trapped ocean waves propagating in a basin with closed boundaries is crucial
for the emergence of ENSO. Further, we show how non-uniformity in delays, and
difference in the strengths of the self-delay coupling of the sub-regions,
affect the rise of oscillations. Interestingly we find that larger delays and
self-delay coupling strengths lead to oscillations, while strong inter-region
coupling kills oscillatory behaviour. Thus, we find that coupling sub-regions
has a very significant effect on the emergence of oscillations, and strong
coupling typically suppresses oscillations, while weak coupling of
non-identical sub-regions can induce oscillations, thereby favouring ENSO.
",physics
"  We present a method for conditional time series forecasting based on an
adaptation of the recent deep convolutional WaveNet architecture. The proposed
network contains stacks of dilated convolutions that allow it to access a broad
range of history when forecasting, a ReLU activation function and conditioning
is performed by applying multiple convolutional filters in parallel to separate
time series which allows for the fast processing of data and the exploitation
of the correlation structure between the multivariate time series. We test and
analyze the performance of the convolutional network both unconditionally as
well as conditionally for financial time series forecasting using the S&P500,
the volatility index, the CBOE interest rate and several exchange rates and
extensively compare it to the performance of the well-known autoregressive
model and a long-short term memory network. We show that a convolutional
network is well-suited for regression-type problems and is able to effectively
learn dependencies in and between the series without the need for long
historical time series, is a time-efficient and easy to implement alternative
to recurrent-type networks and tends to outperform linear and recurrent models.
",statistics
"  This paper studies a new type of 3D bin packing problem (BPP), in which a
number of cuboid-shaped items must be put into a bin one by one orthogonally.
The objective is to find a way to place these items that can minimize the
surface area of the bin. This problem is based on the fact that there is no
fixed-sized bin in many real business scenarios and the cost of a bin is
proportional to its surface area. Based on previous research on 3D BPP, the
surface area is determined by the sequence, spatial locations and orientations
of items. It is a new NP-hard combinatorial optimization problem on
unfixed-sized bin packing, for which we propose a multi-task framework based on
Selected Learning, generating the sequence and orientations of items packed
into the bin simultaneously. During training steps, Selected Learning chooses
one of loss functions derived from Deep Reinforcement Learning and Supervised
Learning corresponding to the training procedure. Numerical results show that
the method proposed significantly outperforms Lego baselines by a substantial
gain of 7.52%. Moreover, we produce large scale 3D Bin Packing order data set
for studying bin packing problems and will release it to the research
community.
",statistics
"  Successful programs are written to be maintained. One aspect to this is that
programmers order the components in the code files in a particular way. This is
part of programming style. While the conventions for ordering are sometimes
given as part of a style guideline, such guidelines are often incomplete and
programmers tend to have their own more comprehensive orderings in mind. This
paper defines a model for ordering program components and shows how this model
can be learned from sample code. Such a model is a useful tool for a
programming environment in that it can be used to find the proper location for
inserting new components or for reordering files to better meet the needs of
the programmer. The model is designed so that it can be fine- tuned by the
programmer. The learning framework is evaluated both by looking at code with
known style guidelines and by testing whether it inserts existing components
into a file correctly.
",computer-science
"  We have investigated tunneling current through a suspended graphene Corbino
disk in high magnetic fields at the Dirac point, i.e. at filling factor $\nu$ =
0. At the onset of the dielectric breakdown the current through the disk grows
exponentially before ohmic behaviour, but in a manner distinct from thermal
activation. We find that Zener tunneling between Landau sublevels dominates,
facilitated by tilting of the source-drain bias potential. According to our
analytic modelling, the Zener tunneling is strongly affected by the gyrotropic
force (Lorentz force) due to the high magnetic field
",physics
"  Image registration is a fundamental issue in multispectral image processing.
In filter wheel based multispectral imaging systems, the non-coplanar placement
of the filters always causes the misalignment of multiple channel images. The
selective characteristic of spectral response in multispectral imaging raises
two challenges to image registration. First, the intensity levels of a local
region may be different in individual channel images. Second, the local
intensity may vary rapidly in some channel images while keeps stationary in
others. Conventional multimodal measures, such as mutual information,
correlation coefficient, and correlation ratio, can register images with
different regional intensity levels, but will fail in the circumstance of
severe local intensity variation. In this paper, a new measure, namely
normalized total gradient (NTG), is proposed for multispectral image
registration. The NTG is applied on the difference between two channel images.
This measure is based on the key assumption (observation) that the gradient of
difference image between two aligned channel images is sparser than that
between two misaligned ones. A registration framework, which incorporates image
pyramid and global/local optimization, is further introduced for rigid
transform. Experimental results validate that the proposed method is effective
for multispectral image registration and performs better than conventional
methods.
",computer-science
"  We suggest an efficient method to resolve electronic cusps in electronic
structure calculations by using an effective transcorrelated Hamiltonian. This
effective Hamiltonian takes a simple form for plane wave bases, containing up
to two-body operators only, and its use incurs almost no additional
computational overhead compared to that of the original Hamiltonian. We apply
this method in combination with the full configuration interaction quantum
Monte Carlo (FCIQMC) method to the homogeneous electron gas. As a projection
technique, the non-Hermitian nature of the transcorrelated Hamiltonian does not
cause complications or numerical difficulties for FCIQMC. The rate of
convergence of the total energy to the complete basis set limit is improved
from ${\cal O}(M^{-1})$ to ${\cal O}\left({M^{-5/3}}\right)$, where $M$ is the
total number of orbital basis functions.
",physics
"  Leakage of polarized Galactic diffuse emission into total intensity can
potentially mimic the 21-cm signal coming from the epoch of reionization (EoR),
as both of them might have fluctuating spectral structure. Although we are
sensitive to the EoR signal only in small fields of view, chromatic sidelobes
from further away can contaminate the inner region. Here, we explore the
effects of leakage into the 'EoR window' of the cylindrically averaged power
spectra (PS) within wide fields of view using both observation and simulation
of the 3C196 and NCP fields, two observing fields of the LOFAR-EoR project. We
present the polarization PS of two one-night observations of the two fields and
find that the NCP field has higher fluctuations along frequency, and
consequently exhibits more power at high-$k_\parallel$ that could potentially
leak to Stokes $I$. Subsequently, we simulate LOFAR observations of Galactic
diffuse polarized emission based on a model to assess what fraction of
polarized power leaks into Stokes $I$ because of the primary beam. We find that
the rms fractional leakage over the instrumental $k$-space is $0.35\%$ in the
3C196 field and $0.27\%$ in the NCP field, and it does not change significantly
within the diameters of $15^\circ$, $9^\circ$ and $4^\circ$. Based on the
observed PS and simulated fractional leakage, we show that a similar level of
leakage into Stokes $I$ is expected in the 3C196 and NCP fields, and the
leakage can be considered to be a bias in the PS.
",physics
"  In a wide variety of applications, including personalization, we want to
measure the difference in outcome due to an intervention and thus have to deal
with counterfactual inference. The feedback from a customer in any of these
situations is only 'bandit feedback' - that is, a partial feedback based on
whether we chose to intervene or not. Typically randomized experiments are
carried out to understand whether an intervention is overall better than no
intervention. Here we present a feature learning algorithm to learn from a
randomized experiment where the intervention in consideration is most effective
and where it is least effective rather than only focusing on the overall
impact, thus adding a context to our learning mechanism and extract more
information. From the randomized experiment, we learn the feature
representations which divide the population into subpopulations where we
observe statistically significant difference in average customer feedback
between those who were subjected to the intervention and those who were not,
with a level of significance l, where l is a configurable parameter in our
model. We use this information to derive the value of the intervention in
consideration for each instance in the population. With experiments, we show
that using this additional learning, in future interventions, the context for
each instance could be leveraged to decide whether to intervene or not.
",statistics
"  In this paper we propose a well-justified synthetic approach of the
projective space. We define the concepts of plane and space of incidence and
also the Gallucci's axiom as an axiom to our classical projective space. To
this purpose we prove from our space axioms, the theorems of Desargues, Pappus,
the fundamental theorem of projectivities, and the fundamental theorem of
central-axial collinearities, respectively. Our building up do not use any
information on analytical projective geometry, as the concept of cross-ratio
and the homogeneous coordinates of points.
",mathematics
"  Spectrally efficient multi-antenna wireless communication systems are a key
challenge as service demands continue to increase. At the same time, powering
up radio access networks is facing environmental and regulation limitations. In
order to achieve more power efficiency, we design a directional modulation
precoder by considering an $M$-QAM constellation, particularly with
$M=4,8,16,32$. First, extended detection regions are defined for desired
constellations using analytical geometry. Then, constellation points are placed
in the optimal positions of these regions while the minimum Euclidean distance
to adjacent constellation points and detection region boundaries is kept as in
the conventional $M$-QAM modulation. For further power efficiency and symbol
error rate similar to that of fixed design in high SNR, relaxed detection
regions are modeled for inner points of $M=16,32$ constellations. The modeled
extended and relaxed detection regions as well as the modulation
characteristics are utilized to formulate symbol-level precoder design problems
for directional modulation to minimize the transmission power while preserving
the minimum required SNR at the destination. In addition, the extended and
relaxed detection regions are used for precoder design to minimize the output
of each power amplifier. We transform the design problems into convex ones and
devise an interior point path-following iterative algorithm to solve the
mentioned problems and provide details on finding the initial values of the
parameters and the starting point. Results show that compared to the benchmark
schemes, the proposed method performs better in terms of power and peak power
reduction as well as symbol error rate reduction for a wide range of SNRs.
",computer-science
"  We overview dataflow matrix machines as a Turing complete generalization of
recurrent neural networks and as a programming platform. We describe vector
space of finite prefix trees with numerical leaves which allows us to combine
expressive power of dataflow matrix machines with simplicity of traditional
recurrent neural networks.
",computer-science
"  Purpose: We propose a phenotype-based artificial intelligence system that can
self-learn and is accurate for screening purposes, and test it on a Level IV
monitoring system. Methods: Based on the physiological knowledge, we
hypothesize that the phenotype information will allow us to find subjects from
a well-annotated database that share similar sleep apnea patterns. Therefore,
for a new-arriving subject, we can establish a prediction model from the
existing database that is adaptive to the subject. We test the proposed
algorithm on a database consisting of 62 subjects with the signals recorded
from a Level IV wearable device measuring the thoracic and abdominal movements
and the SpO2. Results: With the leave-one cross validation, the accuracy of the
proposed algorithm to screen subjects with an apnea-hypopnea index greater or
equal to 15 is 93.6%, the positive likelihood ratio is 6.8, and the negative
likelihood ratio is 0.03. Conclusion: The results confirm the hypothesis and
show that the proposed algorithm has great potential to screen patients with
SAS.
",statistics
"  In this note, we recall Kummer's Fourier series expansion of the 1-periodic
function that coincides with the logarithm of the Gamma function on the unit
interval $(0,1)$, and we use it to find closed forms for some numerical series
related to the generalized Stieltjes constants, and some integrals involving
the function $x\mapsto \ln \ln(1/x)$.
",mathematics
"  Nonlinear systems, whose outputs are not directly proportional to their
inputs, are well known to exhibit many interesting and important phenomena
which have profoundly changed our technological landscape over the last 50
years. Recently the ability to engineer quantum metamaterials through
hybridisation has allowed to explore these nonlinear effects in systems with no
natural analogue. Here we investigate amplitude bistability, which is one of
the most fundamental nonlinear phenomena, in a hybrid system composed of a
superconducting resonator inductively coupled to an ensemble of
nitrogen-vacancy centres. One of the exciting properties of this spin system is
its extremely long spin life-time, more than ten orders of magnitude longer
than other relevant timescales of the hybrid system. This allows us to
dynamically explore this nonlinear regime of cavity quantum electrodynamics
(cQED) and demonstrate a critical slowing down of the cavity population on the
order of several tens of thousands of seconds - a timescale much longer than
observed so far for this effect. Our results provide the foundation for future
quantum technologies based on nonlinear phenomena.
",physics
"  The velocity anisotropy parameter, beta, is a measure of the kinematic state
of orbits in the stellar halo which holds promise for constraining the merger
history of the Milky Way (MW). We determine global trends for beta as a
function of radius from three suites of simulations, including accretion only
and cosmological hydrodynamic simulations. We find that both types of
simulations are consistent and predict strong radial anisotropy (<beta>~0.7)
for Galactocentric radii greater than 10 kpc. Previous observations of beta for
the MW's stellar halo claim a detection of an isotropic or tangential ""dip"" at
r~20 kpc. Using the N-body+SPH simulations, we investigate the temporal
persistence, population origin, and severity of ""dips"" in beta. We find dips in
the in situ stellar halo are long-lived, while dips in the accreted stellar
halo are short-lived and tied to the recent accretion of satellite material. We
also find that a major merger as early as z~1 can result in a present day low
(isotropic to tangential) value of beta over a wide range of radii and angular
expanse. While all of these mechanisms are plausible drivers for the beta dip
observed in the MW, in the simulations, each mechanism has a unique metallicity
signature associated with it, implying that future spectroscopic surveys could
distinguish between them. Since an accurate knowledge of beta(r) is required
for measuring the mass of the MW halo, we note significant transient dips in
beta could cause an overestimate of the halo's mass when using spherical Jeans
equation modeling.
",physics
"  The surface energy of a magnetic Domain Wall (DW) strongly affects its static
and dynamic behaviours. However, this effect was seldom directly observed and
many related phenomena have not been well understood. Moreover, a reliable
method to quantify the DW surface energy is still missing. Here, we report a
series of experiments in which the DW surface energy becomes a dominant
parameter. We observed that a semicircular magnetic domain bubble could
spontaneously collapse under the Laplace pressure induced by DW surface energy.
We further demonstrated that the surface energy could lead to a geometrically
induced pinning when the DW propagates in a Hall cross or from a nanowire into
a nucleation pad. Based on these observations, we developed two methods to
quantify the DW surface energy, which could be very helpful to estimate
intrinsic parameters such as Dzyaloshinskii-Moriya Interactions (DMI) or
exchange stiffness in magnetic ultra-thin films.
",physics
"  Our daily perceptual experience is driven by different neural mechanisms that
yield multisensory interaction as the interplay between exogenous stimuli and
endogenous expectations. While the interaction of multisensory cues according
to their spatiotemporal properties and the formation of multisensory
feature-based representations have been widely studied, the interaction of
spatial-associative neural representations has received considerably less
attention. In this paper, we propose a neural network architecture that models
the interaction of spatial-associative representations to perform causal
inference of audiovisual stimuli. We investigate the spatial alignment of
exogenous audiovisual stimuli modulated by associative congruence. In the
spatial layer, topographically arranged networks account for the interaction of
audiovisual input in terms of population codes. In the associative layer,
congruent audiovisual representations are obtained via the experience-driven
development of feature-based associations. Levels of congruency are obtained as
a by-product of the neurodynamics of self-organizing networks, where the amount
of neural activation triggered by the input can be expressed via a nonlinear
distance function. Our novel proposal is that activity-driven levels of
congruency can be used as top-down modulatory projections to spatially
distributed representations of sensory input, e.g. semantically related
audiovisual pairs will yield a higher level of integration than unrelated
pairs. Furthermore, levels of neural response in unimodal layers may be seen as
sensory reliability for the dynamic weighting of crossmodal cues. We describe a
series of planned experiments to validate our model in the tasks of
multisensory interaction on the basis of semantic congruence and unimodal cue
reliability.
",quantitative-biology
"  A new type of quadrature is developed. The Gauss quadrature, for a given
measure, finds optimal values of a function's argument (nodes) and the
corresponding weights. In contrast, the Lebesgue quadrature developed in this
paper, finds optimal values of function (value-nodes) and the corresponding
weights. The Gauss quadrature groups sums by function argument, it can be
viewed as a $n$-point discrete measure, producing the Riemann integral. The
Lebesgue quadrature groups sums by function value, it can be viewed as a
$n$-point discrete distribution, producing the Lebesgue integral.
Mathematically, the problem is reduced to a generalized eigenvalue problem:
Lebesgue quadrature value-nodes are the eigenvalues and the corresponding
weights are the square of the averaged eigenvectors. A numerical estimation of
an integral as the Lebesgue integral is especially advantageous when analyzing
irregular and stochastic processes. The approach separates the outcome
(value-nodes) and the probability of the outcome (weight). For this reason, it
is especially well-suited for the study of non--Gaussian processes. The
software implementing the theory is available from the authors.
",statistics
"  We develop a Liouville perturbation theory for weakly driven and weakly open
quantum systems in situations when the unperturbed system has a number of
conservations laws. If the perturbation violates the conservation laws, it
drives the system to a new steady state which can be approximately but
efficiently described by a (generalized) Gibbs ensemble characterized by one
Lagrange parameter for each conservation law. The value of those has to be
determined from rate equations for conserved quantities. Remarkably, even weak
perturbations can lead to large responses of conserved quantities. We present a
perturbative expansion of the steady state density matrix; first we give the
condition that fixes the zeroth order expression (Lagrange parameters) and then
determine the higher order corrections via projections of the Liouvillian. The
formalism can be applied to a wide range of problems including two-temperature
models for electron-phonon systems, Bose condensates of excitons or photons or
weakly perturbed integrable models. We test our formalism by studying
interacting fermions coupled to non-thermal reservoirs, approximately described
by a Boltzmann equation.
",physics
"  Smartphones have become the most pervasive devices in people's lives, and are
clearly transforming the way we live and perceive technology. Today's
smartphones benefit from almost ubiquitous Internet connectivity and come
equipped with a plethora of inexpensive yet powerful embedded sensors, such as
accelerometer, gyroscope, microphone, and camera. This unique combination has
enabled revolutionary applications based on the mobile crowdsensing paradigm,
such as real-time road traffic monitoring, air and noise pollution, crime
control, and wildlife monitoring, just to name a few. Differently from prior
sensing paradigms, humans are now the primary actors of the sensing process,
since they become fundamental in retrieving reliable and up-to-date information
about the event being monitored. As humans may behave unreliably or
maliciously, assessing and guaranteeing Quality of Information (QoI) becomes
more important than ever. In this paper, we provide a new framework for
defining and enforcing the QoI in mobile crowdsensing, and analyze in depth the
current state-of-the-art on the topic. We also outline novel research
challenges, along with possible directions of future work.
",computer-science
"  The high-energy non-thermal universe is dominated by power law-like spectra.
Therefore results in high-energy astronomy are often reported as parameters of
power law fits, or, in the case of a non-detection, as an upper limit assuming
the underlying unseen spectrum behaves as a power law. In this paper I
demonstrate a simple and powerful one-to-one relation of the integral upper
limit in the two dimensional power law parameter space into the spectrum
parameter space and use this method to unravel the so far convoluted question
of the sensitivity of astroparticle telescopes.
",physics
"  This study investigates short-crested wave breaking over a planar beach by
using the mesh-free Smoothed Particle Hydrodynamics model, GPUSPH. The
short-crested waves are created by generating intersecting wave trains in a
numerical wave basin. We examine the influence of beach slope, incident wave
height, and incident wave angle on the generated short-crested waves.
Short-crested wave breaking over a steeper beach generates stronger rip
currents, and larger circulation cells in front of the beach. Intersecting wave
trains with a larger incident wave height drive a more complicated
short-crested wave field including isolated breakers and wave amplitude
diffraction. Nearshore circulation induced by short-crested wave breaking is
greatly influenced by the incident wave angle (or the rip current spacing).
There is no secondary circulation cell between the nodal line and the antinodal
line if the rip current spacing is narrow. However, there are multiple
secondary circulation cells observed when the rip current spacing is relatively
large.
",physics
"  We perform direct numerical simulations (DNS) of passive heavy inertial
particles (dust) in homogeneous and isotropic two-dimensional turbulent flows
(gas) for a range of Stokes number, ${\rm St} < 1$, using both Lagrangian and
Eulerian approach (with a shock-capturing scheme). We find that: The
dust-density field in our Eulerian simulations have the same correlation
dimension $d_2$ as obtained from the clustering of particles in the Lagrangian
simulations for ${\rm St} < 1$; The cumulative probability distribution
function of the dust-density coarse-grained over a scale $r$ in the inertial
range has a left-tail with a power-law fall-off indicating presence of voids;
The energy spectrum of the dust-velocity has a power-law range with an exponent
that is same as the gas-velocity spectrum except at very high Fourier modes;
The compressibility of the dust-velocity field is proportional to ${\rm St}^2$.
We quantify the topological properties of the dust-velocity and the
gas-velocity through their gradient matrices, called $\mathcal{A}$ and
$\mathcal{B}$, respectively. The topological properties of $\mathcal{B}$ are
the same in Eulerian and Lagrangian frames only if the Eulerian data are
weighed by the dust-density -- a correspondence that we use to study Lagrangian
properties of $\mathcal{A}$. In the Lagrangian frame, the mean value of the
trace of $\mathcal{A} \sim - \exp(-C/{\rm St}$, with a constant $C\approx 0.1$.
The topology of the dust-velocity fields shows that as ${\rm St} increases the
contribution to negative divergence comes mostly from saddles and the
contribution to positive divergence comes from both vortices and saddles.
Compared to the Eulerian case, the density-weighed Eulerian case has less
inward spirals and more converging saddles. Outward spirals are the least
probable topological structures in both cases.
",physics
"  We derive general expressions for resonant inelastic x-ray scattering (RIXS)
operators for $t_{2g}$ orbital systems, which exhibit a rich array of
unconventional magnetism arising from unquenched orbital moments. Within the
fast collision approximation, which is valid especially for 4$d$ and 5$d$
transition metal compounds with short core-hole lifetimes, the RIXS operators
are expressed in terms of total spin and orbital angular momenta of the
constituent ions. We then map these operators onto pseudospins that represent
spin-orbit entangled magnetic moments in systems with strong spin-orbit
coupling. Applications of our theory to such systems as iridates and ruthenates
are discussed, with a particular focus on compounds based on $d^4$ ions with
Van Vleck-type nonmagnetic ground state.
",physics
"  A sieve for rational points on suitable varieties is developed, together with
applications to counting rational points in thin sets, the number of varieties
in a family which are everywhere locally soluble, and to the notion of friable
rational points with respect to divisors. In the special case of quadrics,
sharper estimates are obtained by developing a version of the Selberg sieve for
rational points.
",mathematics
"  In this paper we present AWEsome (Airborne Wind Energy Standardized
Open-source Model Environment), a test platform for airborne wind energy
systems that consists of low-cost hardware and is entirely based on open-source
software. It can hence be used without the need of large financial investments,
in particular by research groups and startups to acquire first experiences in
their flight operations, to test novel control strategies or technical designs,
or for usage in public relations. Our system consists of a modified
off-the-shelf model aircraft that is controlled by the pixhawk autopilot
hardware and the ardupilot software for fixed wing aircraft. The aircraft is
attached to the ground by a tether. We have implemented new flight modes for
the autonomous tethered flight of the aircraft along periodic patterns. We
present the principal functionality of our algorithms. We report on first
successful tests of these modes in real flights.
",computer-science
"  The control of dynamical, networked systems continues to receive much
attention across the engineering and scientific research fields. Of particular
interest is the proper way to determine which nodes of the network should
receive external control inputs in order to effectively and efficiently control
portions of the network. Published methods to accomplish this task either find
a minimal set of driver nodes to guarantee controllability or a larger set of
driver nodes which optimizes some control metric. Here, we investigate the
control of lattice systems which provides analytical insight into the
relationship between network structure and controllability. First we derive a
closed form expression for the individual elements of the controllability
Gramian of infinite lattice systems. Second, we focus on nearest neighbor
lattices for which the distance between nodes appears in the expression for the
controllability Gramian. We show that common control energy metrics scale
exponentially with respect to the maximum distance between a driver node and a
target node.
",computer-science
"  Unification and generalization are operations on two terms computing
respectively their greatest lower bound and least upper bound when the terms
are quasi-ordered by subsumption up to variable renaming (i.e., $t_1\preceq
t_2$ iff $t_1 = t_2\sigma$ for some variable substitution $\sigma$). When term
signatures are such that distinct functor symbols may be related with a fuzzy
equivalence (called a similarity), these operations can be formally extended to
tolerate mismatches on functor names and/or arity or argument order. We
reformulate and extend previous work with a declarative approach defining
unification and generalization as sets of axioms and rules forming a complete
constraint-normalization proof system. These include the Reynolds-Plotkin
term-generalization procedures, Maria Sessa's ""weak"" unification with partially
fuzzy signatures and its corresponding generalization, as well as novel
extensions of such operations to fully fuzzy signatures (i.e., similar functors
with possibly different arities). One advantage of this approach is that it
requires no modification of the conventional data structures for terms and
substitutions. This and the fact that these declarative specifications are
efficiently executable conditional Horn-clauses offers great practical
potential for fuzzy information-handling applications.
",computer-science
"  Out-of-time-order (OTO) operators have recently become popular diagnostics of
quantum chaos in many-body systems. The usual way they are introduced is via a
quantization of classical Lyapunov growth, which measures the divergence of
classical trajectories in phase space due to the butterfly effect. However, it
is not obvious how exactly they capture the sensitivity of a quantum system to
its initial conditions beyond the classical limit. In this paper, we analyze
sensitivity to initial conditions in the quantum regime by recasting OTO
operators for many-body systems using various formulations of quantum
mechanics. Notably, we utilize the Wigner phase space formulation to derive an
$\hbar$-expansion of the OTO operator for spatial degrees of freedom, and a
large spin $1/s$-expansion for spin degrees of freedom. We find in each case
that the leading term is the Lyapunov growth for the classical limit of the
system and argue that quantum corrections become dominant at around the
scrambling time, which is also when we expect the OTO operator to saturate. We
also express the OTO operator in terms of propagators and see from a different
point of view how it is a quantum generalization of the divergence of classical
trajectories.
",physics
"  New upper bounds on the pointwise behaviour of Christoffel function on convex
domains in ${\mathbb{R}}^d$ are obtained. These estimates are established by
explicitly constructing the corresponding ""needle""-like algebraic polynomials
having small integral norm on the domain, and are stated in terms of few
easy-to-measure geometric characteristics of the location of the point of
interest in the domain. Sharpness of the results is shown and examples of
applications are given.
",mathematics
"  We investigate the Fredholm alternative for the $p$-Laplacian in an exterior
domain which is the complement of the closed unit ball in $\mathbb{R}^N$
($N\geq 2$). By employing techniques of Calculus of Variations we obtain the
multiplicity of solutions. The striking difference between our case and the
entire space case is also discussed.
",mathematics
"  Continuous integration (CI) tools integrate code changes by automatically
compiling, building, and executing test cases upon submission of code changes.
Use of CI tools is getting increasingly popular, yet how proprietary projects
reap the benefits of CI remains unknown. To investigate the influence of CI on
software development, we analyze 150 open source software (OSS) projects, and
123 proprietary projects. For OSS projects, we observe the expected benefits
after CI adoption, e.g., improvements in bug and issue resolution. However, for
the proprietary projects, we cannot make similar observations. Our findings
indicate that only adoption of CI might not be enough to the improve software
development process. CI can be effective for software development if
practitioners use CI's feedback mechanism efficiently, by applying the practice
of making frequent commits. For our set of proprietary projects we observe
practitioners commit less frequently, and hence not use CI effectively for
obtaining feedback on the submitted code changes. Based on our findings we
recommend industry practitioners to adopt the best practices of CI to reap the
benefits of CI tools for example, making frequent commits.
",computer-science
"  During routine state space circuit analysis of an arbitrarily connected set
of nodes representing a lossless LC network, a matrix was formed that was
observed to implicitly capture connectivity of the nodes in a graph similar to
the conventional incidence matrix, but in a slightly different manner. This
matrix has only 0, 1 or -1 as its elements. A sense of direction (of the graph
formed by the nodes) is inherently encoded in the matrix because of the
presence of -1. It differs from the incidence matrix because of leaving out the
datum node from the matrix. Calling this matrix as forward adjacency matrix, it
was found that its inverse also displays useful and interesting physical
properties when a specific style of node-indexing is adopted for the nodes in
the graph. The graph considered is connected but does not have any closed
loop/cycle (corresponding to closed loop of inductors in a circuit) as with its
presence the matrix is not invertible. Incidentally, by definition the graph
being considered is a tree. The properties of the forward adjacency matrix and
its inverse, along with rigorous proof, are presented.
",computer-science
"  We propose a new algorithm for the computation of a singular value
decomposition (SVD) low-rank approximation of a matrix in the Matrix Product
Operator (MPO) format, also called the Tensor Train Matrix format. Our tensor
network randomized SVD (TNrSVD) algorithm is an MPO implementation of the
randomized SVD algorithm that is able to compute dominant singular values and
their corresponding singular vectors. In contrast to the state-of-the-art
tensor-based alternating least squares SVD (ALS-SVD) and modified alternating
least squares SVD (MALS-SVD) matrix approximation methods, TNrSVD can be up to
17 times faster while achieving the same accuracy. In addition, our TNrSVD
algorithm also produces accurate approximations in particular cases where both
ALS-SVD and MALS-SVD fail to converge. We also propose a new algorithm for the
fast conversion of a sparse matrix into its corresponding MPO form, which is up
to 509 times faster than the standard Tensor Train SVD (TT-SVD) method while
achieving machine precision accuracy. The efficiency and accuracy of both
algorithms are demonstrated in numerical experiments.
",computer-science
"  We derive a semi-analytic formula for the transition probability of
three-dimensional Brownian motion in the positive octant with absorption at the
boundaries. Separation of variables in spherical coordinates leads to an
eigenvalue problem for the resulting boundary value problem in the two angular
components. The main theoretical result is a solution to the original problem
expressed as an expansion into special functions and an eigenvalue which has to
be chosen to allow a matching of the boundary condition. We discuss and test
several computational methods to solve a finite-dimensional approximation to
this nonlinear eigenvalue problem. Finally, we apply our results to the
computation of default probabilities and credit valuation adjustments in a
structural credit model with mutual liabilities.
",quantitative-finance
"  Followership is generally defined as a strategy that evolved to solve social
coordination problems, and particularly those involved in group movement.
Followership behaviour is particularly interesting in the context of
road-crossing behaviour because it involves other principles such as
risk-taking and evaluating the value of social information. This study sought
to identify the cognitive mechanisms underlying decision-making by pedestrians
who follow another person across the road at the green or at the red light in
two different countries (France and Japan). We used agent-based modelling to
simulate the road-crossing behaviours of pedestrians. This study showed that
modelling is a reliable means to test different hypotheses and find the exact
processes underlying decision-making when crossing the road. We found that two
processes suffice to simulate pedestrian behaviours. Importantly, the study
revealed differences between the two nationalities and between sexes in the
decision to follow and cross at the green and at the red light. Japanese
pedestrians are particularly attentive to the number of already departed
pedestrians and the number of waiting pedestrians at the red light, whilst
their French counterparts only consider the number of pedestrians that have
already stepped off the kerb, thus showing the strong conformism of Japanese
people. Finally, the simulations are revealed to be similar to observations,
not only for the departure latencies but also for the number of crossing
pedestrians and the rates of illegal crossings. The conclusion suggests new
solutions for safety in transportation research.
",quantitative-biology
"  We start from a variational model for nematic elastomers that involves two
energies: mechanical and nematic. The first one consists of a nonlinear elastic
energy which is influenced by the orientation of the molecules of the nematic
elastomer. The nematic energy is an Oseen--Frank energy in the deformed
configuration. The constraint of the positivity of the determinant of the
deformation gradient is imposed. The functionals are not assumed to have the
usual polyconvexity or quasiconvexity assumptions to be lower semicontinuous.
We instead compute its relaxation, that is, the lower semicontinuous envelope,
which turns out to be the quasiconvexification of the mechanical term plus the
tangential quasiconvexification of the nematic term. The main assumptions are
that the quasiconvexification of the mechanical term is polyconvex and that the
deformation is in the Sobolev space $W^{1,p}$ (with $p>n-1$ and $n$ the
dimension of the space) and does not present cavitation.
",mathematics
"  Artificial ice systems have unique physical properties promising for
potential applications. One of the most challenging issues in this field is to
find novel ice systems that allows a precise control over the geometries and
many-body interactions. Superconducting vortex matter has been proposed as a
very suitable candidate to study artificial ice, mainly due to availability of
tunable vortex-vortex interactions and the possibility to fabricate a variety
of nanoscale pinning potential geometries. So far, a detailed imaging of the
local configurations in a vortex-based artificial ice system is still lacking.
Here we present a direct visualization of the vortex ice state in a
nanostructured superconductor. By using the scanning Hall probe microscopy, a
large area with the vortex ice ground state configuration has been detected,
which confirms the recent theoretical predictions for this new ice system.
Besides the defects analogous to artificial spin ice systems, other types of
defects have been visualized and identified. We also demonstrate the
possibility to realize different types of defects by varying the magnetic
field.
",physics
"  Plasmonic metasurfaces have been employed for tuning and controlling light
enabling various novel applications. Their appeal is enhanced with the
incorporation of an active element with the metasurfaces paving the way for
dynamic control. In this letter, we realize a dynamic polarization state
generator using graphene-integrated anisotropic metasurface (GIAM), where a
linear incidence polarization is controllably converted into an elliptical one.
The anisotropic metasurface leads to an intrinsic polarization conversion when
illuminated with non-orthogonal incident polarization. Additionally, the
single-layer graphene allows us to tune the phase and intensity of the
reflected light on the application of a gate voltage, enabling dynamic
polarization control. The stokes polarization parameters of the reflected light
are measured using rotating polarizer method and it is demonstrated that a
large change in the ellipticity as well as orientation angle can be induced by
this device. We also provide experimental evidence that the titl angle can
change independent of the ellipticity going from positive values to nearly zero
to negative values while ellipticity is constant.
",physics
"  When we are faced with challenging image classification tasks, we often
explain our reasoning by dissecting the image, and pointing out prototypical
aspects of one class or another. The mounting evidence for each of the classes
helps us make our final decision. In this work, we introduce a deep network
architecture that reasons in a similar way: the network dissects the image by
finding prototypical parts, and combines evidence from the prototypes to make a
final classification. The model thus reasons in a way that is qualitatively
similar to the way ornithologists, physicians, geologists, architects, and
others would explain to people on how to solve challenging image classification
tasks. The network uses only image-level labels for training, meaning that
there are no labels for parts of images. We demonstrate our method on the
CUB-200-2011 dataset and the CBIS-DDSM dataset. Our experiments show that our
interpretable network can achieve comparable accuracy with its analogous
standard non-interpretable counterpart as well as other interpretable deep
models.
",statistics
"  Zero-shot learning (ZSL) is a challenging task aiming at recognizing novel
classes without any training instances. In this paper we present a simple but
high-performance ZSL approach by generating pseudo feature representations
(GPFR). Given the dataset of seen classes and side information of unseen
classes (e.g. attributes), we synthesize feature-level pseudo representations
for novel concepts, which allows us access to the formulation of unseen class
predictor. Firstly we design a Joint Attribute Feature Extractor (JAFE) to
acquire understandings about attributes, then construct a cognitive repository
of attributes filtered by confidence margins, and finally generate pseudo
feature representations using a probability based sampling strategy to
facilitate subsequent training process of class predictor. We demonstrate the
effectiveness in ZSL settings and the extensibility in supervised recognition
scenario of our method on a synthetic colored MNIST dataset (C-MNIST). For
several popular ZSL benchmark datasets, our approach also shows compelling
results on zero-shot recognition task, especially leading to tremendous
improvement to state-of-the-art mAP on zero-shot retrieval task.
",computer-science
"  Community analysis is an important way to ascertain whether or not a complex
system consists of sub-structures with different properties. In this paper, we
give a two level community structure analysis for the SSCI journal system by
most similar co-citation pattern. Five different strategies for the selection
of most similar node (journal) pairs are introduced. The efficiency is checked
by the normalized mutual information technique. Statistical properties and
comparisons of the community results show that both of the two level detection
could give instructional information for the community structure of complex
systems. Further comparisons of the five strategies indicates that, the most
efficient strategy is to assign nodes with maximum similarity into the same
community whether the similarity information is complete or not, while random
selection generates small world local community with no inside order. These
results give valuable indication for efficient community detection by most
similar node pairs.
",computer-science
"  We consider free rotation of a body whose parts move slowly with respect to
each other under the action of internal forces. This problem can be considered
as a perturbation of the Euler-Poinsot problem. The dynamics has an approximate
conservation law - an adiabatic invariant. This allows to describe the
evolution of rotation in the adiabatic approximation. The evolution leads to an
overturn in the rotation of the body: the vector of angular velocity crosses
the separatrix of the Euler-Poinsot problem. This crossing leads to a
quasi-random scattering in body's dynamics. We obtain formulas for
probabilities of capture into different domains in the phase space at
separatrix crossings.
",physics
"  This article is concerned with quantitative unique continuation estimates for
equations involving a ""sum of squares"" operator $\mathcal{L}$ on a compact
manifold $\mathcal{M}$ assuming: $(i)$ the Chow-Rashevski-Hörmander condition
ensuring the hypoellipticity of $\mathcal{L}$, and $(ii)$ the analyticity of
$\mathcal{M}$ and the coefficients of $\mathcal{L}$.
The first result is the tunneling estimate $\|\varphi\|_{L^2(\omega)} \geq
Ce^{- \lambda^{\frac{k}{2}}}$ for normalized eigenfunctions $\varphi$ of
$\mathcal{L}$ from a nonempty open set $\omega\subset \mathcal{M}$, where $k$
is the hypoellipticity index of $\mathcal{L}$ and $\lambda$ the eigenvalue.
The main result is a stability estimate for solutions to the hypoelliptic
wave equation $(\partial_t^2+\mathcal{L})u=0$: for $T>2 \sup_{x \in
\mathcal{M}}(dist(x,\omega))$ (here, $dist$ is the sub-Riemannian distance),
the observation of the solution on $(0,T)\times \omega$ determines the data.
The constant involved in the estimate is $Ce^{c\Lambda^k}$ where $\Lambda$ is
the typical frequency of the data.
We then prove the approximate controllability of the hypoelliptic heat
equation $(\partial_t+\mathcal{L})v=1_\omega f$ in any time, with appropriate
(exponential) cost, depending on $k$. In case $k=2$ (Grushin, Heisenberg...),
we further show approximate controllability to trajectories with polynomial
cost in large time.
We also explain how the analyticity assumption can be relaxed, and a boundary
$\partial \mathcal{M}$ can be added in some situations.
Most results turn out to be optimal on a family of Grushin-type operators.
The main proof relies on the general strategy developed by the authors in
arXiv:1506.04254.
",mathematics
"  In this paper, we provide an analysis of self-organized network management,
with an end-to-end perspective of the network. Self-organization as applied to
cellular networks is usually referred to Self-organizing Networks (SONs), and
it is a key driver for improving Operations, Administration, and Maintenance
(OAM) activities. SON aims at reducing the cost of installation and management
of 4G and future 5G networks, by simplifying operational tasks through the
capability to configure, optimize and heal itself. To satisfy 5G network
management requirements, this autonomous management vision has to be extended
to the end to end network. In literature and also in some instances of products
available in the market, Machine Learning (ML) has been identified as the key
tool to implement autonomous adaptability and take advantage of experience when
making decisions. In this paper, we survey how network management can
significantly benefit from ML solutions. We review and provide the basic
concepts and taxonomy for SON, network management and ML. We analyse the
available state of the art in the literature, standardization, and in the
market. We pay special attention to 3rd Generation Partnership Project (3GPP)
evolution in the area of network management and to the data that can be
extracted from 3GPP networks, in order to gain knowledge and experience in how
the network is working, and improve network performance in a proactive way.
Finally, we go through the main challenges associated with this line of
research, in both 4G and in what 5G is getting designed, while identifying new
directions for research.
",computer-science
"  We present an overview of recently developed data-driven tools for safety
analysis of autonomous vehicles and advanced driver assist systems. The core
algorithms combine model-based, hybrid system reachability analysis with
sensitivity analysis of components with unknown or inaccessible models. We
illustrate the applicability of this approach with a new case study of
emergency braking systems in scenarios with two or three vehicles. This problem
is representative of the most common type of rear-end crashes, which is
relevant for safety analysis of automatic emergency braking (AEB) and forward
collision avoidance systems. We show that our verification tool can effectively
prove the safety of certain scenarios (specified by several parameters like
braking profiles, initial velocities, uncertainties in position and reaction
times), and also compute the severity of accidents for unsafe scenarios.
Through hundreds of verification experiments, we quantified the safety envelope
of the system across relevant parameters. These results show that the approach
is promising for design, debugging and certification. We also show how the
reachability analysis can be combined with statistical information about the
parameters, to assess the risk level of the control system, which in turn is
essential, for example, for determining Automotive Safety Integrity Levels
(ASIL) for the ISO26262 standard.
",computer-science
"  The fitness of a species determines its abundance and survival in an
ecosystem. At the same time, species take up resources for growth, so their
abundance affects the availability of resources in an ecosystem. We show here
that such species-resource coupling can be used to assign a quantitative metric
for fitness to each species. This fitness metric also allows for the modeling
of drift in species composition, and hence ecosystem evolution through
speciation and adaptation. Our results provide a foundation for an entirely
computational exploration of evolutionary ecosystem dynamics on any length or
time scale. For example, we can evolve ecosystem dynamics even by initiating
dynamics out of a single primordial ancestor and show that there exists a well
defined ecosystem-averaged fitness dynamics that is resilient against resource
shocks.
",quantitative-biology
"  We report a nontrivial transition in the core structure of vortices in
two-band superconductors as a function of interband impurity scattering. We
demonstrate that, in addition to singular zeros of the order parameter, the
vortices there can acquire a circular nodal line around the singular point in
one of the superconducting components. It results in the formation of the
peculiar ""moat""-like profile in one of the superconducting gaps. The moat-core
vortices occur generically in the vicinity of the impurity-induced crossover
between $s_{\pm}$ and $s_{++}$ states.
",physics
"  The foreseen implementations of the Small Size Telescopes (SST) in CTA will
provide unique insights into the highest energy gamma rays offering fundamental
means to discover and under- stand the sources populating the Galaxy and our
local neighborhood. Aiming at such a goal, the SST-1M is one of the three
different implementations that are being prototyped and tested for CTA. SST-1M
is a Davies-Cotton single mirror telescope equipped with a unique camera
technology based on SiPMs with demonstrated advantages over classical
photomultipliers in terms of duty-cycle. In this contribution, we describe the
telescope components, the camera, and the trigger and readout system. The
results of the commissioning of the camera using a dedicated test setup are
then presented. The performances of the camera first prototype in terms of
expected trigger rates and trigger efficiencies for different night-sky
background conditions are presented, and the camera response is compared to
end-to-end simulations.
",physics
"  In network coding, we discuss the effect of sequential error injection on
information leakage. We show that there is no improvement when the operations
in the network are linear operations. However, when the operations in the
network contains non-linear operations, we find a counterexample to improve
Eve's obtained information. Furthermore, we discuss the asymptotic rate in a
linear network under the secrecy and robustness conditions as well as under the
secrecy condition alone. Finally, we apply our results to network quantum key
distribution, which clarifies the type of network that enables us to realize
secure long distance communication via short distance quantum key distribution.
",computer-science
"  We primarily study a special a weighted low-rank approximation of matrices
and then apply it to solve the background modeling problem. We propose two
algorithms for this purpose: one operates in the batch mode on the entire data
and the other one operates in the batch-incremental mode on the data and
naturally captures more background variations and computationally more
effective. Moreover, we propose a robust technique that learns the background
frame indices from the data and does not require any training frames. We
demonstrate through extensive experiments that by inserting a simple weight in
the Frobenius norm, it can be made robust to the outliers similar to the
$\ell_1$ norm. Our methods match or outperform several state-of-the-art online
and batch background modeling methods in virtually all quantitative and
qualitative measures.
",computer-science
"  In this paper, we consider the problem of identifying the type (local
minimizer, maximizer or saddle point) of a given isolated real critical point
$c$, which is degenerate, of a multivariate polynomial function $f$. To this
end, we introduce the definition of faithful radius of $c$ by means of the
curve of tangency of $f$. We show that the type of $c$ can be determined by the
global extrema of $f$ over the Euclidean ball centered at $c$ with a faithful
radius.We propose algorithms to compute a faithful radius of $c$ and determine
its type.
",mathematics
"  Ordered chains (such as chains of amino acids) are ubiquitous in biological
cells, and these chains perform specific functions contingent on the sequence
of their components. Using the existence and general properties of such
sequences as a theoretical motivation, we study the statistical physics of
systems whose state space is defined by the possible permutations of an ordered
list, i.e., the symmetric group, and whose energy is a function of how certain
permutations deviate from some chosen correct ordering. Such a non-factorizable
state space is quite different from the state spaces typically considered in
statistical physics systems and consequently has novel behavior in systems with
interacting and even non-interacting Hamiltonians. Various parameter choices of
a mean-field model reveal the system to contain five different physical regimes
defined by two transition temperatures, a triple point, and a quadruple point.
Finally, we conclude by discussing how the general analysis can be extended to
state spaces with more complex combinatorial properties and to other standard
questions of statistical mechanics models.
",physics
"  A modified AC method based on micro-fabricated heater and resistive
thermometers has been applied to measure the thermopower of microscale samples.
A sinusoidal current with frequency {\omega} is passed to the heater to
generate an oscillatory temperature difference across the sample at a frequency
2{\omega}, which simultaneously induces an AC thermoelectric voltage, also at
the frequency 2{\omega}. A key step of the method is to extract amplitude and
phase of the oscillatory temperature difference by probing the AC temperature
variation at each individual thermometer. The sign of the thermopower is
determined by examining the phase difference between the oscillatory
temperature difference and the AC thermoelectric voltage. The technique has
been compared with the popular DC method by testing both n-type and p-type thin
film samples. Both methods yielded consistent results, which verified the
reliability of the newly proposed AC method.
",physics
"  Compact substructure is expected to arise in a starless core as mass becomes
concentrated in the central region likely to form a protostar. Additionally,
multiple peaks may form if fragmentation occurs. We present ALMA Cycle 2
observations of 60 starless and protostellar cores in the Ophiuchus molecular
cloud. We detect eight compact substructures which are >15 arcsec from the
nearest Spitzer YSO. Only one of these has strong evidence for being truly
starless after considering ancillary data, e.g., from Herschel and X-ray
telescopes. An additional extended emission structure has tentative evidence
for starlessness. The number of our detections is consistent with estimates
from a combination of synthetic observations of numerical simulations and
analytical arguments. This result suggests that a similar ALMA study in the
Chamaeleon I cloud, which detected no compact substructure in starless cores,
may be due to the peculiar evolutionary state of cores in that cloud.
",physics
"  Several active areas of research in novel energy storage technologies,
including three-dimensional solid state batteries and passivation coatings for
reactive battery electrode components, require conformal solid state
electrolytes. We describe an atomic layer deposition (ALD) process for a member
of the lithium phosphorus oxynitride (LiPON) family, which is employed as a
thin film lithium-conducting solid electrolyte. The reaction between lithium
tert-butoxide (LiO$^t$Bu) and diethyl phosphoramidate (DEPA) produces
conformal, ionically conductive thin films with a stoichiometry close to
Li$_2$PO$_2$N between 250 and 300$^\circ$C. The P/N ratio of the films is
always 1, indicative of a particular polymorph of LiPON which closely resembles
a polyphosphazene. Films grown at 300$^\circ$C have an ionic conductivity of
$6.51\:(\pm0.36)\times10^{-7}$ S/cm at 35$^\circ$C, and are functionally
electrochemically stable in the window from 0 to 5.3V vs. Li/Li$^+$. We
demonstrate the viability of the ALD-grown electrolyte by integrating it into
full solid state batteries, including thin film devices using LiCoO$_2$ as the
cathode and Si as the anode operating at up to 1 mA/cm$^2$. The high quality of
the ALD growth process allows pinhole-free deposition even on rough crystalline
surfaces, and we demonstrate the fabrication and operation of thin film
batteries with the thinnest (<100nm) solid state electrolytes yet reported.
Finally, we show an additional application of the moderate-temperature ALD
process by demonstrating a flexible solid state battery fabricated on a polymer
substrate.
",physics
"  We present a non-perturbative numerical technique for calculating strong
light shifts in atoms under the influence of multiple optical fields with
arbitrary polarization. We confirm our technique experimentally by performing
spectroscopy of a cloud of cold $^{87}$Rb atoms subjected to $\sim$ kW/cm$^2$
intensities of light at 1560.492 nm simultaneous with 1529.269 nm or 1529.282
nm. In these conditions the excited state resonances at 1529.26 nm and 1529.36
nm induce strong level mixing and the shifts are highly nonlinear. By
absorption spectroscopy, we observe that the induced shifts of the 5P3/2
hyperfine Zeeman sublevels agree well with our theoretical predictions.. We
propose the application of our theory and experiment to accurate measurements
of excited-state electric-dipole matrix elements.
",physics
"  Effects of the structural distortion associated with the $\rm OsO_6$
octahedral rotation and tilting on the electronic band structure and magnetic
anisotropy energy for the $5d^3$ compound NaOsO$_3$ are investigated using the
density functional theory (DFT) and within a three-orbital model. Comparison of
the essential features of the DFT band structures with the three-orbital model
for both the undistorted and distorted structures provides insight into the
orbital and directional asymmetry in the electron hopping terms resulting from
the structural distortion. The orbital mixing terms obtained in the transformed
hopping Hamiltonian resulting from the octahedral rotations are shown to
account for the fine features in the DFT band structure. Staggered
magnetization and the magnetic character of states near the Fermi energy
indicate weak coupling behavior.
",physics
"  We find evidence for a strong thermal inversion in the dayside atmosphere of
the highly irradiated hot Jupiter WASP-18b (T$_{eq}=2411K$, $M=10.3M_{J}$)
based on emission spectroscopy from Hubble Space Telescope secondary eclipse
observations and Spitzer eclipse photometry. We demonstrate a lack of water
vapor in either absorption or emission at 1.4$\mu$m. However, we infer emission
at 4.5$\mu$m and absorption at 1.6$\mu$m that we attribute to CO, as well as a
non-detection of all other relevant species (e.g., TiO, VO). The most probable
atmospheric retrieval solution indicates a C/O ratio of 1 and a high
metallicity (C/H=$283^{+395}_{-138}\times$ solar). The derived composition and
T/P profile suggest that WASP-18b is the first example of both a planet with a
non-oxide driven thermal inversion and a planet with an atmospheric metallicity
inconsistent with that predicted for Jupiter-mass planets at $>2\sigma$. Future
observations are necessary to confirm the unusual planetary properties implied
by these results.
",physics
"  Assuming three strongly compact cardinals, it is consistent that \[ \aleph_1
< \mathrm{add}(\mathrm{null}) < \mathrm{cov}(\mathrm{null}) < \mathfrak{b} <
\mathfrak{d} < \mathrm{non}(\mathrm{null}) < \mathrm{cof}(\mathrm{null}) <
2^{\aleph_0}.\] Under the same assumption, it is consistent that \[ \aleph_1 <
\mathrm{add}(\mathrm{null}) < \mathrm{cov}(\mathrm{null}) <
\mathrm{non}(\mathrm{meager}) < \mathrm{cov}(\mathrm{meager}) <
\mathrm{non}(\mathrm{null}) < \mathrm{cof}(\mathrm{null}) < 2^{\aleph_0}.\]
",mathematics
"  Network pruning is aimed at imposing sparsity in a neural network
architecture by increasing the portion of zero-valued weights for reducing its
size regarding energy-efficiency consideration and increasing evaluation speed.
In most of the conducted research efforts, the sparsity is enforced for network
pruning without any attention to the internal network characteristics such as
unbalanced outputs of the neurons or more specifically the distribution of the
weights and outputs of the neurons. That may cause severe accuracy drop due to
uncontrolled sparsity. In this work, we propose an attention mechanism that
simultaneously controls the sparsity intensity and supervised network pruning
by keeping important information bottlenecks of the network to be active. On
CIFAR-10, the proposed method outperforms the best baseline method by 6% and
reduced the accuracy drop by 2.6x at the same level of sparsity.
",statistics
"  Here we deconstruct, and then in a reasoned way reconstruct, the concept of
""entropy of a system,"" paying particular attention to where the randomness may
be coming from. We start with the core concept of entropy as a COUNT associated
with a DESCRIPTION; this count (traditionally expressed in logarithmic form for
a number of good reasons) is in essence the number of possibilities---specific
instances or ""scenarios,"" that MATCH that description. Very natural (and
virtually inescapable) generalizations of the idea of description are the
probability distribution and of its quantum mechanical counterpart, the density
operator.
We track the process of dynamically updating entropy as a system evolves.
Three factors may cause entropy to change: (1) the system's INTERNAL DYNAMICS;
(2) unsolicited EXTERNAL INFLUENCES on it; and (3) the approximations one has
to make when one tries to predict the system's future state. The latter task is
usually hampered by hard-to-quantify aspects of the original description,
limited data storage and processing resource, and possibly algorithmic
inadequacy. Factors 2 and 3 introduce randomness into one's predictions and
accordingly degrade them. When forecasting, as long as the entropy bookkeping
is conducted in an HONEST fashion, this degradation will ALWAYS lead to an
entropy increase.
To clarify the above point we introduce the notion of HONEST ENTROPY, which
coalesces much of what is of course already done, often tacitly, in responsible
entropy-bookkeping practice. This notion, we believe, will help to fill an
expressivity gap in scientific discourse. With its help we shall prove that ANY
dynamical system---not just our physical universe---strictly obeys Clausius's
original formulation of the second law of thermodynamics IF AND ONLY IF it is
invertible. Thus this law is a TAUTOLOGICAL PROPERTY of invertible systems!
",physics
"  Previously, the controllability problem of a linear time-invariant dynamical
system was mapped to the maximum matching (MM) problem on the bipartite
representation of the underlying directed graph, and the sizes of MMs on random
bipartite graphs were calculated analytically with the cavity method at zero
temperature limit. Here we present an alternative theory to estimate MM sizes
based on the core percolation theory and the perfect matching of cores. Our
theory is much more simplified and easily interpreted, and can estimate MM
sizes on random graphs with or without symmetry between out- and in-degree
distributions. Our result helps to illuminate the fundamental connection
between the controllability problem and the underlying structure of complex
systems.
",computer-science
"  Todays, researchers in the field of Pulmonary Embolism (PE) analysis need to
use a publicly available dataset to assess and compare their methods. Different
systems have been designed for the detection of pulmonary embolism (PE), but
none of them have used any public datasets. All papers have used their own
private dataset. In order to fill this gap, we have collected 5160 slices of
computed tomography angiography (CTA) images acquired from 20 patients, and
after labeling the image by experts in this field, we provided a reliable
dataset which is now publicly available. In some situation, PE detection can be
difficult, for example when it occurs in the peripheral branches or when
patients have pulmonary diseases (such as parenchymal disease). Therefore, the
efficiency of CAD systems highly depends on the dataset. In the given dataset,
66% of PE are located in peripheral branches, and different pulmonary diseases
are also included.
",computer-science
"  Crowded environments modify the diffusion of macromolecules, generally
slowing their movement and inducing transient anomalous subdiffusion. The
presence of obstacles also modifies the kinetics and equilibrium behavior of
tracers. While previous theoretical studies of particle diffusion have
typically assumed either impenetrable obstacles or binding interactions that
immobilize the particle, in many cellular contexts bound particles remain
mobile. Examples include membrane proteins or lipids with some entry and
diffusion within lipid domains and proteins that can enter into membraneless
organelles or compartments such as the nucleolus. Using a lattice model, we
studied the diffusive movement of tracer particles which bind to soft
obstacles, allowing tracers and obstacles to occupy the same lattice site. For
sticky obstacles, bound tracer particles are immobile, while for slippery
obstacles, bound tracers can hop without penalty to adjacent obstacles. In both
models, binding significantly alters tracer motion. The type and degree of
motion while bound is a key determinant of the tracer mobility: slippery
obstacles can allow nearly unhindered diffusion, even at high obstacle filling
fraction. To mimic compartmentalization in a cell, we examined how obstacle
size and a range of bound diffusion coefficients affect tracer dynamics. The
behavior of the model is similar in two and three spatial dimensions. Our work
has implications for protein movement and interactions within cells.
",physics
"  We report measurements of the $^{115}$In $7p_{1/2}$ and $7p_{3/2}$ scalar and
tensor polarizabilities using two-step diode laser spectroscopy in an atomic
beam. The scalar polarizabilities are one to two orders of magnitude larger
than for lower lying indium states due to the close proximity of the $7p$ and
$6d$ states. For the scalar polarizabilities, we find values (in atomic units)
of $1.811(4) \times 10^5$ $a_0^3$ and $2.876(6) \times 10^5$ $a_0^3$ for the
$7p_{1/2}$ and $7p_{3/2}$ states respectively. We estimate the smaller tensor
polarizability component of the $7p_{3/2}$ state to be $-1.43(18) \times 10^4$
$a_0^3$. These measurements represent the first high-precision benchmarks of
transition properties of such high excited states of trivalent atomic systems.
We also present new ab initio calculations of these quantities and other In
polarizabilities using two high-precision relativistic methods to make a global
comparison of the accuracies of the two approaches. The precision of the
experiment is sufficient to differentiate between the two theoretical methods
as well as to allow precise determination of the indium $7p-6d$ matrix
elements. The results obtained in this work are applicable to other heavier and
more complicated systems, and provide much needed guidance for the development
of even more precise theoretical approaches.
",physics
"  The goal of the paper is to study the angle between two curves in the
framework of metric (and metric measure) spaces. More precisely, we give a new
notion of angle between two curves in a metric space. Such a notion has a
natural interplay with optimal transportation and is particularly well suited
for metric measure spaces satisfying the curvature-dimension condition. Indeed
one of the main results is the validity of the cosine formula on $RCD^{*}(K,N)$
metric measure spaces. As a consequence, the new introduced notions are
compatible with the corresponding classical ones for Riemannian manifolds,
Ricci limit spaces and Alexandrov spaces.
",mathematics
"  Active communication between robots and humans is essential for effective
human-robot interaction. To accomplish this objective, Cloud Robotics (CR) was
introduced to make robots enhance their capabilities. It enables robots to
perform extensive computations in the cloud by sharing their outcomes. Outcomes
include maps, images, processing power, data, activities, and other robot
resources. But due to the colossal growth of data and traffic, CR suffers from
serious latency issues. Therefore, it is unlikely to scale a large number of
robots particularly in human-robot interaction scenarios, where responsiveness
is paramount. Furthermore, other issues related to security such as privacy
breaches and ransomware attacks can increase. To address these problems, in
this paper, we have envisioned the next generation of social robotic
architectures based on Fog Robotics (FR) that inherits the strengths of Fog
Computing to augment the future social robotic systems. These new architectures
can escalate the dexterity of robots by shoving the data closer to the robot.
Additionally, they can ensure that human-robot interaction is more responsive
by resolving the problems of CR. Moreover, experimental results are further
discussed by considering a scenario of FR and latency as a primary factor
comparing to CR models.
",computer-science
"  In this paper we analyze the local and global boundary rigidity problem for
general Riemannian manifolds with boundary $(M,g)$ whose boundary is strictly
convex. We show that the boundary distance function, i.e., $d_g|_{\partial
M\times\partial M}$, known over suitable open sets of $\partial M$ determines
$g$ in suitable corresponding open subsets of $M$, up to the natural
diffeomorphism invariance of the problem. We also show that if there is a
function on $M$ with suitable convexity properties relative to $g$ then
$d_g|_{\partial M\times\partial M}$ determines $g$ globally in the sense that
if $d_g|_{\partial M\times\partial M}=d_{\tilde g}|_{\partial M\times \partial
M}$ then there is a diffeomorphism $\psi$ fixing $\partial M$ (pointwise) such
that $g=\psi^*\tilde g$. This global assumption is satisfied, for instance, for
the distance function from a given point if the manifold has no focal points
(from that point).
We also consider the lens rigidity problem. The lens relation measures the
point of exit from $M$ and the direction of exit of geodesics issued from the
boundary and the length of the geodesic. The lens rigidity problem is whether
we can determine the metric up to isometry from the lens relation. We solve the
lens rigidity problem under the same global assumption mentioned above. This
shows, for instance, that manifolds with a strictly convex boundary and
non-positive sectional curvature are lens rigid.
The key tool is the analysis of the geodesic X-ray transform on 2-tensors,
corresponding to a metric $g$, in the normal gauge, such as normal coordinates
relative to a hypersurface, where one also needs to allow microlocal weights.
This is handled by refining and extending our earlier results in the solenoidal
gauge.
",mathematics
"  Life-expectancy is a complex outcome driven by genetic, socio-demographic,
environmental and geographic factors. Increasing socio-economic and health
disparities in the United States are propagating the longevity-gap, making it a
cause for concern. Earlier studies have probed individual factors but an
integrated picture to reveal quantifiable actions has been missing. There is a
growing concern about a further widening of healthcare inequality caused by
Artificial Intelligence (AI) due to differential access to AI-driven services.
Hence, it is imperative to explore and exploit the potential of AI for
illuminating biases and enabling transparent policy decisions for positive
social and health impact. In this work, we reveal actionable interventions for
decreasing the longevity-gap in the United States by analyzing a County-level
data resource containing healthcare, socio-economic, behavioral, education and
demographic features. We learn an ensemble-averaged structure, draw inferences
using the joint probability distribution and extend it to a Bayesian Decision
Network for identifying policy actions. We draw quantitative estimates for the
impact of diversity, preventive-care quality and stable-families within the
unified framework of our decision network. Finally, we make this analysis and
dashboard available as an interactive web-application for enabling users and
policy-makers to validate our reported findings and to explore the impact of
ones beyond reported in this work.
",statistics
"  We study four problems in the dynamics of a body moving about a fixed point,
providing a non-complex, analytical solution for all of them. For the first
two, we will work on the motion first integrals. For the symmetrical heavy
body, that is the Lagrange-Poisson case, we compute the second and third Euler
angles in explicit and real forms by means of multiple hypergeometric functions
(Lauricella, functions). Releasing the weight load but adding the complication
of the asymmetry, by means of elliptic integrals of third kind, we provide the
precession angle completing some previous treatments of the Euler-Poinsot case.
Integrating then the relevant differential equation, we reach the finite polar
equation of a special trajectory named the {\it herpolhode}. In the last
problem we keep the symmetry of the first problem, but without the weight, and
take into account a viscous dissipation. The approach of first integrals is no
longer practicable in this situation and the Euler equations are faced directly
leading to dumped goniometric functions obtained as particular occurrences of
Bessel functions of order $-1/2$.
",mathematics
"  Physics phenomena of multi-soliton complexes have enriched the life of
dissipative solitons in fiber lasers. By developing a birefringence-enhanced
fiber laser, we report the first experimental observation of
group-velocity-locked vector soliton (GVLVS) molecules. The
birefringence-enhanced fiber laser facilitates the generation of GVLVSs, where
the two orthogonally polarized components are coupled together to form a
multi-soliton complex. Moreover, the interaction of repulsive and attractive
forces between multiple pulses binds the particle-like GVLVSs together in time
domain to further form compound multi-soliton complexes, namely GVLVS
molecules. By adopting the polarization-resolved measurement, we show that the
two orthogonally polarized components of the GVLVS molecules are both soliton
molecules supported by the strongly modulated spectral fringes and the
double-humped intensity profiles. Additionally, GVLVS molecules with various
soliton separations are also observed by adjusting the pump power and the
polarization controller.
",physics
"  The geodetic VLBI technique is capable of measuring the Sun's gravity light
deflection from distant radio sources around the whole sky. This light
deflection is equivalent to the conventional gravitational delay used for the
reduction of geodetic VLBI data. While numerous tests based on a global set of
VLBI data have shown that the parameter 'gamma' of the post-Newtonian
approximation is equal to unity with a precision of about 0.02 percent, more
detailed analysis reveals some systematic deviations depending on the angular
elongation from the Sun. In this paper a limited set of VLBI observations near
the Sun were adjusted to obtain the estimate of the parameter 'gamma' free of
the elongation angle impact. The parameter 'gamma' is still found to be close
to unity with precision of 0.06 percent, two subsets of VLBI data measured at
short and long baselines produce some statistical inconsistency.
",physics
"  Let $\mathcal{B}$ denote a set of bicolorings of $[n]$, where each bicoloring
is a mapping of the points in $[n]$ to $\{-1,+1\}$.
For each $B \in \mathcal{B}$, let $Y_B=(B(1),\ldots,B(n))$.
For each $A \subseteq [n]$, let $X_A \in \{0,1\}^n$ denote the incidence
vector of $A$.
A non-empty set $A$ is said to be an `unbiased representative' for a
bicoloring $B \in \mathcal{B}$ if $\left\langle X_A,Y_B\right\rangle =0$.
Given a set $\mathcal{B}$ of bicolorings, we study the minimum cardinality of
a family $\mathcal{A}$ consisting of subsets of $[n]$ such that every
bicoloring in $\mathcal{B}$ has an unbiased representative in $\mathcal{A}$.
",computer-science
"  Quantum entanglement serves as a valuable resource for many important quantum
operations. A pair of entangled qubits can be shared between two agents by
first preparing a maximally entangled qubit pair at one agent, and then sending
one of the qubits to the other agent through a quantum channel. In this
process, the deterioration of entanglement is inevitable since the noise
inherent in the channel contaminates the qubit. To address this challenge,
various quantum entanglement distillation (QED) algorithms have been developed.
Among them, recurrence algorithms have advantages in terms of implementability
and robustness. However, the efficiency of recurrence QED algorithms has not
been investigated thoroughly in the literature. This paper put forth two
recurrence QED algorithms that adapt to the quantum channel to tackle the
efficiency issue. The proposed algorithms have guaranteed convergence for
quantum channels with two Kraus operators, which include phase-damping and
amplitude-damping channels. Analytical results show that the convergence speed
of these algorithms is improved from linear to quadratic and one of the
algorithms achieves the optimal speed. Numerical results confirm that the
proposed algorithms significantly improve the efficiency of QED.
",mathematics
"  In this paper, we outline the vision of chatbots that facilitate the
interaction between citizens and policy-makers at the city scale. We report the
results of a co-design session attended by more than 60 participants. We give
an outlook of how some challenges associated with such chatbot systems could be
addressed in the future.
",computer-science
"  Remote Attestation (RA) allows a trusted entity (verifier) to securely
measure internal state of a remote untrusted hardware platform (prover). RA can
be used to establish a static or dynamic root of trust in embedded and
cyber-physical systems. It can also be used as a building block for other
security services and primitives, such as software updates and patches,
verifiable deletion and memory resetting. There are three major classes of RA
designs: hardware-based, software-based, and hybrid, each with its own set of
benefits and drawbacks. This paper presents the first hybrid RA design, called
HYDRA, that builds upon formally verified software components that ensure
memory isolation and protection, as well as enforce access control to memory
and other resources. HYDRA obtains these properties by using the formally
verified seL4 microkernel. (Until now, this was only attainable with purely
hardware-based designs.) Using seL4 requires fewer hardware modifications to
the underlying microprocessor. Building upon a formally verified software
component increases confidence in security of the overall design of HYDRA and
its implementation. We instantiate HYDRA on two commodity hardware platforms
and assess the performance and overhead of performing RA on such platforms via
experimentation; we show that HYDRA can attest 10MB of memory in less than
500msec when using a Speck-based message authentication code (MAC) to compute a
cryptographic checksum over the memory to be attested.
",computer-science
"  GPUs and other accelerators are popular devices for accelerating
compute-intensive, parallelizable applications. However, programming these
devices is a difficult task. Writing efficient device code is challenging, and
is typically done in a low-level programming language. High-level languages are
rarely supported, or do not integrate with the rest of the high-level language
ecosystem. To overcome this, we propose compiler infrastructure to efficiently
add support for new hardware or environments to an existing programming
language.
We evaluate our approach by adding support for NVIDIA GPUs to the Julia
programming language. By integrating with the existing compiler, we
significantly lower the cost to implement and maintain the new compiler, and
facilitate reuse of existing application code. Moreover, use of the high-level
Julia programming language enables new and dynamic approaches for GPU
programming. This greatly improves programmer productivity, while maintaining
application performance similar to that of the official NVIDIA CUDA toolkit.
",computer-science
"  In this joint introduction to an Asterisque volume, we give a short
discussion of the historical developments in the study of nonlinear covering
groups, touching on their structure theory, representation theory and the
theory of automorphic forms. This serves as a historical motivation and sets
the scene for the papers in the volume. Our discussion is necessarily
subjective and will undoubtedly leave out the contributions of many authors, to
whom we apologize in earnest.
",mathematics
"  We establish a large deviation theorem for the empirical spectral
distribution of random covariance matrices whose entries are independent random
variables with mean 0, variance 1 and having controlled forth moments. Some new
properties of Laguerre polynomials are also given.
",mathematics
"  Space photometric missions have been steadily accumulating observations of
Cepheids in recent years, leading to a flow of new discoveries. In this short
review we summarize the findings provided by the early missions such as WIRE,
MOST, and CoRoT, and the recent results of the Kepler and K2 missions. The
surprising and fascinating results from the high-precision, quasi-continuous
data include the detection of the amplitude increase of Polaris, and exquisite
details about V1154 Cyg within the original Kepler field of view. We also
briefly discuss the current opportunities with the K2 mission, and the
prospects of the TESS space telescope regarding Cepheids.
",physics
"  Interbank markets are often characterised in terms of a core-periphery
network structure, with a highly interconnected core of banks holding the
market together, and a periphery of banks connected mostly to the core but not
internally. This paradigm has recently been challenged for short time scales,
where interbank markets seem better characterised by a bipartite structure with
more core-periphery connections than inside the core. Using a novel
core-periphery detection method on the eMID interbank market, we enrich this
picture by showing that the network is actually characterised by multiple
core-periphery pairs. Moreover, a transition from core-periphery to bipartite
structures occurs by shortening the temporal scale of data aggregation. We
further show how the global financial crisis transformed the market, in terms
of composition, multiplicity and internal organisation of core-periphery pairs.
By unveiling such a fine-grained organisation and transformation of the
interbank market, our method can find important applications in the
understanding of how distress can propagate over financial networks.
",quantitative-finance
"  Distributed control, as a potential solution to decreasing communication
demands in microgrids, has drawn much attention in recent years. Advantages of
distributed control have been extensively discussed, while its impacts on
microgrid performance and stability, especially in the case of communication
latency, have not been explicitly studied or fully understood yet. This paper
addresses this gap by proposing a generalized theoretical framework for
small-signal stability analysis and performance evaluation for microgrids using
distributed control. The proposed framework synthesizes generator and load
frequency-domain characteristics, primary and secondary control loops, as well
as the communication latency into a frequency-domain representation which is
further evaluated by the generalized Nyquist theorem. In addition, various
parameters and their impacts on microgrid dynamic performance are investigated
and summarized into guidelines to help better design the system. Case studies
demonstrate the effectiveness of the proposed approach.
",computer-science
"  The R package optimParallel provides a parallel version of the gradient-based
optimization methods of optim(). The main function of the package is
optimParallel(), which has the same usage and output as optim(). Using
optimParallel() can significantly reduce optimization times. We introduce the R
package and illustrate its implementation, which takes advantage of the lexical
scoping mechanism of R.
",statistics
"  Alastair Graham Walker Cameron was an astrophysicist and planetary scientist
of broad interests and exceptional originality. A founder of the field of
nuclear astrophysics, he developed the theoretical understanding of the
chemical elementsâ origins and made pioneering connections between the
abundances of elements in meteorites to advance the theory that the Moon
originated from a giant impact with the young Earth by an object at least the
size of Mars. Cameron was an early and persistent exploiter of computer
technology in the theoretical study of complex astronomical systemsâincluding
nuclear reactions in supernovae, the structure of neutron stars, and planetary
collisions.
",physics
"  We examine the kinematics of the gas in the environments of galaxies hosting
quasars at $z\sim2$. We employ 148 projected quasar pairs to study the
circumgalactic gas of the foreground quasars in absorption. The sample selects
foreground quasars with precise redshift measurements, using emission-lines
with precision $\lesssim300\,{\rm km\,s^{-1}}$ and average offsets from the
systemic redshift $\lesssim|100\,{\rm km\,s^{-1}}|$. We stack the background
quasar spectra at the foreground quasar's systemic redshift to study the mean
absorption in \ion{C}{2}, \ion{C}{4}, and \ion{Mg}{2}. We find that the mean
absorptions exhibit large velocity widths $\sigma_v\approx300\,{\rm
km\,s^{-1}}$. Further, the mean absorptions appear to be asymmetric about the
systemic redshifts. The mean absorption centroids exhibit small redshift
relative to the systemic $\delta v\approx+200\,{\rm km\,s^{-1}}$, with large
intrinsic scatter in the centroid velocities of the individual absorption
systems. We find the observed widths are consistent with gas in gravitational
motion and Hubble flow. However, while the observation of large widths alone
does not require galactic-scale outflows, the observed offsets suggest that the
gas is on average outflowing from the galaxy. The observed offsets also suggest
that the ionizing radiation from the foreground quasars is anisotropic and/or
intermittent.
",physics
"  We investigated the effect of out-of-plane crumpling on the mechanical
response of graphene membranes. In our experiments, stress was applied to
graphene membranes using pressurized gas while the strain state was monitored
through two complementary techniques: interferometric profilometry and Raman
spectroscopy. By comparing the data obtained through these two techniques, we
determined the geometric hidden area which quantifies the crumpling strength.
While the devices with hidden area $\sim0~\%$ obeyed linear mechanics with
biaxial stiffness $428\pm10$ N/m, specimens with hidden area in the range
$0.5-1.0~\%$ were found to obey an anomalous Hooke's law with an exponent
$\sim0.1$.
",physics
"  The study of relays with the scope of energy-harvesting (EH) looks
interesting as a means of enabling sustainable, wireless communication without
the need to recharge or replace the battery driving the relays. However,
reliability of such communication systems becomes an important design challenge
when such relays scavenge energy from the information bearing RF signals
received from the source, using the technique of simultaneous wireless
information and power transfer (SWIPT). To this aim, this work studies
bidirectional communication in a decode-and-forward (DF) relay assisted
cooperative wireless network in presence of co-channel interference (CCI). In
order to quantify the reliability of the bidirectional communication systems, a
closed form expression for the outage probability of the system is derived for
both power splitting (PS) and time switching (TS) mode of operation of the
relay. Simulation results are used to validate the accuracy of our analytical
results and illustrate the dependence of the outage probability on various
system parameters, like PS factor, TS factor, and distance of the relay from
both the users. Results of performance comparison between PS relaying (PSR) and
TS relaying (TSR) schemes are also presented. Besides, simulation results are
also used to illustrate the spectral-efficiency and the energy-efficiency of
the proposed system. The results show that, both in terms of spectral
efficiency and the energy-efficiency, the two-way communication system in
presence of moderate CCI power, performs better than the similar system without
CCI. Additionally, it is also found that PSR is superior to TSR protocol in
terms of peak energy-efficiency.
",computer-science
"  Finite-difference methods are widely used in solving partial differential
equations. In a large problem set, approximations can take days or weeks to
evaluate, yet the bulk of computation may occur within a single loop nest. The
modelling process for researchers is not straightforward either, requiring
models with differential equations to be translated into stencil kernels, then
optimised separately. One tool that seeks to speed up and eliminate mistakes
from this tedious procedure is Devito, used to efficiently employ
finite-difference methods.
In this work, we implement time-tiling, a loop nest optimisation, in Devito
yielding a decrease in runtime of up to 45%, and at least 20% across stencils
from the acoustic wave equation family, widely used in Devito's target domain
of seismic imaging. We present an estimator for arithmetic intensity under
time-tiling and a model to predict runtime improvements in stencil
computations. We also consider generalisation of time-tiling to imperfect loop
nests, a less widely studied problem.
",computer-science
"  Simulations of tidal streams show that close encounters with dark matter
subhalos induce density gaps and distortions in on-sky path along the streams.
Accordingly, observing disrupted streams in the Galactic halo would
substantiate the hypothesis that dark matter substructure exists there, while
in contrast, observing collimated streams with smoothly varying density
profiles would place strong upper limits on the number density and mass
spectrum of subhalos. Here, we examine several measures of stream ""disruption""
and their power to distinguish between halo potentials with and without
substructure and with different global shapes. We create and evolve a
population of 1280 streams on a range of orbits in the Via Lactea II simulation
of a Milky Way-like halo, replete with a full mass range of {\Lambda}CDM
subhalos, and compare it to two control stream populations evolved in smooth
spherical and smooth triaxial potentials, respectively. We find that the number
of gaps observed in a stellar stream is a poor indicator of the halo potential,
but that (i) the thinness of the stream on-sky, (ii) the symmetry of the
leading and trailing tails, and (iii) the deviation of the tails from a
low-order polynomial path on-sky (""path regularity"") distinguish between the
three potentials more effectively. We find that globular cluster streams on
low-eccentricity orbits far from the galactic center (apocentric radius ~ 30-80
kpc) are most powerful in distinguishing between the three potentials. If they
exist, such streams will shortly be discoverable and mapped in high dimensions
with near-future photometric and spectroscopic surveys.
",physics
"  CodRep is a machine learning competition on source code data. It is carefully
designed so that anybody can enter the competition, whether professional
researchers, students or independent scholars, without specific knowledge in
machine learning or program analysis. In particular, it aims at being a common
playground on which the machine learning and the software engineering research
communities can interact. The competition has started on April 14th 2018 and
has ended on October 14th 2018. The CodRep data is hosted at
this https URL.
",computer-science
"  In the paper ""Optimal control of a Vlasov-Poisson plasma by an external
magnetic field - The basics for variational calculus"" [arXiv:1708.02464] we
have already introduced a set of admissible magnetic fields and we have proved
that each of those fields induces a unique strong solution of the
Vlasov-Poisson system. We have also established that the field-state operator
that maps any admissible field onto its corresponding solution is continuous
and weakly compact. In this paper we will show that this operator is also
Fréchet differentiable and we will continue to analyze the optimal control
problem that was introduced in [arXiv:1708.02464]. More precisely, we will
establish necessary and sufficient conditions for local optimality and we will
show that an optimal solution is unique under certain conditions.
",mathematics
"  Spectral shape descriptors have been used extensively in a broad spectrum of
geometry processing applications ranging from shape retrieval and segmentation
to classification. In this pa- per, we propose a spectral graph wavelet
approach for 3D shape classification using the bag-of-features paradigm. In an
effort to capture both the local and global geometry of a 3D shape, we present
a three-step feature description framework. First, local descriptors are
extracted via the spectral graph wavelet transform having the Mexican hat
wavelet as a generating ker- nel. Second, mid-level features are obtained by
embedding lo- cal descriptors into the visual vocabulary space using the soft-
assignment coding step of the bag-of-features model. Third, a global descriptor
is constructed by aggregating mid-level fea- tures weighted by a geodesic
exponential kernel, resulting in a matrix representation that describes the
frequency of appearance of nearby codewords in the vocabulary. Experimental
results on two standard 3D shape benchmarks demonstrate the effective- ness of
the proposed classification approach in comparison with state-of-the-art
methods.
",computer-science
"  We investigate the entanglement properties of an infinite class of excited
states in the quantum Lifshitz model (QLM). The presence of a conformal quantum
critical point in the QLM makes it unusually tractable for a model above one
spatial dimension, enabling the ground state entanglement entropy for an
arbitrary domain to be expressed in terms of geometrical and topological
quantities. Here we extend this result to excited states and find that the
entanglement can be naturally written in terms of quantities which we dub
""entanglement propagator amplitudes"" (EPAs). EPAs are geometrical probabilities
that we explicitly calculate and interpret. A comparison of lattice and
continuum results demonstrates that EPAs are universal. This work shows that
the QLM is an example of a 2+1d field theory where the universal behavior of
excited-state entanglement may be computed analytically.
",physics
"  We searched high resolution spectra of 5600 nearby stars for emission lines
that are both inconsistent with a natural origin and unresolved spatially, as
would be expected from extraterrestrial optical lasers. The spectra were
obtained with the Keck 10-meter telescope, including light coming from within
0.5 arcsec of the star, corresponding typically to within a few to tens of au
of the star, and covering nearly the entire visible wavelength range from 3640
to 7890 angstroms. We establish detection thresholds by injecting synthetic
laser emission lines into our spectra and blindly analyzing them for
detections. We compute flux density detection thresholds for all wavelengths
and spectral types sampled. Our detection thresholds for the power of the
lasers themselves range from 3 kW to 13 MW, independent of distance to the star
but dependent on the competing ""glare"" of the spectral energy distribution of
the star and on the wavelength of the laser light, launched from a benchmark,
diffraction-limited 10-meter class telescope. We found no such laser emission
coming from the planetary region around any of the 5600 stars. As they contain
roughly 2000 lukewarm, Earth-size planets, we rule out models of the Milky Way
in which over 0.1 percent of warm, Earth-size planets harbor technological
civilizations that, intentionally or not, are beaming optical lasers toward us.
A next generation spectroscopic laser search will be done by the Breakthrough
Listen initiative, targeting more stars, especially stellar types overlooked
here including spectral types O, B, A, early F, late M, and brown dwarfs, and
astrophysical exotica.
",physics
"  Chondrules are the dominant bulk silicate constituent of chondritic
meteorites and originate from highly energetic, local processes during the
first million years after the birth of the Sun. So far, an astrophysically
consistent chondrule formation scenario, explaining major chemical, isotopic
and textural features, remains elusive. Here, we examine the prospect of
forming chondrules from planetesimal collisions. We show that intensely melted
bodies with interior magma oceans became rapidly chemically equilibrated and
physically differentiated. Therefore, collisional interactions among such
bodies would have resulted in chondrule-like but basaltic spherules, which are
not observed in the meteoritic record. This inconsistency with the expected
dynamical interactions hints at an incomplete understanding of the planetary
growth regime during the protoplanetary disk phase. To resolve this conundrum,
we examine how the observed chemical and isotopic features of chondrules
constrain the dynamical environment of accreting chondrite parent bodies by
interpreting the meteoritic record as an impact-generated proxy of
planetesimals that underwent repeated collision and reaccretion cycles. Using a
coupled evolution-collision model we demonstrate that the vast majority of
collisional debris feeding the asteroid main belt must be derived from
planetesimals which were partially molten at maximum. Therefore, the precursors
of chondrite parent bodies either formed primarily small, from sub-canonical
aluminum-26 reservoirs, or collisional destruction mechanisms were efficient
enough to shatter planetesimals before they reached the magma ocean phase.
Finally, we outline the window in parameter space for which chondrule formation
from planetesimal collisions can be reconciled with the meteoritic record and
how our results can be used to further constrain early solar system dynamics.
",physics
"  Neural network based machine learning is emerging as a powerful tool for
obtaining phase diagrams when traditional regression schemes using local
equilibrium order parameters are not available, as in many-body localized or
topological phases. Nevertheless, instances of machine learning offering new
insights have been rare up to now. Here we show that a single feed-forward
neural network can decode the defining structures of two distinct MBL phases
and a thermalizing phase, using entanglement spectra obtained from individual
eigenstates. For this, we introduce a simplicial geometry based method for
extracting multi-partite phase boundaries. We find that this method outperforms
conventional metrics (like the entanglement entropy) for identifying MBL phase
transitions, revealing a sharper phase boundary and shedding new insight into
the topology of the phase diagram. Furthermore, the phase diagram we acquire
from a single disorder configuration confirms that the machine-learning based
approach we establish here can enable speedy exploration of large phase spaces
that can assist with the discovery of new MBL phases. To our knowledge this
work represents the first example of a machine learning approach revealing new
information beyond conventional knowledge.
",physics
"  We study transitivity in directed acyclic graphs and its usefulness in
capturing nodes that act as bridges between more densely interconnected parts
in such type of network. In transitively reduced citation networks degree
centrality could be used as a measure of interdisciplinarity or diversity. We
study the measure's ability to capture ""diverse"" nodes in random directed
acyclic graphs and citation networks. We show that transitively reduced degree
centrality is capable of capturing ""diverse"" nodes, thus this measure could be
a timely alternative to text analysis techniques for retrieving papers,
influential in a variety of research fields.
",computer-science
"  Cyber-Physical Systems (CPS) revolutionize various application domains with
integration and interoperability of networking, computing systems, and
mechanical devices. Due to its scale and variety, CPS faces a number of
challenges and opens up a few research questions in terms of management,
fault-tolerance, and scalability. We propose a software-defined approach
inspired by Software-Defined Networking (SDN), to address the challenges for a
wider CPS adoption. We thus design a middleware architecture for the correct
and resilient operation of CPS, to manage and coordinate the interacting
devices centrally in the cyberspace whilst not sacrificing the functionality
and performance benefits inherent to a distributed execution.
",computer-science
"  Working over the prime field of characteristic two, consequences of the
Koszul duality between the Steenrod algebra and the big Dyer-Lashof algebra are
studied, with an emphasis on the interplay between instability for the Steenrod
algebra action and that for the Dyer-Lashof operations. The central algebraic
framework is the category of length-graded modules over the Steenrod algebra
equipped with an unstable action of the Dyer-Lashof algebra, with compatibility
via the Nishida relations.
A first ingredient is a functor defined on modules over the Steenrod algebra
that arose in the work of Kuhn and McCarty on the homology of infinite loop
spaces. This functor is given in terms of derived functors of destabilization
from the category of modules over the Steenrod algebra to unstable modules,
enriched by taking into account the action of Dyer-Lashof operations.
A second ingredient is the derived functors of the Dyer-Lashof
indecomposables functor to length-graded modules over the Steenrod algebra.
These are related to functors used by Miller in his study of a spectral
sequence to calculate the homology of an infinite delooping. An important fact
is that these functors can be calculated as the homology of an explicit Koszul
complex with terms expressed as certain Steinberg functors. The latter are
quadratic dual to the more familiar Singer functors.
By exploiting the explicit complex built from the Singer functors which
calculates the derived functors of destabilization, Koszul duality leads to an
algebraic infinite delooping spectral sequence. This is conceptually similar to
Miller's spectral sequence, but there seems to be no direct relationship.
The spectral sequence sheds light on the relationship between unstable
modules over the Steenrod algebra and all modules.
",mathematics
"  We study small-scale and high-frequency turbulent fluctuations in
three-dimensional flows under Fourier-mode reduction. The Navier-Stokes
equations are evolved on a restricted set of modes, obtained as a projection on
a fractal or homogeneous Fourier set. We find a strong sensitivity (reduction)
of the high-frequency variability of the Lagrangian velocity fluctuations on
the degree of mode decimation, similarly to what is already reported for
Eulerian statistics. This is quantified by a tendency towards a quasi-Gaussian
statistics, i.e., to a reduction of intermittency, at all scales and
frequencies. This can be attributed to a strong depletion of vortex filaments
and of the vortex stretching mechanism. Nevertheless, we found that Eulerian
and Lagrangian ensembles are still connected by a dimensional bridge-relation
which is independent of the degree of Fourier-mode decimation.
",physics
"  In this paper we show how polynomial walks can be used to establish a twisted
recurrence for sets of positive density in $\mathbb{Z}^d$. In particular, we
prove that if $\Gamma \leq \operatorname{GL}_d(\mathbb{Z})$ is finitely
generated by unipotents and acts irreducibly on $\mathbb{R}^d$, then for any
set $B \subset \mathbb{Z}^d$ of positive density, there exists $k \geq 1$ such
that for any $v \in k \mathbb{Z}^d$ one can find $\gamma \in \Gamma$ with
$\gamma v \in B - B$. Our method does not require the linearity of the action,
and we prove a twisted recurrence for semigroups of maps from $\mathbb{Z}^d$ to
$\mathbb{Z}^d$ satisfying some irreducibility and polynomial assumptions. As
one of the consequences, we prove a non-linear analog of Bogolubov's theorem --
for any set $B \subset \mathbb{Z}^2$ of positive density, and $p(n) \in
\mathbb{Z}[n]$, with $p(0) = 0$ and $\operatorname{deg}(p) \geq 2$, there
exists $k \geq 1$ such that $k \mathbb{Z} \subset \{ x - p(y) \, | \, (x,y) \in
B-B \}$. Unlike the previous works on twisted recurrence that used recent
results of Benoist-Quint and Bourgain-Furman-Lindenstrauss-Mozes on
equidistribution of random walks on automorphism groups of tori, our method
relies on the classical Weyl equidistribution for polynomial orbits on tori.
",mathematics
"  We link the theory of optimal transportation to the theory of integer
partitions. Let $\mathscr P(n)$ denote the set of integer partitions of $n \in
\mathbb N$ and write partitions $\pi \in \mathscr P(n)$ as $(n_1, \dots,
n_{k(\pi)})$. Using terminology from optimal transport, we characterize certain
classes of partitions like symmetric partitions and those in Euler's identity
$|\{ \pi \in \mathscr P(n) |$ all $ n_i $ distinct $ \} | = | \{ \pi \in
\mathscr P(n) | $ all $ n_i $ odd $ \}|$.
Then we sketch how optimal transport might help to understand higher
dimensional partitions.
",mathematics
"  We study the fixed point theory of n-valued maps of a space X using the fixed
point theory of maps between X and its configuration spaces. We give some
general results to decide whether an n-valued map can be deformed to a fixed
point free n-valued map. In the case of surfaces, we provide an algebraic
criterion in terms of the braid groups of X to study this problem. If X is
either the k-dimensional ball or an even-dimensional real or complex projective
space, we show that the fixed point property holds for n-valued maps for all n
$\ge$ 1, and we prove the same result for even-dimensional spheres for all n
$\ge$ 2. If X is the 2-torus, we classify the homotopy classes of 2-valued maps
in terms of the braid groups of X. We do not currently have a complete
characterisation of the homotopy classes of split 2-valued maps of the 2-torus
that contain a fixed point free representative, but we give an infinite family
of such homotopy classes.
",mathematics
"  We prove an abstract theorem giving a $\langle t\rangle^\epsilon$ bound
($\forall \epsilon>0$) on the growth of the Sobolev norms in linear
Schrödinger equations of the form $i \dot \psi = H_0 \psi + V(t) \psi $ when
the time $t \to \infty$. The abstract theorem is applied to several cases,
including the cases where (i) $H_0$ is the Laplace operator on a Zoll manifold
and $V(t)$ a pseudodifferential operator of order smaller then 2; (ii) $H_0$ is
the (resonant or nonresonant) Harmonic oscillator in $R^d$ and $V(t)$ a
pseudodifferential operator of order smaller then $H_0$ depending in a
quasiperiodic way on time. The proof is obtained by first conjugating the
system to some normal form in which the perturbation is a smoothing operator
and then applying the results of \cite{MaRo}.
",mathematics
"  We study the convergence of the log-linear non-Bayesian social learning
update rule, for a group of agents that collectively seek to identify a
parameter that best describes a joint sequence of observations. Contrary to
recent literature, we focus on the case where agents assign decaying weights to
its neighbors, and the network is not connected at every time instant but over
some finite time intervals. We provide a necessary and sufficient condition for
the rate at which agents decrease the weights and still guarantees social
learning.
",computer-science
"  We introduce a new critical value $c_\infty(L)$ for Tonelli Lagrangians $L$
on the tangent bundle of the 2-sphere without minimizing measures supported on
a point. We show that $c_\infty(L)$ is strictly larger than the Mañé
critical value $c(L)$, and on every energy level $e\in(c(L),c_\infty(L))$ there
exist infinitely many periodic orbits of the Lagrangian system of $L$, one of
which is a local minimizer of the free-period action functional. This has
applications to Finsler metrics of Randers type on the 2-sphere. We show that,
under a suitable criticality assumption on a given Randers metric, after
rescaling its magnetic part with a sufficiently large multiplicative constant,
the new metric admits infinitely many closed geodesics, one of which is a
waist. Examples of critical Randers metrics include the celebrated Katok
metric.
",mathematics
"  Model Predictive Control (MPC) is the principal control technique used in
industrial applications. Although it offers distinguishable qualities that make
it ideal for industrial applications, it can be questioned its robustness
regarding model uncertainties and external noises. In this paper we propose a
robust MPC controller that merges the simplicity in the design of MPC with
added robustness. In particular, our control system stems from the idea of
adding robustness in the prediction phase of the algorithm through a specific
robust Kalman filter recently introduced. Notably, the overall result is an
algorithm very similar to classic MPC but that also provides the user with the
possibility to tune the robustness of the control. To test the ability of the
controller to deal with errors in modeling, we consider a servomechanism system
characterized by nonlinear dynamics.
",mathematics
"  We show that the equations of reinforcement learning and light transport
simulation are related integral equations. Based on this correspondence, a
scheme to learn importance while sampling path space is derived. The new
approach is demonstrated in a consistent light transport simulation algorithm
that uses reinforcement learning to progressively learn where light comes from.
As using this information for importance sampling includes information about
visibility, too, the number of light transport paths with zero contribution is
dramatically reduced, resulting in much less noisy images within a fixed time
budget.
",computer-science
"  High purity Zinc Selenide (ZnSe) crystals are produced starting from
elemental Zn and Se to be used for the search of the neutrinoless double beta
decay (0{\nu}DBD) of 82Se. In order to increase the number of emitting
nuclides, enriched 82Se is used. Dedicated production lines for the synthesis
and conditioning of the Zn82Se powder in order to make it suitable for crystal
growth were assembled compliant with radio-purity constraints specific to rare
event physics experiments. Besides routine check of impurities concentration,
high sensitivity measurements are made for radio-isotope concentrations in raw
materials, reactants, consumables, ancillaries and intermediary products used
for ZnSe crystals production. Indications are given on the crystals perfection
and how it is achieved. Since very expensive isotopically enriched material
(82Se) is used, a special attention is given for acquiring the maximum yield in
the mass balance of all production stages. Production and certification
protocols are presented and resulting ready-to-use Zn82Se crystals are
described.
",physics
"  We establish effective mean-value estimates for a wide class of
multiplicative arithmetic functions, thereby providing (essentially optimal)
quantitative versions of Wirsing's classical estimates and extending those of
Halász. Several applications are derived, including: estimates for the
difference of mean-values of so-called pretentious functions, local laws for
the distribution of prime factors in an arbitrary set, and weighted
distribution of additive functions.
",mathematics
"  The matrix inversion is an interesting topic in algebra mathematics. However,
to determine an inverse matrix from a given matrix is required many computation
tools and time resource if the size of matrix is huge. In this paper, we have
shown an inverse closed form for an interesting matrix which has much
applications in communication system. Base on this inverse closed form, the
channel capacity closed form of a communication system can be determined via
the error rate parameter alpha
",computer-science
"  Recently, a link between Lorentzian and Finslerian Geometries has been
carried out, leading to the notion of wind Riemannian structure (WRS), a
generalization of Finslerian Randers metrics. Here, we further develop this
notion and its applications to spacetimes, by introducing some
characterizations and criteria for the completeness of WRS's.
As an application, we consider a general class of spacetimes admitting a time
function $t$ generated by the flow of a complete Killing vector field
(generalized standard stationary spacetimes or, more precisely, SSTK ones) and
derive simple criteria ensuring that its slices $t=$ constant are Cauchy.
Moreover, a brief summary on the Finsler/Lorentz link for readers with some
acquaintance in Lorentzian Geometry, plus some simple examples in Mathematical
Relativity, are provided.
",mathematics
"  Observables have a dual nature in both classical and quantum kinematics: they
are at the same time \emph{quantities}, allowing to separate states by means of
their numerical values, and \emph{generators of transformations}, establishing
relations between different states. In this work, we show how this two-fold
role of observables constitutes a key feature in the conceptual analysis of
classical and quantum kinematics, shedding a new light on the distinguishing
feature of the quantum at the kinematical level. We first take a look at the
algebraic description of both classical and quantum observables in terms of
Jordan-Lie algebras and show how the two algebraic structures are the precise
mathematical manifestation of the two-fold role of observables. Then, we turn
to the geometric reformulation of quantum kinematics in terms of Kähler
manifolds. A key achievement of this reformulation is to show that the two-fold
role of observables is the constitutive ingredient defining what an observable
is. Moreover, it points to the fact that, from the restricted point of view of
the transformational role of observables, classical and quantum kinematics
behave in exactly the same way. Finally, we present Landsman's general
framework of Poisson spaces with transition probability, which highlights with
unmatched clarity that the crucial difference between the two kinematics lies
in the way the two roles of observables are related to each other.
",physics
"  Scattering of obliquely incident electromagnetic waves from periodically
space-time modulated slabs is investigated. It is shown that such structures
operate as nonreciprocal harmonic generators and spatial-frequency filters. For
oblique incidences, low-frequency harmonics are filtered out in the form of
surface waves, while high-frequency harmonics are transmitted as space waves.
In the quasisonic regime, where the velocity of the space-time modulation is
close to the velocity of the electromagnetic waves in the background medium,
the incident wave is strongly coupled to space-time harmonics in the forward
direction, while in the backward direction it exhibits low coupling to other
harmonics. This nonreciprocity is leveraged for the realization of an
electromagnetic isolator in the quasisonic regime and is experimentally
demonstrated at microwave frequencies.
",physics
"  We show how simultaneous, back-action evading tracking of non-commuting
observables can be achieved in a widely-used sensing technology, atomic
interferometry. Using high-dynamic-range dynamically-decoupled quantum
non-demolition (QND) measurements on a precessing atomic spin ensemble, we
track the collective spin angle and amplitude with negligible effects from back
action, giving steady-state tracking sensitivity 2.9 dB beyond the standard
quantum limit and 7.0 dB beyond Poisson statistics.
",physics
"  Despite various debugging supports of the existing IDEs for programming
errors and exceptions, software developers often look at web for working
solutions or any up-to-date information. Traditional web search does not
consider the context of the problems that they search solutions for, and thus
it often does not help much in problem solving. In this paper, we propose a
context-aware meta search tool, SurfClipse, that analyzes an encountered
exception and its context in the IDE, and recommends not only suitable search
queries but also relevant web pages for the exception (and its context). The
tool collects results from three popular search engines and a programming Q & A
site against the exception in the IDE, refines the results for relevance
against the context of the exception, and then ranks them before
recommendation. It provides two working modes--interactive and proactive to
meet the versatile needs of the developers, and one can browse the result pages
using a customized embedded browser provided by the tool.
Tool page: www.usask.ca/~masud.rahman/surfclipse
",computer-science
"  Motivated by recent experimental observations in $\alpha$-RuCl$_3$, we study
the $K$-$\Gamma$ model on the honeycomb lattice in an external magnetic field.
By a slave-particle representation and Variational Monte Carlo calculations, we
reproduce the phase transition from zigzag magnetic order to a field-induced
disordered phase. The nature of this state depends crucially on the field
orientation. For particular field directions in the honeycomb plane, we find a
gapless Dirac spin liquid, in agreement with recent experiments on
$\alpha$-RuCl$_3$. For a range of out-of-plane fields, we predict the existence
of a Kalmeyer-Laughlin-type chiral spin liquid, which would show an
integer-quantized thermal Hall effect.
",physics
"  By combining bulk sensitive soft-X-ray angular-resolved photoemission
spectroscopy and accurate first-principles calculations we explored the bulk
electronic properties of WTe$_2$, a candidate type-II Weyl semimetal featuring
a large non-saturating magnetoresistance. Despite the layered geometry
suggesting a two-dimensional electronic structure, we find a three-dimensional
electronic dispersion. We report an evident band dispersion in the reciprocal
direction perpendicular to the layers, implying that electrons can also travel
coherently when crossing from one layer to the other. The measured Fermi
surface is characterized by two well-separated electron and hole pockets at
either side of the $\Gamma$ point, differently from previous more surface
sensitive ARPES experiments that additionally found a significant quasiparticle
weight at the zone center. Moreover, we observe a significant sensitivity of
the bulk electronic structure of WTe$_2$ around the Fermi level to electronic
correlations and renormalizations due to self-energy effects, previously
neglected in first-principles descriptions.
",physics
"  Crowdsourced video systems like YouTube and Twitch.tv have been a major
internet phenomenon and are nowadays entertaining over a billion users. In
addition to video sharing and viewing, over the years they have developed new
features to boost the community engagement and some managed to attract users to
donate, to the community as well as to other users. User donation directly
reflects and influences user engagement in the community, and has a great
impact on the success of such systems. Nevertheless, user donations in
crowdsourced video systems remain trade secrets for most companies and to date
are still unexplored. In this work, we attempt to fill this gap, and we obtain
and provide a publicly available dataset on user donations in one crowdsourced
video system named BiliBili. Based on information on nearly 40 thousand
donators, we examine the dynamics of user donations and their social
relationships, we quantitively reveal the factors that potentially impact user
donation, and we adopt machine-learned classifiers and network representation
learning models to timely and accurately predict the destinations of the
majority and the individual donations.
",computer-science
"  In this paper, we investigate periodic vibrations of a group of particles
with a dihedral configuration in the plane governed by the Lennard-Jones and
Coulomb forces. Using the gradient equivariant degree, we provide a full
topological classification of the periodic solutions with both temporal and
spatial symmetries. In the process, we provide with general formulae for the
spectrum of the linearized system which allows us to obtain the critical
frequencies of the particle motions which indicate the set of all critical
periods of small amplitude periodic solutions emerging from a given stationary
symmetric orbit of solutions.
",mathematics
"  Let $B{ aut}_1X$ be the Dold-Lashof classifying space of orientable
fibrations with fiber $X$. For a rationally weakly trivial map $f:X\to Y$, our
strictly induced map $a_f: (Baut_1X)_0\to (Baut_1Y)_0$ induces a natural map
from a $X_0$-fibration to a $Y_0$-fibration. It is given by a map between the
differential graded Lie algebras of derivations of Sullivan models. We note
some conditions that the map $a_f$ admits a section and note some relations
with the Halperin conjecture. Furthermore we give the obstruction class for a
lifting of a classifying map $h: B\to (Baut_1Y)_0$ and apply it for liftings of
$G$-actions on $Y$ for a compact connected Lie group $G$ as the case of $B=BG$
and evaluating of rational toral ranks as $r_0(Y)\leq r_0(X)$.
",mathematics
"  Efficient communication between qubits relies on robust networks which allow
for fast and coherent transfer of quantum information. It seems natural to
harvest the remarkable properties of systems characterized by topological
invariants to perform this task. Here we show that a linear network of coupled
bosonic degrees of freedom, characterized by topological bands, can be employed
for the efficient exchange of quantum information over large distances.
Important features of our setup are that it is robust against quenched
disorder, all relevant operations can be performed by global variations of
parameters, and the time required for communication between distant qubits
approaches linear scaling with their distance. We demonstrate that our concept
can be extended to an ensemble of qubits embedded in a two-dimensional network
to allow for communication between all of them.
",physics
"  We present $\psi'$MSSM, a model based on a $U(1)_{\psi'}$ extension of the
minimal supersymmetric standard model. The gauge symmetry $U(1)_{\psi'}$, also
known as $U(1)_N$, is a linear combination of the $U(1)_\chi$ and $U(1)_\psi$
subgroups of $E_6$. The model predicts the existence of three sterile neutrinos
with masses $\lesssim 0.1~{\rm eV}$, if the $U(1)_{\psi'}$ breaking scale is of
order 10 TeV. Their contribution to the effective number of neutrinos at
nucleosynthesis is $\Delta N_{\nu}\simeq 0.29$. The model can provide a variety
of possible cold dark matter candidates including the lightest sterile
sneutrino. If the $U(1)_{\psi'}$ breaking scale is increased to $10^3~{\rm
TeV}$, the sterile neutrinos, which are stable on account of a $Z_2$ symmetry,
become viable warm dark matter candidates. The observed value of the standard
model Higgs boson mass can be obtained with relatively light stop quarks thanks
to the D-term contribution from $U(1)_{\psi'}$. The model predicts diquark and
diphoton resonances which may be found at an updated LHC. The well-known $\mu$
problem is resolved and the observed baryon asymmetry of the universe can be
generated via leptogenesis. The breaking of $U(1)_{\psi'}$ produces
superconducting strings that may be present in our galaxy. A $U(1)$ R symmetry
plays a key role in keeping the proton stable and providing the light sterile
neutrinos.
",physics
"  In this paper we show that a $k$-shellable simplicial complex is the
expansion of a shellable complex. We prove that the face ring of a pure
$k$-shellable simplicial complex satisfies the Stanley conjecture. In this way,
by applying expansion functor to the face ring of a given pure shellable
complex, we construct a large class of rings satisfying the Stanley conjecture.
Also, by presenting some characterizations of $k$-shellable graphs, we extend
some results due to Castrillón-Cruz, Cruz-Estrada and Van Tuyl-Villareal.
",mathematics
"  A model in which a three-dimensional elastic medium is represented by a
network of identical masses connected by springs of random strengths and
allowed to vibrate only along a selected axis of the reference frame, exhibits
an Anderson localization transition. To study this transition, we assume that
the dynamical matrix of the network is given by a product of a sparse random
matrix with real, independent, Gaussian-distributed non-zero entries and its
transpose. A finite-time scaling analysis of system's response to an initial
excitation allows us to estimate the critical parameters of the localization
transition. The critical exponent is found to be $\nu = 1.57 \pm 0.02$ in
agreement with previous studies of Anderson transition belonging to the
three-dimensional orthogonal universality class.
",physics
"  In this paper, we consider Burgers' equation with uncertain boundary and
initial conditions. The polynomial chaos (PC) approach yields a hyperbolic
system of deterministic equations, which can be solved by several numerical
methods. Here, we apply the correction procedure via reconstruction (CPR) using
summation-by-parts operators. We focus especially on stability, which is proven
for CPR methods and the systems arising from the PC approach. Due to the usage
of split-forms, the major challenge is to construct entropy stable numerical
fluxes. For the first time, such numerical fluxes are constructed for all
systems resulting from the PC approach for Burgers' equation. In numerical
tests, we verify our results and show also the advantage of the given ansatz
using CPR methods. Moreover, one of the simulations, i.e. Burgers' equation
equipped with an initial shock, demonstrates quite fascinating observations.
The behaviour of the numerical solutions from several methods (finite volume,
finite difference, CPR) differ significantly from each other. Through careful
investigations, we conclude that the reason for this is the high sensitivity of
the system to varying dissipation. Furthermore, it should be stressed that the
system is not strictly hyperbolic with genuinely nonlinear or linearly
degenerate fields.
",mathematics
"  Chromosome conformation capture and Hi-C technologies provide gene-gene
proximity datasets of stationary cells, revealing chromosome territories,
topologically associating domains, and chromosome topology. Imaging of tagged
DNA sequences in live cells through the lac operator reporter system provides
dynamic datasets of chromosomal loci. Chromosome modeling explores the
mechanisms underlying 3D genome structure and dynamics. Here, we automate 4D
genome dataset analysis with network-based tools as an alternative to gene-gene
proximity statistics and visual structure determination. Temporal network
models and community detection algorithms are applied to 4D modeling of G1 in
budding yeast with transient crosslinking of $5 kb$ domains in the nucleolus,
analyzing datasets from four decades of transient binding timescales. Network
tools detect and track transient gene communities (clusters) within the
nucleolus, their size, number, persistence time, and frequency of gene
exchanges. An optimal, weak binding affinity is revealed that maximizes
community-scale plasticity whereby large communities persist, frequently
exchanging genes.
",quantitative-biology
"  Molecular adsorption on surfaces plays an important part in catalysis,
corrosion, desalination, and various other processes that are relevant to
industry and in nature. As a complement to experiments, accurate adsorption
energies can be obtained using various sophisticated electronic structure
methods that can now be applied to periodic systems. The adsorption energy of
water on boron nitride substrates, going from zero to 2-dimensional
periodicity, is particularly interesting as it calls for an accurate treatment
of polarizable electrostatics and dispersion interactions, as well as posing a
practical challenge to experiments and electronic structure methods. Here, we
present reference adsorption energies, static polarizabilities, and dynamic
polarizabilities, for water on BN substrates of varying size and dimension.
Adsorption energies are computed with coupled cluster theory, fixed-node
quantum Monte Carlo (FNQMC), the random phase approximation (RPA), and second
order M{\o}ller-Plesset (MP2) theory. These explicitly correlated methods are
found to agree in molecular as well as periodic systems. The best estimate of
the water/h-BN adsorption energy is $-107\pm7$ meV from FNQMC. In addition, the
water adsorption energy on the BN substrates could be expected to grow
monotonically with the size of the substrate due to increased dispersion
interactions but interestingly, this is not the case here. This peculiar
finding is explained using the static polarizabilities and molecular dispersion
coefficients of the systems, as computed from time-dependent density functional
theory (DFT). Dynamic as well as static polarizabilities are found to be highly
anisotropic in these systems. In addition, the many-body dispersion method in
DFT emerges as a particularly useful estimation of finite size effects for
other expensive, many-body wavefunction based methods.
",physics
"  This paper mainly focus on the front-like entire solution of a classical
nonlocal dispersal equation with ignition nonlinearity. Especially, the
dispersal kernel function $J$ may not be symmetric here. The asymmetry of $J$
has a great influence on the profile of the traveling waves and the sign of the
wave speeds, which further makes the properties of the entire solution more
diverse. We first investigate the asymptotic behavior of the traveling wave
solutions since it plays an essential role in obtaining the front-like entire
solution. Due to the impact of $f'(0)=0$, we can no longer use the common
method which mainly depending on Ikehara theorem and bilateral Laplace
transform to study the asymptotic rates of the nondecreasing traveling wave and
the nonincreasing one tending to 0, respectively, thus we adopt another method
to investigate them. Afterwards, we establish a new entire solution and obtain
its qualitative properties by constructing proper supersolution and subsolution
and by classifying the sign and size of the wave speeds.
",mathematics
"  The Swift test was originally proposed as a formability test to reproduce the
conditions observed in deep drawing operations. This test consists on forming a
cylindrical cup from a circular blank, using a flat bottom cylindrical punch
and has been extensively studied using both analytical and numerical methods.
This test can also be combined with the Demeri test, which consists in cutting
a ring from the wall of a cylindrical cup, in order to open it afterwards to
measure the springback. This combination allows their use as benchmark test, in
order to improve the knowledge concerning the numerical simulation models,
through the comparison between experimental and numerical results. The focus of
this study is the experimental and numerical analyses of the Swift cup test,
followed by the Demeri test, performed with an AA5754-O alloy at room
temperature. In this context, a detailed analysis of the punch force evolution,
the thickness evolution along the cup wall, the earing profile, the strain
paths and their evolution and the ring opening is performed. The numerical
simulation is performed using the finite element code ABAQUS, with solid and
solid-shell elements, in order to compare the computational efficiency of these
type of elements. The results show that the solid-shell element is more
cost-effective than the solid, presenting global accurate predictions, excepted
for the thinning zones. Both the von Mises and the Hill48 yield criteria
predict the strain distributions in the final cup quite accurately. However,
improved knowledge concerning the stress states is still required, because the
Hill48 criterion showed difficulties in the correct prediction of the
springback, whatever the type of finite element adopted.
",physics
"  Inferring directional connectivity from point process data of multiple
elements is desired in various scientific fields such as neuroscience,
geography, economics, etc. Here, we propose an inference procedure for this
goal based on the kinetic Ising model. The procedure is composed of two steps:
(1) determination of the time-bin size for transforming the point-process data
to discrete time binary data and (2) screening of relevant couplings from the
estimated networks. For these, we develop simple methods based on information
theory and computational statistics. Applications to data from artificial and
\textit{in vitro} neuronal networks show that the proposed procedure performs
fairly well when identifying relevant couplings, including the discrimination
of their signs, with low computational cost. These results highlight the
potential utility of the kinetic Ising model to analyze real interacting
systems with event occurrences.
",quantitative-biology
"  The paper presents first results of the CitEcCyr project funded by RANEPA.
The project aims to create a source of open citation data for research papers
written in Russian. Compared to existing sources of citation data, CitEcCyr is
working to provide the following added values: a) a transparent and distributed
architecture of a technology that generates the citation data; b) an openness
of all built/used software and created citation data; c) an extended set of
citation data sufficient for the citation content analysis; d) services for
public control over a quality of the citation data and a citing activity of
researchers.
",computer-science
"  A stochastic orbital approach to the resolution of identity (RI)
approximation for 4-index 2-electron electron repulsion integrals (ERIs) is
presented. The stochastic RI-ERIs are then applied to M\o ller-Plesset
perturbation theory (MP2) utilizing a \textit{multiple stochastic orbital
approach}. The introduction of multiple stochastic orbitals results in an $N^3$
scaling for both the stochastic RI-ERIs and stochastic RI-MP2. We demonstrate
that this method exhibits a small prefactor and an observed scaling of
$N^{2.4}$ for a range of water clusters, already outperforming MP2 for clusters
with as few as 21 water molecules.
",physics
"  Studying the effects of groups of Single Nucleotide Polymorphisms (SNPs), as
in a gene, genetic pathway, or network, can provide novel insight into complex
diseases, above that which can be gleaned from studying SNPs individually.
Common challenges in set-based genetic association testing include weak effect
sizes, correlation between SNPs in a SNP-set, and scarcity of signals, with
single-SNP effects often ranging from extremely sparse to moderately sparse in
number. Motivated by these challenges, we propose the Generalized Berk-Jones
(GBJ) test for the association between a SNP-set and outcome. The GBJ extends
the Berk-Jones (BJ) statistic by accounting for correlation among SNPs, and it
provides advantages over the Generalized Higher Criticism (GHC) test when
signals in a SNP-set are moderately sparse. We also provide an analytic p-value
calculation procedure for SNP-sets of any finite size. Using this p-value
calculation, we illustrate that the rejection region for GBJ can be described
as a compromise of those for BJ and GHC. We develop an omnibus statistic as
well, and we show that this omnibus test is robust to the degree of signal
sparsity. An additional advantage of our method is the ability to conduct
inference using individual SNP summary statistics from a genome-wide
association study. We evaluate the finite sample performance of the GBJ though
simulation studies and application to gene-level association analysis of breast
cancer risk.
",statistics
"  With the help of first principles calculation method based on the density
functional theory we have investigated the structural, elastic, mechanical
properties and Debye temperature of Fe2ScM (M = P and As) compounds under
pressure up to 60 GPa. The optical properties have been investigated under zero
pressure. Our calculated optimized structural parameters of both the compounds
are in good agreement with the other theoretical results. The calculated
elastic constants show that Fe2ScM (M = P and As) compounds are mechanically
stable up to 60 GPa.
",physics
"  We discuss similarity between oscillons and oscillational mode in perturbed
$\phi^4$. For small depths of the perturbing potential it is difficult to
distinguish between oscillons and the mode in moderately long time evolution,
moreover one can transform one into the other by adiabatically switching on and
off the potential. Basins of attraction are presented in the parameter space
describing the potential and initial conditions.
",physics
"  Autonomous robots increasingly depend on third-party off-the-shelf components
and complex machine-learning techniques. This trend makes it challenging to
provide strong design-time certification of correct operation. To address this
challenge, we present SOTER, a programming framework that integrates the core
principles of runtime assurance to enable the use of uncertified controllers,
while still providing safety guarantees.
Runtime Assurance (RTA) is an approach used for safety-critical systems where
design-time analysis is coupled with run-time techniques to switch between
unverified advanced controllers and verified simple controllers. In this paper,
we present a runtime assurance programming framework for modular design of
provably-safe robotics software. \tool provides language primitives to
declaratively construct a \rta module consisting of an advanced controller
(untrusted), a safe controller (trusted), and the desired safety specification
(S). If the RTA module is well formed then the framework provides a formal
guarantee that it satisfies property S. The compiler generates code for
monitoring system state and switching control between the advanced and safe
controller in order to guarantee S. RTA allows complex systems to be
constructed through the composition of RTA modules.
To demonstrate the efficacy of our framework, we consider a real-world
case-study of building a safe drone surveillance system. Our experiments both
in simulation and on actual drones show that RTA-enabled RTA ensures safety of
the system, including when untrusted third-party components have bugs or
deviate from the desired behavior.
",computer-science
"  We present a Las Vegas algorithm for dynamically maintaining a minimum
spanning forest of an $n$-node graph undergoing edge insertions and deletions.
Our algorithm guarantees an $O(n^{o(1)})$ worst-case update time with high
probability. This significantly improves the two recent Las Vegas algorithms by
Wulff-Nilsen [STOC'17] with update time $O(n^{0.5-\epsilon})$ for some constant
$\epsilon>0$ and, independently, by Nanongkai and Saranurak [STOC'17] with
update time $O(n^{0.494})$ (the latter works only for maintaining a spanning
forest).
Our result is obtained by identifying the common framework that both two
previous algorithms rely on, and then improve and combine the ideas from both
works. There are two main algorithmic components of the framework that are
newly improved and critical for obtaining our result. First, we improve the
update time from $O(n^{0.5-\epsilon})$ in Wulff-Nilsen [STOC'17] to
$O(n^{o(1)})$ for decrementally removing all low-conductance cuts in an
expander undergoing edge deletions. Second, by revisiting the ""contraction
technique"" by Henzinger and King [1997] and Holm et al. [STOC'98], we show a
new approach for maintaining a minimum spanning forest in connected graphs with
very few (at most $(1+o(1))n$) edges. This significantly improves the previous
approach in [Wulff-Nilsen STOC'17] and [Nanongkai and Saranurak STOC'17] which
is based on Frederickson's 2-dimensional topology tree and illustrates a new
application to this old technique.
",computer-science
"  We study unsupervised generative modeling in terms of the optimal transport
(OT) problem between true (but unknown) data distribution $P_X$ and the latent
variable model distribution $P_G$. We show that the OT problem can be
equivalently written in terms of probabilistic encoders, which are constrained
to match the posterior and prior distributions over the latent space. When
relaxed, this constrained optimization problem leads to a penalized optimal
transport (POT) objective, which can be efficiently minimized using stochastic
gradient descent by sampling from $P_X$ and $P_G$. We show that POT for the
2-Wasserstein distance coincides with the objective heuristically employed in
adversarial auto-encoders (AAE) (Makhzani et al., 2016), which provides the
first theoretical justification for AAEs known to the authors. We also compare
POT to other popular techniques like variational auto-encoders (VAE) (Kingma
and Welling, 2014). Our theoretical results include (a) a better understanding
of the commonly observed blurriness of images generated by VAEs, and (b)
establishing duality between Wasserstein GAN (Arjovsky and Bottou, 2017) and
POT for the 1-Wasserstein distance.
",statistics
"  We study weighted $H^\infty$ spaces of analytic functions on the open unit
disc in the case of non-doubling weights, which decrease rapidly with respect
to the boundary distance. We characterize the solid hulls of such spaces and
give quite explicit representations of them in the case of the most natural
exponentially decreasing weights.
",mathematics
"  The algorithmic Markov condition states that the most likely causal direction
between two random variables X and Y can be identified as that direction with
the lowest Kolmogorov complexity. Due to the halting problem, however, this
notion is not computable.
We hence propose to do causal inference by stochastic complexity. That is, we
propose to approximate Kolmogorov complexity via the Minimum Description Length
(MDL) principle, using a score that is mini-max optimal with regard to the
model class under consideration. This means that even in an adversarial
setting, such as when the true distribution is not in this class, we still
obtain the optimal encoding for the data relative to the class.
We instantiate this framework, which we call CISC, for pairs of univariate
discrete variables, using the class of multinomial distributions. Experiments
show that CISC is highly accurate on synthetic, benchmark, as well as
real-world data, outperforming the state of the art by a margin, and scales
extremely well with regard to sample and domain sizes.
",computer-science
"  While going deeper has been witnessed to improve the performance of
convolutional neural networks (CNN), going smaller for CNN has received
increasing attention recently due to its attractiveness for mobile/embedded
applications. It remains an active and important topic how to design a small
network while retaining the performance of large and deep CNNs (e.g., Inception
Nets, ResNets). Albeit there are already intensive studies on compressing the
size of CNNs, the considerable drop of performance is still a key concern in
many designs. This paper addresses this concern with several new contributions.
First, we propose a simple yet powerful method for compressing the size of deep
CNNs based on parameter binarization. The striking difference from most
previous work on parameter binarization/quantization lies at different
treatments of $1\times 1$ convolutions and $k\times k$ convolutions ($k>1$),
where we only binarize $k\times k$ convolutions into binary patterns. The
resulting networks are referred to as pattern networks. By doing this, we show
that previous deep CNNs such as GoogLeNet and Inception-type Nets can be
compressed dramatically with marginal drop in performance. Second, in light of
the different functionalities of $1\times 1$ (data projection/transformation)
and $k\times k$ convolutions (pattern extraction), we propose a new block
structure codenamed the pattern residual block that adds transformed feature
maps generated by $1\times 1$ convolutions to the pattern feature maps
generated by $k\times k$ convolutions, based on which we design a small network
with $\sim 1$ million parameters. Combining with our parameter binarization, we
achieve better performance on ImageNet than using similar sized networks
including recently released Google MobileNets.
",computer-science
"  We define a generalization of the free Lie algebra based on an $n$-ary
commutator and call it the free LAnKe. We show that the action of the symmetric
group $S_{2n-1}$ on the multilinear component with $2n-1$ generators is given
by the representation $S^{2^{n-1}1}$, whose dimension is the $n$th Catalan
number. An application involving Specht modules of staircase shape is
presented. We also introduce a conjecture that extends the relation between the
Whitehouse representation and Lie($k$).
",mathematics
"  We give infinitely many $2$-component links with unknotted components which
are topologically concordant to the Hopf link, but not smoothly concordant to
any $2$-component link with trivial Alexander polynomial. Our examples are
pairwise non-concordant.
",mathematics
"  This paper addresses structures of state space in quasiperiodically forced
dynamical systems. We develop a theory of ergodic partition of state space in a
class of measure-preserving and dissipative flows, which is a natural extension
of the existing theory for measure-preserving maps. The ergodic partition
result is based on eigenspace at eigenvalue 0 of the associated Koopman
operator, which is realized via time-averages of observables, and provides a
constructive way to visualize a low-dimensional slice through a
high-dimensional invariant set. We apply the result to the systems with a
finite number of attractors and show that the time-average of a continuous
observable is well-defined and reveals the invariant sets, namely, a finite
number of basins of attraction. We provide a characterization of invariant sets
in the quasiperiodically forced systems. A theorem on uniform boundedness of
the invariant sets is proved. The series of analytical results enables
numerical analysis of invariant sets in the quasiperiodically forced systems
based on the ergodic partition and time-averages. Using this, we analyze a
nonlinear model of complex power grids that represents the short-term swing
instability, named the coherent swing instability. We show that our analytical
results can be used to understand stability regions in such complex systems.
",computer-science
"  The profitability of fraud in online systems such as app markets and social
networks marks the failure of existing defense mechanisms. In this paper, we
propose FraudSys, a real-time fraud preemption approach that imposes
Bitcoin-inspired computational puzzles on the devices that post online system
activities, such as reviews and likes. We introduce and leverage several novel
concepts that include (i) stateless, verifiable computational puzzles, that
impose minimal performance overhead, but enable the efficient verification of
their authenticity, (ii) a real-time, graph-based solution to assign fraud
scores to user activities, and (iii) mechanisms to dynamically adjust puzzle
difficulty levels based on fraud scores and the computational capabilities of
devices. FraudSys does not alter the experience of users in online systems, but
delays fraudulent actions and consumes significant computational resources of
the fraudsters. Using real datasets from Google Play and Facebook, we
demonstrate the feasibility of FraudSys by showing that the devices of honest
users are minimally impacted, while fraudster controlled devices receive daily
computational penalties of up to 3,079 hours. In addition, we show that with
FraudSys, fraud does not pay off, as a user equipped with mining hardware
(e.g., AntMiner S7) will earn less than half through fraud than from honest
Bitcoin mining.
",computer-science
"  In the context of fitness coaching or for rehabilitation purposes, the motor
actions of a human participant must be observed and analyzed for errors in
order to provide effective feedback. This task is normally carried out by human
coaches, and it needs to be solved automatically in technical applications that
are to provide automatic coaching (e.g. training environments in VR). However,
most coaching systems only provide coarse information on movement quality, such
as a scalar value per body part that describes the overall deviation from the
correct movement. Further, they are often limited to static body postures or
rather simple movements of single body parts. While there are many approaches
to distinguish between different types of movements (e.g., between walking and
jumping), the detection of more subtle errors in a motor performance is less
investigated. We propose a novel approach to classify errors in sports or
rehabilitation exercises such that feedback can be delivered in a rapid and
detailed manner: Homogeneous sub-sequences of exercises are first temporally
aligned via Dynamic Time Warping. Next, we extract a feature vector from the
aligned sequences, which serves as a basis for feature selection using Random
Forests. The selected features are used as input for Support Vector Machines,
which finally classify the movement errors. We compare our algorithm to a well
established state-of-the-art approach in time series classification, 1-Nearest
Neighbor combined with Dynamic Time Warping, and show our algorithm's
superiority regarding classification quality as well as computational cost.
",computer-science
"  I present a simple phenomenological model for the observed linear scaling of
the stellar mass in old globular clusters (GCs) with $z=0$ halo mass in which
the stellar mass in GCs scales linearly with progenitor halo mass at $z=6$
above a minimum halo mass for GC formation. This model reproduces the observed
$M_{\rm GCs}-M_{\rm halo}$ relation at $z=0$ and results in a prediction for
the minimum halo mass at $z=6$ required for hosting one GC: $M_{\rm
min}(z=6)=1.07 \times 10^9\,M_{\odot}$. Translated to $z=0$, the mean threshold
mass is $M_{\rm halo}(z=0) \approx 2\times 10^{10}\,M_{\odot}$. I explore the
observability of GCs in the reionization era and their contribution to cosmic
reionization, both of which depend sensitively on the (unknown) ratio of GC
birth mass to present-day stellar mass, $\xi$. Based on current detections of
$z \gtrsim 6$ objects with $M_{1500} < -17$, values of $\xi > 10$ are strongly
disfavored; this, in turn, has potentially important implications for GC
formation scenarios. Even for low values of $\xi$, some observed high-$z$
galaxies may actually be GCs, complicating estimates of reionization-era galaxy
ultraviolet luminosity functions and constraints on dark matter models. GCs are
likely important reionization sources if $5 \lesssim \xi \lesssim 10$. I also
explore predictions for the fraction of accreted versus in situ GCs in the
local Universe and for descendants of systems at the halo mass threshold of GC
formation (dwarf galaxies). An appealing feature of the model presented here is
the ability to make predictions for GC properties based solely on dark matter
halo merger trees.
",physics
"  The star EPIC 210894022 has been identified from a light curve acquired
through the K2 space mission as possibly orbited by a transiting planet. Our
aim is to confirm the planetary nature of the object and derive its fundamental
parameters. We combine the K2 photometry with reconnaissance spectroscopy and
radial velocity (RV) measurements obtained using three separate telescope and
spectrograph combinations. The spectroscopic synthesis package SME has been
used to derive the stellar photospheric parameters that were used as input to
various stellar evolutionary tracks in order to derive the parameters of the
system. The planetary transit was also validated to occur on the assumed host
star through adaptive imaging and statistical analysis. The star is found to be
located in the background of the Hyades cluster at a distance at least 4 times
further away from Earth than the cluster itself. The spectrum and the space
velocities of EPIC 210894022 strongly suggest it to be a member of the thick
disk population. We find that the star is a metal poor ([Fe/H]=-0.53+/-0.05
dex) and alpha-rich somewhat evolved solar-like object of spectral type G3 with
Teff=5730+/-50 K, logg=4.15+/-0.1 (cgs), radius of 1.3+/-0.1 R_Sun, and mass of
0.88+/-0.02 M_Sun. The RV detection together with the imaging confirms with a
high level of significance that the transit signature is caused by a
super-Earth orbiting the star EPIC 210894022. We measure a mass of 8.6+/-3.9
M_Earth and a radius of 1.9+/-0.2 R_Earth. A second more massive object with a
period longer than about 120 days is indicated by a long term linear
acceleration. With an age of > 10 Gyrs this system is one of the oldest where
planets is hitherto detected. Further studies of this planetary system is
important since it contains information about the planetary formation process
during a very early epoch of the history of our Galaxy.
",physics
"  We propose a new indexing structure for parameterized strings, called
parameterized position heap. Parameterized position heap is applicable for
parameterized pattern matching problem, where the pattern matches a substring
of the text if there exists a bijective mapping from the symbols of the pattern
to the symbols of the substring. We propose an online construction algorithm of
parameterized position heap of a text and show that our algorithm runs in
linear time with respect to the text size. We also show that by using
parameterized position heap, we can find all occurrences of a pattern in the
text in linear time with respect to the product of the pattern size and the
alphabet size.
",computer-science
"  Using the Purple Mountain Observatory Delingha (PMODLH) 13.7 m telescope, we
report a 96-square-degree 12CO/13CO/C18O mapping observation toward the
Galactic region of l = [139.75, 149.75]$^\circ$, b = [-5.25, 5.25]$^\circ$. The
molecular structure of the Local Arm and Perseus Arm are presented. Combining
HI data and part of the Outer Arm results, we obtain that the warp structure of
both atomic and molecular gas is obvious, while the flare structure only exists
in atomic gas in this observing region. In addition, five filamentary giant
molecular clouds on the Perseus Arm are identified. Among them, four are newly
identified. Their relations with the Milky Way large-scale structure are
discussed.
",physics
"  As computer scientists working in bioinformatics/computational biology, we
often face the challenge of coming up with an algorithm to answer a biological
question. This occurs in many areas, such as variant calling, alignment, and
assembly. In this tutorial, we use the example of the genome assembly problem
to demonstrate how to go from a question in the biological realm to a solution
in the computer science realm. We show the modeling process step-by-step,
including all the intermediate failed attempts.
",computer-science
"  The system of dynamic equations for Bose-Einstein condensate at zero
temperature with account of pair correlations is obtained. The spectrum of
small oscillations of the condensate in a spatially homogeneous state is
explored. It is shown that this spectrum has two branches: the sound wave
branch and branch with an energy gap.
",physics
"  We discuss such Maltsev conditions that consist of just one linear equation,
we call them loop conditions. To every such condition can be assigned a graph.
We provide a classification of conditions with undirected graphs. It follows
that the Siggers term is the weakest non-trivial loop condition.
",mathematics
"  Yes, but only for a parameter value that makes it almost coincide with the
standard model. We reconsider the cosmological dynamics of a generalized
Chaplygin gas (gCg) which is split into a cold dark matter (CDM) part and a
dark energy (DE) component with constant equation of state. This model, which
implies a specific interaction between CDM and DE, has a $\Lambda$CDM limit and
provides the basis for studying deviations from the latter. Including matter
and radiation, we use the (modified) CLASS code \cite{class} to construct the
CMB and matter power spectra in order to search for a gCg-based concordance
model that is in agreement with the SNIa data from the JLA sample and with
recent Planck data. The results reveal that the gCg parameter $\alpha$ is
restricted to $|\alpha|\lesssim 0.05$, i.e., to values very close to the
$\Lambda$CDM limit $\alpha =0$. This excludes, in particular, models in which
DE decays linearly with the Hubble rate.
",physics
"  We give topological and game theoretic definitions and theorems nec- essary
for defining a Banach-Mazur game, and apply these definitions to formalize the
game. We then state and prove two theorems which give necessary conditions for
existence of winning strategies for players in a Banach-Mazur game.
",mathematics
"  The support vector machine (SVM) is a powerful and widely used classification
algorithm. This paper uses the Karush-Kuhn-Tucker conditions to provide
rigorous mathematical proof for new insights into the behavior of SVM. These
insights provide perhaps unexpected relationships between SVM and two other
linear classifiers: the mean difference and the maximal data piling direction.
For example, we show that in many cases SVM can be viewed as a cropped version
of these classifiers. By carefully exploring these connections we show how SVM
tuning behavior is affected by characteristics including: balanced vs.
unbalanced classes, low vs. high dimension, separable vs. non-separable data.
These results provide further insights into tuning SVM via cross-validation by
explaining observed pathological behavior and motivating improved
cross-validation methodology. Finally, we also provide new results on the
geometry of complete data piling directions in high dimensional space.
",statistics
"  Sequential decision making problems, such as structured prediction, robotic
control, and game playing, require a combination of planning policies and
generalisation of those plans. In this paper, we present Expert Iteration
(ExIt), a novel reinforcement learning algorithm which decomposes the problem
into separate planning and generalisation tasks. Planning new policies is
performed by tree search, while a deep neural network generalises those plans.
Subsequently, tree search is improved by using the neural network policy to
guide search, increasing the strength of new plans. In contrast, standard deep
Reinforcement Learning algorithms rely on a neural network not only to
generalise plans, but to discover them too. We show that ExIt outperforms
REINFORCE for training a neural network to play the board game Hex, and our
final tree search agent, trained tabula rasa, defeats MoHex 1.0, the most
recent Olympiad Champion player to be publicly released.
",computer-science
"  Descent theory for linear categories is developed. Given a linear category as
an extension of a diagonal category, we introduce descent data, and the
category of descent data is isomorphic to the category of representations of
the diagonal category, if some flatness assumptions are satisfied. Then
Hopf-Galois descent theory for linear Hopf categories, the Hopf algebra version
of a linear category, is developed. This leads to the notion of Hopf-Galois
category extension. We have a dual theory, where actions by dual linear Hopf
categories on linear categories are considered. Hopf-Galois category extensions
over groupoid algebras correspond to strongly graded linear categories.
",mathematics
"  Novice programmers often struggle with the formal syntax of programming
languages. To assist them, we design a novel programming language correction
framework amenable to reinforcement learning. The framework allows an agent to
mimic human actions for text navigation and editing. We demonstrate that the
agent can be trained through self-exploration directly from the raw input, that
is, program text itself, without any knowledge of the formal syntax of the
programming language. We leverage expert demonstrations for one tenth of the
training data to accelerate training. The proposed technique is evaluated on
6975 erroneous C programs with typographic errors, written by students during
an introductory programming course. Our technique fixes 14% more programs and
29% more compiler error messages relative to those fixed by a state-of-the-art
tool, DeepFix, which uses a fully supervised neural machine translation
approach.
",computer-science
"  We propose a consistent polynomial-time method for the unseeded node matching
problem for networks with smooth underlying structures. Despite widely
conjectured by the research community that the structured graph matching
problem to be significantly easier than its worst case counterpart, well-known
to be NP-hard, the statistical version of the problem has stood a challenge
that resisted any solution both provable and polynomial-time. The closest
existing work requires quasi-polynomial time. Our method is based on the latest
advances in graphon estimation techniques and analysis on the concentration of
empirical Wasserstein distances. Its core is a simple yet unconventional
sampling-and-matching scheme that reduces the problem from unseeded to seeded.
Our method allows flexible efficiencies, is convenient to analyze and
potentially can be extended to more general settings. Our work enables a rich
variety of subsequent estimations and inferences.
",statistics
"  We study the Strichartz estimates for Schrödinger equation on a metric cone
$X$, where the metric cone $X=C(Y)=(0,\infty)_r\times Y$ and the cross section
$Y$ is a $(n-1)$-dimensional closed Riemannian manifold $(Y,h)$. The equipped
metric on $X$ is given by $g=dr^2+r^2h$, and let $\Delta_g$ be the Friedrich
extension positive Laplacian on $X$ and $V=V_0 r^{-2}$ where
$V_0\in\CC^\infty(Y)$ is a real function such that the operator
$\Delta_h+V_0+(n-2)^2/4$ is a strictly positive operator on $L^2(Y)$. We
establish the full range of the global-in-time Strichartz estimate without loss
for the Schödinger equation associated with the operator $\LL_V=\Delta_g+V_0
r^{-2}$ including the endpoint estimate both in homogeneous and inhomogeneous
cases. As an application, we study the well-posed theory and scattering theory
for the Schödinger equation with cubic nonlinearity on this setting.
",mathematics
"  We study transient behaviour in the dynamics of complex systems described by
a set of non-linear ODE's. Destabilizing nature of transient trajectories is
discussed and its connection with the eigenvalue-based linearization procedure.
The complexity is realized as a random matrix drawn from a modified May-Wigner
model. Based on the initial response of the system, we identify a novel
stable-transient regime. We calculate exact abundances of typical and extreme
transient trajectories finding both Gaussian and Tracy-Widom distributions
known in extreme value statistics. We identify degrees of freedom driving
transient behaviour as connected to the eigenvectors and encoded in a
non-orthogonality matrix $T_0$. We accordingly extend the May-Wigner model to
contain a phase with typical transient trajectories present. An exact norm of
the trajectory is obtained in the vanishing $T_0$ limit where it describes a
normal matrix.
",physics
"  We propose a multi-view network for text classification. Our method
automatically creates various views of its input text, each taking the form of
soft attention weights that distribute the classifier's focus among a set of
base features. For a bag-of-words representation, each view focuses on a
different subset of the text's words. Aggregating many such views results in a
more discriminative and robust representation. Through a novel architecture
that both stacks and concatenates views, we produce a network that emphasizes
both depth and width, allowing training to converge quickly. Using our
multi-view architecture, we establish new state-of-the-art accuracies on two
benchmark tasks.
",computer-science
"  The critcal exponent $\omega$ is evaluated at $O(1/N)$ in $d$-dimensions in
the Gross-Neveu model using the large $N$ critical point formalism. It is shown
to be in agreement with the recently determined three loop $\beta$-functions of
the Gross-Neveu-Yukawa model in four dimensions. The same exponent is computed
for the chiral Gross-Neveu and non-abelian Nambu-Jona-Lasinio universality
classes.
",physics
"  K. Borsuk in 1979, in the Topological Conference in Moscow, introduced the
concept of the capacity of a compactum. In this paper, we compute the capacity
of the product of two spheres of the same or different dimensions and the
capacity of lense spaces. Also, we present an upper bound for the capacity of a
$\mathbb{Z}_n$-complex, i.e., a connected finite 2-dimensional CW-complex with
finite cyclic fundamental group $\mathbb{Z}_n$.
",mathematics
"  Low-textured image stitching remains a challenging problem. It is difficult
to achieve good alignment and it is easy to break image structures due to
insufficient and unreliable point correspondences. Moreover, because of the
viewpoint variations between multiple images, the stitched images suffer from
projective distortions. To solve these problems, this paper presents a
line-guided local warping method with a global similarity constraint for image
stitching. Line features which serve well for geometric descriptions and scene
constraints, are employed to guide image stitching accurately. On one hand, the
line features are integrated into a local warping model through a designed
weight function. On the other hand, line features are adopted to impose strong
geometric constraints, including line correspondence and line colinearity, to
improve the stitching performance through mesh optimization. To mitigate
projective distortions, we adopt a global similarity constraint, which is
integrated with the projective warps via a designed weight strategy. This
constraint causes the final warp to slowly change from a projective to a
similarity transformation across the image. Finally, the images undergo a
two-stage alignment scheme that provides accurate alignment and reduces
projective distortion. We evaluate our method on a series of images and compare
it with several other methods. The experimental results demonstrate that the
proposed method provides a convincing stitching performance and that it
outperforms other state-of-the-art methods.
",computer-science
"  Spectral Clustering (SC) is a widely used data clustering method which first
learns a low-dimensional embedding $U$ of data by computing the eigenvectors of
the normalized Laplacian matrix, and then performs k-means on $U^\top$ to get
the final clustering result. The Sparse Spectral Clustering (SSC) method
extends SC with a sparse regularization on $UU^\top$ by using the block
diagonal structure prior of $UU^\top$ in the ideal case. However, encouraging
$UU^\top$ to be sparse leads to a heavily nonconvex problem which is
challenging to solve and the work (Lu, Yan, and Lin 2016) proposes a convex
relaxation in the pursuit of this aim indirectly. However, the convex
relaxation generally leads to a loose approximation and the quality of the
solution is not clear. This work instead considers to solve the nonconvex
formulation of SSC which directly encourages $UU^\top$ to be sparse. We propose
an efficient Alternating Direction Method of Multipliers (ADMM) to solve the
nonconvex SSC and provide the convergence guarantee. In particular, we prove
that the sequences generated by ADMM always exist a limit point and any limit
point is a stationary point. Our analysis does not impose any assumptions on
the iterates and thus is practical. Our proposed ADMM for nonconvex problems
allows the stepsize to be increasing but upper bounded, and this makes it very
efficient in practice. Experimental analysis on several real data sets verifies
the effectiveness of our method.
",computer-science
"  We study a stochastic particle system with a logarithmically-singular
inter-particle interaction potential which allows for inelastic particle
collisions. We relate the squared Bessel process to the evolution of localized
clusters of particles, and develop a numerical method capable of detecting
collisions of many point particles without the use of pairwise computations, or
very refined adaptive timestepping. We show that when the system is in an
appropriate parameter regime, the hydrodynamic limit of the empirical mass
density of the system is a solution to a nonlinear Fokker-Planck equation, such
as the Patlak-Keller-Segel (PKS) model, or its multispecies variant. We then
show that the presented numerical method is well-suited for the simulation of
the formation of finite-time singularities in the PKS, as well as PKS pre- and
post-blow-up dynamics. Additionally, we present numerical evidence that blow-up
with an increasing total second moment in the two species Keller-Segel system
occurs with a linearly increasing second moment in one component, and a
linearly decreasing second moment in the other component.
",mathematics
"  This work presents an innovative application of the well-known concept of
cortico-muscular coherence for the classification of various motor tasks, i.e.,
grasps of different kinds of objects. Our approach can classify objects with
different weights (motor-related features) and different surface frictions
(haptics-related features) with high accuracy (over 0:8). The outcomes
presented here provide information about the synchronization existing between
the brain and the muscles during specific activities; thus, this may represent
a new effective way to perform activity recognition.
",quantitative-biology
"  The thermoelectric voltage developed across an atomic metal junction (i.e., a
nanostructure in which one or a few atoms connect two metal electrodes) in
response to a temperature difference between the electrodes, results from the
quantum interference of electrons that pass through the junction multiple times
after being scattered by the surrounding defects. Here we report successfully
tuning this quantum interference and thus controlling the magnitude and sign of
the thermoelectric voltage by applying a mechanical force that deforms the
junction. The observed switching of the thermoelectric voltage is reversible
and can be cycled many times. Our ab initio and semi-empirical calculations
elucidate the detailed mechanism by which the quantum interference is tuned. We
show that the applied strain alters the quantum phases of electrons passing
through the narrowest part of the junction and hence modifies the electronic
quantum interference in the device. Tuning the quantum interference causes the
energies of electronic transport resonances to shift, which affects the
thermoelectric voltage. These experimental and theoretical studies reveal that
Au atomic junctions can be made to exhibit both positive and negative
thermoelectric voltages on demand, and demonstrate the importance and
tunability of the quantum interference effect in the atomic-scale metal
nanostructures.
",physics
"  Sensor fusion is a fundamental process in robotic systems as it extends the
perceptual range and increases robustness in real-world operations. Current
multi-sensor deep learning based semantic segmentation approaches do not
provide robustness to under-performing classes in one modality, or require a
specific architecture with access to the full aligned multi-sensor training
data. In this work, we analyze statistical fusion approaches for semantic
segmentation that overcome these drawbacks while keeping a competitive
performance. The studied approaches are modular by construction, allowing to
have different training sets per modality and only a much smaller subset is
needed to calibrate the statistical models. We evaluate a range of statistical
fusion approaches and report their performance against state-of-the-art
baselines on both real-world and simulated data. In our experiments, the
approach improves performance in IoU over the best single modality segmentation
results by up to 5%. We make all implementations and configurations publicly
available.
",computer-science
"  The misalignment of the solar rotation axis and the magnetic axis of the Sun
produces a periodic reversal of the Parker spiral magnetic field and the
sectored solar wind. The compression of the sectors is expected to lead to
reconnection in the heliosheath (HS). We present particle-in-cell simulations
of the sectored HS that reflect the plasma environment along the Voyager 1 and
2 trajectories, specifically including unequal positive and negative azimuthal
magnetic flux as seen in the Voyager data \citep{Burlaga03}. Reconnection
proceeds on individual current sheets until islands on adjacent current layers
merge. At late time bands of the dominant flux survive, separated by bands of
deep magnetic field depletion. The ambient plasma pressure supports the strong
magnetic pressure variation so that pressure is anti-correlated with magnetic
field strength. There is little variation in the magnetic field direction
across the boundaries of the magnetic depressions. At irregular intervals
within the magnetic depressions are long-lived pairs of magnetic islands where
the magnetic field direction reverses so that spacecraft data would reveal
sharp magnetic field depressions with only occasional crossings with jumps in
magnetic field direction. This is typical of the magnetic field data from the
Voyager spacecraft \citep{Burlaga11,Burlaga16}. Voyager 2 data reveals that
fluctuations in the density and magnetic field strength are anti-correlated in
the sector zone as expected from reconnection but not in unipolar regions. The
consequence of the annihilation of subdominant flux is a sharp reduction in the
""number of sectors"" and a loss in magnetic flux as documented from the Voyager
1 magnetic field and flow data \citep{Richardson13}.
",physics
"  Natural language and symbols are intimately correlated. Recent advances in
machine learning (ML) and in natural language processing (NLP) seem to
contradict the above intuition: symbols are fading away, erased by vectors or
tensors called distributed and distributional representations. However, there
is a strict link between distributed/distributional representations and
symbols, being the first an approximation of the second. A clearer
understanding of the strict link between distributed/distributional
representations and symbols will certainly lead to radically new deep learning
networks. In this paper we make a survey that aims to draw the link between
symbolic representations and distributed/distributional representations. This
is the right time to revitalize the area of interpreting how symbols are
represented inside neural networks.
",computer-science
"  Shock wave interactions with defects, such as pores, are known to play a key
role in the chemical initiation of energetic materials. The shock response of
hexanitrostilbene is studied through a combination of large scale reactive
molecular dynamics and mesoscale hydrodynamic simulations. In order to extend
our simulation capability at the mesoscale to include weak shock conditions (<
6 GPa), atomistic simulations of pore collapse are used to define a strain rate
dependent strength model. Comparing these simulation methods allows us to
impose physically-reasonable constraints on the mesoscale model parameters. In
doing so, we have been able to study shock waves interacting with pores as a
function of this viscoplastic material response. We find that the pore collapse
behavior of weak shocks is characteristically different to that of strong
shocks.
",physics
"  Nickel oxide (NiO) has been studied extensively for various applications
ranging from electrochemistry to solar cells [1,2]. In recent years, NiO
attracted much attention as an antiferromagnetic (AF) insulator material for
spintronic devices [3-10]. Understanding the spin - phonon coupling in NiO is a
key to its functionalization, and enabling AF spintronics' promise of
ultra-high-speed and low-power dissipation [11,12]. However, despite its status
as an exemplary AF insulator and a benchmark material for the study of
correlated electron systems, little is known about the spin - phonon
interaction, and the associated energy dissipation channel, in NiO. In
addition, there is a long-standing controversy over the large discrepancies
between the experimental and theoretical values for the electron, phonon, and
magnon energies in NiO [13-23]. This gap in knowledge is explained by NiO
optical selection rules, high Neel temperature and dominance of the magnon band
in the visible Raman spectrum, which precludes a conventional approach for
investigating such interaction. Here we show that by using ultraviolet (UV)
Raman spectroscopy one can extract the spin - phonon coupling coefficients in
NiO. We established that unlike in other materials, the spins of Ni atoms
interact more strongly with the longitudinal optical (LO) phonons than with the
transverse optical (TO) phonons, and produce opposite effects on the phonon
energies. The peculiarities of the spin - phonon coupling are consistent with
the trends given by density functional theory calculations. The obtained
results shed light on the nature of the spin - phonon coupling in AF insulators
and may help in developing innovative spintronic devices.
",physics
"  We present a framework for vision-based model predictive control (MPC) for
the task of aggressive, high-speed autonomous driving. Our approach uses deep
convolutional neural networks to predict cost functions from input video which
are directly suitable for online trajectory optimization with MPC. We
demonstrate the method in a high speed autonomous driving scenario, where we
use a single monocular camera and a deep convolutional neural network to
predict a cost map of the track in front of the vehicle. Results are
demonstrated on a 1:5 scale autonomous vehicle given the task of high speed,
aggressive driving.
",computer-science
"  Given a poset $P$ and a standard closure operator $\Gamma:\wp(P)\to\wp(P)$ we
give a necessary and sufficient condition for the lattice of $\Gamma$-closed
sets of $\wp(P)$ to be a frame in terms of the recursive construction of the
$\Gamma$-closure of sets. We use this condition to show that given a set
$\mathcal{U}$ of distinguished joins from $P$, the lattice of
$\mathcal{U}$-ideals of $P$ fails to be a frame if and only if it fails to be
$\sigma$-distributive, with $\sigma$ depending on the cardinalities of sets in
$\mathcal{U}$. From this we deduce that if a poset has the property that
whenever $a\wedge(b\vee c)$ is defined for $a,b,c\in P$ it is necessarily equal
to $(a\wedge b)\vee (a\wedge c)$, then it has an $(\omega,3)$-representation.
This answers a question from the literature.
",mathematics
"  The aim of this work is to establish that two recently published projection
theorems, one dealing with a parametric generalization of relative entropy and
another dealing with Rényi divergence, are equivalent under a
correspondence on the space of probability measures. Further, we demonstrate
that the associated ""Pythagorean"" theorems are equivalent under this
correspondence. Finally, we apply Eguchi's method of obtaining Riemannian
metrics from general divergence functions to show that the geometry arising
from the above divergences are equivalent under the aforementioned
correspondence.
",computer-science
"  Unsupervised neural nets such as Restricted Boltzmann Machines(RBMs) and Deep
Belif Networks(DBNs), are powerful in automatic feature extraction,unsupervised
weight initialization and density estimation. In this paper,we demonstrate that
the parameters of these neural nets can be dramatically reduced without
affecting their performance. We describe a method to reduce the parameters
required by RBM which is the basic building block for deep architectures.
Further we propose an unsupervised sparse deep architectures selection
algorithm to form sparse deep neural networks.Experimental results show that
there is virtually no loss in either generative or discriminative performance.
",computer-science
"  For an endofunctor $H$ on a hyper-extensive category preserving countable
coproducts we describe the free corecursive algebra on $Y$ as the coproduct of
the final coalgebra for $H$ and the free $H$-algebra on $Y$. As a consequence,
we derive that $H$ is a cia functor, i.e., its corecursive algebras are
precisely the cias (completely iterative algebras). Also all functors $H(-) +
Y$ are then cia functors. For finitary set functors we prove that, conversely,
if $H$ is a cia functor, then it has the form $H = W \times (-) + Y$ for some
sets $W$ and $Y$.
",computer-science
"  We report on the development of a versatile cryogen-free laboratory cryostat
based upon a commercial pulse tube cryocooler. It provides enough cooling power
for continuous recondensation of circulating $^4$He gas at a condensation
pressure of approximately 250~mbar. Moreover, the cryostat allows for exchange
of different cryostat-inserts as well as fast and easy ""wet"" top-loading of
samples directly into the 1 K pot with a turn-over time of less than 75~min.
Starting from room temperature and using a $^4$He cryostat-insert, a base
temperature of 1.0~K is reached within approximately seven hours and a cooling
power of 250~mW is established at 1.24~K.
",physics
"  We prove that a representation of the fundamental group of a quasi-projective
manifold into the group of formal diffeomorphisms of one variable either is
virtually abelian or, after taking the quotient by its center, factors through
an orbicurve.
",mathematics
"  Magnetic fields quench the kinetic energy of two dimensional electrons,
confining them to highly degenerate Landau levels. In the absence of disorder,
the ground state at partial Landau level filling is determined only by Coulomb
interactions, leading to a variety of correlation-driven phenomena. Here, we
realize a quantum Hall analog of the Neél-to-valence bond solid transition
within the spin- and sublattice- degenerate monolayer graphene zero energy
Landau level by experimentally controlling substrate-induced sublattice
symmetry breaking. The transition is marked by unusual isospin transitions in
odd denominator fractional quantum Hall states for filling factors $\nu$ near
charge neutrality, and the unexpected appearance of incompressible even
denominator fractional quantum Hall states at $\nu=\pm1/2$ and $\pm1/4$
associated with pairing between composite fermions on different carbon
sublattices.
",physics
"  Daily operation of a large-scale experiment is a challenging task,
particularly from perspectives of routine monitoring of quality for data being
taken. We describe an approach that uses Machine Learning for the automated
system to monitor data quality, which is based on partial use of data qualified
manually by detector experts. The system automatically classifies marginal
cases: both of good an bad data, and use human expert decision to classify
remaining ""grey area"" cases.
This study uses collision data collected by the CMS experiment at LHC in
2010. We demonstrate that proposed workflow is able to automatically process at
least 20\% of samples without noticeable degradation of the result.
",computer-science
"  Thermoelectric (TE) measurements have been performed on the workhorses of
today's data storage devices, exhibiting either the giant or the anisotropic
magnetoresistance effect (GMR and AMR). The temperature-dependent (50-300 K)
and magnetic field-dependent (up to 1 T) TE power factor (PF) has been
determined for several Co-Ni alloy nanowires with varying Co:Ni ratios as well
as for Co-Ni/Cu multilayered nanowires with various Cu layer thicknesses, which
were all synthesized via a template-assisted electrodeposition process. A
systematic investigation of the resistivity, as well as the Seebeck
coefficient, is performed for Co-Ni alloy nanowires and Co-Ni/Cu multilayered
nanowires. At room temperature, measured values of TE PFs up to 3.6 mWK-2m-1
for AMR samples and 2.0 mWK-2m-1 for GMR nanowires are obtained. Furthermore,
the TE PF is found to increase by up to 13.1 % for AMR Co-Ni alloy nanowires
and by up to 52 % for GMR Co-Ni/Cu samples in an external applied magnetic
field. The magnetic nanowires exhibit TE PFs that are of the same order of
magnitude as TE PFs of Bi-Sb-Se-Te based thermoelectric materials and,
additionally, give the opportunity to adjust the TE power output to changing
loads and hotspots through external magnetic fields.
",physics
"  In Web search, entity-seeking queries often trigger a special Question
Answering (QA) system. It may use a parser to interpret the question to a
structured query, execute that on a knowledge graph (KG), and return direct
entity responses. QA systems based on precise parsing tend to be brittle: minor
syntax variations may dramatically change the response. Moreover, KG coverage
is patchy. At the other extreme, a large corpus may provide broader coverage,
but in an unstructured, unreliable form. We present AQQUCN, a QA system that
gracefully combines KG and corpus evidence. AQQUCN accepts a broad spectrum of
query syntax, between well-formed questions to short `telegraphic' keyword
sequences. In the face of inherent query ambiguities, AQQUCN aggregates signals
from KGs and large corpora to directly rank KG entities, rather than commit to
one semantic interpretation of the query. AQQUCN models the ideal
interpretation as an unobservable or latent variable. Interpretations and
candidate entity responses are scored as pairs, by combining signals from
multiple convolutional networks that operate collectively on the query, KG and
corpus. On four public query workloads, amounting to over 8,000 queries with
diverse query syntax, we see 5--16% absolute improvement in mean average
precision (MAP), compared to the entity ranking performance of recent systems.
Our system is also competitive at entity set retrieval, almost doubling F1
scores for challenging short queries.
",computer-science
"  This paper presents KeypointNet, an end-to-end geometric reasoning framework
to learn an optimal set of category-specific 3D keypoints, along with their
detectors. Given a single image, KeypointNet extracts 3D keypoints that are
optimized for a downstream task. We demonstrate this framework on 3D pose
estimation by proposing a differentiable objective that seeks the optimal set
of keypoints for recovering the relative pose between two views of an object.
Our model discovers geometrically and semantically consistent keypoints across
viewing angles and instances of an object category. Importantly, we find that
our end-to-end framework using no ground-truth keypoint annotations outperforms
a fully supervised baseline using the same neural network architecture on the
task of pose estimation. The discovered 3D keypoints on the car, chair, and
plane categories of ShapeNet are visualized at this http URL.
",statistics
"  The actin cytoskeleton is an active semi-flexible polymer network whose
non-equilibrium properties coordinate both stable and contractile behaviors to
maintain or change cell shape. While myosin motors drive the actin cytoskeleton
out-of-equilibrium, the role of myosin-driven active stresses in the
accumulation and dissipation of mechanical energy is unclear. To investigate
this, we synthesize an actomyosin material in vitro whose active stress content
can tune the network from stable to contractile. Each increment in activity
determines a characteristic spectrum of actin filament fluctuations which is
used to calculate the total mechanical work and the production of entropy in
the material. We find that the balance of work and entropy does not increase
monotonically and, surprisingly, the entropy production rate is maximized in
the non-contractile, stable state. Our study provides evidence that the origins
of system entropy production and activity-dependent dissipation arise from
disorder in the molecular interactions between actin and myosin
",quantitative-biology
"  The classical idea of evolutionarily stable strategy (ESS) modeling animal
behavior does not involve any spatial dependence. We considered a spatial
Hawk-Dove game played by animals in a patchy environment with wrap around
boundaries. We posit that each site contains the same number of individuals. An
evolution equation for analyzing the stability of the ESS is found as the mean
dynamics of the classical frequency dependent Moran process coupled via
migration and nonlocal payoff calculation in 1D and 2D habitats. The linear
stability analysis of the model is performed and conditions to observe spatial
patterns are investigated. For the nearest neighbor interactions (including von
Neumann and Moore neighborhoods in 2D) we concluded that it is possible to
destabilize the ESS of the game and observe pattern formation when the
dispersal rate is small enough. We numerically investigate the spatial patterns
arising from the replicator equations coupled via nearest neighbor payoff
calculation and dispersal.
",quantitative-biology
"  In the quest for scalable Bayesian computational algorithms we need to
exploit the full potential of existing methodologies. In this note we point out
that message passing algorithms, which are very well developed for inference in
graphical models, appear to be largely unexplored for scalable inference in
Bayesian multilevel regression models. We show that nested multilevel
regression models with Gaussian errors lend themselves very naturally to the
combined use of belief propagation and MCMC. Specifically, the posterior
distribution of the regression parameters conditionally on covariance
hyperparameters is a high-dimensional Gaussian that can be sampled exactly (as
well as marginalized) using belief propagation at a cost that scales linearly
in the number of parameters and data. We derive an algorithm that works
efficiently even for conditionally singular Gaussian distributions, e.g., when
there are linear constraints between the parameters at different levels. We
show that allowing for such non-invertible Gaussians is critical for belief
propagation to be applicable to a large class of nested multilevel models. From
a different perspective, the methodology proposed can be seen as a
generalization of forward-backward algorithms for sampling to multilevel
regressions with tree-structure graphical models, as opposed to single-branch
trees used in classical Kalman filter contexts.
",statistics
"  We propose a new method for input variable selection in nonlinear regression.
The method is embedded into a kernel regression machine that can model general
nonlinear functions, not being a priori limited to additive models. This is the
first kernel-based variable selection method applicable to large datasets. It
sidesteps the typical poor scaling properties of kernel methods by mapping the
inputs into a relatively low-dimensional space of random features. The
algorithm discovers the variables relevant for the regression task together
with learning the prediction model through learning the appropriate nonlinear
random feature maps. We demonstrate the outstanding performance of our method
on a set of large-scale synthetic and real datasets.
",statistics
"  It is known that connected translation invariant $n$-dimensional
noncommutative differentials $d x^i$ on the algebra $k[x^1,\cdots,x^n]$ of
polynomials in $n$-variables over a field $k$ are classified by commutative
algebras $V$ on the vector space spanned by the coordinates. This data also
applies to construct differentials on the Heisenberg algebra `spacetime' with
relations $[x^\mu,x^\nu]=\lambda\Theta^{\mu\nu}$ where $ \Theta$ is an
antisymmetric matrix as well as to Lie algebras with pre-Lie algebra
structures. We specialise the general theory to the field $k={\ \mathbb{F}}_2$
of two elements, in which case translation invariant metrics (i.e. with
constant coefficients) are equivalent to making $V$ a Frobenius algebras. We
classify all of these and their quantum Levi-Civita bimodule connections for
$n=2,3$, with partial results for $n=4$. For $n=2$ we find 3 inequivalent
differential structures admitting 1,2 and 3 invariant metrics respectively. For
$n=3$ we find 6 differential structures admitting $0,1,2,3,4,7$ invariant
metrics respectively. We give some examples for $n=4$ and general $n$.
Surprisingly, not all our geometries for $n\ge 2$ have zero quantum Riemann
curvature. Quantum gravity is normally seen as a weighted `sum' over all
possible metrics but our results are a step towards a deeper approach in which
we must also `sum' over differential structures. Over ${\mathbb{F}}_2$ we
construct some of our algebras and associated structures by digital gates,
opening up the possibility of `digital geometry'.
",mathematics
"  This article is an attempt to generalize Riemann's bilinear relations on
compact Riemann surface of genus at least 2, which may lead to new structures
in the theory of hyperbolic Riemann surfaces. No significant result is
obtained, the article serves to bring the readers' attention to the observation
made by [Bol-1949], and some easy consequences.
",mathematics
"  To date, the only limit on graviton mass using galaxy clusters was obtained
by Goldhaber and Nieto in 1974, using the fact that the orbits of galaxy
clusters are bound and closed, and extend up to 580 kpc. From positing that
only a Newtonian potential gives rise to such stable bound orbits, a limit on
the graviton mass $m_g<10^{-29}$ eV was obtained (PRD 9,1119, 1974). Recently,
it has been shown that one can obtain closed bound orbits for Yukawa potential
(arXiv:1705.02444), thus invalidating the main \emph{ansatz} used in Goldhaber
and Nieto to obtain the graviton mass bound. In order to obtain a revised
estimate using galaxy clusters, we use dynamical mass models of the Abell 1689
(A1689) galaxy cluster to check their compatibility with a Yukawa gravitational
potential. We assume mass models for the gas, dark matter, and galaxies for
A1689 from arXiv:1703.10219 and arXiv:1610.01543, who used this cluster to test
various alternate gravity theories, which dispense with the need for dark
matter. We quantify the deviations in the acceleration profile using these mass
models assuming a Yukawa potential and that obtained assuming a Newtonian
potential by calculating the $\chi^2$ residuals between the two profiles. Our
estimated bound on the graviton mass ($m_g$) is thereby given by, $m_g < 1.37
\times 10^{-29}$ eV or in terms of the graviton Compton wavelength of,
$\lambda_g>9.1 \times 10^{19}$ km at 90\% confidence level.
",physics
"  In this paper, we present a new R package COREclust dedicated to the
detection of representative variables in high dimensional spaces with a
potentially limited number of observations. Variable sets detection is based on
an original graph clustering strategy denoted CORE-clustering algorithm that
detects CORE-clusters, i.e. variable sets having a user defined size range and
in which each variable is very similar to at least another variable.
Representative variables are then robustely estimate as the CORE-cluster
centers. This strategy is entirely coded in C++ and wrapped by R using the Rcpp
package. A particular effort has been dedicated to keep its algorithmic cost
reasonable so that it can be used on large datasets. After motivating our work,
we will explain the CORE-clustering algorithm as well as a greedy extension of
this algorithm. We will then present how to use it and results obtained on
synthetic and real data.
",statistics
"  We consider the problem of estimating an expected outcome from a stochastic
simulation model using importance sampling. We propose a two-stage procedure
that involves a regression stage and a sampling stage to construct our
estimator. We introduce a parametric and a nonparametric regression estimator
in the first stage and study how the allocation between the two stages affects
the performance of final estimator. We derive the oracle property for both
approaches. We analyze the empirical performances of our approaches using two
simulated data and a case study on wind turbine reliability evaluation.
",statistics
"  In this paper, we introduce an iterative linearization scheme that allows to
approximate the weak solution of the $p$-Poisson problem
\begin{align*}
-\operatorname{div}(|\nabla u|^{p-2}\nabla u) &= f\quad\text{in }\Omega,
u&= 0\quad\text{on}\partial\Omega
\end{align*} for $1 < p \leq 2$. The algorithm can be interpreted as a
relaxed Kačanov iteration. We prove that the algorithm converges at least
with an algebraic rate.
",mathematics
"  We present a simple apparatus for femtosecond laser induced generation of
X-rays. The apparatus consists of a vacuum chamber containing an off-axis
parabolic focusing mirror, a reel system, a debris protection setup, a quartz
window for the incoming laser beam, and an X-ray window. Before entering the
vacuum chamber, the femtosecond laser is expanded with an all reflective
telescope design to minimize laser intensity losses and pulse broadening while
allowing for focusing as well as peak intensity optimization. The laser pulse
duration was characterized by second-harmonic generation frequency resolved
optical gating. A high spatial resolution knife-edge technique was implemented
to characterize the beam size at the focus of the X-ray generation apparatus.
We have characterized x-ray spectra obtained with three different samples:
titanium, iron:chromium alloy, and copper. In all three cases, the femtosecond
laser generated X-rays give spectral lines consistent with literature reports.
We present a rms amplitude analysis of the generated X-ray pulses, and provide
an upper bound for the duration of the X-ray pulses.
",physics
"  Debate and deliberation play essential roles in politics and government, but
most models presume that debates are won mainly via superior style or agenda
control. Ideally, however, debates would be won on the merits, as a function of
which side has the stronger arguments. We propose a predictive model of debate
that estimates the effects of linguistic features and the latent persuasive
strengths of different topics, as well as the interactions between the two.
Using a dataset of 118 Oxford-style debates, our model's combination of content
(as latent topics) and style (as linguistic features) allows us to predict
audience-adjudicated winners with 74% accuracy, significantly outperforming
linguistic features alone (66%). Our model finds that winning sides employ
stronger arguments, and allows us to identify the linguistic features
associated with strong or weak arguments.
",computer-science
"  Given a network of nodes, minimizing the spread of a contagion using a
limited budget is a well-studied problem with applications in network security,
viral marketing, social networks, and public health. In real graphs, virus may
infect a node which in turn infects its neighbor nodes and this may trigger an
epidemic in the whole graph. The goal thus is to select the best k nodes
(budget constraint) that are immunized (vaccinated, screened, filtered) so as
the remaining graph is less prone to the epidemic. It is known that the problem
is, in all practical models, computationally intractable even for moderate
sized graphs. In this paper we employ ideas from spectral graph theory to
define relevance and importance of nodes. Using novel graph theoretic
techniques, we then design an efficient approximation algorithm to immunize the
graph. Theoretical guarantees on the running time of our algorithm show that it
is more efficient than any other known solution in the literature. We test the
performance of our algorithm on several real world graphs. Experiments show
that our algorithm scales well for large graphs and outperforms state of the
art algorithms both in quality (containment of epidemic) and efficiency
(runtime and space complexity).
",computer-science
"  A rational projective plane ($\mathbb{QP}^2$) is a simply connected, smooth,
closed manifold $M$ such that $H^*(M;\mathbb{Q}) \cong
\mathbb{Q}[\alpha]/\langle \alpha^3 \rangle$. An open problem is to classify
the dimensions at which such a manifold exists. The Barge-Sullivan rational
surgery realization theorem provides necessary and sufficient conditions that
include the Hattori-Stong integrality conditions on the Pontryagin numbers. In
this article, we simplify these conditions and combine them with the signature
equation to give a single quadratic residue equation that determines whether a
given dimension supports a $\mathbb{QP}^2$. We then confirm existence of a
$\mathbb{QP}^2$ in two new dimensions and prove several non-existence results
using factorizations of numerators of divided Bernoulli numbers. We also
resolve the existence question in the Spin case, and we discuss existence
results for the more general class of rational projective spaces.
",mathematics
"  Let $\Omega$ be a pseudoconvex domain in $\mathbb C^n$ with smooth boundary
$b\Omega$. We define general estimates $(f\text{-}\mathcal M)^k_{\Omega}$ and
$(f\text{-}\mathcal M)^k_{b\Omega}$ on $k$-forms for the complex Laplacian
$\Box$ on $\Omega$ and the Kohn-Laplacian $\Box_b$ on $b\Omega$. For $1\le k\le
n-2$, we show that $(f\text{-}\mathcal M)^k_{b\Omega}$ holds if and only if
$(f\text{-}\mathcal M)^k_{\Omega}$ and $(f\text{-}\mathcal M)^{n-k-1}_{\Omega}$
hold. Our proof relies on Kohn's method in [Ann. of Math. (2), 156(1):213--248,
2002].
",mathematics
"  Chemical or enzymatic cross-linking of casein micelles (CMs) increases their
stability against dissociating agents. In this paper, a comparative study of
stability between native CMs and CMs cross-linked with genipin (CMs-GP) as a
function of pH is described. Stability to temperature and ethanol were
investigated in the pH range 2.0-7.0. The size and the charge
($\zeta$-potential) of the particles were determined by dynamic light
scattering. Native CMs precipitated below pH 5.5, CMs-GP precipitated from pH
3.5 to 4.5, whereas no precipitation was observed at pH 2.0-3.0 or pH 4.5-7.0.
The isoelectric point of CMs-GP was determined to be pH 3.7. Highest stability
against heat and ethanol was observed for CMs-GP at pH 2, where visible
coagulation was determined only after 800 s at 140 $^\circ$C or 87.5% (v/v) of
ethanol. These results confirmed the hypothesis that cross-linking by GP
increased the stability of CMs.
",physics
"  The distance between the true and numerical solutions in some metric is
considered as the discretization error magnitude. If error magnitude ranging is
known, the triangle inequality enables the estimation of the vicinity of the
approximate solution that contains the exact one (exact solution enclosure).
The analysis of distances between the numerical solutions enables
discretization error ranging, if solutions errors are significantly different.
Numerical tests conducted using the steady supersonic flows, governed by the
two dimensional Euler equations, demonstrate the properties of the exact
solution enclosure. The set of solutions generated by solvers of different
orders of approximation is used. The success of this approach depends on the
choice of metric.
",physics
"  We show that after forming a connected sum with a homotopy sphere, all
(2j-1)-connected 2j-parallelisable manifolds in dimension 4j+1, j > 0, can be
equipped with Riemannian metrics of 2-positive Ricci curvature. When j=1 we
extend the above to certain classes of simply-connected non-spin 5-manifolds.
The condition of 2-positive Ricci curvature is defined to mean that the sum of
the two smallest eigenvalues of the Ricci tensor is positive at every point.
This result is a counterpart to a previous result of the authors concerning the
existence of positive Ricci curvature on highly connected manifolds in
dimensions 4j-1 for j > 1, and in dimensions 4j+1 for j > 0 with torsion-free
cohomology.
",mathematics
"  The field enhancement factor at the emitter tip and its variation in a close
neighbourhood determines the emitter current in a Fowler-Nordheim like
formulation. For an axially symmetric emitter with a smooth tip, it is shown
that the variation can be accounted by a $\cos{\tilde{\theta}}$ factor in
appropriately defined normalized co-ordinates. This is shown analytically for a
hemi-ellipsoidal emitter and confirmed numerically for other emitter shapes
with locally quadratic tips.
",physics
"  We introduce PULSEDYN, a particle dynamics program in $C++$, to solve
many-body nonlinear systems in one dimension. PULSEDYN is designed to make
computing accessible to non-specialists in the field of nonlinear dynamics of
many-body systems and to ensure transparency and easy benchmarking of numerical
results for an integrable model (Toda chain) and three non-integrable models
(Fermi-Pasta-Ulam-Tsingou, Morse and Lennard-Jones). To achieve the latter, we
have made our code open source and free to distribute. We examine (i) soliton
propagation and two-soliton collision in the Toda system, (ii) the recurrence
phenomenon in the Fermi-Pasta-Ulam-Tsingou system and the decay of a single
localized nonlinear excitation in the same system through quasi-equilibrium to
an equipartitioned state, and SW propagation in chains with (iii) Morse and
(iv) Lennard-Jones potentials. We recover well known results from theory and
other numerical results in the literature. We have obtained these results by
setting up a parameter file interface which allows the code to be used as a
black box. Therefore, we anticipate that the code would prove useful to
students and non-specialists. At the same time, PULSEDYN provides
scientifically accurate simulations thus making the study of rich dynamical
processes broadly accessible.
",physics
"  This study concentrates on advancing mathematical and computational
methodology for radar tomography imaging in which the unknown volumetric
velocity distribution of a wave within a bounded domain is to be reconstructed.
Our goal is to enable effective simulation and inversion of a large amount of
full-wave data within a realistic 2D or 3D geometry. For propagating and
inverting the wave, we present a rigorous multigrid-based forward approach
which utilizes the finite-difference time-domain method and a nested finite
element grid structure. Based on the multigrid approach, we introduce and
validate a multiresolution algorithm which allows regularization of the unknown
distribution through a coarse-to-fine inversion scheme. In this approach,
sparse signals can be effectively inverted, as the coarse fluctuations are
reconstructed before the finer ones. Furthermore, the number of nonzero entries
in the system matrix can be compressed and thus the inversion procedure can be
speeded up. As a test scenario we investigate satellite-based asteroid interior
reconstruction. We use both full-wave and projected wave data and estimate the
accuracy of the inversion under different error sources: noise and positioning
inaccuracies. The results suggest that the present full-wave inversion approach
allows recovering the interior with a single satellite recording backscattering
data. It seems that robust results can be achieved, when the peak-to-peak
signal-to-noise ratio is above 10 dB. Furthermore, it seems that reconstructing
the deep interior can be enhanced if two satellites can be utilized in the
measurements.
",physics
"  Let k be an infinite perfect field. We provide a general criterion for a
spectrum in the stable homotopy category over k to be effective, i.e. to be in
the localizing subcategory generated by the suspension spectra of smooth
schemes. As a consequence, we show that two recent versions of generalized
motivic cohomology theories coincide.
",mathematics
"  We report on the detailed analysis of a gravitationally-lensed Y-band
dropout, A2744_YD4, selected from deep Hubble Space Telescope imaging in the
Frontier Field cluster Abell 2744. Band 7 observations with the Atacama Large
Millimeter Array (ALMA) indicate the proximate detection of a significant 1mm
continuum flux suggesting the presence of dust for a star-forming galaxy with a
photometric redshift of $z\simeq8$. Deep X-SHOOTER spectra confirms the high
redshift identity of A2744_YD4 via the detection of Lyman $\alpha$ emission at
a redshift $z$=8.38. The association with the ALMA detection is confirmed by
the presence of [OIII] 88$\mu$m emission at the same redshift. Although both
emission features are only significant at the 4 $\sigma$ level, we argue their
joint detection and the positional coincidence with a high redshift dropout in
the HST images confirms the physical association. Analysis of the available
photometric data and the modest gravitational magnification ($\mu\simeq2$)
indicates A2744_YD4 has a stellar mass of $\sim$ 2$\times$10$^9$ M$_{\odot}$, a
star formation rate of $\sim20$ M$_{\odot}$/yr and a dust mass of
$\sim$6$\times$10$^{6}$ M$_{\odot}$. We discuss the implications of the
formation of such a dust mass only $\simeq$200 Myr after the onset of cosmic
reionisation.
",physics
"  Automatic question-answering is a classical problem in natural language
processing, which aims at designing systems that can automatically answer a
question, in the same way as human does. In this work, we propose a deep
learning based model for automatic question-answering. First the questions and
answers are embedded using neural probabilistic modeling. Then a deep
similarity neural network is trained to find the similarity score of a pair of
answer and question. Then for each question, the best answer is found as the
one with the highest similarity score. We first train this model on a
large-scale public question-answering database, and then fine-tune it to
transfer to the customer-care chat data. We have also tested our framework on a
public question-answering database and achieved very good performance.
",computer-science
"  Research on automated vehicles has experienced an explosive growth over the
past decade. A main obstacle to their practical realization, however, is a
convincing safety concept. This question becomes ever more important as more
sophisticated algorithms are used and the vehicle automation level increases.
The field of functional safety offers a systematic approach to identify
possible sources of risk and to improve the safety of a vehicle. It is based on
practical experience across the aerospace, process and other industries over
multiple decades. This experience is compiled in the functional safety standard
for the automotive domain, ISO 26262, which is widely adopted throughout the
automotive industry. However, its applicability and relevance for highly
automated vehicles is subject to a controversial debate. This paper takes a
critical look at the discussion and summarizes the main steps of ISO 26262 for
a safe control design for automated vehicles.
",computer-science
"  Three properties of the dielectric relaxation in ultra-pure single
crystalline H$_{2}$O ice Ih were probed at temperatures between 80-250 K; the
thermally stimulated depolarization current, static electrical conductivity,
and dielectric relaxation time. The measurements were made with a guarded
parallel-plate capacitor constructed of fused quartz with Au electrodes. The
data agree with relaxation-based models and provide for the determination of
activation energies, which suggest that relaxation in ice is dominated by
Bjerrum defects below 140 K. Furthermore, anisotropy in the dielectric
relaxation data reveals that molecular reorientations along the
crystallographic $c$-axis are energetically favored over those along the
$a$-axis between 80-140 K. These results lend support for the postulate of a
shared origin between the dielectric relaxation dynamics and the thermodynamic
partial proton-ordering in ice near 100 K, and suggest a preference for
ordering along the $c$-axis.
",physics
"  The latent feature relational model (LFRM) is a generative model for
graph-structured data to learn a binary vector representation for each node in
the graph. The binary vector denotes the node's membership in one or more
communities. At its core, the LFRM miller2009nonparametric is an overlapping
stochastic blockmodel, which defines the link probability between any pair of
nodes as a bilinear function of their community membership vectors. Moreover,
using a nonparametric Bayesian prior (Indian Buffet Process) enables learning
the number of communities automatically from the data. However, despite its
appealing properties, inference in LFRM remains a challenge and is typically
done via MCMC methods. This can be slow and may take a long time to converge.
In this work, we develop a small-variance asymptotics based framework for the
non-parametric Bayesian LFRM. This leads to an objective function that retains
the nonparametric Bayesian flavor of LFRM, while enabling us to design
deterministic inference algorithms for this model, that are easy to implement
(using generic or specialized optimization routines) and are fast in practice.
Our results on several benchmark datasets demonstrate that our algorithm is
competitive to methods such as MCMC, while being much faster.
",statistics
"  The origin of colossal magnetoresistance (CMR) is still controversial. The
spin dynamics of La$_{1-x}$Sr$_x$MnO$_3$ is revisited along the Mn-O-Mn
direction at $x\leq 0.5$, $T\leq T_C$ with a new study at $x$=0.4. A new
lattice dynamics study is also reported at $x_0$=0.2,representative of the
optimal doping for CMR. In large-$q$ wavevector range, typical of the scale of
polarons, spin dynamics exhibits a discrete spectrum, $E^n_{\rm mag}$ with $n$
equal to the degeneracy of orbital-pseudospin transitions and energy values in
coincidence with the phonon ones. It corresponds to the spin-orbital excitation
spectrum of short life-time polarons, in which the orbital pseudospin
degeneracy is lift by phonons. For $x\neq x_0$, its q-range reveals a $\ell
\approx 1.7a$ size of polarons with a dimension $2d$ at $x=1/8$ partly
increasing to $\approx$ $3d$ at $x=0.3$. At $x_0=0.2$ ($T<T_C$) two distinct
$q$ and energy ranges appear separated by $\Delta E(q_0\approx 0.35)=3meV$. The
same $\Delta E(q_0)$ value separates two unusual transverse acoustic branches
($T>T_C$). Both characterize a nematic-phase defined by chains of ""orbital
polarons"" of $2a$ size, distant from $3a$, typical of $x_0=1/6$. It could
explain CMR.
",physics
"  Electronic charge carriers in ionic materials can self-trap to form large
polarons. Interference between the ionic displacements associated with
oppositely charged large polarons increases as they approach one another.
Initially this interference produces an attractive potential that fosters their
merger. However, for small enough separations this interference generates a
repulsive interaction between oppositely charged large polarons. In suitable
circumstances this repulsion can overwhelm their direct Coulomb attraction.
Then the resulting net repulsion between oppositely charged large polarons
constitutes a potential barrier which impedes their recombination.
",physics
"  A simulation study of energy resolution, position resolution, and
$\pi^0$-$\gamma$ separation using multivariate methods of a sampling
calorimeter is presented. As a realistic example, the geometry of the
calorimeter is taken from the design geometry of the Shashlik calorimeter which
was considered as a candidate for CMS endcap for the phase II of LHC running.
The methods proposed in this paper can be easily adapted to various geometrical
layouts of a sampling calorimeter. Energy resolution is studied for different
layouts and different absorber-scintillator combinations of the Shashlik
detector. It is shown that a boosted decision tree using fine grained
information of the calorimeter can perform three times better than a cut-based
method for separation of $\pi^0$ from $\gamma$ over a large energy range of 20
GeV-200 GeV.
",physics
"  In this paper we propose a novel methodology for static analysis of binary
code using abstract interpretation. We use an abstract domain based on
polyhedra and two mapping functions that associate polyhedra variables with
registers and memory. We demonstrate our methodology to the problem of
computing upper bounds to loop iterations in the code. This problem is
particularly important in the domain of Worst-Case Execution Time (WCET)
analysis of safety-critical real-time code. However, our approach is general
and it can applied to other static analysis problems.
",computer-science
"  In this work, answer-set programs that specify repairs of databases are used
as a basis for solving computational and reasoning problems about causes for
query answers from databases.
",computer-science
"  Data are often labeled by many different experts with each expert only
labeling a small fraction of the data and each data point being labeled by
several experts. This reduces the workload on individual experts and also gives
a better estimate of the unobserved ground truth. When experts disagree, the
standard approaches are to treat the majority opinion as the correct label or
to model the correct label as a distribution. These approaches, however, do not
make any use of potentially valuable information about which expert produced
which label. To make use of this extra information, we propose modeling the
experts individually and then learning averaging weights for combining them,
possibly in sample-specific ways. This allows us to give more weight to more
reliable experts and take advantage of the unique strengths of individual
experts at classifying certain types of data. Here we show that our approach
leads to improvements in computer-aided diagnosis of diabetic retinopathy. We
also show that our method performs better than competing algorithms by Welinder
and Perona (2010), and by Mnih and Hinton (2012). Our work offers an innovative
approach for dealing with the myriad real-world settings that use expert
opinions to define labels for training.
",computer-science
"  Identification of differentially expressed genes (DE-genes) is commonly
conducted in modern biomedical researches. However, unwanted variation
inevitably arises during the data collection process, which could make the
detection results heavily biased. It is suggested to remove the unwanted
variation while keeping the biological variation to ensure a reliable analysis
result. Removing Unwanted Variation (RUV) is recently proposed for this purpose
by the virtue of negative control genes. On the other hand, outliers are
frequently appear in modern high-throughput genetic data that can heavily
affect the performances of RUV and its downstream analysis. In this work, we
propose a robust RUV-testing procedure via gamma-divergence. The advantages of
our method are twofold: (1) it does not involve any modeling for the outlier
distribution, which is applicable to various situations, (2) it is easy to
implement in the sense that its robustness is controlled by a single tuning
parameter gamma of gamma-divergence, and a data-driven criterion is developed
to select $\gamma$. In the Gender Study, our method can successfully remove
unwanted variation, and is able to identify more DE-genes than conventional
methods.
",statistics
"  Quantum phase transitions are ubiquitous in many exotic behaviors of
strongly-correlated materials. However the microscopic complexity impedes their
quantitative understanding. Here, we observe thoroughly and comprehend the rich
strongly-correlated physics in two profoundly dissimilar regimes of quantum
criticality. With a circuit implementing a quantum simulator for the
three-channel Kondo model, we reveal the universal scalings toward different
low-temperature fixed points and along the multiple crossovers from quantum
criticality. Notably, an unanticipated violation of the maximum conductance for
ballistic free electrons is uncovered. The present charge pseudospin
implementation of a Kondo impurity opens access to a broad variety of
strongly-correlated phenomena.
",physics
"  We investigate multiparticle excitation effect on a collective density
excitation as well as a single-particle excitation in a weakly interacting
Bose--Einstein condensate (BEC). We find that although the weakly interacting
BEC offers weak multiparticle excitation spectrum at low temperatures, this
multiparticle excitation effect may not remain hidden, but emerges as
bimodality in the density response function through the single-particle
excitation. Identification of spectra in the BEC between the single-particle
excitation and the density excitation is also assessed at nonzero temperatures,
which has been known to be unique nature in the BEC at absolute zero
temperature.
",physics
"  Reynold's parametricity theory captures the property that parametrically
polymorphic functions behave uniformly: they produce related results on related
instantiations. In dependently-typed programming languages, such relations and
uniformity proofs can be expressed internally, and generated as a program
translation.
We present a new parametricity translation for a significant fragment of Coq.
Previous translations of parametrically polymorphic propositions allowed
non-uniformity. For example, on related instantiations, a function may return
propositions that are logically inequivalent (e.g. True and False). We show
that uniformity of polymorphic propositions is not achievable in general.
Nevertheless, our translation produces proofs that the two propositions are
logically equivalent and also that any two proofs of those propositions are
related. This is achieved at the cost of potentially requiring more assumptions
on the instantiations, requiring them to be isomorphic in the worst case.
Our translation augments the previous one for Coq by carrying and
compositionally building extra proofs about parametricity relations. It is made
easier by a new method for translating inductive types and pattern matching.
The new method builds upon and generalizes previous such translations for
dependently-typed programming languages.
Using reification and reflection, we have implemented our translation as Coq
programs. We obtain several stronger free theorems applicable to an ongoing
compiler-correctness project. Previously, proofs of some of these theorems took
several hours to finish.
",computer-science
"  We propose a class of Particle-In-Cell (PIC) methods for the Vlasov-Poisson
system with a strong and inhomogeneous external magnetic field with fixed
direction, where we focus on the motion of particles in the plane orthogonal to
the magnetic field (so-called poloidal directions). In this regime, the time
step can be subject to stability constraints related to the smallness of Larmor
radius and plasma frequency. To avoid this limitation, our approach is based on
first and higher-order semi-implicit numerical schemes already validated on
dissipative systems [3] and for homogeneous magnetic fields [10]. Thus, when
the magnitude of the external magnetic field becomes large, this method
provides a consistent PIC discretization of the guiding-center system taking
into account variations of the magnetic field. We carry out some theoretical
proofs and perform several numerical experiments that establish a solid
validation of the method and its underlying concepts.
",mathematics
"  A highly-efficient multi-resonant RF energy-harvesting rectenna based on a
metamaterial perfect absorber featuring closely-spaced polarization-independent
absorption modes is presented. Its effective area is larger than its physical
area, and so efficiencies of 230% and 130% are measured at power densities of
10 uW/cm2 and 1 uW/cm2 respectively, for a linear absorption mode at 0.75 GHz.
The rectenna exhibits a broad polarization-independent region between 1.4 GHz
and 1.7 GHz with maximum efficiencies of 167% and 36% for those same power
densities. Additionally, by adjustment of the distance between the rectenna and
a reflecting ground plane, the absorption frequency can be adjusted to a
limited extent within the polarization-independent region. Lastly, the rectenna
should be capable of delivering 100 uW of power to a device located within 50 m
of a cell-phone tower under ideal conditions.
",physics
"  Development of high strength carbon fibers (CFs) requires an understanding of
the relationship between the processing conditions, microstructure and
resulting properties. We developed a molecular model that combines kinetic
Monte Carlo (KMC) and molecular dynamics (MD) techniques to predict the
microstructure evolution during the carbonization process of carbon fiber
manufacturing. The model accurately predicts the cross-sectional microstructure
of carbon fibers, predicting features such as graphitic sheets and hairpin
structures that have been observed experimentally. We predict the transverse
modulus of the resulting fibers and find that the modulus is slightly lower
than experimental values, but is up to an order of magnitude lower than ideal
graphite. We attribute this to the perfect longitudinal texture of our
simulated structures, as well as the chain sliding mechanism that governs the
deformation of the fibers, rather than the van der Waals interaction that
governs the modulus for graphite. We also observe that high reaction rates
result in porous structures that have lower moduli.
",physics
"  Unidirectional control of optically induced spin waves in a rare-earth iron
garnet crystal is demonstrated. We observed the interference of two spin-wave
packets with different initial phases generated by circularly polarized light
pulses. This interference results in unidirectional propagation if the
spin-wave sources are spaced apart at 1/4 of the wavelength of the spin waves
and the initial phase difference is set to pi/2. The propagating direction of
the spin wave is switched by the polarization helicity of the light pulses.
Moreover, in a numerical simulation, applying more than two spin-wave sources
with a suitable polarization and spot shape, arbitrary manipulation of the spin
wave by the phased array method was replicated.
",physics
"  A nice differential-geometric framework for (non-abelian) higher gauge theory
is provided by principal 2-bundles, i.e. categorified principal bundles. Their
total spaces are Lie groupoids, local trivializations are kinds of Morita
equivalences, and connections are Lie-2-algebra-valued 1-forms. In this
article, we construct explicitly the parallel transport of a connection on a
principal 2-bundle. Parallel transport along a path is a Morita equivalence
between the fibres over the end points, and parallel transport along a surface
is an intertwiner between Morita equivalences. We prove that our constructions
fit into the general axiomatic framework for categorified parallel transport
and surface holonomy.
",mathematics
"  Big, fine-grained enterprise registration data that includes time and
location information enables us to quantitatively analyze, visualize, and
understand the patterns of industries at multiple scales across time and space.
However, data quality issues like incompleteness and ambiguity, hinder such
analysis and application. These issues become more challenging when the volume
of data is immense and constantly growing. High Performance Computing (HPC)
frameworks can tackle big data computational issues, but few studies have
systematically investigated imputation methods for enterprise registration data
in this type of computing environment. In this paper, we propose a big data
imputation workflow based on Apache Spark as well as a bare-metal computing
cluster, to impute enterprise registration data. We integrated external data
sources, employed Natural Language Processing (NLP), and compared several
machine-learning methods to address incompleteness and ambiguity problems found
in enterprise registration data. Experimental results illustrate the
feasibility, efficiency, and scalability of the proposed HPC-based imputation
framework, which also provides a reference for other big georeferenced text
data processing. Using these imputation results, we visualize and briefly
discuss the spatiotemporal distribution of industries in China, demonstrating
the potential applications of such data when quality issues are resolved.
",computer-science
"  The widespread use of smartphones gives rise to new security and privacy
concerns. Smartphone thefts account for the largest percentage of thefts in
recent crime statistics. Using a victim's smartphone, the attacker can launch
impersonation attacks, which threaten the security of the victim and other
users in the network. Our threat model includes the attacker taking over the
phone after the user has logged on with his password or pin. Our goal is to
design a mechanism for smartphones to better authenticate the current user,
continuously and implicitly, and raise alerts when necessary. In this paper, we
propose a multi-sensors-based system to achieve continuous and implicit
authentication for smartphone users. The system continuously learns the owner's
behavior patterns and environment characteristics, and then authenticates the
current user without interrupting user-smartphone interactions. Our method can
adaptively update a user's model considering the temporal change of user's
patterns. Experimental results show that our method is efficient, requiring
less than 10 seconds to train the model and 20 seconds to detect the abnormal
user, while achieving high accuracy (more than 90%). Also the combination of
more sensors provide better accuracy. Furthermore, our method enables adjusting
the security level by changing the sampling rate.
",computer-science
"  The Jordan decomposition theorem states that every function $f \colon [0,1]
\to \mathbb{R}$ of bounded variation can be written as the difference of two
non-decreasing functions. Combining this fact with a result of Lebesgue, every
function of bounded variation is differentiable almost everywhere in the sense
of Lebesgue measure. We analyze the strength of these theorems in the setting
of reverse mathematics. Over $\mathsf{RCA}_0$, a stronger version of Jordan's
result where all functions are continuous is equivalent to $\mathsf{ACA}_0$,
while the version stated is equivalent to $\mathsf{WKL}_0$. The result that
every function on $[0,1]$ of bounded variation is almost everywhere
differentiable is equivalent to $\mathsf{WWKL}_0$. To state this equivalence in
a meaningful way, we develop a theory of Martin-Löf randomness over
$\mathsf{RCA}_0$.
",mathematics
"  We present the methodology for and detail the implementation of the Dark
Energy Survey (DES) 3x2pt DES Year 1 (Y1) analysis, which combines
configuration-space two-point statistics from three different cosmological
probes: cosmic shear, galaxy-galaxy lensing, and galaxy clustering, using data
from the first year of DES observations. We have developed two independent
modeling pipelines and describe the code validation process. We derive
expressions for analytical real-space multi-probe covariances, and describe
their validation with numerical simulations. We stress-test the inference
pipelines in simulated likelihood analyses that vary 6-7 cosmology parameters
plus 20 nuisance parameters and precisely resemble the analysis to be presented
in the DES 3x2pt analysis paper, using a variety of simulated input data
vectors with varying assumptions.
We find that any disagreement between pipelines leads to changes in assigned
likelihood $\Delta \chi^2 \le 0.045$ with respect to the statistical error of
the DES Y1 data vector. We also find that angular binning and survey mask do
not impact our analytic covariance at a significant level. We determine lower
bounds on scales used for analysis of galaxy clustering (8 Mpc$~h^{-1}$) and
galaxy-galaxy lensing (12 Mpc$~h^{-1}$) such that the impact of modeling
uncertainties in the non-linear regime is well below statistical errors, and
show that our analysis choices are robust against a variety of systematics.
These tests demonstrate that we have a robust analysis pipeline that yields
unbiased cosmological parameter inferences for the flagship 3x2pt DES Y1
analysis. We emphasize that the level of independent code development and
subsequent code comparison as demonstrated in this paper is necessary to
produce credible constraints from increasingly complex multi-probe analyses of
current data.
",physics
"  The case of the classical Hill problem is numerically investigated by
performing a thorough and systematic classification of the initial conditions
of the orbits. More precisely, the initial conditions of the orbits are
classified into four categories: (i) non-escaping regular orbits; (ii) trapped
chaotic orbits; (iii) escaping orbits; and (iv) collision orbits. In order to
obtain a more general and complete view of the orbital structure of the
dynamical system our exploration takes place in both planar (2D) and the
spatial (3D) version of the Hill problem. For the 2D system we numerically
integrate large sets of initial conditions in several types of planes, while
for the system with three degrees of freedom, three-dimensional distributions
of initial conditions of orbits are examined. For distinguishing between
ordered and chaotic bounded motion the Smaller ALingment Index (SALI) method is
used. We managed to locate the several bounded basins, as well as the basins of
escape and collision and also to relate them with the corresponding escape and
collision time of the orbits. Our numerical calculations indicate that the
overall orbital dynamics of the Hamiltonian system is a complicated but highly
interested problem. We hope our contribution to be useful for a further
understanding of the orbital properties of the classical Hill problem.
",physics
"  Although timely sepsis diagnosis and prompt interventions in Intensive Care
Unit (ICU) patients are associated with reduced mortality, early clinical
recognition is frequently impeded by non-specific signs of infection and
failure to detect signs of sepsis-induced organ dysfunction in a constellation
of dynamically changing physiological data. The goal of this work is to
identify patient at risk of life-threatening sepsis utilizing a data-centered
and machine learning-driven approach. We derive a mortality risk predictive
dynamic Bayesian network (DBN) guided by a customized sepsis knowledgebase and
compare the predictive accuracy of the derived DBN with the Sepsis-related
Organ Failure Assessment (SOFA) score, the Quick SOFA (qSOFA) score, the
Simplified Acute Physiological Score (SAPS-II) and the Modified Early Warning
Score (MEWS) tools.
A customized sepsis ontology was used to derive the DBN node structure and
semantically characterize temporal features derived from both structured
physiological data and unstructured clinical notes. We assessed the performance
in predicting mortality risk of the DBN predictive model and compared
performance to other models using Receiver Operating Characteristic (ROC)
curves, area under curve (AUROC), calibration curves, and risk distributions.
The derived dataset consists of 24,506 ICU stays from 19,623 patients with
evidence of suspected infection, with 2,829 patients deceased at discharge. The
DBN AUROC was found to be 0.91, which outperformed the SOFA (0.843), qSOFA
(0.66), MEWS (0.73), and SAPS-II (0.77) scoring tools. Continuous Net
Reclassification Index and Integrated Discrimination Improvement analysis
supported the superiority DBN. Compared with conventional rule-based risk
scoring tools, the sepsis knowledgebase-driven DBN algorithm offers improved
performance for predicting mortality of infected patients in ICUs.
",statistics
"  In this paper, we derive the pointwise upper bounds and lower bounds on the
gradients of solutions to the Lamé systems with partially infinite
coefficients as the surface of discontinuity of the coefficients of the system
is located very close to the boundary. When the distance tends to zero, the
optimal blow-up rates of the gradients are established for inclusions with
arbitrary shapes and in all dimensions.
",mathematics
"  We consider co-rotational wave maps from the $(1+d)$-dimensional Minkowski
space into the $d$-sphere for $d\geq 3$ odd. This is an energy-supercritical
model which is known to exhibit finite-time blowup via self-similar solutions.
Based on a method developed by the second author and Schörkhuber, we prove
the asymptotic nonlinear stability of the ""ground-state"" self-similar solution.
",mathematics
"  We extend our previous results characterizing the loading properties of a
diffusing passive scalar advected by a laminar shear flow in ducts and channels
to more general cross-sectional shapes, including regular polygons and smoothed
corner ducts originating from deformations of ellipses. For the case of the
triangle, short time skewness is calculated exactly to be positive, while
long-time asymptotics shows it to be negative. Monte-Carlo simulations confirm
these predictions, and document the time scale for sign change. Interestingly,
the equilateral triangle is the only regular polygon with this property, all
other polygons possess positive skewness at all times, although this cannot
cannot be proved on finite times due to the lack of closed form flow solutions
for such geometries. Alternatively, closed form flow solutions can be
constructed for smooth deformations of ellipses, and illustrate how the
possibility of multiple sign switching in time is unrelated to domain corners.
Exact conditions relating the median and the skewness to the mean are developed
which guarantee when the sign for the skewness implies front (back) loading
properties of the evolving tracer distribution along the pipe. Short and long
time asymptotics confirm this condition, and Monte-Carlo simulations verify
this at all times.
",physics
"  Security threats such as jamming and route manipulation can have significant
consequences on the performance of modern wireless networks. To increase the
efficacy and stealthiness of such threats, a number of extremely challenging,
cross-layer attacks have been recently unveiled. Although existing research has
thoroughly addressed many single-layer attacks, the problem of detecting and
mitigating cross-layer attacks still remains unsolved. For this reason, in this
paper we propose a novel framework to analyze and address cross-layer attacks
in wireless networks. Specifically, our framework consists of a detection and a
mitigation component. The attack detection component is based on a Bayesian
learning detection scheme that constructs a model of observed evidence to
identify stealthy attack activities. The mitigation component comprises a
scheme that achieves the desired trade-off between security and performance. We
specialize and evaluate the proposed framework by considering a specific
cross-layer attack that uses jamming as an auxiliary tool to achieve route
manipulation. Simulations and experimental results obtained with a test-bed
made up by USRP software-defined radios demonstrate the effectiveness of the
proposed methodology.
",computer-science
"  Using the Panama Papers, we show that the beginning of media reporting on
expropriations and property confiscations in a country increases the
probability that offshore entities are incorporated by agents from the same
country in the same month. This result is robust to the use of country-year
fixed effects and the exclusion of tax havens. Further analysis shows that the
effect is driven by countries with non-corrupt and effective governments, which
supports the notion that offshore entities are incorporated when reasonably
well-intended and well-functioning governments become more serious about
fighting organized crime by confiscating proceeds of crime.
",quantitative-finance
"  Domain shift refers to the well known problem that a model trained in one
source domain performs poorly when applied to a target domain with different
statistics. {Domain Generalization} (DG) techniques attempt to alleviate this
issue by producing models which by design generalize well to novel testing
domains. We propose a novel {meta-learning} method for domain generalization.
Rather than designing a specific model that is robust to domain shift as in
most previous DG work, we propose a model agnostic training procedure for DG.
Our algorithm simulates train/test domain shift during training by synthesizing
virtual testing domains within each mini-batch. The meta-optimization objective
requires that steps to improve training domain performance should also improve
testing domain performance. This meta-learning procedure trains models with
good generalization ability to novel domains. We evaluate our method and
achieve state of the art results on a recent cross-domain image classification
benchmark, as well demonstrating its potential on two classic reinforcement
learning tasks.
",computer-science
"  Let $M$ be a Liouville 6-manifold which is the smooth fiber of a Lefschetz
fibration on $\mathbb{C}^4$ constructed by suspending a Lefschetz fibration on
$\mathbb{C}^3$. We prove that for many examples including stabilizations of
Milnor fibers of hypersurface cusp singularities, the compact Fukaya category
$\mathcal{F}(M)$ and the wrapped Fukaya category $\mathcal{W}(M)$ are related
through $A_\infty$-Koszul duality, by identifying them with cyclic and
Calabi-Yau completions of the same quiver algebra. This implies the
split-generation of the compact Fukaya category $\mathcal{F}(M)$ by vanishing
cycles. Moreover, new examples of Liouville manifolds which admit
quasi-dilations in the sense of Seidel-Solomon are obtained.
",mathematics
"  In this work, we extend the solid harmonics derivation, which was used by
Ackroyd et al to derive the steady-state SP$_N$ equations, to transient
problems. The derivation expands the angular flux in ordinary surface harmonics
but uses harmonic polynomials to generate additional surface spherical harmonic
terms to be used in Galerkin projection. The derivation shows the equivalence
between the SP$_N$ and the P$_N$ approximation. Also, we use the line source
problem and McClarren's ""box"" problem to demonstrate such equivalence
numerically. Both problems were initially proposed for isotropic scattering,
but here we add higher-order scattering moments to them. Results show that the
difference between the SP$_N$ and P$_N$ scalar flux solution is at the roundoff
level.
",physics
"  We study the optimal pricing strategy of a monopolist selling homogeneous
goods to customers over multiple periods. The customers choose their time of
purchase to maximize their payoff that depends on their valuation of the
product, the purchase price, and the utility they derive from past purchases of
others, termed the network effect. We first show that the optimal price
sequence is non-decreasing. Therefore, by postponing purchase to future rounds,
customers trade-off a higher utility from the network effects with a higher
price. We then show that a customer's equilibrium strategy can be characterized
by a threshold rule in which at each round a customer purchases the product if
and only if her valuation exceeds a certain threshold. This implies that
customers face an inference problem regarding the valuations of others, i.e.,
observing that a customer has not yet purchased the product, signals that her
valuation is below a threshold. We consider a block model of network
interactions, where there are blocks of buyers subject to the same network
effect. A natural benchmark, this model allows us to provide an explicit
characterization of the optimal price sequence asymptotically as the number of
agents goes to infinity, which notably is linearly increasing in time with a
slope that depends on the network effect through a scalar given by the sum of
entries of the inverse of the network weight matrix. Our characterization shows
that increasing the ""imbalance"" in the network defined as the difference
between the in and out degree of the nodes increases the revenue of the
monopolist. We further study the effects of price discrimination and show that
in earlier periods monopolist offers lower prices to blocks with higher
Bonacich centrality to encourage them to purchase, which in turn further
incentivizes other customers to buy in subsequent periods.
",computer-science
"  We consider an elastic composite material containing particulate inclusions
in a soft elastic matrix that is bounded by a rigid wall, e.g., the substrate.
If such a composite serves as a soft actuator, forces are imposed on or induced
between the embedded particles. We investigate how the presence of the rigid
wall affects the interactions between the inclusions in the elastic matrix. For
no-slip boundary conditions, we transfer Blake's derivation of a corresponding
Green's function from low-Reynolds-number hydrodynamics to the linearly elastic
case. Results for no-slip and free-slip surface conditions are compared to each
other and to the bulk behavior. Our results suggest that walls with free-slip
surface conditions are preferred when they serve as substrates for soft
actuators made from elastic composite materials. As we further demonstrate, the
presence of a rigid wall can qualitatively change the interactions between the
inclusions. In effect, it can switch attractive interactions into repulsive
ones (and vice versa). It should be straightforward to observe the effects in
future experiments and to combine our results, e.g., with the modeling of
biological cells and tissue on rigid surfaces.
",physics
"  Let $M$ be an atomic monoid and let $x$ be a non-unit element of $M$. The
elasticity of $x$, denoted by $\rho(x)$, is the ratio of its largest
factorization length to its shortest factorization length, and it measures how
far is $x$ from having a unique factorization. The elasticity $\rho(M)$ of $M$
is the supremum of the elasticities of all non-unit elements of $M$. The monoid
$M$ has accepted elasticity if $\rho(M) = \rho(m)$ for some $m \in M$. In this
paper, we study the elasticity of Puiseux monoids (i.e., additive submonoids of
$\mathbb{Q}_{\ge 0}$). First, we characterize the Puiseux monoids $M$ having
finite elasticity and find a formula for $\rho(M)$. Then we classify the
Puiseux monoids having accepted elasticity in terms of their sets of atoms.
When $M$ is a primary Puiseux monoid, we describe the topology of the set of
elasticities of $M$, including a characterization of when $M$ is a bounded
factorization monoid. Lastly, we give an example of a Puiseux monoid that is
bifurcus (that is, every nonzero element has a factorization of length at most
$2$).
",mathematics
"  The use of color in QR codes brings extra data capacity, but also inflicts
tremendous challenges on the decoding process due to chromatic distortion,
cross-channel color interference and illumination variation. Particularly, we
further discover a new type of chromatic distortion in high-density color QR
codes, cross-module color interference, caused by the high density which also
makes the geometric distortion correction more challenging. To address these
problems, we propose two approaches, namely, LSVM-CMI and QDA-CMI, which
jointly model these different types of chromatic distortion. Extended from SVM
and QDA, respectively, both LSVM-CMI and QDA-CMI optimize over a particular
objective function to learn a color classifier. Furthermore, a robust geometric
transformation method and several pipeline refinements are proposed to boost
the decoding performance for mobile applications. We put forth and implement a
framework for high-capacity color QR codes equipped with our methods, called
HiQ. To evaluate the performance of HiQ, we collect a challenging large-scale
color QR code dataset, CUHK-CQRC, which consists of 5390 high-density color QR
code samples. The comparison with the baseline method [2] on CUHK-CQRC shows
that HiQ at least outperforms [2] by 188% in decoding success rate and 60% in
bit error rate. Our implementation of HiQ in iOS and Android also demonstrates
the effectiveness of our framework in real-world applications.
",computer-science
"  In this paper, we propose a framework for cross-layer optimization to ensure
ultra-high reliability and ultra-low latency in radio access networks, where
both transmission delay and queueing delay are considered. With short
transmission time, the blocklength of channel codes is finite, and the Shannon
Capacity cannot be used to characterize the maximal achievable rate with given
transmission error probability. With randomly arrived packets, some packets may
violate the queueing delay. Moreover, since the queueing delay is shorter than
the channel coherence time in typical scenarios, the required transmit power to
guarantee the queueing delay and transmission error probability will become
unbounded even with spatial diversity. To ensure the required
quality-of-service (QoS) with finite transmit power, a proactive packet
dropping mechanism is introduced. Then, the overall packet loss probability
includes transmission error probability, queueing delay violation probability,
and packet dropping probability. We optimize the packet dropping policy, power
allocation policy, and bandwidth allocation policy to minimize the transmit
power under the QoS constraint. The optimal solution is obtained, which depends
on both channel and queue state information. Simulation and numerical results
validate our analysis, and show that setting packet loss probabilities equal is
a near optimal solution.
",computer-science
"  We propose foundations for a synthetic theory of $(\infty,1)$-categories
within homotopy type theory. We axiomatize a directed interval type, then
define higher simplices from it and use them to probe the internal categorical
structures of arbitrary types. We define Segal types, in which binary
composites exist uniquely up to homotopy; this automatically ensures
composition is coherently associative and unital at all dimensions. We define
Rezk types, in which the categorical isomorphisms are additionally equivalent
to the type-theoretic identities - a ""local univalence"" condition. And we
define covariant fibrations, which are type families varying functorially over
a Segal type, and prove a ""dependent Yoneda lemma"" that can be viewed as a
directed form of the usual elimination rule for identity types. We conclude by
studying homotopically correct adjunctions between Segal types, and showing
that for a functor between Rezk types to have an adjoint is a mere proposition.
To make the bookkeeping in such proofs manageable, we use a three-layered
type theory with shapes, whose contexts are extended by polytopes within
directed cubes, which can be abstracted over using ""extension types"" that
generalize the path-types of cubical type theory. In an appendix, we describe
the motivating semantics in the Reedy model structure on bisimplicial sets, in
which our Segal and Rezk types correspond to Segal spaces and complete Segal
spaces.
",mathematics
"  Variational systems allow effective building of many custom variants by using
features (configuration options) to mark the variable functionality. In many of
the applications, their quality assurance and formal verification are of
paramount importance. Family-based model checking allows simultaneous
verification of all variants of a variational system in a single run by
exploiting the commonalities between the variants. Yet, its computational cost
still greatly depends on the number of variants (often huge).
In this work, we show how to achieve efficient family-based model checking of
CTL* temporal properties using variability abstractions and off-the-shelf
(single-system) tools. We use variability abstractions for deriving abstract
family-based model checking, where the variability model of a variational
system is replaced with an abstract (smaller) version of it, called modal
featured transition system, which preserves the satisfaction of both universal
and existential temporal properties, as expressible in CTL*. Modal featured
transition systems contain two kinds of transitions, termed may and must
transitions, which are defined by the conservative (over-approximating)
abstractions and their dual (under-approximating) abstractions, respectively.
The variability abstractions can be combined with different partitionings of
the set of variants to infer suitable divide-and-conquer verification plans for
the variational system. We illustrate the practicality of this approach for
several variational systems.
",computer-science
"  Background: As most of the software development organizations are
male-dominated, female developers encountering various negative workplace
experiences reported feeling like they ""do not belong"". Exposures to
discriminatory expletives or negative critiques from their male colleagues may
further exacerbate those feelings. Aims: The primary goal of this study is to
identify the differences in expressions of sentiments between male and female
developers during various software engineering tasks. Method: On this goal, we
mined the code review repositories of six popular open source projects. We used
a semi-automated approach leveraging the name as well as multiple social
networks to identify the gender of a developer. Using SentiSE, a customized and
state-of-the-art sentiment analysis tool for the software engineering domain,
we classify each communication as negative, positive, or neutral. We also
compute the frequencies of sentiment words, emoticons, and expletives used by
each developer. Results: Our results suggest that the likelihood of using
sentiment words, emoticons, and expletives during code reviews varies based on
the gender of a developer, as females are significantly less likely to express
sentiments than males. Although female developers were more neutral to their
male colleagues than to another female, male developers from three out of the
six projects were not only writing more frequent negative comments but also
withholding positive encouragements from their female counterparts. Conclusion:
Our results provide empirical evidence of another factor behind the negative
work place experiences encountered by the female developers that may be
contributing to the diminishing number of females in the SE industry.
",computer-science
"  For natural microswimmers, the interplay of swimming activity and external
flow can promote robust motion, e.g. propulsion against (""upstream rheotaxis"")
or perpendicular to the direction of flow. These effects are generally
attributed to their complex body shapes and flagellar beat patterns. Here,
using catalytic Janus particles as a model experimental system, we report on a
strong directional response that occurs for spherical active particles in a
channel flow. The particles align their propulsion axes to be nearly
perpendicular to both the direction of flow and the normal vector of a nearby
bounding surface. We develop a deterministic theoretical model of spherical
microswimmers near a planar wall that captures the experimental observations.
We show how the directional response emerges from the interplay of shear flow
and near-surface swimming activity. Finally, adding the effect of thermal
noise, we obtain probability distributions for the swimmer orientation that
semi-quantitatively agree with the experimental distributions.
",physics
"  Advances in deep generative networks have led to impressive results in recent
years. Nevertheless, such models can often waste their capacity on the minutiae
of datasets, presumably due to weak inductive biases in their decoders. This is
where graphics engines may come in handy since they abstract away low-level
details and represent images as high-level programs. Current methods that
combine deep learning and renderers are limited by hand-crafted likelihood or
distance functions, a need for large amounts of supervision, or difficulties in
scaling their inference algorithms to richer datasets. To mitigate these
issues, we present SPIRAL, an adversarially trained agent that generates a
program which is executed by a graphics engine to interpret and sample images.
The goal of this agent is to fool a discriminator network that distinguishes
between real and rendered data, trained with a distributed reinforcement
learning setup without any supervision. A surprising finding is that using the
discriminator's output as a reward signal is the key to allow the agent to make
meaningful progress at matching the desired output rendering. To the best of
our knowledge, this is the first demonstration of an end-to-end, unsupervised
and adversarial inverse graphics agent on challenging real world (MNIST,
Omniglot, CelebA) and synthetic 3D datasets.
",statistics
"  Let $G$ be a real linear semisimple algebraic group without compact factors
and $\Gamma$ a Zariski dense subgroup of $G$. In this paper, we use a
probabilistic counting in order to study the asymptotic properties of $\Gamma$
acting on the Furstenberg boundary of $G$. First, we show that the $K$
components of the elements of $\Gamma$ in the KAK decomposition of $G$ become
asymptotically independent. This result is an analog of a result of Gorodnik-Oh
in the context of the Archimedean counting. Then, we give a new proof of a
result of Guivarc'h concerning the positivity of the Hausdorff dimension of the
unique stationary probability measure on the Furstenberg Boundary of $G$.
Finally, we show how these results can be combined to give a probabilistic
proof of the Tit's alternative; namely that two independent random walks on
$\Gamma$ will eventually generate a free subgroup. This result answered a
question of Guivarc'h and was published earlier by the author. Since we're
working with the field of real numbers, we give here a more direct proof and a
more general statement.
",mathematics
"  Let $K$ be a field of characteristic zero, $\mathcal A$ a $K$-algebra and
$\delta$ a $K$-derivation of $\mathcal A$ or $K$-$\mathcal E$-derivation of
$\mathcal A$ (i.e., $\delta=\operatorname{Id}_A-\phi$ for some $K$-algebra
endomorphism $\phi$ of $\mathcal A$). Motivated by the Idempotent conjecture
proposed in [Z4], we first show that for every idempotent $e$ lying in both the
kernel ${\mathcal A}^\delta$ and the image $\operatorname{Im}\delta \!:=\delta
({\mathcal A})$ of $\delta$, the principal ideal $(e)\subseteq
\operatorname{Im} \delta$ if $\delta$ is a locally finite $K$-derivation or a
locally nilpotent $K$-$\mathcal E$-derivation of $\mathcal A$; and $e{\mathcal
A}, {\mathcal A}e \subseteq \operatorname{Im} \delta$ if $\delta$ is a locally
finite $K$-$\mathcal E$-derivation of $\mathcal A$. Consequently, the
Idempotent conjecture holds for all locally finite $K$-derivations and all
locally nilpotent $K$-$\mathcal E$-derivations of $\mathcal A$. We then show
that $1_{\mathcal A} \in \operatorname{Im} \delta$, (if and) only if $\delta$
is surjective, which generalizes the same result [GN, W] for locally nilpotent
$K$-derivations of commutative $K$-algebras to locally finite $K$-derivations
and $K$-$\mathcal E$-derivations $\delta$ of all $K$-algebras $\mathcal A$.
",mathematics
"  Most end devices are now equipped with multiple network interfaces.
Applications can exploit all available interfaces and benefit from multipath
transmission. Recently Multipath TCP (MPTCP) was proposed to implement
multipath transmission at the transport layer and has attracted lots of
attention from academia and industry. However, MPTCP only supports TCP-based
applications and its multipath routing flexibility is limited. In this paper,
we investigate the possibility of orchestrating multipath transmission from the
network layer of end devices, and develop a Multipath IP (MPIP) design
consisting of signaling, session and path management, multipath routing, and
NAT traversal. We implement MPIP in Linux and Android kernels. Through
controlled lab experiments and Internet experiments, we demonstrate that MPIP
can effectively achieve multipath gains at the network layer. It not only
supports the legacy TCP and UDP protocols, but also works seamlessly with
MPTCP. By facilitating user-defined customized routing, MPIP can route traffic
from competing applications in a coordinated fashion to maximize the aggregate
user Quality-of-Experience.
",computer-science
"  We construct a model of random groups of rank 7/4, and show that in this
model the random group has the exponential mesoscopic rank property.
",mathematics
"  We investigate the addition of symmetry and temporal context information to a
deep Convolutional Neural Network (CNN) with the purpose of detecting malignant
soft tissue lesions in mammography. We employ a simple linear mapping that
takes the location of a mass candidate and maps it to either the contra-lateral
or prior mammogram and Regions Of Interest (ROI) are extracted around each
location. We subsequently explore two different architectures (1) a fusion
model employing two datastreams were both ROIs are fed to the network during
training and testing and (2) a stage-wise approach where a single ROI CNN is
trained on the primary image and subsequently used as feature extractor for
both primary and symmetrical or prior ROIs. A 'shallow' Gradient Boosted Tree
(GBT) classifier is then trained on the concatenation of these features and
used to classify the joint representation. Results shown a significant increase
in performance using the first architecture and symmetry information, but only
marginal gains in performance using temporal data and the other setting. We
feel results are promising and can greatly be improved when more temporal data
becomes available.
",computer-science
"  The prediction of cancer prognosis and metastatic potential immediately after
the initial diagnoses is a major challenge in current clinical research. The
relevance of such a signature is clear, as it will free many patients from the
agony and toxic side-effects associated with the adjuvant chemotherapy
automatically and sometimes carelessly subscribed to them. Motivated by this
issue, Ein-Dor (2006) and Zuk (2007) presented a Bayesian model which leads to
the following conclusion: Thousands of samples are needed to generate a robust
gene list for predicting outcome. This conclusion is based on existence of some
statistical assumptions. The current work raises doubts over this determination
by showing that: (1) These assumptions are not consistent with additional
assumptions such as sparsity and Gaussianity. (2) The empirical Bayes
methodology which was suggested in order to test the relevant assumptions
doesn't detect severe violations of the model assumptions and consequently an
overestimation of the required sample size might be incurred.
",statistics
"  According to a traditional point of view Boltzmann entropy is intimately
related to linear Fokker-Planck equations (Smoluchowski, Klein-Kramers, and
Rayleigh equations) that describe a well-known nonequilibrium phenomenon:
(normal) Brownian motion of a particle immersed in a thermal bath.
Nevertheless, current researches have claimed that non-Boltzmann entropies
(Tsallis and Renyi entropies, for instance) may give rise to anomalous Brownian
motion through nonlinear Fokker-Planck equations. The novelty of the present
article is to show that anomalous diffusion could be investigated within the
framework of non-Markovian linear Fokker-Planck equations. So on the ground of
this non-Markovian approach to Brownian motion, we find out anomalous diffusion
characterized by the mean square displacement of a free particle and a harmonic
oscillator in absence of inertial force as well as the mean square momentum of
a free particle in presence of inertial force.
",physics
"  We compute the Hochschild cohomology ring of the algebras $A= k\langle X,
Y\rangle/ (X^a, XY-qYX, Y^a)$ over a field $k$ where $a\geq 2$ and where $q\in
k$ is a primitive $a$-th root of unity. We find the the dimension of
$\mathrm{HH}^n(A)$ and show that it is independent of $a$. We compute
explicitly the ring structure of the even part of the Hochschild cohomology
modulo homogeneous nilpotent elements.
",mathematics
"  Infra-Red(IR) astronomical databases, namely, IRAS, 2MASS, WISE, and Spitzer,
are used to analyze photometric data of 126 carbon stars whose spectra are
visible in the First Byurakan Survey low-resolution spectral plates. Among
these, six new objects, recently confirmed on the digitized FBS plates, are
included. For three of them, moderate-resolution CCD optical spectra are also
presented. In this work several IR color-color diagrams are studied. Early and
late-type C stars are separated in the JHK Near-Infra-Red(NIR) color-color
plots, as well as in the WISE W3-W4 versus W1-W2 diagram. Late N-type
Asymptotic Giant Branch stars are redder in W1-W2, while early-types(CH and R
giants) are redder in W3-W4 as expected. Objects with W2-W3 > 1.0 mag. show
double-peaked spectral energy distribution, indicating the existence of the
circumstellar envelopes around them. 26 N-type stars have IRAS Point Source
Catalog(PSC) associations. For FBS 1812+455 IRAS Low-Resolution Spectra in the
wavelength range 7.7 - 22.6micron and Spitzer Space Telescope Spectra in the
range 5 - 38micro are presented clearly showing absorption features of
C2H2(acetylene) molecule at 7.5 and 13.7micron , and the SiC(silicone carbide)
emission at 11.3micron. The mass-loss rates for eight Mira-type variables are
derived from the K-[12] color and from the pulsation periods. The reddest
object among the targets is N-type C star FBS 2213+421, which belong to the
group of the cold post-AGB R Coronae Borealis(R CrB) variables.
",physics
"  Variational autoencoders (VAE) are directed generative models that learn
factorial latent variables. As noted by Burda et al. (2015), these models
exhibit the problem of factor over-pruning where a significant number of
stochastic factors fail to learn anything and become inactive. This can limit
their modeling power and their ability to learn diverse and meaningful latent
representations. In this paper, we evaluate several methods to address this
problem and propose a more effective model-based approach called the epitomic
variational autoencoder (eVAE). The so-called epitomes of this model are groups
of mutually exclusive latent factors that compete to explain the data. This
approach helps prevent inactive units since each group is pressured to explain
the data. We compare the approaches with qualitative and quantitative results
on MNIST and TFD datasets. Our results show that eVAE makes efficient use of
model capacity and generalizes better than VAE.
",computer-science
"  Perovskite solar cells with record power conversion efficiency are fabricated
by alloying both hybrid and fully inorganic compounds. While the basic
electronic properties of the hybrid perovskites are now well understood, key
electronic parameters for solar cell performance, such as the exciton binding
energy of fully inorganic perovskites, are still unknown. By performing magneto
transmission measurements, we determine with high accuracy the exciton binding
energy and reduced mass of fully inorganic CsPbX$_3$ perovskites (X=I, Br, and
an alloy of these). The well behaved (continuous) evolution of the band gap
with temperature in the range $4-270$\,K suggests that fully inorganic
perovskites do not undergo structural phase transitions like their hybrid
counterparts. The experimentally determined dielectric constants indicate that
at low temperature, when the motion of the organic cation is frozen, the
dielectric screening mechanism is essentially the same both for hybrid and
inorganic perovskites, and is dominated by the relative motion of atoms within
the lead-halide cage.
",physics
"  Usage of online textual media is steadily increasing. Daily, more and more
news stories, blog posts and scientific articles are added to the online
volumes. These are all freely accessible and have been employed extensively in
multiple research areas, e.g. automatic text summarization, information
retrieval, information extraction, etc. Meanwhile, online debate forums have
recently become popular, but have remained largely unexplored. For this reason,
there are no sufficient resources of annotated debate data available for
conducting research in this genre. In this paper, we collected and annotated
debate data for an automatic summarization task. Similar to extractive gold
standard summary generation our data contains sentences worthy to include into
a summary. Five human annotators performed this task. Inter-annotator
agreement, based on semantic similarity, is 36% for Cohen's kappa and 48% for
Krippendorff's alpha. Moreover, we also implement an extractive summarization
system for online debates and discuss prominent features for the task of
summarizing online debate data automatically.
",computer-science
"  The Fisher information matrix (FIM) is a fundamental quantity to represent
the characteristics of a stochastic model, including deep neural networks
(DNNs). The present study reveals novel statistics of FIM that are universal
among a wide class of DNNs. To this end, we use random weights and large width
limits, which enables us to utilize mean field theories. We investigate the
asymptotic statistics of the FIM's eigenvalues and reveal that most of them are
close to zero while the maximum takes a huge value. This implies that the
eigenvalue distribution has a long tail. Because the landscape of the parameter
space is defined by the FIM, it is locally flat in most dimensions, but
strongly distorted in others. We also demonstrate the potential usage of the
derived statistics through two exercises. First, small eigenvalues that induce
flatness can be connected to a norm-based capacity measure of generalization
ability. Second, the maximum eigenvalue that induces the distortion enables us
to quantitatively estimate an appropriately sized learning rate for gradient
methods to converge.
",statistics
"  We address the problem of a lightly doped spin-liquid through a large-scale
density-matrix renormalization group (DMRG) study of the $t$-$J$ model on a
Kagome lattice with a small but non-zero concentration, $\delta$, of doped
holes. It is now widely accepted that the undoped ($\delta=0$) spin 1/2
Heisenberg antiferromagnet has a spin-liquid groundstate. Theoretical arguments
have been presented that light doping of such a spin-liquid could give rise to
a high temperature superconductor or an exotic topological Fermi liquid metal
(FL$^\ast$). Instead, we infer that the doped holes form an insulating
charge-density wave state with one doped-hole per unit cell - i.e. a Wigner
crystal (WC). Spin correlations remain short-ranged, as in the spin-liquid
parent state, from which we infer that the state is a crystal of spinless
holons (WC$^\ast$), rather than of holes. Our results may be relevant to Kagome
lattice Herbertsmithite $\rm ZnCu_3(OH)_6Cl_2$ upon doping.
",physics
"  Quantum charge pumping phenomenon connects band topology through the dynamics
of a one-dimensional quantum system. In terms of a microscopic model, the
Su-Schrieffer-Heeger/Rice-Mele quantum pump continues to serve as a fruitful
starting point for many considerations of topological physics. Here we present
a generalized Creutz scheme as a distinct two-band quantum pump model. By
noting that it undergoes two kinds of topological band transitions accompanying
with a Zak-phase-difference of $\pi$ and $2\pi$, respectively, various charge
pumping schemes are studied by applying an elaborate Peierl's phase
substitution. Translating into real space, the transportation of quantized
charges is a result of cooperative quantum interference effect. In particular,
an all-flux quantum pump emerges which operates with time-varying fluxes only
and transports two charge units. This puts cold atoms with artificial gauge
fields as an unique system where this kind of phenomena can be realized.
",physics
"  The entropy of a quantum system is a measure of its randomness, and has
applications in measuring quantum entanglement. We study the problem of
measuring the von Neumann entropy, $S(\rho)$, and Rényi entropy,
$S_\alpha(\rho)$ of an unknown mixed quantum state $\rho$ in $d$ dimensions,
given access to independent copies of $\rho$.
We provide an algorithm with copy complexity $O(d^{2/\alpha})$ for estimating
$S_\alpha(\rho)$ for $\alpha<1$, and copy complexity $O(d^{2})$ for estimating
$S(\rho)$, and $S_\alpha(\rho)$ for non-integral $\alpha>1$. These bounds are
at least quadratic in $d$, which is the order dependence on the number of
copies required for learning the entire state $\rho$. For integral $\alpha>1$,
on the other hand, we provide an algorithm for estimating $S_\alpha(\rho)$ with
a sub-quadratic copy complexity of $O(d^{2-2/\alpha})$. We characterize the
copy complexity for integral $\alpha>1$ up to constant factors by providing
matching lower bounds. For other values of $\alpha$, and the von Neumann
entropy, we show lower bounds on the algorithm that achieves the upper bound.
This shows that we either need new algorithms for better upper bounds, or
better lower bounds to tighten the results.
For non-integral $\alpha$, and the von Neumann entropy, we consider the well
known Empirical Young Diagram (EYD) algorithm, which is the analogue of
empirical plug-in estimator in classical distribution estimation. As a
corollary, we strengthen a lower bound on the copy complexity of the EYD
algorithm for learning the maximally mixed state by showing that the lower
bound holds with exponential probability (which was previously known to hold
with a constant probability). For integral $\alpha>1$, we provide new
concentration results of certain polynomials that arise in Kerov algebra of
Young diagrams.
",computer-science
"  Neuroscience has been carried into the domain of big data and high
performance computing (HPC) on the backs of initiatives in data collection and
an increasingly compute-intensive tools. While managing HPC experiments
requires considerable technical acumen, platforms and standards have been
developed to ease this burden on scientists. While web-portals make resources
widely accessible, data organizations such as the Brain Imaging Data Structure
and tool description languages such as Boutiques provide researchers with a
foothold to tackle these problems using their own datasets, pipelines, and
environments. While these standards lower the barrier to adoption of HPC and
cloud systems for neuroscience applications, they still require the
consolidation of disparate domain-specific knowledge. We present Clowdr, a
lightweight tool to launch experiments on HPC systems and clouds, record rich
execution records, and enable the accessible sharing of experimental summaries
and results. Clowdr uniquely sits between web platforms and bare-metal
applications for experiment management by preserving the flexibility of
do-it-yourself solutions while providing a low barrier for developing,
deploying and disseminating neuroscientific analysis.
",computer-science
"  Thermal properties of graphene monolayers are studied by path-integral
molecular dynamics (PIMD) simulations, which take into account the quantization
of vibrational modes in the crystalline membrane, and allow one to consider
anharmonic effects in these properties. This system was studied at temperatures
in the range from 12 to 2000~K and zero external stress, by describing the
interatomic interactions through the LCBOPII effective potential. We analyze
the internal energy and specific heat and compare the results derived from the
simulations with those yielded by a harmonic approximation for the vibrational
modes. This approximation turns out to be rather precise up to temperatures of
about 400~K. At higher temperatures, we observe an influence of the elastic
energy, due to the thermal expansion of the graphene sheet. Zero-point and
thermal effects on the in-plane and ""real"" surface of graphene are discussed.
The thermal expansion coefficient $\alpha$ of the real area is found to be
positive at all temperatures, in contrast to the expansion coefficient
$\alpha_p$ of the in-plane area, which is negative at low temperatures, and
becomes positive for $T \gtrsim$ 1000~K.
",physics
"  We study the asymptotic behavior of a sequence of positive solutions
$(u_{\epsilon})_{\epsilon >0}$ as $\epsilon \to 0$ to the family of equations
\begin{equation*} \left\{\begin{array}{ll} \Delta
u_{\epsilon}+a(x)u_{\epsilon}=
\frac{u_{\epsilon}^{2^*(s_{\epsilon})-1}}{|x|^{s_{\epsilon}}}& \hbox{ in
}\Omega\\ u_{\epsilon}=0 & \hbox{ on }\partial\Omega. \end{array}\right.
\end{equation*} where $(s_{\epsilon})_{\epsilon >0}$ is a sequence of positive
real numbers such that $\lim \limits_{\epsilon \rightarrow 0} s_{\epsilon}=0$,
$2^{*}(s_{\epsilon}):= \frac{2(n-s_{\epsilon})}{n-2}$ and $\Omega \subset
\mathbb{R}^{n}$ is a bounded smooth domain such that $0 \in \partial \Omega$.
When the sequence $(u_{\epsilon})_{\epsilon >0}$ is uniformly bounded in
$L^{\infty}$, then upto a subsequence it converges strongly to a minimizing
solution of the stationary Schrödinger equation with critical growth. In
case the sequence blows up, we obtain strong pointwise control on the blow up
sequence, and then using the Pohozaev identity localize the point of
singularity, which in this case can at most be one, and derive precise blow up
rates. In particular when $n=3$ or $a\equiv 0$ then blow up can occur only at
an interior point of $\Omega$ or the point $0 \in \partial \Omega$.
",mathematics
"  Summarizes recent work on the wakefields and impedances of flat, metallic
plates with small corrugations
",physics
"  Recently, Lawson has shown that the 2-primary Brown-Peterson spectrum does
not admit the structure of an $E_{12}$ ring spectrum, thus answering a question
of May in the negative. We extend Lawson's result to odd primes by proving that
the p-primary Brown-Peterson spectrum does not admit the structure of an
$E_{2(p^2+2)}$ ring spectrum. We also show that there can be no map $MU \to BP$
of $E_{2p+3}$ ring spectra at any prime.
",mathematics
"  This paper addresses the challenge of humanoid robot teleoperation in a
natural indoor environment via a Brain-Computer Interface (BCI). We leverage
deep Convolutional Neural Network (CNN) based image and signal understanding to
facilitate both real-time object detection and dry-Electroencephalography (EEG)
based human cortical brain bio-signal decoding. We employ recent advances in
dry-EEG technology to stream and collect the cortical waveforms from subjects
while the subjects fixate on variable Steady-State Visual Evoked Potential
(SSVEP) stimuli generated directly from the environment the robot is
navigating. To these ends, we propose the use of novel variable BCI stimuli by
utilising the real-time video streamed via the on-board robot camera as visual
input for SSVEP where the CNN detected natural scene objects are altered and
flickered with differing frequencies (10Hz, 12Hz and 15Hz). These stimuli are
not akin to traditional stimuli - as both the dimensions of the flicker regions
and their on-screen position changes depending on the scene objects detected in
the scene. On-screen object selection via dry-EEG enabled SSVEP in this way,
facilitates the on-line decoding of human cortical brain signals via a
secondary CNN approach into teleoperation robot commands (approach object, move
in a specific direction: right, left or back). This SSVEP decoding model is
trained via a priori offline experimental data in which very similar visual
input is present for all subjects. The resulting offline classification
demonstrates extremely high performance and with mean accuracies of 96% and 90%
for the real-time robot navigation experiment across multiple test subjects.
",computer-science
"  By establishing a connection between bi-directional Helmholtz machines and
information theory, we propose a generalized Helmholtz machine. Theoretical and
experimental results show that given \textit{shallow} architectures, the
generalized model outperforms the previous ones substantially.
",statistics
"  The Hohenberg-Kohn theorem plays a fundamental role in density functional
theory, which has become a basic tool for the study of electronic structure of
matter. In this article, we study the Hohenberg-Kohn theorem for a class of
external potentials based on a unique continuation principle.
",physics
"  Human relations are driven by social events - people interact, exchange
information, share knowledge and emotions, or gather news from mass media.
These events leave traces in human memory. The initial strength of a trace
depends on cognitive factors such as emotions or attention span. Each trace
continuously weakens over time unless another related event activity
strengthens it. Here, we introduce a novel Cognition-driven Social Network
(CogSNet) model that accounts for cognitive aspects of social perception and
explicitly represents human memory dynamics. For validation, we apply our model
to NetSense data on social interactions among university students. The results
show that CogSNet significantly improves quality of modeling of human
interactions in social networks.
",computer-science
"  A sparse stochastic block model (SBM) with two communities is defined by the
community probability $\pi_0,\pi_1$, and the connection probability between
communities $a,b\in\{0,1\}$, namely $q_{ab} = \frac{\alpha_{ab}}{n}$. When
$q_{ab}$ is constant in $a,b$, the random graph is simply the
Erdős-Rény random graph. We evaluate the log partition function of the
Ising model on sparse SBM with two communities.
As an application, we give consistent parameter estimation of the sparse SBM
with two communities in a special case. More specifically, let $d_0,d_1$ be the
average degree of the two communities, i.e.,
$d_0\overset{def}{=}\pi_0\alpha_{00}+\pi_1\alpha_{01},d_1\overset{def}{=}\pi_0\alpha_{10}+\pi_1\alpha_{11}$.
We focus on the regime $d_0=d_1$ (the regime $d_0\ne d_1$ is trivial). In this
regime, there exists $d,\lambda$ and $r\geq 0$ with $\pi_0=\frac{1}{1+r},
\pi_1=\frac{r}{1+r}$, $\alpha_{00}=d(1+r\lambda), \alpha_{01}=\alpha_{10} =
d(1-\lambda), \alpha_{11} = d(1+\frac{\lambda}{r})$. We give a consistent
estimator of $r$ when $\lambda<0$. The estimator of $\lambda$ given by
\citep{mossel2015reconstruction} is valid in the general situation. We also
provide a random clustering algorithm which does not require knowledge of
parameters and which is positively correlated with the true community label
when $\lambda<0$.
",statistics
"  In real world, there is a significant relation between human behaviors and
epidemic spread. Especially, the reactions among individuals in different
communities to epidemics may be different, which lead to cluster
synchronization of human behaviors. So, a mathematical model that embeds
community structures, behavioral evolution and epidemic transmission is
constructed to study the interaction between cluster synchronization and
epidemic spread. The epidemic threshold of the model is obtained by using
Gersgorin Lemma and dynamical system theory. By applying the Lyapunov stability
method, the stability analysis of cluster synchronization and spreading
dynamics are presented. Then, some numerical simulations are performed to
illustrate and complement our theoretical results. As far as we know, this work
is the first one to address the interplay between cluster synchronization and
epidemic transmission in community networks, so it may deepen the understanding
of the impact of cluster behaviors on infectious disease dynamics.
",physics
"  Ultrafast X-ray imaging provides high resolution information on individual
fragile specimens such as aerosols, metastable particles, superfluid quantum
systems and live biospecimen, which is inaccessible with conventional imaging
techniques. Coherent X-ray diffractive imaging, however, suffers from intrinsic
loss of phase, and therefore structure recovery is often complicated and not
always uniquely-defined. Here, we introduce the method of in-flight holography,
where we use nanoclusters as reference X-ray scatterers in order to encode
relative phase information into diffraction patterns of a virus. The resulting
hologram contains an unambiguous three-dimensional map of a virus and two
nanoclusters with the highest lat- eral resolution so far achieved via single
shot X-ray holography. Our approach unlocks the benefits of holography for
ultrafast X-ray imaging of nanoscale, non-periodic systems and paves the way to
direct observation of complex electron dynamics down to the attosecond time
scale.
",physics
"  Let $\tau(n)$ be the number of divisors of $n$. We give an elementary proof
of the fact that $$ \sum_{n\le x} \tau(n)^r =xC_{r} (\log x)^{2^r-1}+O(x(\log
x)^{2^r-2}), $$ for any integer $r\ge 2$. Here, $$ C_{r}=\frac{1}{(2^r-1)!}
\prod_{p\ge 2}\left( \left(1-\frac{1}{p}\right)^{2^r} \left(\sum_{\alpha\ge 0}
\frac{(\alpha+1)^r}{p^{\alpha}}\right)\right). $$
",mathematics
"  A new Bayesian framework is presented that can constrain projections of
future climate using historical observations by exploiting robust estimates of
emergent relationships between multiple climate models. We argue that emergent
relationships can be interpreted as constraints on model inadequacy, but that
projections may be biased if we do not account for internal variability in
climate model projections. We extend the previously proposed coexchangeable
framework to account for natural variability in the Earth system and internal
variability simulated by climate models. A detailed theoretical comparison with
previous multi-model projection frameworks is provided.
The proposed framework is applied to projecting surface temperature in the
Arctic at the end of the 21st century. A subset of available climate models are
selected in order to satisfy the assumptions of the framework. All available
initial condition runs from each model are utilized in order maximize the
utility of the data. Projected temperatures in some regions are more than 2C
lower when constrained by historical observations. The uncertainty about the
climate response is reduced by up to 30% where strong constraints exist.
",statistics
"  Artificial Intelligence (AI) is an effective science which employs strong
enough approaches, methods, and techniques to solve unsolvable real world based
problems. Because of its unstoppable rise towards the future, there are also
some discussions about its ethics and safety. Shaping an AI friendly
environment for people and a people friendly environment for AI can be a
possible answer for finding a shared context of values for both humans and
robots. In this context, objective of this paper is to address the ethical
issues of AI and explore the moral dilemmas that arise from ethical algorithms,
from pre set or acquired values. In addition, the paper will also focus on the
subject of AI safety. As general, the paper will briefly analyze the concerns
and potential solutions to solving the ethical issues presented and increase
readers awareness on AI safety as another related research interest.
",computer-science
"  The interaction that occurs between a light solid object and a horizontal
soap film of a bamboo foam contained in a cylindrical tube is simulated in 3D.
We vary the shape of the falling object from a sphere to a cube by changing a
single shape parameter as well as varying the initial orientation and position
of the object. We investigate in detail how the soap film deforms in all these
cases, and determine the network and pressure forces that a foam exerts on a
falling object, due to surface tension and bubble pressure respectively. We
show that a cubic particle in a particular orientation experiences the largest
drag force, and that this orientation is also the most likely outcome of
dropping a cube from an arbitrary orientation through a bamboo foam.
",physics
"  Automatic testing is a widely adopted technique for improving software
quality. Software developers add, remove and update test methods and test
classes as part of the software development process as well as during the
evolution phase, following the initial release. In this work we conduct a large
scale study of 61 popular open source projects and report the relationships we
have established between test maintenance, production code maintenance, and
semantic changes (e.g, statement added, method removed, etc.). performed in
developers' commits.
We build predictive models, and show that the number of tests in a software
project can be well predicted by employing code maintenance profiles (i.e., how
many commits were performed in each of the maintenance activities: corrective,
perfective, adaptive). Our findings also reveal that more often than not,
developers perform code fixes without performing complementary test maintenance
in the same commit (e.g., update an existing test or add a new one). When
developers do perform test maintenance, it is likely to be affected by the
semantic changes they perform as part of their commit.
Our work is based on studying 61 popular open source projects, comprised of
over 240,000 commits consisting of over 16,000,000 semantic change type
instances, performed by over 4,000 software engineers.
",computer-science
"  Recent research in computational linguistics has developed algorithms which
associate matrices with adjectives and verbs, based on the distribution of
words in a corpus of text. These matrices are linear operators on a vector
space of context words. They are used to construct the meaning of composite
expressions from that of the elementary constituents, forming part of a
compositional distributional approach to semantics. We propose a Matrix Theory
approach to this data, based on permutation symmetry along with Gaussian
weights and their perturbations. A simple Gaussian model is tested against word
matrices created from a large corpus of text. We characterize the cubic and
quartic departures from the model, which we propose, alongside the Gaussian
parameters, as signatures for comparison of linguistic corpora. We propose that
perturbed Gaussian models with permutation symmetry provide a promising
framework for characterizing the nature of universality in the statistical
properties of word matrices. The matrix theory framework developed here
exploits the view of statistics as zero dimensional perturbative quantum field
theory. It perceives language as a physical system realizing a universality
class of matrix statistics characterized by permutation symmetry.
",computer-science
"  The maximum gap $g(f)$ of a polynomial $f$ is the maximum of the differences
(gaps) between two consecutive exponents that appear in $f$. Let $\Phi_{n}$ and
$\Psi_{n}$ denote the $n$-th cyclotomic and $n$-th inverse cyclotomic
polynomial, respectively. In this paper, we give several lower bounds for
$g(\Phi_{n})$ and $g(\Psi_{n})$, where $n$ is the product of odd primes. We
observe that they are very often exact. We also give an exact expression for
$g(\Psi_{n})$ under a certain condition. Finally we conjecture an exact
expression for $g(\Phi_{n})$ under a certain condition.
",mathematics
"  For primordial black holes (PBH) to be the dark matter in single-field
inflation, the slow-roll approximation must be violated by at least ${\cal
O}(1)$ in order to enhance the curvature power spectrum within the required
number of efolds between CMB scales and PBH mass scales. Power spectrum
predictions which rely on the inflaton remaining on the slow-roll attractor can
fail dramatically leading to qualitatively incorrect conclusions in models like
an inflection potential and misestimate the mass scale in a running mass model.
We show that an optimized temporal evaluation of the Hubble slow-roll
parameters to second order remains a good description for a wide range of PBH
formation models where up to a $10^7$ amplification of power occurs in $10$
efolds or more.
",physics
"  In this paper, we propose an encoder-decoder convolutional neural network
(CNN) architecture for estimating camera pose (orientation and location) from a
single RGB-image. The architecture has a hourglass shape consisting of a chain
of convolution and up-convolution layers followed by a regression part. The
up-convolution layers are introduced to preserve the fine-grained information
of the input image. Following the common practice, we train our model in
end-to-end manner utilizing transfer learning from large scale classification
data. The experiments demonstrate the performance of the approach on data
exhibiting different lighting conditions, reflections, and motion blur. The
results indicate a clear improvement over the previous state-of-the-art even
when compared to methods that utilize sequence of test frames instead of a
single frame.
",computer-science
"  Animal groups exhibit emergent properties that are a consequence of local
interactions. Linking individual-level behaviour to coarse-grained descriptions
of animal groups has been a question of fundamental interest. Here, we present
two complementary approaches to deriving coarse-grained descriptions of
collective behaviour at so-called mesoscopic scales, which account for the
stochasticity arising from the finite sizes of animal groups. We construct
stochastic differential equations (SDEs) for a coarse-grained variable that
describes the order/consensus within a group. The first method of construction
is based on van Kampen's system-size expansion of transition rates. The second
method employs Gillespie's chemical Langevin equations. We apply these two
methods to two microscopic models from the literature, in which organisms
stochastically interact and choose between two directions/choices of foraging.
These `binary-choice' models differ only in the types of interactions between
individuals, with one assuming simple pair-wise interactions, and the other
incorporating higher-order effects. In both cases, the derived mesoscopic SDEs
have multiplicative, or state-dependent, noise. However, the different models
demonstrate the contrasting effects of noise: increasing order in the pair-wise
interaction model, whilst reducing order in the higher-order interaction model.
Although both methods yield identical SDEs for such binary-choice, or
one-dimensional, systems, the relative tractability of the chemical Langevin
approach is beneficial in generalizations to higher-dimensions. In summary,
this book chapter provides a pedagogical review of two complementary methods to
construct mesoscopic descriptions from microscopic rules and demonstrates how
resultant multiplicative noise can have counter-intuitive effects on shaping
collective behaviour.
",quantitative-biology
"  Poynting's theorem is used to obtain an expression for the turbulent
power-spectral density as function of frequency and wavenumber in low-frequency
magnetic turbulence. No reference is made to Elsasser variables as is usually
done in magnetohydrodynamic turbulence mixing mechanical and electromagnetic
turbulence. We rather stay with an implicit form of the mechanical part of
turbulence as suggested by electromagnetic theory in arbitrary media. All of
mechanics and flows is included into a turbulent response function which by
appropriate observations can be determined from knowledge of the turbulent
fluctuation spectra. This approach is not guided by the wish of developing a
complete theory of turbulence. It aims on the identification of the response
function from observations as input into a theory which afterwards attempts its
interpretation. Combination of both the magnetic and electric power spectral
densities leads to a representation of the turbulent response function, i.e.
the turbulent conductivity spectrum $\sigma_{\omega k}$ as function of
frequency $\omega$ and wavenumber $k$. {It is given as the ratio of magnetic to
electric power spectral densities in frequency space. This knowledge allows for
formally writing down a turbulent dispersion relation. Power law inertial range
spectra result in a power law turbulent conductivity spectrum. These can be
compared with observations in the solar wind. Keywords: MHD turbulence,
turbulent dispersion relation, turbulent response function, solar wind
turbulence
",physics
"  We study the orbital properties of dark matter haloes by combining a spectral
method and cosmological simulations of Milky Way-sized galaxies. We compare the
dynamics and orbits of individual dark matter particles from both hydrodynamic
and $N$-body simulations, and find that the fraction of box, tube and resonant
orbits of the dark matter halo decreases significantly due to the effects of
baryons. In particular, the central region of the dark matter halo in the
hydrodynamic simulation is dominated by regular, short-axis tube orbits, in
contrast to the chaotic, box and thin orbits dominant in the $N$-body run. This
leads to a more spherical dark matter halo in the hydrodynamic run compared to
a prolate one as commonly seen in the $N$-body simulations. Furthermore, by
using a kernel based density estimator, we compare the coarse-grained
phase-space densities of dark matter haloes in both simulations and find that
it is lower by $\sim0.5$ dex in the hydrodynamic run due to changes in the
angular momentum distribution, which indicates that the baryonic process that
affects the dark matter is irreversible. Our results imply that baryons play an
important role in determining the shape, kinematics and phase-space density of
dark matter haloes in galaxies.
",physics
"  For a finite field of odd cardinality $q$, we show that the sequence of
iterates of $aX^2+c$, starting at $0$, always recurs after $O(q/\log\log q)$
steps. For $X^2+1$ the same is true for any starting value. We suggest that the
traditional ""Birthday Paradox"" model is inappropriate for iterates of $X^3+c$,
when $q$ is 2 mod 3.
",mathematics
"  We study the problem of defining maps on link Floer homology induced by
unoriented link cobordisms. We provide a natural notion of link cobordism,
disoriented link cobordism, which tracks the motion of index zero and index
three critical points. Then we construct a map on unoriented link Floer
homology associated to a disoriented link cobordism. Furthermore, we give a
comparison with Oszváth-Stipsicz-Szabó's and Manolescu's constructions of
link cobordism maps for an unoriented band move.
",mathematics
"  Characteristic classes in space-time manifolds are discussed for both even-
and odd-dimensional spacetimes. In particular, it is shown that the
Einstein--Hilbert action is equivalent to a second Chern-class on a modified
Poincare bundle in four dimensions. Consequently, the cosmological constant and
the trace of an energy-momentum tensor become divisible modulo R/Z.
",physics
"  Metabolic fluxes in cells are governed by physical, biochemical,
physiological, and economic principles. Cells may show ""economical"" behaviour,
trading metabolic performance against the costly side-effects of high enzyme or
metabolite concentrations. Some constraint-based flux prediction methods score
fluxes by heuristic flux costs as proxies of enzyme investments. However,
linear cost functions ignore enzyme kinetics and the tight coupling between
fluxes, metabolite levels and enzyme levels. To derive more realistic cost
functions, I define an apparent ""enzymatic flux cost"" as the minimal enzyme
cost at which the fluxes can be realised in a given kinetic model, and a
""kinetic flux cost"", which includes metabolite cost. I discuss the mathematical
properties of such flux cost functions, their usage for flux prediction, and
their importance for cells' metabolic strategies. The enzymatic flux cost
scales linearly with the fluxes and is a concave function on the flux polytope.
The costs of two flows are usually not additive, due to an additional
""compromise cost"". Between flux polytopes, where fluxes change their
directions, the enzymatic cost shows a jump. With strictly concave flux cost
functions, cells can reduce their enzymatic cost by running different fluxes in
different cell compartments or at different moments in time. The enzymactic
flux cost can be translated into an approximated cell growth rate, a convex
function on the flux polytope. Growth-maximising metabolic states can be
predicted by Flux Cost Minimisation (FCM), a variant of FBA based on general
flux cost functions. The solutions are flux distributions in corners of the
flux polytope, i.e. typically elementary flux modes. Enzymatic flux costs can
be linearly or nonlinearly approximated, providing model parameters for linear
FBA based on kinetic parameters and extracellular concentrations, and justified
by a kinetic model.
",quantitative-biology
"  We study topological structure of the $\omega$-limit sets of the skew-product
semiflow generated by the following scalar reaction-diffusion equation
\begin{equation*} u_{t}=u_{xx}+f(t,u,u_{x}),\,\,t>0,\,x\in
S^{1}=\mathbb{R}/2\pi \mathbb{Z}, \end{equation*} where $f(t,u,u_x)$ is
$C^2$-admissible with time-recurrent structure including almost-periodicity and
almost-automorphy. Contrary to the time-periodic cases (for which any
$\omega$-limit set can be imbedded into a periodically forced circle flow), it
is shown that one cannot expect that any $\omega$-limit set can be imbedded
into an almost-periodically forced circle flow even if $f$ is uniformly
almost-periodic in $t$.
More precisely, we prove that, for a given $\omega$-limit set $\Omega$, if
${\rm dim}V^c(\Omega)\leq 1$ ($V^c(\Omega)$ is the center space associated with
$\Omega$), then $\Omega$ is either spatially-homogeneous or
spatially-inhomogeneous; and moreover, any spatially-inhomogeneous $\Omega$ can
be imbedded into a time-recurrently forced circle flow (resp. imbedded into an
almost periodically-forced circle flow if $f$ is uniformly almost-periodic in
$t$). On the other hand, when ${\rm dim}V^c(\Omega>1$, it is pointed out that
the above embedding property cannot hold anymore. Furthermore, we also show the
new phenomena of the residual imbedding into a time-recurrently forced circle
flow (resp. into an almost automorphically-forced circle flow if $f$ is
uniformly almost-periodic in $t$) provided that $\dim V^c(\Omega)=2$ and $\dim
V^u(\Omega)$ is odd. All these results reveal that for such system there are
essential differences between time-periodic cases and non-periodic cases.
",mathematics
"  We revisit the fundamental problem of liquid-liquid dewetting and perform a
detailed comparison of theoretical predictions based on thin-film models with
experimental measurements obtained by atomic force microscopy (AFM).
Specifically, we consider the dewetting of a liquid polystyrene (PS) layer from
a liquid polymethyl methacrylate (PMMA) layer, where the thicknesses and the
viscosities of PS and PMMA layers are similar. The excellent agreement of
experiment and theory reveals that dewetting rates for such systems follow no
universal power law, in contrast to dewetting scenarios on solid substrates.
Our new energetic approach allows to assess the physical importance of
different contributions to the energy-dissipation mechanism, for which we
analyze the local flow fields and the local dissipation rates.
",physics
"  We introduce a few variants on Frank-Wolfe style algorithms suitable for
large scale optimization. We show how to modify the standard Frank-Wolfe
algorithm using stochastic gradients, approximate subproblem solutions, and
sketched decision variables in order to scale to enormous problems while
preserving (up to constants) the optimal convergence rate
$\mathcal{O}(\frac{1}{k})$.
",statistics
"  Users of Virtual Reality (VR) systems often experience vection, the
perception of self-motion in the absence of any physical movement. While
vection helps to improve presence in VR, it often leads to a form of motion
sickness called cybersickness. Cybersickness is a major deterrent to large
scale adoption of VR.
Prior work has discovered that changing vection (changing the perceived speed
or moving direction) causes more severe cybersickness than steady vection
(walking at a constant speed or in a constant direction). Based on this idea,
we try to reduce the cybersickness caused by character movements in a First
Person Shooter (FPS) game in VR. We propose Rotation Blurring (RB), uniformly
blurring the screen during rotational movements to reduce cybersickness. We
performed a user study to evaluate the impact of RB in reducing cybersickness.
We found that the blurring technique led to an overall reduction in sickness
levels of the participants and delayed its onset. Participants who experienced
acute levels of cybersickness benefited significantly from this technique.
",computer-science
"  In this paper, we present some extensions of interpolation between the
arithmetic-geometric means inequality. Among other inequalities, it is shown
that if $A, B, X$ are $n\times n$ matrices, then \begin{align*}
\|AXB^*\|^2\leq\|f_1(A^*A)Xg_1(B^*B)\|\,\|f_2(A^*A)Xg_2(B^*B)\|, \end{align*}
where $f_1,f_2,g_1,g_2$ are non-negative continues functions such that
$f_1(t)f_2(t)=t$ and $g_1(t)g_2(t)=t\,\,(t\geq0)$. We also obtain the
inequality \begin{align*}
\left|\left|\left|AB^*\right|\right|\right|^2\nonumber&\leq
\left|\left|\left|p(A^*A)^{\frac{m}{p}}+
(1-p)(B^*B)^{\frac{s}{1-p}}\right|\right|\right|\,\left|\left|\left|(1-p)(A^*A)^{\frac{n}{1-p}}+
p(B^*B)^{\frac{t}{p}}\right|\right|\right|, \end{align*} in which $m,n,s,t$ are
real numbers such that $m+n=s+t=1$, $|||\cdot|||$ is an arbitrary unitarily
invariant norm and $p\in[0,1]$.
",mathematics
"  Despite significant recent progress in the area of Brain-Computer Interface,
there are numerous shortcomings associated with collecting
Electroencephalography (EEG) signals in real-world environments. These include,
but are not limited to, subject and session data variance, long and arduous
calibration processes and performance generalisation issues across
differentsubjects or sessions. This implies that many downstream applications,
including Steady State Visual Evoked Potential (SSVEP) based classification
systems, can suffer from a shortage of reliable data. Generating meaningful and
realistic synthetic data can therefore be of significant value in circumventing
this problem. We explore the use of modern neural-based generative models
trained on a limited quantity of EEG data collected from different subjects to
generate supplementary synthetic EEG signal vectors subsequently utilised to
train an SSVEP classifier. Extensive experimental analyses demonstrate the
efficacy of our generated data, leading to significant improvements across a
variety of evaluations, with the crucial task of cross-subject generalisation
improving by over 35% with the use of synthetic data.
",quantitative-biology
"  The technical details of a balloon stratospheric mission that is aimed at
measuring the Schumann resonances are described. The gondola is designed
specifically for the measuring of faint effects of ELF (Extremely Low Frequency
electromagnetic waves) phenomena. The prototype met the design requirements.
The ELF measuring system worked properly for entire mission; however, the level
of signal amplification that was chosen taking into account ground-level
measurements was too high. Movement of the gondola in the Earth magnetic field
induced the signal in the antenna that saturated the measuring system. This
effect will be taken into account in the planning of future missions. A large
telemetry dataset was gathered during the experiment and is currently under
processing. The payload consists also of biological material as well as
electronic equipment that was tested under extreme conditions.
",physics
"  Relativistic effects in the non-resonant two-photon K-shell ionization of
neutral atoms are studied theoretically within the framework of second-order
perturbation theory. The non-relativistic results are compared with the
relativistic calculations in the dipole and no-pair approximations as well as
with the complete relativistic approach. The calculations are performed in both
velocity and length gauges. Our results show a significant decrease of the
total cross section for heavy atoms as compared to the non-relativistic
treatment, which is mainly due to the relativistic wavefunction contraction.
The effects of higher multipoles and negative continuum energy states
counteract the relativistic contraction contribution, but are generally much
weaker. While the effects beyond the dipole approximation are equally important
in both gauges, the inclusion of negative continuum energy states visibly
contributes to the total cross section only in the velocity gauge.
",physics
"  Let (G, \mu) be a pair of a reductive group G over the p-adic integers and a
minuscule cocharacter {\mu} of G defined over an unramified extension. We
introduce and study ""(G, \mu)-displays"" which generalize Zink's Witt vector
displays. We use these to define certain Rapoport-Zink formal schemes purely
group theoretically, i.e. without p-divisible groups.
",mathematics
"  In recent years there has been great interest in variational analysis of a
class of nonsmooth functions called the minimal time function. In this paper we
continue this line of research by providing new results on generalized
differentiation of this class of functions, relaxing assumptions imposed on the
functions and sets involved for the results. In particular, we focus on the
singular subdifferential and the limiting subdifferential of this class of
functions.
",mathematics
"  We investigate spatial evolutionary games with death-birth updating in large
finite populations. Within growing spatial structures subject to appropriate
conditions, the density processes of a fixed type are proven to converge to the
Wright-Fisher diffusions with drift. In addition, convergence in the
Wasserstein distance of the laws of their occupation measures holds. The proofs
of these results develop along an equivalence between the laws of the
evolutionary games and certain voter models and rely on the analogous results
of voter models on large finite sets by convergences of the Radon-Nikodym
derivative processes. As another application of this equivalence of laws, we
show that in a general, large population of size $N$, for which the stationary
probabilities of the corresponding voting kernel are comparable to uniform
probabilities, a first-derivative test among the major methods for these
evolutionary games is applicable at least up to weak selection strengths in the
usual biological sense (that is, selection strengths of the order $\mathcal
O(1/N)$).
",mathematics
"  We present a machine learning framework for multi-agent systems to learn both
the optimal policy for maximizing the rewards and the encoding of the high
dimensional visual observation. The encoding is useful for sharing local visual
observations with other agents under communication resource constraints. The
actor-encoder encodes the raw images and chooses an action based on local
observations and messages sent by the other agents. The machine learning agent
generates not only an actuator command to the physical device, but also a
communication message to the other agents. We formulate a reinforcement
learning problem, which extends the action space to consider the communication
action as well. The feasibility of the reinforcement learning framework is
demonstrated using a 3D simulation environment with two collaborating agents.
The environment provides realistic visual observations to be used and shared
between the two agents.
",computer-science
"  The magnetorotational instability (MRI) is thought to be a powerful source of
turbulence in Keplerian accretion disks. Motivated by recent laboratory
experiments, we study the MRI driven by an azimuthal magnetic field in an
electrically conducting fluid sheared between two concentric rotating
cylinders. By adjusting the rotation rates of the cylinders, we approximate
angular velocity profiles $\omega \propto r^{q}$. We perform direct numerical
simulations of a steep profile close to the Rayleigh line $q \gtrsim -2 $ and a
quasi-Keplerian profile $q \approx -3/2$ and cover wide ranges of Reynolds
($Re\le 4\cdot10^4$) and magnetic Prandtl numbers ($0\le Pm \le 1$). In the
quasi-Keplerian case, the onset of instability depends on the magnetic Reynolds
number, with $Rm_c \approx 50$, and angular momentum transport scales as
$\sqrt{Pm} Re^2$ in the turbulent regime. The ratio of Maxwell to Reynolds
stresses is set by $Rm$. At the onset of instability both stresses have similar
magnitude, whereas the Reynolds stress vanishes or becomes even negative as
$Rm$ increases. For the profile close to the Rayleigh line, the instability
shares these properties as long as $Pm\gtrsim0.1$, but exhibits a markedly
different character if $Pm\rightarrow 0$, where the onset of instability is
governed by the Reynolds number, with $Re_c \approx 1250$, transport is via
Reynolds stresses and scales as $Re^2$. At intermediate $Pm=0.01$ we observe a
continuous transition from one regime to the other, with a crossover at
$Rm=\mathcal{O}(100)$. Our results give a comprehensive picture of angular
momentum transport of the MRI with an imposed azimuthal field.
",physics
"  Internship assignment is a complicated process for universities since it is
necessary to take into account a multiplicity of variables to establish a
compromise between companies' requirements and student competencies acquired
during the university training. These variables build up a complex relations
map that requires the formulation of an exhaustive and rigorous conceptual
scheme. In this research a domain ontological model is presented as support to
the student's decision making for opportunities of University studies level of
the University Lumiere Lyon 2 (ULL) education system. The ontology is designed
and created using methodological approach offering the possibility of improving
the progressive creation, capture and knowledge articulation. In this paper, we
draw a balance taking the demands of the companies across the capabilities of
the students. This will be done through the establishment of an ontological
model of an educational learners' profile and the internship postings which are
written in a free text and using uncontrolled vocabulary. Furthermore, we
outline the process of semantic matching which improves the quality of query
results.
",computer-science
"  In this paper, we consider the problem of formally verifying the safety of an
autonomous robot equipped with a Neural Network (NN) controller that processes
LiDAR images to produce control actions. Given a workspace that is
characterized by a set of polytopic obstacles, our objective is to compute the
set of safe initial conditions such that a robot trajectory starting from these
initial conditions is guaranteed to avoid the obstacles. Our approach is to
construct a finite state abstraction of the system and use standard
reachability analysis over the finite state abstraction to compute the set of
the safe initial states. The first technical problem in computing the finite
state abstraction is to mathematically model the imaging function that maps the
robot position to the LiDAR image. To that end, we introduce the notion of
imaging-adapted sets as partitions of the workspace in which the imaging
function is guaranteed to be affine. We develop a polynomial-time algorithm to
partition the workspace into imaging-adapted sets along with computing the
corresponding affine imaging functions. Given this workspace partitioning, a
discrete-time linear dynamics of the robot, and a pre-trained NN controller
with Rectified Linear Unit (ReLU) nonlinearity, the second technical challenge
is to analyze the behavior of the neural network. To that end, we utilize a
Satisfiability Modulo Convex (SMC) encoding to enumerate all the possible
segments of different ReLUs. SMC solvers then use a Boolean satisfiability
solver and a convex programming solver and decompose the problem into smaller
subproblems. To accelerate this process, we develop a pre-processing algorithm
that could rapidly prune the space feasible ReLU segments. Finally, we
demonstrate the efficiency of the proposed algorithms using numerical
simulations with increasing complexity of the neural network controller.
",computer-science
"  The DArk Matter Particle Explorer (DAMPE) is one of the four satellites
within Strategic Pioneer Research Program in Space Science of the Chinese
Academy of Science (CAS). DAMPE can detect electrons, photons in a wide energy
range (5 GeV to 10 TeV) and ions up to iron (100GeV to 100 TeV).
Silicon-Tungsten Tracker (STK) is one of the four subdetectors in DAMPE,
providing photon-electron conversion, track reconstruction and charge
identification for ions. Ion beam test was carried out in CERN with 60GeV/u
Lead primary beams. Charge reconstruction and charge resolution of STK
detectors were investigated.
",physics
"  Ionization by relativistically intense short laser pulses is studied in the
framework of strong-field quantum electrodynamics. Distinctive patterns are
found in the energy probability distributions of photoelectrons. Except of the
already observed patterns, which were studied in Phys. Rev. A {\bf 94}, 013402
(2016), we discover an additional interference-free smooth supercontinuum in
the high-energy portion of the spectrum, reaching tens of kiloelectronovolts.
As we show, the latter is sensitive to the driving field intensity and it can
be detected in a narrow polar-angular window. Once these high-energy electrons
are collected, they can form solitary attosecond pulses. This is particularly
important in light of various applications of attosecond electron beams such as
in ultrafast electron diffraction and crystallography, or in time-resolved
electron microscopy of physical, chemical, and biological processes.
",physics
"  The field of speech recognition is in the midst of a paradigm shift:
end-to-end neural networks are challenging the dominance of hidden Markov
models as a core technology. Using an attention mechanism in a recurrent
encoder-decoder architecture solves the dynamic time alignment problem,
allowing joint end-to-end training of the acoustic and language modeling
components. In this paper we extend the end-to-end framework to encompass
microphone array signal processing for noise suppression and speech enhancement
within the acoustic encoding network. This allows the beamforming components to
be optimized jointly within the recognition architecture to improve the
end-to-end speech recognition objective. Experiments on the noisy speech
benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system
outperformed the attention-based baseline with input from a conventional
adaptive beamformer.
",computer-science
"  Learning-based hashing methods are widely used for nearest neighbor
retrieval, and recently, online hashing methods have demonstrated good
performance-complexity trade-offs by learning hash functions from streaming
data. In this paper, we first address a key challenge for online hashing: the
binary codes for indexed data must be recomputed to keep pace with updates to
the hash functions. We propose an efficient quality measure for hash functions,
based on an information-theoretic quantity, mutual information, and use it
successfully as a criterion to eliminate unnecessary hash table updates. Next,
we also show how to optimize the mutual information objective using stochastic
gradient descent. We thus develop a novel hashing method, MIHash, that can be
used in both online and batch settings. Experiments on image retrieval
benchmarks (including a 2.5M image dataset) confirm the effectiveness of our
formulation, both in reducing hash table recomputations and in learning
high-quality hash functions.
",computer-science
"  Compared with the two-component Camassa-Holm system, the modified
two-component Camassa-Holm system introduces a regularized density which makes
possible the existence of solutions of lower regularity, and in particular of
multipeakon solutions. In this paper, we derive a new pointwise invariant for
the modified two-component Camassa-Holm system. The derivation of the invariant
uses directly the symmetry of the system, following the classical argument of
Noether's theorem. The existence of the multipeakon solutions can be directly
inferred from this pointwise invariant. This derivation shows the strong
connection between symmetries and the existence of special solutions. The
observation also holds for the scalar Camassa-Holm equation and, for
comparison, we have also included the corresponding derivation. Finally, we
compute explicitly the solutions obtained for the peakon-antipeakon case. We
observe the existence of a periodic solution which has not been reported in the
literature previously. This case shows the attractive effect that the
introduction of an elastic potential can have on the solutions.
",mathematics
"  Assessing the impact of the individual actions performed by soccer players
during games is a crucial aspect of the player recruitment process.
Unfortunately, most traditional metrics fall short in addressing this task as
they either focus on rare events like shots and goals alone or fail to account
for the context in which the actions occurred. This paper introduces a novel
advanced soccer metric for valuing any type of individual player action on the
pitch, be it with or without the ball. Our metric values each player action
based on its impact on the game outcome while accounting for the circumstances
under which the action happened. When applied to on-the-ball actions like
passes, dribbles, and shots alone, our metric identifies Argentine forward
Lionel Messi, French teenage star Kylian Mbappé, and Belgian winger Eden
Hazard as the most effective players during the 2016/2017 season.
",statistics
"  We found an easy and quick post-learning method named ""Icing on the Cake"" to
enhance a classification performance in deep learning. The method is that we
train only the final classifier again after an ordinary training is done.
",statistics
"  We present Sequential Attend, Infer, Repeat (SQAIR), an interpretable deep
generative model for videos of moving objects. It can reliably discover and
track objects throughout the sequence of frames, and can also generate future
frames conditioning on the current frame, thereby simulating expected motion of
objects. This is achieved by explicitly encoding object presence, locations and
appearances in the latent variables of the model. SQAIR retains all strengths
of its predecessor, Attend, Infer, Repeat (AIR, Eslami et. al., 2016),
including learning in an unsupervised manner, and addresses its shortcomings.
We use a moving multi-MNIST dataset to show limitations of AIR in detecting
overlapping or partially occluded objects, and show how SQAIR overcomes them by
leveraging temporal consistency of objects. Finally, we also apply SQAIR to
real-world pedestrian CCTV data, where it learns to reliably detect, track and
generate walking pedestrians with no supervision.
",statistics
"  The detection of intermediate mass black holes (IMBHs) in Galactic globular
clusters (GCs) has so far been controversial. In order to characterize the
effectiveness of integrated-light spectroscopy through integral field units, we
analyze realistic mock data generated from state-of-the-art Monte Carlo
simulations of GCs with a central IMBH, considering different setups and
conditions varying IMBH mass, cluster distance, and accuracy in determination
of the center. The mock observations are modeled with isotropic Jeans models to
assess the success rate in identifying the IMBH presence, which we find to be
primarily dependent on IMBH mass. However, even for a IMBH of considerable mass
(3% of the total GC mass), the analysis does not yield conclusive results in 1
out of 5 cases, because of shot noise due to bright stars close to the IMBH
line-of-sight. This stochastic variability in the modeling outcome grows with
decreasing BH mass, with approximately 3 failures out of 4 for IMBHs with 0.1%
of total GC mass. Finally, we find that our analysis is generally unable to
exclude at 68% confidence an IMBH with mass of $10^3~M_\odot$ in snapshots
without a central BH. Interestingly, our results are not sensitive to GC
distance within 5-20 kpc, nor to mis-identification of the GC center by less
than 2'' (<20% of the core radius). These findings highlight the value of
ground-based integral field spectroscopy for large GC surveys, where systematic
failures can be accounted for, but stress the importance of discrete kinematic
measurements that are less affected by stochasticity induced by bright stars.
",physics
"  N-polar GaN p-n diodes are realized on single-crystal N-polar GaN bulk wafers
by plasma-assisted molecular beam epitaxy growth. The current-voltage
characteristics show high-quality rectification and electroluminescence
characteristics with a high on/off current ratio and interband photon emission.
The measured electroluminescence spectrum is dominated by strong near-band edge
emission, while deep level luminescence is greatly suppressed. A very low
dislocation density leads to a high reverse breakdown electric field. The low
leakage current N-polar diodes open up several potential applications in
polarization-engineered photonic and electronic devices.
",physics
"  In this note we derive the backward (automatic) differentiation (adjoint
[automatic] differentiation) for an algorithm containing a conditional
expectation operator. As an example we consider the backward algorithm as it is
used in Bermudan product valuation, but the method is applicable in full
generality.
The method relies on three simple properties: 1) a forward or backward
(automatic) differentiation of an algorithm containing a conditional
expectation operator results in a linear combination of the conditional
expectation operators; 2) the differential of an expectation is the expectation
of the differential $\frac{d}{dx} E(Y) = E(\frac{d}{dx}Y)$; 3) if we are only
interested in the expectation of the final result (as we are in all valuation
problems), we may use $E(A \cdot E(B\vert\mathcal{F})) = E(E(A\vert\mathcal{F})
\cdot B)$, i.e., instead of applying the (conditional) expectation operator to
a function of the underlying random variable (continuation values), it may be
applied to the adjoint differential. \end{enumerate}
The methodology not only allows for a very clean and simple implementation,
but also offers the ability to use different conditional expectation estimators
in the valuation and the differentiation.
",computer-science
"  We study the uniqueness of complete biconservative surfaces in the Euclidean
space $\mathbb{R}^3$, and prove that the only complete biconservative regular
surfaces in $\mathbb{R}^3$ are either $CMC$ or certain surfaces of revolution.
In particular, any compact biconservative regular surface in $\mathbb{R}^3$ is
a round sphere.
",mathematics
"  Working over an infinite field of positive characteristic, an upper bound is
given for the nilpotency index of a finitely generated nil algebra of bounded
nil index $n$ in terms of the maximal degree in a minimal homogenous generating
system of the ring of simultaneous conjugation invariants of tuples of $n$ by
$n$ matrices. This is deduced from a result of Zubkov. As a consequence, a
recent degree bound due to Derksen and Makam for the generators of the ring of
matrix invariants yields an upper bound for the nilpotency index of a finitely
generated nil algebra that is polynomial in the number of generators and the
nil index. Furthermore, a characteristic free treatment is given to Kuzmin's
lower bound for the nilpotency index.
",mathematics
"  We reexamine interactions between the dark sectors of cosmology, with a focus
on robust constraints that can be obtained using only mildly nonlinear scales.
While it is well known that couplings between dark matter and dark energy can
be constrained to the percent level when including the full range of scales
probed by future optical surveys, calibrating matter power spectrum emulators
to all possible choices of potentials and couplings requires many
computationally expensive n-body simulations. Here we show that lensing and
clustering of galaxies in combination with the Cosmic Microwave Background
(CMB) is capable of probing the dark sector coupling to the few percent level
for a given class of models, using only linear and quasi-linear Fourier modes.
These scales can, in principle, be described by semi-analytical techniques such
as the effective field theory of large-scale structure.
",physics
"  We present a significantly different reflection process from an optically
thin flat metallic or dielectric layer and propose a strikingly simple method
to form approximately unipolar half-cycle optical pulses via reflection of a
single-cycle optical pulse. Unipolar pulses in reflection arise due to
specifics of effectively one-dimensional pulse propagation. Namely, we show
that in considered system the field emitted by a flat medium layer is
proportional to the velocity of oscillating medium charges instead of their
acceleration as it is usually the case. When the single-cycle pulse interacts
with linear optical medium, the oscillation velocity of medium charges can be
then forced to keep constant sign throughout the pulse duration. Our results
essentially differ from the direct mirror reflection and suggest a possibility
of unusual transformations of the few-cycle light pulses in linear optical
systems.
",physics
"  In this paper, we analyze in depth a simplicial decomposition like
algorithmic framework for large scale convex quadratic programming. In
particular, we first propose two tailored strategies for handling the master
problem. Then, we describe a few techniques for speeding up the solution of the
pricing problem. We report extensive numerical experiments on both real
portfolio optimization and general quadratic programming problems, showing the
efficiency and robustness of the method when compared to Cplex.
",mathematics
"  Let $F_n$ denote the $n^{th}$ term of the Fibonacci sequence. In this paper,
we investigate the Diophantine equation $F_1^p+2F_2^p+\cdots+kF_{k}^p=F_{n}^q$
in the positive integers $k$ and $n$, where $p$ and $q$ are given positive
integers. A complete solution is given if the exponents are included in the set
$\{1,2\}$. Based on the specific cases we could solve, and a computer search
with $p,q,k\le100$ we conjecture that beside the trivial solutions only
$F_8=F_1+2F_2+3F_3+4F_4$, $F_4^2=F_1+2F_2+3F_3$, and
$F_4^3=F_1^3+2F_2^3+3F_3^3$ satisfy the title equation.
",mathematics
"  Independent component analysis (ICA) is a cornerstone of modern data
analysis. Its goal is to recover a latent random vector S with independent
components from samples of X=AS where A is an unknown mixing matrix.
Critically, all existing methods for ICA rely on and exploit strongly the
assumption that S is not Gaussian as otherwise A becomes unidentifiable. In
this paper, we show that in fact one can handle the case of Gaussian components
by imposing structure on the matrix A. Specifically, we assume that A is sparse
and generic in the sense that it is generated from a sparse Bernoulli-Gaussian
ensemble. Under this condition, we give an efficient algorithm to recover the
columns of A given only the covariance matrix of X as input even when S has
several Gaussian components.
",statistics
"  A receiver with perfect channel state information (CSI) in a point-to-point
multiple-input multiple-output (MIMO) channel can compute the transmit
beamforming vector that maximizes the transmission rate. For frequency-division
duplex, a transmitter is not able to estimate CSI directly and has to obtain a
quantized transmit beamforming vector from the receiver via a rate-limited
feedback channel. We assume that time evolution of MIMO channels is modeled as
a Gauss-Markov process parameterized by a temporal-correlation coefficient.
Since feedback rate is usually low, we assume rank-one transmit beamforming or
transmission with single data stream. For given feedback rate, we analyze the
optimal feedback interval that maximizes the average received power of the
systems with two transmit or two receive antennas. For other system sizes, the
optimal feedback interval is approximated by maximizing the rate difference in
a large system limit. Numerical results show that the large system
approximation can predict the optimal interval for finite-size system quite
accurately. Numerical results also show that quantizing transmit beamforming
with the optimal feedback interval gives larger rate than the existing
Kalman-filter scheme does by as much as 10% and than feeding back for every
block does by 44% when the number of feedback bits is small.
",computer-science
"  This article investigates a fast and stable method to solve Henderson's mixed
model equation. The proposed algorithm is stable in that it avoids inverting a
matrix of a large dimension and hence is free from the curse of dimensionality.
This tactic is enabled through row operations performed on the design matrix.
",statistics
"  In this article, we discuss a probabilistic interpretation of McShane's
identity as describing a finite measure on the space of embedded paths though a
point.
",mathematics
"  There has been a recent explosion in applications for dialogue interaction
ranging from direction-giving and tourist information to interactive story
systems. Yet the natural language generation (NLG) component for many of these
systems remains largely handcrafted. This limitation greatly restricts the
range of applications; it also means that it is impossible to take advantage of
recent work in expressive and statistical language generation that can
dynamically and automatically produce a large number of variations of given
content. We propose that a solution to this problem lies in new methods for
developing language generation resources. We describe the ES-Translator, a
computational language generator that has previously been applied only to
fables, and quantitatively evaluate the domain independence of the EST by
applying it to personal narratives from weblogs. We then take advantage of
recent work on language generation to create a parameterized sentence planner
for story generation that provides aggregation operations, variations in
discourse and in point of view. Finally, we present a user evaluation of
different personal narrative retellings.
",computer-science
"  Let $n >3$ and $ 0< k < \frac{n}{2} $ be integers. In this paper, we
investigate some algebraic properties of the line graph of the graph $
{Q_n}(k,k+1) $ where $ {Q_n}(k,k+1) $ is the subgraph of the hypercube $Q_n$
which is induced by the set of vertices of weights $k$ and $k+1$. In the first
step, we determine the automorphism groups of these graphs for all values of
$k$. In the second step, we study Cayley properties of the line graph of these
graphs. In particular, we show that for $ k>2, $ if $ 2k+1 \neq n$, then the
line graph of the graph $ {Q_n}(k,k+1) $ is a vertex-transitive non Cayley
graph. Also, we show that the line graph of the graph $ {Q_n}(1,2) $ is a
Cayley graph if and only if $ n$ is a power of a prime $p$.
",mathematics
"  In this paper we propose a supervised learning system for counting and
localizing palm trees in high-resolution, panchromatic satellite imagery
(40cm/pixel to 1.5m/pixel). A convolutional neural network classifier trained
on a set of palm and no-palm images is applied across a satellite image scene
in a sliding window fashion. The resultant confidence map is smoothed with a
uniform filter. A non-maximal suppression is applied onto the smoothed
confidence map to obtain peaks. Trained with a small dataset of 500 images of
size 40x40 cropped from satellite images, the system manages to achieve a tree
count accuracy of over 99%.
",computer-science
"  An $A_1-A_\infty$ estimate improving a previous result in arXiv:1607.06432 is
obtained. Also new a result in terms of the ${A_\infty}$ constant and the one
supremum $A_q-A_\infty^{\exp}$ constant, is proved, providing a counterpart for
the result obained in arXiv:1705.08364. Both of the preceding results rely upon
a sparse domination in terms of bilinear forms for $[b,T_\Omega]$ with
$\Omega\in L^\infty(\mathbb{S}^{n-1})$ and $b\in BMO$ which is established
relying upon techniques from arXiv:1705.07397.
",mathematics
"  A general greedy approach to construct coverings of compact metric spaces by
metric balls is given and analyzed. The analysis is a continuous version of
Chvatal's analysis of the greedy algorithm for the weighted set cover problem.
The approach is demonstrated in an exemplary manner to construct efficient
coverings of the n-dimensional sphere and n-dimensional Euclidean space to give
short and transparent proofs of several best known bounds obtained from
deterministic constructions in the literature on sphere coverings.
",mathematics
"  We consider a fundamental integer programming (IP) model for cost-benefit
analysis flood protection through dike building in the Netherlands, due to
Verweij and Zwaneveld.
Experimental analysis with data for the Ijsselmeer lead to integral optimal
solution of the linear programming relaxation of the IP model.
This naturally led to the question of integrality of the polytope associated
with the IP model.
In this paper we first give a negative answer to this question by
establishing non-integrality of the polytope.
Second, we establish natural conditions that guarantee the linear programming
relaxation of the IP model to be integral.
We then test the most recent data on flood probabilities, damage and
investment costs of the IJsselmeer for these conditions.
Third, we show that the IP model can be solved in polynomial time when the
number of dike segments, or the number of feasible barrier heights, are
constant.
",quantitative-finance
"  This paper introduces consensus-based primal-dual methods for distributed
online optimization where the time-varying system objective function
$f_t(\mathbf{x})$ is given as the sum of local agents' objective functions,
i.e., $f_t(\mathbf{x}) = \sum_i f_{i,t}(\mathbf{x}_i)$, and the system
constraint function $\mathbf{g}(\mathbf{x})$ is given as the sum of local
agents' constraint functions, i.e., $\mathbf{g}(\mathbf{x}) = \sum_i
\mathbf{g}_i (\mathbf{x}_i) \preceq \mathbf{0}$. At each stage, each agent
commits to an adaptive decision pertaining only to the past and locally
available information, and incurs a new cost function reflecting the change in
the environment. Our algorithm uses weighted averaging of the iterates for each
agent to keep local estimates of the global constraints and dual variables. We
show that the algorithm achieves a regret of order $O(\sqrt{T})$ with the time
horizon $T$, in scenarios when the underlying communication topology is
time-varying and jointly-connected. The regret is measured in regard to the
cost function value as well as the constraint violation. Numerical results for
online routing in wireless multi-hop networks with uncertain channel rates are
provided to illustrate the performance of the proposed algorithm.
",mathematics
"  This paper describes the procedure to estimate the parameters in mean
reversion processes with functional tendency defined by a periodic continuous
deterministic function, expressed as a series of truncated Fourier. Two phases
of estimation are defined, in the first phase through Gaussian techniques using
the Euler-Maruyama discretization, we obtain the maximum likelihood function,
that will allow us to find estimators of the external parameters and an
estimation of the expected value of the process. In the second phase, a
reestimate of the periodic functional tendency with it's parameters of phase
and amplitude is carried out, this will allow, improve the initial estimation.
Some experimental result using simulated data sets are graphically illustrated.
",statistics
"  We prove effective Nullstellensatz and elimination theorems for difference
equations in sequence rings. More precisely, we compute an explicit function of
geometric quantities associated to a system of difference equations (and these
geometric quantities may themselves be bounded by a function of the number of
variables, the order of the equations, and the degrees of the equations) so
that for any system of difference equations in variables $\mathbf{x} = (x_1,
\ldots, x_m)$ and $\mathbf{u} = (u_1, \ldots, u_r)$, if these equations have
any nontrivial consequences in the $\mathbf{x}$ variables, then such a
consequence may be seen algebraically considering transforms up to the order of
our bound. Specializing to the case of $m = 0$, we obtain an effective method
to test whether a given system of difference equations is consistent.
",mathematics
"  Over the past decade, the idea of smart homes has been conceived as a
potential solution to counter energy crises or to at least mitigate its
intensive destructive consequences in the residential building sector.
",computer-science
"  Fast, byte-addressable non-volatile memory (NVM) embraces both near-DRAM
latency and disk-like persistence, which has generated considerable interests
to revolutionize system software stack and programming models. However, it is
less understood how NVM can be combined with managed runtime like Java virtual
machine (JVM) to ease persistence management. This paper proposes Espresso, a
holistic extension to Java and its runtime, to enable Java programmers to
exploit NVM for persistence management with high performance. Espresso first
provides a general persistent heap design called Persistent Java Heap (PJH) to
manage persistent data as normal Java objects. The heap is then strengthened
with a recoverable mechanism to provide crash consistency for heap metadata. It
then provides a new abstraction called Persistent Java Object (PJO) to provide
an easy-to-use but safe persistent programming model for programmers to persist
application data. The evaluation confirms that Espresso significantly
outperforms state-of-art NVM support for Java (i.e., JPA and PCJ) while being
compatible to existing data structures in Java programs.
",computer-science
"  Discrete event simulators, such as OMNeT++, provide fast and convenient
methods for the assessment of algorithms and protocols, especially in the
context of wired and wireless networks. Usually, simulation parameters such as
topology and traffic patterns are predefined to observe the behaviour
reproducibly. However, for learning about the dynamic behaviour of a system, a
live interaction that allows changing parameters on the fly is very helpful.
This is especially interesting for providing interactive demonstrations at
conferences and fairs. In this paper, we present a remote interface to OMNeT++
simulations that can be used to control the simulations while visualising
real-time data merged from multiple OMNeT++ instances. We explain the software
architecture behind our framework and how it can be used to build
demonstrations on the foundation of OMNeT++.
",computer-science
"  We study the action of monads on categories equipped with several monoidal
structures. We identify the structure and conditions that guarantee that the
higher monoidal structure is inherited by the category of algebras over the
monad. Monoidal monads and comonoidal monads appear as the base cases in this
hierarchy. Monads acting on duoidal categories constitute the next case. We
cover the general case of $n$-monoidal categories and discuss several naturally
occurring examples in which $n\leq 3$.
",mathematics
"  We present a multimodal non-linear optical (NLO) laser-scanning microscope,
based on a compact fiber-format excitation laser and integrating coherent
anti-Stokes Raman scattering (CARS), stimulated Raman scattering (SRS) and
two-photon-excitation fluorescence (TPEF) on a single platform. We demonstrate
its capabilities in simultaneously acquiring CARS and SRS images of a blend of
6-{\mu}m poly(methyl methacrylate) beads and 3-{\mu}m polystyrene beads. We
then apply it to visualize cell walls and chloroplast of an unprocessed fresh
leaf of Elodea aquatic plant via SRS and TPEF modalities, respectively. The
presented NLO microscope, developed in house using off-the-shelf components,
offers full accessibility to the optical path and ensures its easy
re-configurability and flexibility.
",physics
"  Learning high quality class representations from few examples is a key
problem in metric-learning approaches to few-shot learning. To accomplish this,
we introduce a novel architecture where class representations are conditioned
for each few-shot trial based on a target image. We also deviate from
traditional metric-learning approaches by training a network to perform
comparisons between classes rather than relying on a static metric comparison.
This allows the network to decide what aspects of each class are important for
the comparison at hand. We find that this flexible architecture works well in
practice, achieving state-of-the-art performance on the Caltech-UCSD birds
fine-grained classification task.
",statistics
"  In this paper, we theoretically study x-ray multiphoton ionization dynamics
of heavy atoms taking into account relativistic and resonance effects. When an
atom is exposed to an intense x-ray pulse generated by an x-ray free-electron
laser (XFEL), it is ionized to a highly charged ion via a sequence of
single-photon ionization and accompanying relaxation processes, and its final
charge state is limited by the last ionic state that can be ionized by a
single-photon ionization. If x-ray multiphoton ionization involves deep
inner-shell electrons in heavy atoms, energy shifts by relativistic effects
play an important role in ionization dynamics, as pointed out in [Phys.\ Rev.\
Lett.\ \textbf{110}, 173005 (2013)]. On the other hand, if the x-ray beam has a
broad energy bandwidth, the high-intensity x-ray pulse can drive resonant
photo-excitations for a broad range of ionic states and ionize even beyond the
direct one-photon ionization limit, as first proposed in [Nature\ Photon.\
\textbf{6}, 858 (2012)]. To investigate both relativistic and resonance
effects, we extend the \textsc{xatom} toolkit to incorporate relativistic
energy corrections and resonant excitations in x-ray multiphoton ionization
dynamics calculations. Charge-state distributions are calculated for Xe atoms
interacting with intense XFEL pulses at a photon energy of 1.5~keV and 5.5~keV,
respectively. For both photon energies, we demonstrate that the role of
resonant excitations in ionization dynamics is altered due to significant
shifts of orbital energy levels by relativistic effects. Therefore it is
necessary to take into account both effects to accurately simulate multiphoton
multiple ionization dynamics at high x-ray intensity.
",physics
"  We formulate a quasiclassical theory ($\omega_c\tau \lesssim 1$ with
$\omega_c$ as the cyclotron frequency and $\tau$ as the relaxation time) to
study the influence of magnetic field on electron-impurity scattering process
in the two-dimensional electron gas. We introduce a general recipe based on an
abstraction of the detailed impurity scattering process to define the
scattering parameter such as the incoming and outgoing momentum and coordinate
jump. In this picture, we can conveniently describe the skew scattering and
coordinate jump, which will eventually modify the Boltzmann equation. We find
an anomalous Hall resistivity different from the conventional Boltzmann-Drude
result and a negative magnetoresistivity parabolic in magnetic field. The
origin of these results has been analyzed. The relevance between our theory and
recent simulation and experimental works is also discussed. Our theory
dominates in dilute impurity system where the correlation effect is negligible.
",physics
"  We consider the problem of recovering a $d-$dimensional manifold $\mathcal{M}
\subset \mathbb{R}^n$ when provided with noiseless samples from $\mathcal{M}$.
There are many algorithms (e.g., Isomap) that are used in practice to fit
manifolds and thus reduce the dimensionality of a given data set. Ideally, the
estimate $\mathcal{M}_\mathrm{put}$ of $\mathcal{M}$ should be an actual
manifold of a certain smoothness; furthermore, $\mathcal{M}_\mathrm{put}$
should be arbitrarily close to $\mathcal{M}$ in Hausdorff distance given a
large enough sample. Generally speaking, existing manifold learning algorithms
do not meet these criteria. Fefferman, Mitter, and Narayanan (2016) have
developed an algorithm whose output is provably a manifold. The key idea is to
define an approximate squared-distance function (asdf) to $\mathcal{M}$. Then,
$\mathcal{M}_\mathrm{put}$ is given by the set of points where the gradient of
the asdf is orthogonal to the subspace spanned by the largest $n - d$
eigenvectors of the Hessian of the asdf. As long as the asdf meets certain
regularity conditions, $\mathcal{M}_\mathrm{put}$ is a manifold that is
arbitrarily close in Hausdorff distance to $\mathcal{M}$. In this paper, we
define two asdfs that can be calculated from the data and show that they meet
the required regularity conditions. The first asdf is based on kernel density
estimation, and the second is based on estimation of tangent spaces using local
principal components analysis.
",statistics
"  Neural machine translation models rely on the beam search algorithm for
decoding. In practice, we found that the quality of hypotheses in the search
space is negatively affected owing to the fixed beam size. To mitigate this
problem, we store all hypotheses in a single priority queue and use a universal
score function for hypothesis selection. The proposed algorithm is more
flexible as the discarded hypotheses can be revisited in a later step. We
further design a penalty function to punish the hypotheses that tend to produce
a final translation that is much longer or shorter than expected. Despite its
simplicity, we show that the proposed decoding algorithm is able to select
hypotheses with better qualities and improve the translation performance.
",computer-science
"  We are concerned with the inverse scattering problem of recovering an
inhomogeneous medium by the associated acoustic wave measurement. We prove that
under certain assumptions, a single far-field pattern determines the values of
a perturbation to the refractive index on the corners of its support. These
assumptions are satisfied for example in the low acoustic frequency regime. As
a consequence if the perturbation is piecewise constant with either a
polyhedral nest geometry or a known polyhedral cell geometry, such as a pixel
or voxel array, we establish the injectivity of the perturbation to far-field
map given a fixed incident wave. This is the first unique determinancy result
of its type in the literature, and all of the existing results essentially make
use of infinitely many measurements.
",mathematics
"  In the typical framework for boolean games (BG) each player can change the
truth value of some propositional atoms, while attempting to make her goal
true. In standard BG goals are propositional formulas, whereas in iterated BG
goals are formulas of Linear Temporal Logic. Both notions of BG are
characterised by the fact that agents have exclusive control over their set of
atoms, meaning that no two agents can control the same atom. In the present
contribution we drop the exclusivity assumption and explore structures where an
atom can be controlled by multiple agents. We introduce Concurrent Game
Structures with Shared Propositional Control (CGS-SPC) and show that they ac-
count for several classes of repeated games, including iterated boolean games,
influence games, and aggregation games. Our main result shows that, as far as
verification is concerned, CGS-SPC can be reduced to concurrent game structures
with exclusive control. This result provides a polynomial reduction for the
model checking problem of specifications in Alternating-time Temporal Logic on
CGS-SPC.
",computer-science
"  We study coupled motion of a 1-D closed elastic string immersed in a 2-D
Stokes flow, known as the Stokes immersed boundary problem in two dimensions.
Using the fundamental solution of the Stokes equation and the Lagrangian
coordinate of the string, we write the problem into a contour dynamic
formulation, which is a nonlinear non-local equation solely keeping track of
evolution of the string configuration. We prove existence and uniqueness of
local-in-time solution starting from an arbitrary initial configuration that is
an $H^{5/2}$-function in the Lagrangian coordinate satisfying the so-called
well-stretched assumption. We also prove that when the initial string
configuration is sufficiently close to an equilibrium, which is an evenly
parameterized circular configuration, then global-in-time solution uniquely
exists and it will converge to an equilibrium configuration exponentially as
$t\rightarrow +\infty$. The technique in this paper may also apply to the
Stokes immersed boundary problem in three dimensions.
",mathematics
"  In this article, we propose a new class of priors for Bayesian inference with
multiple Gaussian graphical models. We introduce fully Bayesian treatments of
two popular procedures, the group graphical lasso and the fused graphical
lasso, and extend them to a continuous spike-and-slab framework to allow
self-adaptive shrinkage and model selection simultaneously. We develop an EM
algorithm that performs fast and dynamic explorations of posterior modes. Our
approach selects sparse models efficiently with substantially smaller bias than
would be induced by alternative regularization procedures. The performance of
the proposed methods are demonstrated through simulation and two real data
examples.
",statistics
"  We train multi-task autoencoders on linguistic tasks and analyze the learned
hidden sentence representations. The representations change significantly when
translation and part-of-speech decoders are added. The more decoders a model
employs, the better it clusters sentences according to their syntactic
similarity, as the representation space becomes less entangled. We explore the
structure of the representation space by interpolating between sentences, which
yields interesting pseudo-English sentences, many of which have recognizable
syntactic structure. Lastly, we point out an interesting property of our
models: The difference-vector between two sentences can be added to change a
third sentence with similar features in a meaningful way.
",statistics
"  Building a voice conversion (VC) system from non-parallel speech corpora is
challenging but highly valuable in real application scenarios. In most
situations, the source and the target speakers do not repeat the same texts or
they may even speak different languages. In this case, one possible, although
indirect, solution is to build a generative model for speech. Generative models
focus on explaining the observations with latent variables instead of learning
a pairwise transformation function, thereby bypassing the requirement of speech
frame alignment. In this paper, we propose a non-parallel VC framework with a
variational autoencoding Wasserstein generative adversarial network (VAW-GAN)
that explicitly considers a VC objective when building the speech model.
Experimental results corroborate the capability of our framework for building a
VC system from unaligned data, and demonstrate improved conversion quality.
",computer-science
"  The attention for personalized mental health care is thriving. Research data
specific to the individual, such as time series sensor data or data from
intensive longitudinal studies, is relevant from a research perspective, as
analyses on these data can reveal the heterogeneity among the participants and
provide more precise and individualized results than with group-based methods.
However, using this data for self-management and to help the individual to
improve his or her mental health has proven to be challenging.
The present work describes a novel approach to automatically generate
personalized advice for the improvement of the well-being of individuals by
using time series data from intensive longitudinal studies: Automated Impulse
Response Analysis (AIRA). AIRA analyzes vector autoregression models of
well-being by generating impulse response functions. These impulse response
functions are used in simulations to determine which variables in the model
have the largest influence on the other variables and thus on the well-being of
the participant. The effects found can be used to support self-management.
We demonstrate the practical usefulness of AIRA by performing analysis on
longitudinal self-reported data about psychological variables. To evaluate its
effectiveness and efficacy, we ran its algorithms on two data sets ($N=4$ and
$N=5$), and discuss the results. Furthermore, we compare AIRA's output to the
results of a previously published study and show that the results are
comparable. By automating Impulse Response Function Analysis, AIRA fulfills the
need for accurate individualized models of health outcomes at a low resource
cost with the potential for upscaling.
",computer-science
"  Gallium arsenide (GaAs) is the widest used second generation semiconductor
with a direct band gap and increasingly used as nanofilms. However, the
magnetic properties of GaAs nanofilms have never been studied. Here we find by
comprehensive density functional theory calculations that GaAs nanofilms
cleaved along the <111> and <100> directions become intrinsically metallic
films with strong surface magnetism and magnetoelectric (ME) effect. The
surface magnetism and electrical conductivity are realized via a combined
effect of transferring charge induced by spontaneous electric-polarization
through the film thickness and spin-polarized surface states. The surface
magnetism of <111> nanofilms can be significantly and linearly tuned by
vertically applied electric field, endowing the nanofilms unexpectedly high ME
coefficients, which are tens of times higher than those of ferromagnetic metals
and transition metal oxides.
",physics
"  We show that the tensor product $A\otimes B$ over $\mathbb{C}$ of two $C^*
$-algebras satisfying the \textit{NCDL} conditions has again the same property.
We use this result to describe the $C^* $-algebra of the Heisenberg motion
groups $G_n = \mathbb{T}^n \ltimes \mathbb{H}_n$ as algebra of operator fields
defined over the spectrum of $G_n $.
",mathematics
"  The problem of construction a quantum mechanical evolution for the
Schrodinger equation with a degenerate Hamiltonian which is a symmetric
operator that does not have self-adjoint extensions is considered. Self-adjoint
regularization of the Hamiltonian does not lead to a preserving probability
limiting evolution for vectors from the Hilbert space but it is used to
construct a limiting evolution of states on a C*-algebra of compact operators
and on an abelian subalgebra of operators in the Hilbert space. The limiting
evolution of the states on the abelian algebra can be presented by the Kraus
decomposition with two terms. Both of this terms are corresponded to the
unitary and shift components of Wold's decomposition of isometric semigroup
generated by the degenerate Hamiltonian. Properties of the limiting evolution
of the states on the C*-algebras are investigated and it is shown that pure
states could evolve into mixed states.
",mathematics
"  Knotted solutions to electromagnetism and fluid dynamics are investigated,
based on relations we find between the two subjects. We can write fluid
dynamics in electromagnetism language, but only on an initial surface, or for
linear perturbations, and we use this map to find knotted fluid solutions, as
well as new electromagnetic solutions. We find that knotted solutions of
Maxwell electromagnetism are also solutions of more general nonlinear theories,
like Born-Infeld, and including ones which contain quantum corrections from
couplings with other modes, like Euler-Heisenberg and string theory DBI. Null
configurations in electromagnetism can be described as a null pressureless
fluid, and from this map we can find null fluid knotted solutions. A type of
nonrelativistic reduction of the relativistic fluid equations is described,
which allows us to find also solutions of the (nonrelativistic) Euler's
equations.
",physics
"  What if someone built a ""box"" that applies quantum superposition not just to
quantum bits in the microscopic but also to macroscopic everyday ""objects"",
such as Schrödinger's cat or a human being? If that were possible, and if the
different ""copies"" of a man could exploit quantum interference to synchronize
and collapse into their preferred state, then one (or they?) could in a sense
choose their future, win the lottery, break codes and other security devices,
and become king of the world, or actually of the many-worlds. We set up the
plot-line of a new episode of Black Mirror to reflect on what might await us if
one were able to build such a technology.
",computer-science
"  This paper demonstrates end-to-end neural network architectures for
Vietnamese named entity recognition. Our best model is a combination of
bidirectional Long Short-Term Memory (Bi-LSTM), Convolutional Neural Network
(CNN), Conditional Random Field (CRF), using pre-trained word embeddings as
input, which achieves an F1 score of 88.59% on a standard test set. Our system
is able to achieve a comparable performance to the first-rank system of the
VLSP campaign without using any syntactic or hand-crafted features. We also
give an extensive empirical study on using common deep learning models for
Vietnamese NER, at both word and character level.
",computer-science
"  We present an experimental study of the local and collective magnetism of
$\mathrm{EuFe_2As_2}$, that is isostructural with the high temperature
superconductor parent compound $\mathrm{BaFe_2As_2}$. In contrast to
$\mathrm{BaFe_2As_2}$, where only Fe spins order, $\mathrm{EuFe_2As_2}$ has an
additional magnetic transition below 20 K due to the ordering of the Eu$^{2+}$
spins ($J =7/2$, with $L=0$ and $S=7/2$) in an A-type antiferromagnetic texture
(ferromagnetic layers stacked antiferromagnetically). This may potentially
affect the FeAs layer and its local and correlated magnetism. Fe-K$_\beta$
x-ray emission experiments on $\mathrm{EuFe_2As_2}$ single crystals reveal a
local magnetic moment of 1.3$\pm0.15~\mu_B$ at 15 K that slightly increases to
1.45$\pm0.15~\mu_B$ at 300 K. Resonant inelastic x-ray scattering (RIXS)
experiments performed on the same crystals show dispersive broad (in energy)
magnetic excitations along $\mathrm{(0, 0)\rightarrow(1, 0)}$ and $\mathrm{(0,
0)\rightarrow(1, 1)}$ with a bandwidth on the order of 170-180 meV. These
results on local and collective magnetism are in line with other parent
compounds of the $\mathrm{AFe_2As_2}$ series ($A=$ Ba, Ca, and Sr), especially
the well characterized $\mathrm{BaFe_2As_2}$. Thus, our experiments lead us to
the conclusion that the effect of the high magnetic moment of Eu on the
magnitude of both Fe local magnetic moment and spin excitations is small and
confined to low energy excitations.
",physics
"  In the first part of this paper we will prove the Voevodsky's nilpotence
conjecture for smooth cubic fourfolds and ordinary generic Gushel-Mukai
fourfolds. Then, making use of noncommutative motives, we will prove the
Voevodsky's nilpotence conjecture for generic Gushel-Mukai fourfolds containing
a $\tau$-plane $\G(2,3)$ and for ordinary Gushel-Mukai fourfolds containing a
quintic del Pezzo surface.
",mathematics
"  Using density-functional theory calculations, we analyze the optical
absorption properties of lead (Pb)-free metal halide perovskites
(AB$^{2+}$X$_3$) and double perovskites (AB$^+$B$^{3+}$X$_6$) (A = Cs or
monovalent organic ion, B$^{2+}$ = non-Pb divalent metal, B$^+$ = monovalent
metal, B$^{3+}$ = trivalent metal, X = halogen). We show that, if B$^{2+}$ is
not Sn or Ge, Pb-free metal halide perovskites exhibit poor optical absorptions
because of their indirect bandgap nature. Among the nine possible types of
Pb-free metal halide double perovskites, six have direct bandgaps. Of these six
types, four show inversion symmetry-induced parity-forbidden or weak
transitions between band edges, making them not ideal for thin-film solar cell
application. Only one type of Pb-free double perovskite shows optical
absorption and electronic properties suitable for solar cell applications,
namely those with B$^+$ = In, Tl and B$^{3+}$ = Sb, Bi. Our results provide
important insights for designing new metal halide perovskites and double
perovskites for optoelectronic applications.
",physics
"  TUS is the world's first orbital detector of extreme energy cosmic rays
(EECRs), which operates as a part of the scientific payload of the Lomonosov
satellite since May 19, 2016. TUS employs the nocturnal atmosphere of the Earth
to register ultraviolet (UV) fluorescence and Cherenkov radiation from
extensive air showers generated by EECRs as well as UV radiation from lightning
strikes and transient luminous events, micro-meteors and space debris. The
first months of its operation in orbit have demonstrated an unexpectedly rich
variety of UV radiation in the atmosphere. We briefly review the design of TUS
and present a few examples of events recorded in a mode dedicated to
registering EECRs.
",physics
"  Let $k$ be a fixed integer. We determine the complexity of finding a
$p$-partition $(V_1, \dots, V_p)$ of the vertex set of a given digraph such
that the maximum out-degree of each of the digraphs induced by $V_i$, ($1\leq
i\leq p$) is at least $k$ smaller than the maximum out-degree of $D$. We show
that this problem is polynomial-time solvable when $p\geq 2k$ and ${\cal
NP}$-complete otherwise. The result for $k=1$ and $p=2$ answers a question
posed in \cite{bangTCS636}. We also determine, for all fixed non-negative
integers $k_1,k_2,p$, the complexity of deciding whether a given digraph of
maximum out-degree $p$ has a $2$-partition $(V_1,V_2)$ such that the digraph
induced by $V_i$ has maximum out-degree at most $k_i$ for $i\in [2]$. It
follows from this characterization that the problem of deciding whether a
digraph has a 2-partition $(V_1,V_2)$ such that each vertex $v\in V_i$ has at
least as many neighbours in the set $V_{3-i}$ as in $V_i$, for $i=1,2$ is
${\cal NP}$-complete. This solves a problem from \cite{kreutzerEJC24} on
majority colourings.
",computer-science
"  Observations of the CMB today allow us to answer detailed questions about the
properties of our Universe, targeting both standard and non-standard physics.
In this paper, we study the effects of varying fundamental constants (i.e., the
fine-structure constant, $\alpha_{\rm EM}$, and electron rest mass, $m_{\rm
e}$) around last scattering using the recombination codes CosmoRec and
Recfast++. We approach the problem in a pedagogical manner, illustrating the
importance of various effects on the free electron fraction, Thomson visibility
function and CMB power spectra, highlighting various degeneracies. We
demonstrate that the simpler Recfast++ treatment (based on a three-level atom
approach) can be used to accurately represent the full computation of CosmoRec.
We also include explicit time-dependent variations using a phenomenological
power-law description. We reproduce previous Planck 2013 results in our
analysis. Assuming constant variations relative to the standard values, we find
the improved constraints $\alpha_{\rm EM}/\alpha_{\rm EM,0}=0.9993\pm 0.0025$
(CMB only) and $m_{\rm e}/m_{\rm e,0}= 1.0039 \pm 0.0074$ (including BAO) using
Planck 2015 data. For a redshift-dependent variation, $\alpha_{\rm
EM}(z)=\alpha_{\rm EM}(z_0)\,[(1+z)/1100]^p$ with $\alpha_{\rm
EM}(z_0)\equiv\alpha_{\rm EM,0}$ at $z_0=1100$, we obtain $p=0.0008\pm 0.0025$.
Allowing simultaneous variations of $\alpha_{\rm EM}(z_0)$ and $p$ yields
$\alpha_{\rm EM}(z_0)/\alpha_{\rm EM,0} = 0.9998\pm 0.0036$ and $p = 0.0006\pm
0.0036$. We also discuss combined limits on $\alpha_{\rm EM}$ and $m_{\rm e}$.
Our analysis shows that existing data is not only sensitive to the value of the
fundamental constants around recombination but also its first time derivative.
This suggests that a wider class of varying fundamental constant models can be
probed using the CMB.
",physics
"  Winds are predicted to be ubiquitous in low-mass, actively star-forming
galaxies. Observationally, winds have been detected in relatively few local
dwarf galaxies, with even fewer constraints placed on their timescales. Here,
we compare galactic outflows traced by diffuse, soft X-ray emission from
Chandra Space Telescope archival observations to the star formation histories
derived from Hubble Space Telescope imaging of the resolved stellar populations
in six starburst dwarfs. We constrain the longevity of a wind to have an upper
limit of 25 Myr based on galaxies whose starburst activity has already
declined, although a larger sample is needed to confirm this result. We find an
average 16% efficiency for converting the mechanical energy of stellar feedback
to thermal, soft X-ray emission on the 25 Myr timescale, somewhat higher than
simulations predict. The outflows have likely been sustained for timescales
comparable to the duration of the starbursts (i.e., 100's Myr), after taking
into account the time for the development and cessation of the wind. The wind
timescales imply that material is driven to larger distances in the
circumgalactic medium than estimated by assuming short, 5-10 Myr starburst
durations, and that less material is recycled back to the host galaxy on short
timescales. In the detected outflows, the expelled hot gas shows various
morphologies which are not consistent with a simple biconical outflow
structure. The sample and analysis are part of a larger program, the STARBurst
IRregular Dwarf Survey (STARBIRDS), aimed at understanding the lifecycle and
impact of starburst activity in low-mass systems.
",physics
"  We experimentally confirmed the threshold behavior and scattering length
scaling law of the three-body loss coefficients in an ultracold spin-polarized
gas of $^6$Li atoms near a $p$-wave Feshbach resonance. We measured the
three-body loss coefficients as functions of temperature and scattering volume,
and found that the threshold law and the scattering length scaling law hold in
limited temperature and magnetic field regions. We also found that the
breakdown of the scaling laws is due to the emergence of the effective-range
term. This work is an important first step toward full understanding of the
loss of identical fermions with $p$-wave interactions.
",physics
"  We develop a quantitative theory of stochastic homogenization for linear,
uniformly parabolic equations with coefficients depending on space and time.
Inspired by recent works in the elliptic setting, our analysis is focused on
certain subadditive quantities derived from a variational interpretation of
parabolic equations. These subadditive quantities are intimately connected to
spatial averages of the fluxes and gradients of solutions. We implement a
renormalization-type scheme to obtain an algebraic rate for their convergence,
which is essentially a quantification of the weak convergence of the gradients
and fluxes of solutions to their homogenized limits. As a consequence, we
obtain estimates of the homogenization error for the Cauchy-Dirichlet problem
which are optimal in stochastic integrability. We also develop a higher
regularity theory for solutions of the heterogeneous equation, including a
uniform $C^{0,1}$-type estimate and a Liouville theorem of every finite order.
",mathematics
"  In this work, we jointly address the problem of text detection and
recognition in natural scene images based on convolutional recurrent neural
networks. We propose a unified network that simultaneously localizes and
recognizes text with a single forward pass, avoiding intermediate processes
like image cropping and feature re-calculation, word separation, or character
grouping. In contrast to existing approaches that consider text detection and
recognition as two distinct tasks and tackle them one by one, the proposed
framework settles these two tasks concurrently. The whole framework can be
trained end-to-end, requiring only images, the ground-truth bounding boxes and
text labels. Through end-to-end training, the learned features can be more
informative, which improves the overall performance. The convolutional features
are calculated only once and shared by both detection and recognition, which
saves processing time. Our proposed method has achieved competitive performance
on several benchmark datasets.
",computer-science
"  Disk migration and high-eccentricity migration are two well-studied theories
to explain the formation of hot Jupiters. The former predicts that these
planets can migrate up until the planet-star Roche separation ($a_{Roche}$) and
the latter predicts they will tidally circularize at a minimum distance of
2$a_{Roche}$. Considering long-running radial velocity and transit surveys have
identified a couple hundred hot Jupiters to date, we can revisit the classic
question of hot Jupiter formation in a data-driven manner. We approach this
problem using data from several exoplanet surveys (radial velocity, Kepler,
HAT, and WASP) allowing for either a single population or a mixture of
populations associated with these formation channels, and applying a
hierarchical Bayesian mixture model of truncated power laws of the form
$x^{\gamma-1}$ to constrain the population-level parameters of interest (e.g.,
location of inner edges, $\gamma$, mixture fractions). Within the limitations
of our chosen models, we find the current radial velocity and Kepler sample of
hot Jupiters can be well explained with a single truncated power law
distribution with a lower cutoff near 2$a_{Roche}$, a result that still holds
after a decade, and $\gamma=-0.51\pm^{0.19}_{0.20}$. However, the HAT and WASP
data show evidence for multiple populations (Bayes factor $\approx 10^{21}$).
We find that $15\pm^{9}_{6}\%$ reside in a component consistent with disk
migration ($\gamma=-0.04\pm^{0.53}_{1.27}$) and $85\pm^{6}_{9}\%$ in one
consistent with high-eccentricity migration ($\gamma=-1.38\pm^{0.32}_{0.47}$).
We find no immediately strong connections with some observed host star
properties and speculate on how future exoplanet surveys could improve upon hot
Jupiter population inference.
",physics
"  Transfer learning has the potential to reduce the burden of data collection
and to decrease the unavoidable risks of the training phase. In this letter, we
introduce a multirobot, multitask transfer learning framework that allows a
system to complete a task by learning from a few demonstrations of another task
executed on another system. We focus on the trajectory tracking problem where
each trajectory represents a different task, since many robotic tasks can be
described as a trajectory tracking problem. The proposed multirobot transfer
learning framework is based on a combined $\mathcal{L}_1$ adaptive control and
an iterative learning control approach. The key idea is that the adaptive
controller forces dynamically different systems to behave as a specified
reference model. The proposed multitask transfer learning framework uses
theoretical control results (e.g., the concept of vector relative degree) to
learn a map from desired trajectories to the inputs that make the system track
these trajectories with high accuracy. This map is used to calculate the inputs
for a new, unseen trajectory. Experimental results using two different
quadrotor platforms and six different trajectories show that, on average, the
proposed framework reduces the first-iteration tracking error by 74% when
information from tracking a different single trajectory on a different
quadrotor is utilized.
",computer-science
"  These are reminiscences of my interactions with Julian Schwinger from 1968
through 1981 and beyond.
",physics
"  Data diversity is critical to success when training deep learning models.
Medical imaging data sets are often imbalanced as pathologic findings are
generally rare, which introduces significant challenges when training deep
learning models. In this work, we propose a method to generate synthetic
abnormal MRI images with brain tumors by training a generative adversarial
network using two publicly available data sets of brain MRI. We demonstrate two
unique benefits that the synthetic images provide. First, we illustrate
improved performance on tumor segmentation by leveraging the synthetic images
as a form of data augmentation. Second, we demonstrate the value of generative
models as an anonymization tool, achieving comparable tumor segmentation
results when trained on the synthetic data versus when trained on real subject
data. Together, these results offer a potential solution to two of the largest
challenges facing machine learning in medical imaging, namely the small
incidence of pathological findings, and the restrictions around sharing of
patient data.
",statistics
"  Thanks to multi-spacecraft mission, it has recently been possible to directly
estimate the current density in space plasmas, by using magnetic field time
series from four satellites flying in a quasi perfect tetrahedron
configuration. The technique developed, commonly called 'curlometer' permits a
good estimation of the current density when the magnetic field time series vary
linearly in space. This approximation is generally valid for small spacecraft
separation. The recent space missions Cluster and Magnetospheric Multiscale
(MMS) have provided high resolution measurements with inter-spacecraft
separation up to 100 km and 10 km, respectively. The former scale corresponds
to the proton gyroradius/ion skin depth in 'typical' solar wind conditions,
while the latter to sub-proton scale. However, some works have highlighted an
underestimation of the current density via the curlometer technique with
respect to the current computed directly from the velocity distribution
functions, measured at sub-proton scales resolution with MMS. In this paper we
explore the limit of the curlometer technique studying synthetic data sets
associated to a cluster of four artificial satellites allowed to fly in a
static turbulent field, spanning a wide range of relative separation. This
study tries to address the relative importance of measuring plasma moments at
very high resolution from a single spacecraft with respect to the
multi-spacecraft missions in the current density evaluation.
",physics
"  Sharir and Welzl [1] derived a bound on crossing-free matchings primarily
based on solving a recurrence based on the size of the matchings. We show that
the recurrence given in Lemma 2.3 in Sharir and Welzl can be improve to
$(2n-6s)\textbf{Ma}_{m}(P)\leq\frac{68}{3}(s+2)\textbf{Ma}_{m-1}(P)$ and
$(3n-7s)\textbf{Ma}_{m}(P)\leq44.5(s+2)\textbf{Ma}_{m-1}(P)$, thereby improving
the upper bound for crossing-free matchings.
",mathematics
"  The usability of small devices such as smartphones or interactive watches is
often hampered by the limited size of command vocabularies. This paper is an
attempt at better understanding how finger identification may help users invoke
commands on touch screens, even without recourse to multi-touch input. We
describe how finger identification can increase the size of input vocabularies
under the constraint of limited real estate, and we discuss some visual cues to
communicate this novel modality to novice users. We report a controlled
experiment that evaluated, over a large range of input-vocabulary sizes, the
efficiency of single-touch command selections with vs. without finger
identification. We analyzed the data not only in terms of traditional time and
error metrics, but also in terms of a throughput measure based on Shannon's
theory, which we show offers a synthetic and parsimonious account of users'
performance. The results show that the larger the input vocabulary needed by
the designer, the more promising the identification of individual fingers.
",computer-science
"  In this work, we study the extent to which structural connectomes and
topological derivative measures are unique to individual changes within human
brains. To do so, we classify structural connectome pairs from two large
longitudinal datasets as either belonging to the same individual or not. Our
data is comprised of 227 individuals from the Alzheimer's Disease Neuroimaging
Initiative (ADNI) and 226 from the Parkinson's Progression Markers Initiative
(PPMI). We achieve 0.99 area under the ROC curve score for features which
represent either weights or network structure of the connectomes (node degrees,
PageRank and local efficiency). Our approach may be useful for eliminating
noisy features as a preprocessing step in brain aging studies and early
diagnosis classification problems.
",computer-science
"  Recent observations of lensed galaxies at cosmological distances have
detected individual stars that are extremely magnified when crossing the
caustics of lensing clusters. In idealized cluster lenses with smooth mass
distributions, two images of a star of radius $R$ approaching a caustic
brighten as $t^{-1/2}$ and reach a peak magnification $\sim 10^{6}\, (10\,
R_{\odot}/R)^{1/2}$ before merging on the critical curve. We show that a mass
fraction ($\kappa_\star \gtrsim \, 10^{-4.5}$) in microlenses inevitably
disrupts the smooth caustic into a network of corrugated microcaustics, and
produces light curves with numerous peaks. Using analytical calculations and
numerical simulations, we derive the characteristic width of the network,
caustic-crossing frequencies, and peak magnifications. For the lens parameters
of a recent detection and a population of intracluster stars with $\kappa_\star
\sim 0.01$, we find a source-plane width of $\sim 20 \, {\rm pc}$ for the
caustic network, which spans $0.2 \, {\rm arcsec}$ on the image plane. A source
star takes $\sim 2\times 10^4$ years to cross this width, with a total of $\sim
6 \times 10^4$ crossings, each one lasting for $\sim 5\,{\rm
hr}\,(R/10\,R_\odot)$ with typical peak magnifications of $\sim 10^{4} \left(
R/ 10\,R_\odot \right)^{-1/2}$. The exquisite sensitivity of caustic-crossing
events to the granularity of the lens-mass distribution makes them ideal probes
of dark matter components, such as compact halo objects and ultralight axion
dark matter.
",physics
"  We introduce an inhomogeneous bosonic mixture composed of two kinds of
hard-core and semi-hard-core bosons with different nilpotency conditions and
demonstrate that in contrast with the standard hard-core Bose-Hubbard model,
our bosonic mixture with nearest- and next-nearest-neighbor interactions on a
square lattice develops the checkerboard supersolid phase characterized by the
simultaneous superfluid and checkerboard solid orders. Our bosonic mixture is
created from a two-orbital Bose-Hubbard model including two kinds of bosons: a
single-orbital boson and a two-orbital boson. By mapping the bosonic mixture to
an anisotropic inhomogeneous spin model in the presence of a magnetic field, we
study the ground-state phase diagram of the model by means of cluster mean
field theory and linear spin-wave theory and show that various phases such as
solid, superfluid, supersolid, and Mott insulator appear in the phase diagram
of the mixture. Competition between the interactions and magnetic field causes
the mixture to undergo different kinds of first- and second-order phase
transitions. By studying the behavior of the spin-wave excitations, we find the
reasons of all first- and second-order phase transitions. We also obtain the
temperature phase diagram of the system using cluster mean field theory. We
show that the checkerboard supersolid phase persists at finite temperature
comparable with the interaction energies of bosons.
",physics
"  A photodetector may be characterized by various figures of merit such as
response time, bandwidth, dark count rate, efficiency, wavelength resolution,
and photon-number resolution. On the other hand, quantum theory says that any
measurement device is fully described by its POVM, which stands for
Positive-Operator-Valued Measure, and which generalizes the textbook notion of
the eigenstates of the appropriate hermitian operator (the ""observable"") as
measurement outcomes. Here we show how to define a multitude of photodetector
figures of merit in terms of a given POVM. We distinguish classical and quantum
figures of merit and issue a conjecture regarding trade-off relations between
them. We discuss the relationship between POVM elements and photodetector
clicks, and how models of photodetectors may be tested by measuring either POVM
elements or figures of merit. Finally, the POVM is advertised as a
platform-independent way of comparing different types of photodetectors, since
any such POVM refers to the Hilbert space of the incoming light, and not to any
Hilbert space internal to the detector.
",physics
"  To improve the efficiency of elderly assessments, an influence-based fast
preceding questionnaire model (FPQM) is proposed. Compared with traditional
assessments, the FPQM optimizes questionnaires by reordering their attributes.
The values of low-ranking attributes can be predicted by the values of the
high-ranking attributes. Therefore, the number of attributes can be reduced
without redesigning the questionnaires. A new function for calculating the
influence of the attributes is proposed based on probability theory. Reordering
and reducing algorithms are given based on the attributes' influences. The
model is verified through a practical application. The practice in an
elderly-care company shows that the FPQM can reduce the number of attributes by
90.56% with a prediction accuracy of 98.39%. Compared with other methods, such
as the Expert Knowledge, Rough Set and C4.5 methods, the FPQM achieves the best
performance. In addition, the FPQM can also be applied to other questionnaires.
",computer-science
"  We propose a new partial decoding algorithm for one-point Hermitian codes
that can decode up to the same number of errors as the Guruswami--Sudan
decoder. Simulations suggest that it has a similar failure probability as the
latter one. The algorithm is based on a recent generalization of the power
decoding algorithm for Reed--Solomon codes and does not require an expensive
root-finding step. In addition, it promises improvements for decoding
interleaved Hermitian codes.
",computer-science
"  We propose a vision-based method that localizes a ground vehicle using
publicly available satellite imagery as the only prior knowledge of the
environment. Our approach takes as input a sequence of ground-level images
acquired by the vehicle as it navigates, and outputs an estimate of the
vehicle's pose relative to a georeferenced satellite image. We overcome the
significant viewpoint and appearance variations between the images through a
neural multi-view model that learns location-discriminative embeddings in which
ground-level images are matched with their corresponding satellite view of the
scene. We use this learned function as an observation model in a filtering
framework to maintain a distribution over the vehicle's pose. We evaluate our
method on different benchmark datasets and demonstrate its ability localize
ground-level images in environments novel relative to training, despite the
challenges of significant viewpoint and appearance variations.
",computer-science
"  We consider the global optimization of a function over a continuous domain.
At every evaluation attempt, we can observe the function at a chosen point in
the domain and we reap the reward of the value observed. We assume that drawing
these observations are expensive and noisy. We frame it as a continuum-armed
bandit problem with a Gaussian Process prior on the function. In this regime,
most algorithms have been developed to minimize some form of regret. Contrary
to this popular norm, in this paper, we study the convergence of the sequential
point $\boldsymbol{x}^t$ to the global optimizer $\boldsymbol{x}^*$ for the
Thompson Sampling approach. Under some assumptions and regularity conditions,
we show an exponential rate of convergence to the true optimal.
",statistics
"  We review, from a didactic point of view, the definition of a toric section
and the different shapes it can take. We'll then discuss some properties of
this curve, investigate its analogies and differences with the most renowned
conic section and show how to build its general quartic equation. A curious and
unexpected result was to find that, with some algebraic manipulation, a toric
section can also be obtained as the intersection of a cylinder with a cone.
Finally we'll show how it is possible to construct and represent toric sections
in the 3D Graphics view of Geogebra. In the article only elementary algebra is
used, and the requirements to follow it are just some notion of goniometry and
of tridimensional analytic geometry.
",mathematics
"  Monotone systems preserve a partial ordering of states along system
trajectories and are often amenable to separable Lyapunov functions that are
either the sum or the maximum of a collection of functions of a scalar
argument. In this paper, we consider constructing separable Lyapunov functions
for monotone systems that are also contractive, that is, the distance between
any pair of trajectories exponentially decreases. The distance is defined in
terms of a possibly state-dependent norm. When this norm is a weighted
one-norm, we obtain conditions which lead to sum-separable Lyapunov functions,
and when this norm is a weighted infinity-norm, symmetric conditions lead to
max-separable Lyapunov functions. In addition, we consider two classes of
Lyapunov functions: the first class is separable along the system's state, and
the second class is separable along components of the system's vector field.
The latter case is advantageous for many practically motivated systems for
which it is difficult to measure the system's state but easier to measure the
system's velocity or rate of change. In addition, we present an algorithm based
on sum-of-squares programming to compute such separable Lyapunov functions. We
provide several examples to demonstrate our results.
",computer-science
"  Given a large number of unlabeled face images, face grouping aims at
clustering the images into individual identities present in the data. This task
remains a challenging problem despite the remarkable capability of deep
learning approaches in learning face representation. In particular, grouping
results can still be egregious given profile faces and a large number of
uninteresting faces and noisy detections. Often, a user needs to correct the
erroneous grouping manually. In this study, we formulate a novel face grouping
framework that learns clustering strategy from ground-truth simulated behavior.
This is achieved through imitation learning (a.k.a apprenticeship learning or
learning by watching) via inverse reinforcement learning (IRL). In contrast to
existing clustering approaches that group instances by similarity, our
framework makes sequential decision to dynamically decide when to merge two
face instances/groups driven by short- and long-term rewards. Extensive
experiments on three benchmark datasets show that our framework outperforms
unsupervised and supervised baselines.
",computer-science
"  We present Calipso, an interactive method for editing images and videos in a
physically-coherent manner. Our main idea is to realize physics-based
manipulations by running a full physics simulation on proxy geometries given by
non-rigidly aligned CAD models. Running these simulations allows us to apply
new, unseen forces to move or deform selected objects, change physical
parameters such as mass or elasticity, or even add entire new objects that
interact with the rest of the underlying scene. In Calipso, the user makes
edits directly in 3D; these edits are processed by the simulation and then
transfered to the target 2D content using shape-to-image correspondences in a
photo-realistic rendering process. To align the CAD models, we introduce an
efficient CAD-to-image alignment procedure that jointly minimizes for rigid and
non-rigid alignment while preserving the high-level structure of the input
shape. Moreover, the user can choose to exploit image flow to estimate scene
motion, producing coherent physical behavior with ambient dynamics. We
demonstrate Calipso's physics-based editing on a wide range of examples
producing myriad physical behavior while preserving geometric and visual
consistency.
",computer-science
"  It is shown that the Orlik-Terao algebra is graded isomorphic to the special
fiber of the ideal $I$ generated by the $(n-1)$-fold products of the members of
a central arrangement of size $n$. This momentum is carried over to the Rees
algebra (blowup) of $I$ and it is shown that this algebra is of fiber-type and
Cohen-Macaulay. It follows by a result of Simis-Vasconcelos that the special
fiber of $I$ is Cohen-Macaulay, thus giving another proof of a result of
Proudfoot-Speyer about the Cohen-Macauleyness of the Orlik-Terao algebra.
",mathematics
"  With the purpose of modeling, specifying and reasoning in an integrated
fashion with procedural and declarative aspects (both commonly present in cases
or scenarios), the paper introduces Logic Programming Petri Nets (LPPN), an
extension to the Petri Net notation providing an interface to logic programming
constructs. Two semantics are presented. First, a hybrid operational semantics
that separates the process component, treated with Petri nets, from the
constraint/terminological component, treated with Answer Set Programming (ASP).
Second, a denotational semantics maps the notation to ASP fully, via Event
Calculus. These two alternative specifications enable a preliminary evaluation
in terms of reasoning efficiency.
",computer-science
"  We analytically study the spontaneous emission of a single optical dipole
emitter in the vicinity of a plasmonic nanoshell, based on the Lorenz-Mie
theory. We show that the fluorescence enhancement due to the coupling between
optical emitter and sphere can be tuned by the aspect ratio of the core-shell
nanosphere and by the distance between the quantum emitter and its surface. In
particular, we demonstrate that both the enhancement and quenching of the
fluorescence intensity are associated with plasmonic Fano resonances induced by
near- and far-field interactions. These Fano resonances have asymmetry
parameters whose signs depend on the orientation of the dipole with respect to
the spherical nanoshell. We also show that if the atomic dipole is oriented
tangentially to the nanoshell, the interaction exhibits saddle points in the
near-field energy flow. This results in a Lorentzian fluorescence enhancement
response in the near field and a Fano line-shape in the far field. The
signatures of this interaction may have interesting applications for sensing
the presence and the orientation of optical emitters in close proximity to
plasmonic nanoshells.
",physics
"  We construct, for any finite commutative ring $R$, a family of
representations of the general linear group $\mathrm{GL}_n(R)$ whose
intertwining properties mirror those of the principal series for
$\mathrm{GL}_n$ over a finite field.
",mathematics
"  In an imaginary conversation with Guido Altarelli, I express my views on the
status of particle physics beyond the Standard Model and its future prospects.
",physics
"  In this paper, an original heuristic algorithm of empty vehicles management
in personal rapid transit network is presented. The algorithm is used for the
delivery of empty vehicles for waiting passengers, for balancing the
distribution of empty vehicles within the network, and for providing an empty
space for vehicles approaching a station. Each of these tasks involves a
decision on the trip that has to be done by a selected empty vehicle from its
actual location to some determined destination. The decisions are based on a
multi-parameter function involving a set of factors and thresholds. An
important feature of the algorithm is that it does not use any central database
of passenger input (demand) and locations of free vehicles. Instead, it is
based on the local exchange of data between stations: on their states and on
the vehicles they expect. Therefore, it seems well-tailored for a distributed
implementation. The algorithm is uniform, meaning that the same basic procedure
is used for multiple tasks using a task-specific set of parameters.
",computer-science
"  We show that for an elliptic curve E defined over a number field K, the group
E(A) of points of E over the adele ring A of K is a topological group that can
be analyzed in terms of the Galois representation associated to the torsion
points of E. An explicit description of E(A) is given, and we prove that for K
of degree n, almost all elliptic curves over K have an adelic point group
topologically isomorphic to a universal group depending on n. We also show that
there exist infinitely many elliptic curves over K having a different adelic
point group.
",mathematics
"  Manifolds with infinite cylindrical ends have continuous spectrum of
increasing multiplicity as energy grows, and in general embedded resonances and
eigenvalues can accumulate at infinity. However, we prove that if geodesic
trapping is sufficiently mild, then such an accumulation is ruled out, and
moreover the cutoff resolvent is uniformly bounded at high energies. We obtain
as a corollary the existence of resonance free regions near the continuous
spectrum.
We also obtain improved estimates when the resolvent is cut off away from
part of the trapping, and along the way we prove some resolvent estimates for
repulsive potentials on the half line which may be of independent interest.
",mathematics
"  Density estimation is a fundamental problem in statistical learning. This
problem is especially challenging for complex high-dimensional data due to the
curse of dimensionality. A promising solution to this problem is given here in
an inference-free hierarchical framework that is built on score matching. We
revisit the Bayesian interpretation of the score function and the Parzen score
matching, and construct a multilayer perceptron with a scalable objective for
learning the energy (i.e. the unnormalized log-density), which is then
optimized with stochastic gradient descent. In addition, the resulting deep
energy estimator network (DEEN) is designed as products of experts. We present
the utility of DEEN in learning the energy, the score function, and in
single-step denoising experiments for synthetic and high-dimensional data. We
also diagnose stability problems in the direct estimation of the score function
that had been observed for denoising autoencoders.
",statistics
"  We present a finite-temperature extension of the retarded cumulant Green's
function for calculations of exited-state and thermodynamic properties of
electronic systems. The method incorporates a cumulant to leading order in the
screened Coulomb interaction $W$ and improves excited state properties compared
to the $GW$ approximation of many-body perturbation theory. Results for the
homogeneous electron gas are presented for a wide range of densities and
temperatures, from cool to warm dense matter regime, which reveal several
hitherto unexpected properties. For example, correlation effects remain strong
at high $T$ while the exchange-correlation energy becomes small. In addition,
the spectral function broadens and damping increases with temperature, blurring
the usual quasi-particle picture. Similarly Compton scattering exhibits
substantial many-body corrections that persist at normal densities and
intermediate $T$. Results for exchange-correlation energies and potentials are
in good agreement with existing theories and finite-temperature DFT
functionals.
",physics
"  We fabricate high-mobility p-type few-layer WSe2 field-effect transistors and
surprisingly observe a series of quantum Hall (QH) states following an
unconventional sequence predominated by odd-integer states under a moderate
strength magnetic field. By tilting the magnetic field, we discover Landau
level (LL) crossing effects at ultra-low coincident angles, revealing that the
Zeeman energy is about three times as large as the cyclotron energy near the
valence band top at {\Gamma} valley. This result implies the significant roles
played by the exchange interactions in p-type few-layer WSe2, in which
itinerant or QH ferromagnetism likely occurs. Evidently, the {\Gamma} valley of
few-layer WSe2 offers a unique platform with unusually heavy hole-carriers and
a substantially enhanced g-factor for exploring strongly correlated phenomena.
",physics
"  The current work combines the Cluster Dynamics (CD) technique and
CALPHAD-based precipitation modeling to address the second phase precipitation
in cold-worked (CW) 316 stainless steels (SS) under irradiation at 300-400 C.
CD provides the radiation enhanced diffusion and dislocation evolution as
inputs for the precipitation model. The CALPHAD-based precipitation model
treats the nucleation, growth and coarsening of precipitation processes based
on classical nucleation theory and evolution equations, and simulates the
composition, size and size distribution of precipitate phases. We benchmark the
model against available experimental data at fast reactor conditions (9.4 x
10^-7 dpa/s and 390 C) and then use the model to predict the phase instability
of CW 316 SS under light water reactor (LWR) extended life conditions (7 x
10^-8 dpa/s and 275 C). The model accurately predicts the gamma-prime (Ni3Si)
precipitation evolution under fast reactor conditions and that the formation of
this phase is dominated by radiation enhanced segregation. The model also
predicts a carbide volume fraction that agrees well with available experimental
data from a PWR reactor but is much higher than the volume fraction observed in
fast reactors. We propose that radiation enhanced dissolution and/or carbon
depletion at sinks that occurs at high flux could be the main sources of this
inconsistency. The integrated model predicts ~1.2% volume fraction for carbide
and ~3.0% volume fraction for gamma-prime for typical CW 316 SS (with 0.054
wt.% carbon) under LWR extended life conditions. This work provides valuable
insights into the magnitudes and mechanisms of precipitation in irradiated CW
316 SS for nuclear applications.
",physics
"  Part I of this work [2] developed the exact diffusion algorithm to remove the
bias that is characteristic of distributed solutions for deterministic
optimization problems. The algorithm was shown to be applicable to a larger set
of combination policies than earlier approaches in the literature. In
particular, the combination matrices are not required to be doubly stochastic,
which impose stringent conditions on the graph topology and communications
protocol. In this Part II, we examine the convergence and stability properties
of exact diffusion in some detail and establish its linear convergence rate. We
also show that it has a wider stability range than the EXTRA consensus
solution, meaning that it is stable for a wider range of step-sizes and can,
therefore, attain faster convergence rates. Analytical examples and numerical
simulations illustrate the theoretical findings.
",mathematics
"  Context: Information Technology consumes up to 10\% of the world's
electricity generation, contributing to CO2 emissions and high energy costs.
Data centers, particularly databases, use up to 23% of this energy. Therefore,
building an energy-efficient (green) database engine could reduce energy
consumption and CO2 emissions.
Goal: To understand the factors driving databases' energy consumption and
execution time throughout their evolution.
Method: We conducted an empirical case study of energy consumption by two
MySQL database engines, InnoDB and MyISAM, across 40 releases. We examined the
relationships of four software metrics to energy consumption and execution time
to determine which metrics reflect the greenness and performance of a database.
Results: Our analysis shows that database engines' energy consumption and
execution time increase as databases evolve. Moreover, the Lines of Code metric
is correlated moderately to strongly with energy consumption and execution time
in 88% of cases.
Conclusions: Our findings provide insights to both practitioners and
researchers. Database administrators may use them to select a fast, green
release of the MySQL database engine. MySQL database-engine developers may use
the software metric to assess products' greenness and performance. Researchers
may use our findings to further develop new hypotheses or build models to
predict greenness and performance of databases.
",computer-science
"  A general methodology is proposed to differentiate the likelihood of
energetic-particle-driven instabilities to produce frequency chirping or
fixed-frequency oscillations. The method employs numerically calculated
eigenstructures and multiple resonance surfaces of a given mode in the presence
of energetic ion drag and stochasticity (due to collisions and
micro-turbulence). Toroidicity-induced, reversed-shear and beta-induced
Alfven-acoustic eigenmodes are used as examples. Waves measured in experiments
are characterized and compatibility is found between the proposed criterion
predictions and the experimental observation or lack of observation of chirping
behavior of Alfvenic modes in different tokamaks. It is found that the
stochastic diffusion due to micro-turbulence can be the dominant energetic
particle detuning mechanism near the resonances in many plasma experiments, and
its strength is the key as to whether chirping solutions are likely to arise.
The proposed criterion constitutes a useful predictive tool in assessing
whether the nature of the transport for fast ion losses in fusion devices will
be dominated by convective or diffusive processes.
",physics
"  Si Li and author suggested in that, in some cases, the AdS/CFT correspondence
can be formulated in terms of the algebraic operation of Koszul duality. In
this paper this suggestion is checked explicitly for $M2$ branes in an
$\Omega$-background. The algebra of supersymmetric operators on a stack of $K$
$M2$ branes is shown to be Koszul dual, in large $K$, to the algebra of
supersymmetric operators of $11$-dimensional supergravity in an
$\Omega$-background (using the formulation of supergravity in an
$\Omega$-background presented in arXiv:1610.04144).
The twisted form of supergravity that is used here can be quantized to all
orders in perturbation theory. We find that the Koszul duality result holds to
all orders in perturbation theory, in both the gravitational theory and the
theory on the $M2$. (However, there is a certain non-linear identification of
the coupling constants on each side which I was unable to determine
explicitly).
It is also shown that the algebra of operators on $K$ $M2$ branes, as $K \to
\infty$, is a quantum double-loop algebra (a two-variable analog of the
Yangian). This algebra is also the Koszul dual of the algebra of operators on
the gravitational theory. An explicit presentation for this algebra is
presented, and it is shown that this algebra is the unique quantization of its
classical limit. Some conjectural applications to enumerative geometry of
Calabi-Yau threefolds are also presented.
",mathematics
"  It was proven in [B.-Y. Chen, F. Dillen, J. Van der Veken and L. Vrancken,
Curvature inequalities for Lagrangian submanifolds: the final solution, Differ.
Geom. Appl. 31 (2013), 808-819] that every Lagrangian submanifold $M$ of a
complex space form $\tilde M^{n}(4c)$ of constant holomorphic sectional
curvature $4c$ satisfies the following optimal inequality: \begin{align*}
\delta(2,n-2) \leq \frac{n^2(n-2)}{4(n-1)} H^2 + 2(n-2) c, \end{align*} where
$H^2$ is the squared mean curvature and $\delta(2,n-2)$ is a $\delta$-invariant
on $M$. In this paper we classify Lagrangian submanifolds of complex space
forms $\tilde M^{n}(4c)$, $n \geq 5$, which satisfy the equality case of this
inequality at every point.
",mathematics
"  We study the signs of the Fourier coefficients of a newform. Let $f$ be a
normalized newform of weight $k$ for $\Gamma_0(N)$. Let $a_f(n)$ be the $n$th
Fourier coefficient of $f$. For any fixed positive integer $m$, we study the
distribution of the signs of $\{a_f(p^m)\}_p$, where $p$ runs over all prime
numbers. We also find out the abscissas of absolute convergence of two
Dirichlet series with coefficients involving the Fourier coefficients of cusp
forms and the coefficients of symmetric power $L$-functions.
",mathematics
"  We consider the problem of generating relevant execution traces to test rich
interactive applications. Rich interactive applications, such as apps on mobile
platforms, are complex stateful and often distributed systems where
sufficiently exercising the app with user-interaction (UI) event sequences to
expose defects is both hard and time-consuming. In particular, there is a
fundamental tension between brute-force random UI exercising tools, which are
fully-automated but offer low relevance, and UI test scripts, which are manual
but offer high relevance. In this paper, we consider a middle way---enabling a
seamless fusion of scripted and randomized UI testing. This fusion is
prototyped in a testing tool called ChimpCheck for programming, generating, and
executing property-based randomized test cases for Android apps. Our approach
realizes this fusion by offering a high-level, embedded domain-specific
language for defining custom generators of simulated user-interaction event
sequences. What follows is a combinator library built on industrial strength
frameworks for property-based testing (ScalaCheck) and Android testing (Android
JUnit and Espresso) to implement property-based randomized testing for Android
development. Driven by real, reported issues in open source Android apps, we
show, through case studies, how ChimpCheck enables expressing effective testing
patterns in a compact manner.
",computer-science
"  We show that any totally geodesic submanifold of Teichmuller space of
dimension greater than one covers a totally geodesic subvariety, and only
finitely many totally geodesic subvarieties of dimension greater than one exist
in each moduli space.
",mathematics
"  With the wide adoption of the multi-community setting in many popular social
media platforms, the increasing user engagements across multiple online
communities warrant research attention. In this paper, we introduce a novel
analogy between the movements in the cyber space and the physical space. This
analogy implies a new way of studying human online activities by modelling the
activities across online communities in a similar fashion as the movements
among locations. First, we quantitatively validate the analogy by comparing
several important properties of human online activities and physical movements.
Our experiments reveal striking similarities between the cyber space and the
physical space. Next, inspired by the established methodology on human mobility
in the physical space, we propose a framework to study human ""mobility"" across
online platforms. We discover three interesting patterns of user engagements in
online communities. Furthermore, our experiments indicate that people with
different mobility patterns also exhibit divergent preferences to online
communities. This work not only attempts to achieve a better understanding of
human online activities, but also intends to open a promising research
direction with rich implications and applications.
",computer-science
"  We generalise some well-known graph parameters to operator systems by
considering their underlying quantum channels. In particular, we introduce the
quantum complexity as the dimension of the smallest co-domain Hilbert space a
quantum channel requires to realise a given operator system as its
non-commutative confusability graph. We describe quantum complexity as a
generalised minimum semidefinite rank and, in the case of a graph operator
system, as a quantum intersection number. The quantum complexity and a closely
related quantum version of orthogonal rank turn out to be upper bounds for the
Shannon zero-error capacity of a quantum channel, and we construct examples for
which these bounds beat the best previously known general upper bound for the
capacity of quantum channels, given by the quantum Lovász theta number.
",mathematics
"  In recent publications, we presented a novel formal symbolic process virtual
machine (FSPVM) framework that combined higher-order theorem proving and
symbolic execution for verifying the reliability and security of smart
contracts developed in the Ethereum blockchain system without suffering the
standard issues surrounding reusability, consistency, and automation. A
specific FSPVM, denoted as FSPVM-E, was developed in Coq based on a general,
extensible, and reusable formal memory (GERM) framework, an extensible and
universal formal intermediate programming language, denoted as Lolisa, which is
a large subset of the Solidity programming language that uses generalized
algebraic datatypes, and a corresponding formally verified interpreter for
Lolisa, denoted as FEther, which serves as a crucial component of FSPVM-E.
However, our past work has demonstrated that the execution efficiency of the
standard development of FEther is extremely low. As a result, FSPVM-E fails to
achieve its expected verification effect. The present work addresses this issue
by first identifying three root causes of the low execution efficiency of
formal interpreters. We then build abstract models of these causes, and present
respective optimization schemes for rectifying the identified conditions.
Finally, we apply these optimization schemes to FEther, and demonstrate that
its execution efficiency has been improved significantly.
",computer-science
"  We employ the Grand Canonical Adaptive Resolution Molecular Dynamics
Technique (GC-AdResS) to test the spatial locality of the 1-ethyl 3-methyl
imidazolium chloride liquid. In GC-AdResS atomistic details are kept only in an
open sub-region of the system while the environment is treated at
coarse-grained level, thus if spatial quantities calculated in such a
sub-region agree with the equivalent quantities calculated in a full atomistic
simulation then the atomistic degrees of freedom outside the sub-region play a
negligible role. The size of the sub-region fixes the degree of spatial
locality of a certain quantity. We show that even for sub-regions whose radius
corresponds to the size of a few molecules, spatial properties are reasonably
{reproduced} thus suggesting a higher degree of spatial locality, a hypothesis
put forward also by other {researchers} and that seems to play an important
role for the characterization of fundamental properties of a large class of
ionic liquids.
",physics
"  Recommendation systems are widely used by different user service providers
specially those who have interactions with the large community of users. This
paper introduces a recommender system based on community detection. The
recommendation is provided using the local and global similarities between
users. The local information is obtained from communities, and the global ones
are based on the ratings. Here, a new fuzzy community detection using the
personalized PageRank metaphor is introduced. The fuzzy membership values of
the users to the communities are utilized to define a similarity measure. The
method is evaluated by using two well-known datasets: MovieLens and FilmTrust.
The results show that our method outperforms recent recommender systems.
",computer-science
"  We construct an explicit projective bimodule resolution for the Leavitt path
algebra of a row-finite quiver. We prove that the Leavitt path algebra of a
row-countable quiver has Hochschild cohomolgical dimension at most one, that
is, it is quasi-free in the sense of Cuntz-Quillen. The construction of the
resolution relies on an explicit derivation of the Leavitt path algebra.
",mathematics
"  Weiyi Zhang noticed recently a gap in the proof of the main theorem of the
authors article ""Tamed to compatible: Symplectic forms via moduli space
integration"" [T] for the case when the symplectic 4-manifold in question has
first Betti number 2 (and necessarily self-dual second Betti number 1). This
note explains how to fill this gap.
",mathematics
"  We develop a feedback control method for networked epidemic spreading
processes. In contrast to most prior works which consider mean field, open-loop
control schemes, the present work develops a novel framework for feedback
control of epidemic processes which leverages incomplete observations of the
stochastic epidemic process in order to control the exact dynamics of the
epidemic outbreak. We develop an observation model for the epidemic process,
and demonstrate that if the set of observed nodes is sufficiently well
structured, then the random variables which denote the process' infections are
conditionally independent given the observations. We then leverage the attained
conditional independence property to construct tractable mechanisms for the
inference and prediction of the process state, avoiding the need to use mean
field approximations or combinatorial representations. We conclude by
formulating a one-step lookahead controller for the discrete-time
Susceptible-Infected-Susceptible (SIS) epidemic process which leverages the
developed Bayesian inference and prediction mechanisms, and causes the epidemic
to die out at a chosen rate.
",mathematics
"  In this paper, we are concerned with the existence of the least energy
sign-changing solutions for the following fractional Schrödinger-Poisson
system: \begin{align*}
\left\{ \begin{aligned} &(-\Delta)^{s} u+V(x)u+\lambda\phi(x)u=f(x, u),\quad
&\text{in}\, \ \mathbb{R}^{3},\\ &(-\Delta)^{t}\phi=u^{2},& \text{in}\,\
\mathbb{R}^{3}, \end{aligned} \right. \end{align*} where $\lambda\in
\mathbb{R}^{+}$ is a parameter, $s, t\in (0, 1)$ and $4s+2t>3$, $(-\Delta)^{s}$
stands for the fractional Laplacian. By constraint variational method and
quantitative deformation lemma, we prove that the above problem has one least
energy sign-changing solution. Moreover, for any $\lambda>0$, we show that the
energy of the least energy sign-changing solutions is strictly larger than two
times the ground state energy.
Finally, we consider $\lambda$ as a parameter and study the convergence
property of the least energy sign-changing solutions as $\lambda\searrow 0$.
",mathematics
"  Centrality metrics are among the main tools in social network analysis. Being
central for a user of a network leads to several benefits to the user: central
users are highly influential and play key roles within the network. Therefore,
the optimization problem of increasing the centrality of a network user
recently received considerable attention. Given a network and a target user
$v$, the centrality maximization problem consists in creating $k$ new links
incident to $v$ in such a way that the centrality of $v$ is maximized,
according to some centrality metric. Most of the algorithms proposed in the
literature are based on showing that a given centrality metric is monotone and
submodular with respect to link addition. However, this property does not hold
for several shortest-path based centrality metrics if the links are undirected.
In this paper we study the centrality maximization problem in undirected
networks for one of the most important shortest-path based centrality measures,
the coverage centrality. We provide several hardness and approximation results.
We first show that the problem cannot be approximated within a factor greater
than $1-1/e$, unless $P=NP$, and, under the stronger gap-ETH hypothesis, the
problem cannot be approximated within a factor better than $1/n^{o(1)}$, where
$n$ is the number of users. We then propose two greedy approximation
algorithms, and show that, by suitably combining them, we can guarantee an
approximation factor of $\Omega(1/\sqrt{n})$. We experimentally compare the
solutions provided by our approximation algorithm with optimal solutions
computed by means of an exact IP formulation. We show that our algorithm
produces solutions that are very close to the optimum.
",computer-science
"  We investigate the differential equation for the Jacobi-type polynomials
which are orthogonal on the interval $[-1,1]$ with respect to the classical
Jacobi measure and an additional point mass at one endpoint. This scale of
higher-order equations was introduced by J. and R. Koekoek in 1999 essentially
by using special function methods. In this paper, a completely elementary
representation of the Jacobi-type differential operator of any even order is
given. This enables us to trace the orthogonality relation of the Jacobi-type
polynomials back to their differential equation. Moreover, we establish a new
factorization of the Jacobi-type operator which gives rise to a recurrence
relation with respect to the order of the equation.
",mathematics
"  All four giant planets in the Solar System feature zonal flows on the order
of 100 m/s in the cloud deck, and large-scale intrinsic magnetic fields on the
order of 1 Gauss near the surface. The vertical structure of the zonal flows
remains obscure. The end-member scenarios are shallow flows confined in the
radiative atmosphere and deep flows throughout the entire planet. The
electrical conductivity increases rapidly yet smoothly as a function of depth
inside Jupiter and Saturn. Deep zonal flows will inevitably interact with the
magnetic field, at depth with even modest electrical conductivity. Here we
investigate the interaction between zonal flows and magnetic fields in the
semi-conducting region of giant planets. Employing mean-field electrodynamics,
we show that the interaction will generate detectable poloidal magnetic field
perturbations spatially correlated with the deep zonal flows. Assuming the peak
amplitude of the dynamo alpha-effect to be 0.1 mm/s, deep zonal flows on the
order of 0.1 - 1 m/s in the semi-conducting region of Jupiter and Saturn would
generate poloidal magnetic perturbations on the order of 0.01% - 1% of the
background dipole field. These poloidal perturbations should be detectable with
the in-situ magnetic field measurements from the Juno mission and the Cassini
Grand Finale. This implies that magnetic field measurements can be employed to
constrain the properties of deep zonal flows in the semi-conducting region of
giant planets.
",physics
"  For any $r\geq 1$ and $\mathbf{n} \in \mathbb{Z}_{\geq0}^r \setminus
\{\mathbf0\}$ we construct a poset $W_{\mathbf{n}}$ called a 2-associahedron.
The 2-associahedra arose in symplectic geometry, where they are expected to
control maps between Fukaya categories of different symplectic manifolds. We
prove that the completion $\widehat{W_{\mathbf{n}}}$ is an abstract polytope of
dimension $|\mathbf{n}|+r-3$. There are forgetful maps $W_{\mathbf{n}} \to
K_r$, where $K_r$ is the $(r-2)$-dimensional associahedron, and the
2-associahedra specialize to the associahedra (in two ways) and to the
multiplihedra. In an appendix, we work out the 2- and 3-dimensional
associahedra in detail.
",mathematics
"  In magnetic resonant coupling (MRC) enabled multiple-input multiple-output
(MIMO) wireless power transfer (WPT) systems, multiple transmitters (TXs) each
with one single coil are used to enhance the efficiency of simultaneous power
transfer to multiple single-coil receivers (RXs) by constructively combining
their induced magnetic fields at the RXs, a technique termed ""magnetic
beamforming"". In this paper, we study the optimal magnetic beamforming design
in a multi-user MIMO MRC-WPT system. We introduce the multi-user power region
that constitutes all the achievable power tuples for all RXs, subject to the
given total power constraint over all TXs as well as their individual peak
voltage and current constraints. We characterize each boundary point of the
power region by maximizing the sum-power deliverable to all RXs subject to
their minimum harvested power constraints. For the special case without the TX
peak voltage and current constraints, we derive the optimal TX current
allocation for the single-RX setup in closed-form as well as that for the
multi-RX setup. In general, the problem is a non-convex quadratically
constrained quadratic programming (QCQP), which is difficult to solve. For the
case of one single RX, we show that the semidefinite relaxation (SDR) of the
problem is tight. For the general case with multiple RXs, based on SDR we
obtain two approximate solutions by applying time-sharing and randomization,
respectively. Moreover, for practical implementation of magnetic beamforming,
we propose a novel signal processing method to estimate the magnetic MIMO
channel due to the mutual inductances between TXs and RXs. Numerical results
show that our proposed magnetic channel estimation and adaptive beamforming
schemes are practically effective, and can significantly improve the power
transfer efficiency and multi-user performance trade-off in MIMO MRC-WPT
systems.
",computer-science
"  We study some basic properties of the class of universal operators on Hilbert
space, and provide new examples of universal operators and universal pairs.
",mathematics
"  In the research of the impact of gestures using by a lecturer, one
challenging task is to infer the attention of a group of audiences. Two
important measurements that can help infer the level of attention are eye
movement data and Electroencephalography (EEG) data. Under the fundamental
assumption that a group of people would look at the same place if they all pay
attention at the same time, we apply a method, ""Time Warp Edit Distance"", to
calculate the similarity of their eye movement trajectories. Moreover, we also
cluster eye movement pattern of audiences based on these pair-wised similarity
metrics. Besides, since we don't have a direct metric for the ""attention""
ground truth, a visual assessment would be beneficial to evaluate the
gesture-attention relationship. Thus we also implement a visualization tool.
",computer-science
"  We investigate the basic thermal, mechanical and structural properties of
body centred cubic iron ($\alpha$-Fe) at several temperatures and positive
loading by means of Molecular Dynamics simulations in conjunction with the
embedded-atom method potential and its modified counterpart one. Computations
of its thermal properties like average energy and density of atoms, transport
sound velocities at finite temperatures and pressures are detailed studied as
well. Moreover, there are suggestions to obtain hexagonal close- packed
structure ($\varepsilon$-phase) of this metal under positive loading. To
demonstrate that, one can increase sufficiently the pressure of simulated
system at several temperature's ranges; these structural changes depend only on
potential type used. The ensuring structures are studied via the pair radial
distribution functions (PRDF) and precise common- neighbour analysis method
(CNA) as well.
",physics
"  We investigate the impact of an external pressure on the structure of
self-gravitating polytropes for axially symmetric ellipsoids and rings. The
confinement of the fluid by photons is accounted for through a boundary
condition on the enthalpy $H$. Equilibrium configurations are determined
numerically from a generalised ""Self-Consistent-Field""-method. The new
algorithm incorporates an intra-loop re-scaling operator ${\cal R}(H)$, which
is essential for both convergence and getting self-normalised solutions. The
main control parameter is the external-to-core enthalpy ratio. In the case of
uniform rotation rate and uniform surrounding pressure, we compute the mass,
the volume, the rotation rate and the maximum enthalpy. This is repeated for a
few polytropic indices $n$. For a given axis ratio, over-pressurization
globally increases all output quantities, and this is more pronounced for large
$n$. Density profiles are flatter than in the absence of an external pressure.
When the control parameter asymptotically tends to unity, the fluid converges
toward the incompressible solution, whatever the index, but becomes
geometrically singular. Equilibrium sequences, obtained by varying the axis
ratio, are built. States of critical rotation are greatly exceeded or even
disappear. The same trends are observed with differential rotation. Finally,
the typical response to a photon point source is presented. Strong irradiation
favours sharp edges. Applications concern star forming regions and matter
orbiting young stars and black holes.
",physics
"  In relativistic quantum field theories, compact objects of interacting bosons
can become stable owing to conservation of an additive quantum number $Q$.
Discovering such $Q$-balls propagating in the Universe would confirm
supersymmetric extensions of the standard model and may shed light on the
mysteries of dark matter, but no unambiguous experimental evidence exists. We
report observation of a propagating long-lived $Q$-ball in superfluid $^3$He,
where the role of $Q$-ball is played by a Bose-Einstein condensate of magnon
quasiparticles. We achieve accurate representation of the $Q$-ball Hamiltonian
using the influence of the number of magnons, corresponding to the charge $Q$,
on the orbital structure of the superfluid $^3$He order parameter. This
realisation supports multiple coexisting $Q$-balls which in future allows
studies of $Q$-ball dynamics, interactions, and collisions.
",physics
"  We study stochastic multi-armed bandits with many players. The players do not
know the number of players, cannot communicate with each other and if multiple
players select a common arm they collide and none of them receive any reward.
We consider the static scenario, where the number of players remains fixed, and
the dynamic scenario, where the players enter and leave at any time. We provide
algorithms based on a novel `trekking approach' that guarantees constant regret
for the static case and sub-linear regret for the dynamic case with high
probability. The trekking approach eliminates the need to estimate the number
of players resulting in fewer collisions and improved regret performance
compared to the state-of-the-art algorithms. We also develop an epoch-less
algorithm that eliminates any requirement of time synchronization across the
players provided each player can detect the presence of other players on an
arm. We validate our theoretical guarantees using simulation based and real
test-bed based experiments.
",statistics
"  We describe a communication game, and a conjecture about this game, whose
proof would imply the well-known Sensitivity Conjecture asserting a polynomial
relation between sensitivity and block sensitivity for Boolean functions. The
author defined this game and observed the connection in Dec. 2013 - Jan. 2014.
The game and connection were independently discovered by Gilmer, Koucký, and
Saks, who also established further results about the game (not proved by us)
and published their results in ITCS '15 [GKS15].
This note records our independent work, including some observations that did
not appear in [GKS15]. Namely, the main conjecture about this communication
game would imply not only the Sensitivity Conjecture, but also a stronger
hypothesis raised by Chung, Füredi, Graham, and Seymour [CFGS88]; and,
another related conjecture we pose about a ""query-bounded"" variant of our
communication game would suffice to answer a question of Aaronson, Ambainis,
Balodis, and Bavarian [AABB14] about the query complexity of the ""Weak Parity""
problem---a question whose resolution was previously shown by [AABB14] to
follow from a proof of the Chung et al. hypothesis.
",computer-science
"  In this sequel to earlier papers by three of the authors, we obtain a new
bound on the complexity of a closed 3--manifold, as well as a characterisation
of manifolds realising our complexity bounds. As an application, we obtain the
first infinite families of minimal triangulations of Seifert fibred spaces
modelled on Thurston's geometry $\widetilde{\text{SL}_2(\mathbb{R})}.$
",mathematics
"  We study the one dimensional t-t'-J model for generic couplings using two
complementary theories, the extremely correlated Fermi liquid theory and
time-dependent density matrix renormalization group over a broad energy scale.
The two methods provide a unique insight into the strong momentum dependence of
the self-energy of this prototypical non-Fermi liquid, described at low
energies as a Tomonaga-Luttinger liquid. We also demonstrate its intimate
relationship to spin-charge separation, i.e. the splitting of Landau
quasiparticles of higher dimensions into two constituents, driven by strong
quantum fluctuations inherent in one dimension. The momentum distribution
function, the spectral function, and the excitation dispersion of these two
methods also compare well.
",physics
"  Object tracking systems play important roles in tracking moving objects and
overcoming problems such as safety, security and other location-related
applications. Problems arise from the difficulties in creating a well-defined
and understandable description of tracking systems. Nowadays, describing such
processes results in fragmental representation that most of the time leads to
difficulties creating documentation. Additionally, once learned by assigned
personnel, repeated tasks result in them continuing on autopilot in a way that
often degrades their effectiveness. This paper proposes the modeling of
tracking systems in terms of a new diagrammatic methodology to produce
engineering-like schemata. The resultant diagrams can be used in documentation,
explanation, communication, education and control.
",computer-science
"  We develop a method to study the implied volatility for exotic options and
volatility derivatives with European payoffs such as VIX options. Our approach,
based on Malliavin calculus techniques, allows us to describe the properties of
the at-the-money implied volatility (ATMI) in terms of the Malliavin
derivatives of the underlying process. More precisely, we study the short-time
behaviour of the ATMI level and skew. As an application, we describe the
short-term behavior of the ATMI of VIX and realized variance options in terms
of the Hurst parameter of the model, and most importantly we describe the class
of volatility processes that generate a positive skew for the VIX implied
volatility. In addition, we find that our ATMI asymptotic formulae perform very
well even for large maturities. Several numerical examples are provided to
support our theoretical results.
",quantitative-finance
"  This paper studies mechanism of preconcentration of charged particles in a
straight micro-channel embedded with permselective membranes, by numerically
solving coupled transport equations of ions, charged particles and solvent
fluid without any simplifying assumptions. It is demonstrated that trapping and
preconcentration of charged particles are determined by the interplay between
drag force from the electroosmotic fluid flow and the electrophoretic force
applied trough the electric field. Several insightful characteristics are
revealed, including the diverse dynamics of co-ions and counter ions,
replacement of co-ions by focused particles, lowered ion concentrations in
particle enriched zone, and enhanced electroosmotic pumping effect etc.
Conditions for particles that may be concentrated are identified in terms of
charges, sizes and electrophoretic mobilities of particles and co-ions.
Dependences of enrichment factor on cross-membrane voltage, initial particle
concentration and buffer ion concentrations are analyzed and the underlying
reasons are elaborated. Finally, post priori a condition for validity of
decoupled simulation model is given based on charges carried by focused charge
particles and that by buffer co-ions. These results provide important guidance
in the design and optimization of nanofluidic preconcentration and other
related devices.
",physics
"  This works presents a formulation for visual navigation that unifies map
based spatial reasoning and path planning, with landmark based robust plan
execution in noisy environments. Our proposed formulation is learned from data
and is thus able to leverage statistical regularities of the world. This allows
it to efficiently navigate in novel environments given only a sparse set of
registered images as input for building representations for space. Our
formulation is based on three key ideas: a learned path planner that outputs
path plans to reach the goal, a feature synthesis engine that predicts features
for locations along the planned path, and a learned goal-driven closed loop
controller that can follow plans given these synthesized features. We test our
approach for goal-driven navigation in simulated real world environments and
report performance gains over competitive baseline approaches.
",computer-science
"  Our manuscript investigates a self-consistent solution of the statistical
atom model proposed by Berthold-Georg Englert and Julian Schwinger (the ES
model) and benchmarks it against atomic Kohn-Sham and two orbital-free models
of the Thomas-Fermi-Dirac (TFD)-$\lambda$vW family. Results show that the ES
model generally offers the same accuracy as the well-known TFD-$\frac{1}{5}$vW
model; however, the ES model corrects the failure in Pauli potential
near-nucleus region. We also point to the inability of describing low-$Z$ atoms
as the foremost concern in improving the present model.
",physics
"  We study the temperature dependence of the Rashba-split bands in the bismuth
tellurohalides BiTe$X$ $(X=$ I, Br, Cl) from first principles. We find that
increasing temperature reduces the Rashba splitting, with the largest effect
observed in BiTeI with a reduction of the Rashba parameter of $40$% when
temperature increases from $0$ K to $300$ K. These results highlight the
inadequacy of previous interpretations of the observed Rashba splitting in
terms of static-lattice calculations alone. Notably, we find the opposite
trend, a strengthening of the Rashba splitting with rising temperature, in the
pressure-stabilized topological-insulator phase of BiTeI. We propose that the
opposite trends with temperature on either side of the topological phase
transition could be an experimental signature for identifying it. The predicted
temperature dependence is consistent with optical conductivity measurements,
and should also be observable using photoemission spectroscopy, which could
provide further insights into the nature of spin splitting and topology in the
bismuth tellurohalides.
",physics
"  An elliptic curve $E$ defined over a $p$-adic field $K$ with a $p$-isogeny
$\phi:E\rightarrow E^\prime$ comes equipped with an invariant $\alpha_{\phi/K}$
that measures the valuation of the leading term of the formal group
homomorphism $\Phi:\hat E \rightarrow \hat E^\prime$. We prove that if
$K/\mathbb{Q}_p$ is unramified and $E$ has additive, potentially supersingular
reduction, then $\alpha_{\phi/K}$ is determined by the number of distinct
geometric components on the special fibers of the minimal proper regular models
of $E$ and $E^\prime$.
",mathematics
"  In this work, we introduce the class of $h$-${\rm{MN}}$-convex functions by
generalizing the concept of ${\rm{MN}}$-convexity and combining it with
$h$-convexity. Namely, Let $I,J$ be two intervals subset of
$\left(0,\infty\right)$ such that $\left(0,1\right)\subseteq J$ and
$\left[a,b\right]\subseteq I$. Consider a non-negative function $h:
(0,\infty)\to \left(0,\infty\right)$ and let ${\rm{M}}:\left[0,1\right]\to
\left[a,b\right] $ $(0<a<b)$ be a Mean function given by
${\rm{\rm{M}}}\left(t\right)={\rm{\rm{M}}}\left( {h(t);a,b} \right)$; where
by ${\rm{\rm{M}}}\left( {h(t);a,b} \right)$ we mean one of the following
functions: $A_h\left( {a,b} \right):=h\left( {1 - t} \right)a + h(t) b$,
$G_h\left( {a,b} \right)=a^{h(1-t)} b^{h(t)}$ and $H_h\left( {a,b}
\right):=\frac{ab}{h(t) a + h\left( {1 - t} \right)b} = \frac{1}{A_h\left(
{\frac{1}{a},\frac{1}{b}} \right)}$; with the property that
${\rm{\rm{M}}}\left( {h(0);a,b} \right)=a$ and ${\rm{M}}\left( {h(1);a,b}
\right)=b$.
A function $f : I \to \left(0,\infty\right)$ is said to be
$h$-${\rm{\rm{MN}}}$-convex (concave) if the inequality \begin{align*} f
\left({\rm{M}}\left(t;x, y\right)\right) \le (\ge) \, {\rm{N}}\left(h(t);f (x),
f (y)\right), \end{align*} holds for all $x,y \in I$ and $t\in [0,1]$, where M
and N are two mean functions. In this way, nine classes of
$h$-${\rm{MN}}$-convex functions are established and some of their analytic
properties are explored and investigated. Characterizations of each type are
given. Various Jensen's type inequalities and their converses are proved.
",mathematics
"  We consider co-rotational wave maps from (1+3)-dimensional Minkowski space
into the three-sphere. This model exhibits an explicit blowup solution and we
prove the asymptotic nonlinear stability of this solution in the whole space
under small perturbations of the initial data. The key ingredient is the
introduction of a novel coordinate system that allows one to track the
evolution past the blowup time and almost up to the Cauchy horizon of the
singularity. As a consequence, we also obtain a result on continuation beyond
blowup.
",mathematics
"  Topological Dirac and Weyl semimetals not only host quasiparticles analogous
to the elementary fermionic particles in high-energy physics, but also have
nontrivial band topology manifested by exotic Fermi arcs on the surface. Recent
advances suggest new types of topological semimetals, in which spatial
symmetries protect gapless electronic excitations without high-energy analogy.
Here we observe triply-degenerate nodal points (TPs) near the Fermi level of
WC, in which the low-energy quasiparticles are described as three-component
fermions distinct from Dirac and Weyl fermions. We further observe the surface
states whose constant energy contours are pairs of Fermi arcs connecting the
surface projection of the TPs, proving the nontrivial topology of the newly
identified semimetal state.
",physics
"  The popular BFGS quasi-Newton minimization algorithm under reasonable
conditions converges globally on smooth convex functions. This result was
proved by Powell in 1976: we consider its implications for functions that are
not smooth. In particular, an analogous convergence result holds for functions,
like the Euclidean norm, that are nonsmooth at the minimizer.
",mathematics
"  While strong progress has been made in image captioning over the last years,
machine and human captions are still quite distinct. A closer look reveals that
this is due to the deficiencies in the generated word distribution, vocabulary
size, and strong bias in the generators towards frequent captions. Furthermore,
humans -- rightfully so -- generate multiple, diverse captions, due to the
inherent ambiguity in the captioning task which is not considered in today's
systems.
To address these challenges, we change the training objective of the caption
generator from reproducing groundtruth captions to generating a set of captions
that is indistinguishable from human generated captions. Instead of
handcrafting such a learning target, we employ adversarial training in
combination with an approximate Gumbel sampler to implicitly match the
generated distribution to the human one. While our method achieves comparable
performance to the state-of-the-art in terms of the correctness of the
captions, we generate a set of diverse captions, that are significantly less
biased and match the word statistics better in several aspects.
",computer-science
"  In this paper, we present a concolic execution technique for detecting SQL
injection vulnerabilities in Android apps, with a new tool we called
ConsiDroid. We extend the source code of apps with mocking technique, such that
the execution of original source code is not affected. The extended source
codes can be treated as Java applications and may be executed by SPF with
concolic execution. We automatically produce a DummyMain class out of static
analysis such that the essential functions are called sequentially and, the
events leading to vulnerable functions are triggered. We extend SPF with taint
analysis in ConsiDroid. For making taint analysis possible, we introduce a new
technique of symbolic mock classes in order to ease the propagation of tainted
values in the code. An SQL injection vulnerability is detected through
receiving a tainted value by a vulnerable function. Besides, ConsiDroid takes
advantage of static analysis to adjust SPF in order to inspect only suspicious
paths. To illustrate the applicability of ConsiDroid, we have inspected
randomly selected 140 apps from F-Droid repository. From these apps, we found
three apps vulnerable to SQL injection. To verify their vulnerability, we
analyzed the apps manually based on ConsiDroid's reports by using Robolectric.
",computer-science
"  In this paper we propose a novel approach to manage the throughput vs latency
tradeoff that emerges when managing updates in geo-replicated systems. Our
approach consists in allowing full concurrency when processing local updates
and using a deferred local serialisation procedure before shipping updates to
remote datacenters. This strategy allows to implement inexpensive mechanisms to
ensure system consistency requirements while avoiding intrusive effects on
update operations, a major performance limitation of previous systems. We have
implemented our approach as a variant of Riak KV. Our extensive evaluation
shows that we outperform sequencer-based approaches by almost an order of
magnitude in the maximum achievable throughput. Furthermore, unlike previous
sequencer-free solutions, our approach reaches nearly optimal remote update
visibility latencies without limiting throughput.
",computer-science
"  Gaussian belief propagation (BP) has been widely used for distributed
estimation in large-scale networks such as the smart grid, communication
networks, and social networks, where local measurements/observations are
scattered over a wide geographical area. However, the convergence of Gaus- sian
BP is still an open issue. In this paper, we consider the convergence of
Gaussian BP, focusing in particular on the convergence of the information
matrix. We show analytically that the exchanged message information matrix
converges for arbitrary positive semidefinite initial value, and its dis- tance
to the unique positive definite limit matrix decreases exponentially fast.
",computer-science
"  This article describes the motivation, design, and progress of the Journal of
Open Source Software (JOSS). JOSS is a free and open-access journal that
publishes articles describing research software. It has the dual goals of
improving the quality of the software submitted and providing a mechanism for
research software developers to receive credit. While designed to work within
the current merit system of science, JOSS addresses the dearth of rewards for
key contributions to science made in the form of software. JOSS publishes
articles that encapsulate scholarship contained in the software itself, and its
rigorous peer review targets the software components: functionality,
documentation, tests, continuous integration, and the license. A JOSS article
contains an abstract describing the purpose and functionality of the software,
references, and a link to the software archive. The article is the entry point
of a JOSS submission, which encompasses the full set of software artifacts.
Submission and review proceed in the open, on GitHub. Editors, reviewers, and
authors work collaboratively and openly. Unlike other journals, JOSS does not
reject articles requiring major revision; while not yet accepted, articles
remain visible and under review until the authors make adequate changes (or
withdraw, if unable to meet requirements). Once an article is accepted, JOSS
gives it a DOI, deposits its metadata in Crossref, and the article can begin
collecting citations on indexers like Google Scholar and other services.
Authors retain copyright of their JOSS article, releasing it under a Creative
Commons Attribution 4.0 International License. In its first year, starting in
May 2016, JOSS published 111 articles, with more than 40 additional articles
under review. JOSS is a sponsored project of the nonprofit organization
NumFOCUS and is an affiliate of the Open Source Initiative.
",computer-science
"  We have developed polynomial-time algorithms to generate terms of the
cogrowth series for groups $\mathbb{Z}\wr \mathbb{Z},$ the lamplighter group,
$(\mathbb{Z}\wr \mathbb{Z})\wr \mathbb{Z}$ and the Navas-Brin group $B.$ We
have also given an improved algorithm for the coefficients of Thompson's group
$F,$ giving 32 terms of the cogrowth series. We develop numerical techniques to
extract the asymptotics of these various cogrowth series. We present improved
rigorous lower bounds on the growth-rate of the cogrowth series for Thompson's
group $F$ using the method from \cite{HHR15} applied to our extended series. We
also generalise their method by showing that it applies to loops on any locally
finite graph. Unfortunately, lower bounds less than 16 do not help in
determining amenability.
Again for Thompson's group $F$ we prove that, if the group is amenable, there
cannot be a sub-dominant stretched exponential term in the
asymptotics\footnote{ }. Yet the numerical data provides compelling evidence
for the presence of such a term. This observation suggests a potential path to
a proof of non-amenability: If the universality class of the cogrowth sequence
can be determined rigorously, it will likely prove non-amenability.
We estimate the asymptotics of the cogrowth coefficients of $F$ to be $$ c_n
\sim c \cdot \mu^n \cdot \kappa^{n^\sigma \log^\delta{n}} \cdot n^g,$$ where
$\mu \approx 15,$ $\kappa \approx 1/e,$ $\sigma \approx 1/2,$ $\delta \approx
1/2,$ and $g \approx -1.$ The growth constant $\mu$ must be 16 for amenability.
These two approaches, plus a third based on extrapolating lower bounds, support
the conjecture \cite{ERvR15, HHR15} that the group is not amenable.
",mathematics
"  Let $F$ be a non-archimedean locally compact field. We study a class of
Langlands-Shahidi pairs $({\bf H},{\bf L})$, consisting of a quasi-split
connected reductive group $\bf H$ over $F$ and a Levi subgroup $\bf L$ which is
closely related to a product of restriction of scalars of ${\rm GL}_1$'s or
${\rm GL}_2$'s. We prove the compatibility of the resulting local factors with
the Langlands correspondence. In particular, let $E$ be a cubic separable
extension of $F$. We consider a simply connected quasi-split semisimple group
$\bf H$ over $F$ of type $D_4$, with triality corresponding to $E$, and let
$\bf L$ be its Levi subgroup with derived group ${\rm Res}_{E/F} {\rm SL}_2$.
In this way we obtain Asai cube local factors attached to irreducible smooth
representations of ${\rm GL}_2(E)$; we prove that they are Weil-Deligne factors
obtained via the local Langlands correspondence for ${\rm GL}_2(E)$ and tensor
induction from $E$ to $F$. A consequence is that Asai cube $\gamma$- and
$\varepsilon$-factors become stable under twists by highly ramified characters.
",mathematics
"  We obtain alternative explicit Specht filtrations for the induced and the
restricted Specht modules in the Hecke algebra of the symmetric group (defined
over the ring $A=\mathbb Z[q^{1/2},q^{-1/2}]$ where $q$ is an indeterminate)
using $C$-bases for these modules. Moreover, we provide a link between a
certain $C$-basis for the induced Specht module and the notion of pairs of
partitions.
",mathematics
"  We propose a deformable generator model to disentangle the appearance and
geometric information from images into two independent latent vectors. The
appearance generator produces the appearance information, including color,
illumination, identity or category, of an image. The geometric generator
produces displacement of the coordinates of each pixel and performs geometric
warping, such as stretching and rotation, on the appearance generator to obtain
the final synthesized image. The proposed model can learn both representations
from image data in an unsupervised manner. The learned geometric generator can
be conveniently transferred to the other image datasets to facilitate
downstream AI tasks.
",statistics
"  In this work, we introduce an online model for communication complexity.
Analogous to how online algorithms receive their input piece-by-piece, our
model presents one of the players, Bob, his input piece-by-piece, and has the
players Alice and Bob cooperate to compute a result each time before the next
piece is revealed to Bob. This model has a closer and more natural
correspondence to dynamic data structures than classic communication models do,
and hence presents a new perspective on data structures.
We first present a tight lower bound for the online set intersection problem
in the online communication model, demonstrating a general approach for proving
online communication lower bounds. The online communication model prevents a
batching trick that classic communication complexity allows, and yields a
stronger lower bound. We then apply the online communication model to prove
data structure lower bounds for two dynamic data structure problems: the Group
Range problem and the Dynamic Connectivity problem for forests. Both of the
problems admit a worst case $O(\log n)$-time data structure. Using online
communication complexity, we prove a tight cell-probe lower bound for each:
spending $o(\log n)$ (even amortized) time per operation results in at best an
$\exp(-\delta^2 n)$ probability of correctly answering a
$(1/2+\delta)$-fraction of the $n$ queries.
",computer-science
"  We define an infinite measure-preserving transformation to have infinite
symmetric ergodic index if all finite Cartesian products of the transformation
and its inverse are ergodic, and show that infinite symmetric ergodic index
does not imply that all products of powers are conservative, so does not imply
power weak mixing. We provide a sufficient condition for $k$-fold and infinite
symmetric ergodic index and use it to answer a question on the relationship
between product conservativity and product ergodicity. We also show that a
class of rank-one transformations that have infinite symmetric ergodic index
are not power weakly mixing, and precisely characterize a class of power weak
transformations that generalizes existing examples.
",mathematics
"  Electronic health records (EHR) data provide a cost and time-effective
opportunity to conduct cohort studies of the effects of multiple time-point
interventions in the diverse patient population found in real-world clinical
settings. Because the computational cost of analyzing EHR data at daily (or
more granular) scale can be quite high, a pragmatic approach has been to
partition the follow-up into coarser intervals of pre-specified length. Current
guidelines suggest employing a 'small' interval, but the feasibility and
practical impact of this recommendation has not been evaluated and no formal
methodology to inform this choice has been developed. We start filling these
gaps by leveraging large-scale EHR data from a diabetes study to develop and
illustrate a fast and scalable targeted learning approach that allows to follow
the current recommendation and study its practical impact on inference. More
specifically, we map daily EHR data into four analytic datasets using 90, 30,
15 and 5-day intervals. We apply a semi-parametric and doubly robust estimation
approach, the longitudinal TMLE, to estimate the causal effects of four dynamic
treatment rules with each dataset, and compare the resulting inferences. To
overcome the computational challenges presented by the size of these data, we
propose a novel TMLE implementation, the 'long-format TMLE', and rely on the
latest advances in scalable data-adaptive machine-learning software, xgboost
and h2o, for estimation of the TMLE nuisance parameters.
",statistics
"  Context: In the past decade, sensitive, resolved Sunyaev-Zel'dovich (SZ)
studies of galaxy clusters have become common. Whereas many previous SZ studies
have parameterized the pressure profiles of galaxy clusters, non-parametric
reconstructions will provide insights into the thermodynamic state of the
intracluster medium (ICM). Aims: We seek to recover the non-parametric pressure
profiles of the high redshift ($z=0.89$) galaxy cluster CLJ 1226.9+3332 as
inferred from SZ data from the MUSTANG, NIKA, Bolocam, and Planck instruments,
which all probe different angular scales. Methods: Our non-parametric algorithm
makes use of logarithmic interpolation, which under the assumption of
ellipsoidal symmetry is analytically integrable. For MUSTANG, NIKA, and Bolocam
we derive a non-parametric pressure profile independently and find good
agreement among the instruments. In particular, we find that the non-parametric
profiles are consistent with a fitted gNFW profile. Given the ability of Planck
to constrain the total signal, we include a prior on the integrated Compton Y
parameter as determined by Planck. Results: For a given instrument, constraints
on the pressure profile diminish rapidly beyond the field of view. The overlap
in spatial scales probed by these four datasets is therefore critical in
checking for consistency between instruments. By using multiple instruments,
our analysis of CLJ 1226.9+3332 covers a large radial range, from the central
regions to the cluster outskirts: $0.05 R_{500} < r < 1.1 R_{500}$. This is a
wider range of spatial scales than is typical recovered by SZ instruments.
Similar analyses will be possible with the new generation of SZ instruments
such as NIKA2 and MUSTANG2.
",physics
"  In this research, we employ accurate time-dependent density functional
calculations for ultrashort laser spectroscopy of nitrogen molecule. Laser
pulses with different frequencies, intensities, and durations are applied to
the molecule and the resulting photoelectron spectra are analyzed. It is argued
that relative orientation of the molecule in the laser pulse significantly
influence the orbital character of the emitted photoelectrons. Moreover, the
duration of the laser pulse is also found to be very effective in controlling
the orbital resolution and intensity of photoelectrons. Angular resolved
distribution of photoelectrons are computed at different pulse frequencies and
recording times. By exponential growth of the laser pulse intensity, the
theoretical threshold of two photons absorption in nitrogen molecule is
determined.
",physics
"  We show that there exist complete and minimal systems of time-frequency
shifts of Gaussians in $L^2(\mathbb{R})$ which are not strong Markushevich
basis (do not admit the spectral synthesis). In particular, it implies that
there is no linear summation method for general Gaussian Gabor expansions. On
the other hand we prove that the spectral synthesis for such Gabor systems
holds up to one dimensional defect.
",mathematics
"  In [MMO] (arXiv:1704.03413), we reworked and generalized equivariant infinite
loop space theory, which shows how to construct $G$-spectra from $G$-spaces
with suitable structure. In this paper, we construct a new variant of the
equivariant Segal machine that starts from the category $\scr{F}$ of finite
sets rather than from the category ${\scr{F}}_G$ of finite $G$-sets and which
is equivalent to the machine studied by Shimakawa and in [MMO]. In contrast to
the machine in [MMO], the new machine gives a lax symmetric monoidal functor
from the symmetric monoidal category of $\scr{F}$-$G$-spaces to the symmetric
monoidal category of orthogonal $G$-spectra. We relate it multiplicatively to
suspension $G$-spectra and to Eilenberg-MacLane $G$-spectra via lax symmetric
monoidal functors from based $G$-spaces and from abelian groups to
$\scr{F}$-$G$-spaces. Even non-equivariantly, this gives an appealing new
variant of the Segal machine. This new variant makes the equivariant
generalization of the theory essentially formal, hence is likely to be
applicable in other contexts.
",mathematics
"  We introduce a new variant of the game of Cops and Robbers played on graphs,
where the robber is invisible unless outside the neighbor set of a cop. The
hyperopic cop number is the corresponding analogue of the cop number, and we
investigate bounds and other properties of this parameter. We characterize the
cop-win graphs for this variant, along with graphs with the largest possible
hyperopic cop number. We analyze the cases of graphs with diameter 2 or at
least 3, focusing on when the hyperopic cop number is at most one greater than
the cop number. We show that for planar graphs, as with the usual cop number,
the hyperopic cop number is at most 3. The hyperopic cop number is considered
for countable graphs, and it is shown that for connected chains of graphs, the
hyperopic cop density can be any real number in $[0,1/2].$
",computer-science
"  A Pilot unit of a closed loop gas (CLS) mixing and distribution system for
the INO project was designed and is being operated with (1.8 x 1.9) m^2 glass
RPCs (Resistive Plate Chamber). The performance of an RPC depends on the
quality and quantity of gas mixture being used, a number of studies on
controlling the flow and optimization of the gas mixture is being carried out.
In this paper the effect of capillary as a dynamic impedance element on the
differential pressure across RPC detector in a closed loop gas system is being
highlighted. The flow versus the pressure variation with different types of
capillaries and also with different types of gasses that are being used in an
RPC is presented. An attempt is also made to measure the transient time of the
gas flow through the capillary.
",physics
"  We propose a novel approach to 3D human pose estimation from a single depth
map. Recently, convolutional neural network (CNN) has become a powerful
paradigm in computer vision. Many of computer vision tasks have benefited from
CNNs, however, the conventional approach to directly regress 3D body joint
locations from an image does not yield a noticeably improved performance. In
contrast, we formulate the problem as estimating per-voxel likelihood of key
body joints from a 3D occupancy grid. We argue that learning a mapping from
volumetric input to volumetric output with 3D convolution consistently improves
the accuracy when compared to learning a regression from depth map to 3D joint
coordinates. We propose a two-stage approach to reduce the computational
overhead caused by volumetric representation and 3D convolution: Holistic 2D
prediction and Local 3D prediction. In the first stage, Planimetric Network
(P-Net) estimates per-pixel likelihood for each body joint in the holistic 2D
space. In the second stage, Volumetric Network (V-Net) estimates the per-voxel
likelihood of each body joints in the local 3D space around the 2D estimations
of the first stage, effectively reducing the computational cost. Our model
outperforms existing methods by a large margin in publicly available datasets.
",computer-science
"  An analytical model of Human-Robot (H-R) coordination is presented for a
Human-Robot system executing a collaborative task in which a high level of
synchronization among the agents is desired. The influencing parameters and
decision variables that affect the waiting time of the collaborating agents
were analyzed. The performance of the model was evaluated based on the costs of
the waiting times of each of the agents at the pre-defined spatial point of
handover. The model was tested for two cases of dynamic H-R coordination
scenarios. Results indicate that this analytical model can be used as a tool
for designing an H-R system that optimizes the agent waiting time thereby
increasing the joint-efficiency of the system and making coordination fluent
and natural.
",computer-science
"  This paper aims at one-shot learning of deep neural nets, where a highly
parallel setting is considered to address the algorithm calibration problem -
selecting the best neural architecture and learning hyper-parameter values
depending on the dataset at hand. The notoriously expensive calibration problem
is optimally reduced by detecting and early stopping non-optimal runs. The
theoretical contribution regards the optimality guarantees within the multiple
hypothesis testing framework. Experimentations on the Cifar10, PTB and Wiki
benchmarks demonstrate the relevance of the approach with a principled and
consistent improvement on the state of the art with no extra hyper-parameter.
",computer-science
"  We extend the approach of wall modeling via function enrichment to
detached-eddy simulation. The wall model aims at using coarse cells in the
near-wall region by modeling the velocity profile in the viscous sublayer and
log-layer. However, unlike other wall models, the full Navier-Stokes equations
are still discretely fulfilled, including the pressure gradient and convective
term. This is achieved by enriching the elements of the high-order
discontinuous Galerkin method with the law-of-the-wall. As a result, the
Galerkin method can ""choose"" the optimal solution among the polynomial and
enrichment shape functions. The detached-eddy simulation methodology provides a
suitable turbulence model for the coarse near-wall cells. The approach is
applied to wall-modeled LES of turbulent channel flow in a wide range of
Reynolds numbers. Flow over periodic hills shows the superiority compared to an
equilibrium wall model under separated flow conditions.
",physics
"  We consider a piecewise deterministic Markov decision process, where the
expected exponential utility of total (nonnegative) cost is to be minimized.
The cost rate, transition rate and post-jump distributions are under control.
The state space is Borel, and the transition and cost rates are locally
integrable along the drift. Under natural conditions, we establish the
optimality equation, justify the value iteration algorithm, and show the
existence of a deterministic stationary optimal policy. Applied to special
cases, the obtained results already significantly improve some existing results
in the literature on finite horizon and infinite horizon discounted
risk-sensitive continuous-time Markov decision processes.
",mathematics
"  We present an application of deep generative models in the context of
partial-differential equation (PDE) constrained inverse problems. We combine a
generative adversarial network (GAN) representing an a priori model that
creates subsurface geological structures and their petrophysical properties,
with the numerical solution of the PDE governing the propagation of acoustic
waves within the earth's interior. We perform Bayesian inversion using an
approximate Metropolis-adjusted Langevin algorithm (MALA) to sample from the
posterior given seismic observations. Gradients with respect to the model
parameters governing the forward problem are obtained by solving the adjoint of
the acoustic wave equation. Gradients of the mismatch with respect to the
latent variables are obtained by leveraging the differentiable nature of the
deep neural network used to represent the generative model. We show that
approximate MALA sampling allows efficient Bayesian inversion of model
parameters obtained from a prior represented by a deep generative model,
obtaining a diverse set of realizations that reflect the observed seismic
response.
",statistics
"  Response delay is an inherent and essential part of human actions. In the
context of human balance control, the response delay is traditionally modeled
using the formalism of delay-differential equations, which adopts the
approximation of fixed delay. However, experimental studies revealing
substantial variability, adaptive anticipation, and non-stationary dynamics of
response delay provide evidence against this approximation. In this paper, we
call for development of principally new mathematical formalism describing human
response delay. To support this, we present the experimental data from a simple
virtual stick balancing task. Our results demonstrate that human response delay
is a widely distributed random variable with complex properties, which can
exhibit oscillatory and adaptive dynamics characterized by long-range
correlations. Given this, we argue that the fixed-delay approximation ignores
essential properties of human response, and conclude with possible directions
for future developments of new mathematical notions describing human control.
",quantitative-biology
"  Porous silicon layers (PS) have been prepared in this work via
Photoelectrochemical etching process (PEC) of n type silicon wafer of 0.8
ohm.cm resistivity in hydrofluoric (HF) acid of 24.5 precent concentration at
different etching times (5 to 25 min.). The irradiation has been achieved using
Tungsten lamp with different wavelengths (450 nm, 535 nm and 700 nm). The
morphological properties of these layers such as surface morphology, Porosity,
layer thickness, and also the etching rate have been investigated using optical
microscopy and the gravimetric method.
",physics
"  We present the results of three-dimensional (3D) ideal magnetohydrodynamics
(MHD) simulations on the dynamics of a perpendicularly inhomogeneous plasma
disturbed by propagating Alfvénic waves. Simpler versions of this scenario
have been extensively studied as the phenomenon of phase mixing. We show that,
by generalizing the textbook version of phase mixing, interesting phenomena are
obtained, such as turbulence-like behavior and complex current-sheet structure,
a novelty in longitudinally homogeneous plasma excited by unidirectionally
propagating waves. This constitutes an important finding for turbulence-related
phenomena in astrophysics in general, relaxing the conditions that have to be
fulfilled in order to generate turbulent behavior.
",physics
"  Extremal Graph Theory aims to determine bounds for graph invariants as well
as the graphs attaining those bounds.
We are currently developping PHOEG, an ecosystem of tools designed to help
researchers in Extremal Graph Theory.
It uses a big relational database of undirected graphs and works with the
convex hull of the graphs as points in the invariants space in order to exactly
obtain the extremal graphs and optimal bounds on the invariants for some fixed
parameters. The results obtained on the restricted finite class of graphs can
later be used to infer conjectures. This database also allows us to make
queries on those graphs. Once the conjecture defined, PHOEG goes one step
further by helping in the process of designing a proof guided by successive
applications of transformations from any graph to an extremal graph. To this
aim, we use a second database based on a graph data model.
The paper presents ideas and techniques used in PHOEG to assist the study of
Extremal Graph Theory.
",computer-science
"  Privacy has become a serious concern for modern Information Societies. The
sensitive nature of much of the data that are daily exchanged or released to
untrusted parties requires that responsible organizations undertake appropriate
privacy protection measures. Nowadays, much of these data are texts (e.g.,
emails, messages posted in social media, healthcare outcomes, etc.) that,
because of their unstructured and semantic nature, constitute a challenge for
automatic data protection methods. In fact, textual documents are usually
protected manually, in a process known as document redaction or sanitization.
To do so, human experts identify sensitive terms (i.e., terms that may reveal
identities and/or confidential information) and protect them accordingly (e.g.,
via removal or, preferably, generalization). To relieve experts from this
burdensome task, in a previous work we introduced the theoretical basis of
C-sanitization, an inherently semantic privacy model that provides the basis to
the development of automatic document redaction/sanitization algorithms and
offers clear and a priori privacy guarantees on data protection; even though
its potential benefits C-sanitization still presents some limitations when
applied to practice (mainly regarding flexibility, efficiency and accuracy). In
this paper, we propose a new more flexible model, named (C, g(C))-sanitization,
which enables an intuitive configuration of the trade-off between the desired
level of protection (i.e., controlled information disclosure) and the
preservation of the utility of the protected data (i.e., amount of semantics to
be preserved). Moreover, we also present a set of technical solutions and
algorithms that provide an efficient and scalable implementation of the model
and improve its practical accuracy, as we also illustrate through empirical
experiments.
",computer-science
"  Online experiments are a fundamental component of the development of
web-facing products. Given the large user-base, even small product improvements
can have a large impact on an absolute scale. As a result, accurately
estimating the relative impact of these changes is extremely important. I
propose an approach based on an objective Bayesian model to improve the
sensitivity of percent change estimation in A/B experiments. Leveraging
pre-period information, this approach produces more robust and accurate point
estimates and up to 50% tighter credible intervals than traditional methods.
The R package abpackage provides an implementation of the approach.
",statistics
"  Singular limits of 6D F-theory compactifications are often captured by
T-branes, namely a non-abelian configuration of intersecting 7-branes with a
nilpotent matrix of normal deformations. The long distance approximation of
such 7-branes is a Hitchin-like system in which simple and irregular poles
emerge at marked points of the geometry. When multiple matter fields localize
at the same point in the geometry, the associated Higgs field can exhibit
irregular behavior, namely poles of order greater than one. This provides a
geometric mechanism to engineer wild Higgs bundles. Physical constraints such
as anomaly cancellation and consistent coupling to gravity also limit the order
of such poles. Using this geometric formulation, we unify seemingly different
wild Hitchin systems in a single framework in which orders of poles become
adjustable parameters dictated by tuning gauge singlet moduli of the F-theory
model.
",mathematics
"  An \emph{ab initio} Langevin dynamics approach is developed based on
stochastic density functional theory (sDFT) within a new \emph{embedded
saturated } \emph{fragment }formalism, applicable to covalently bonded systems.
The forces on the nuclei generated by sDFT contain a random component natural
to Langevin dynamics and its standard deviation is used to estimate the
friction term on each atom by satisfying the fluctuation\textendash dissipation
relation. The overall approach scales linearly with system size even if the
density matrix is not local and is thus applicable to ordered as well as
disordered extended systems. We implement the approach for a series of silicon
nanocrystals (NCs) of varying size with a diameter of up to $3$nm corresponding
to $N_{e}=3000$ electrons and generate a set of configurations that are
distributed canonically at a fixed temperature, ranging from cryogenic to room
temperature. We also analyze the structure properties of the NCs and discuss
the reconstruction of the surface geometry.
",physics
"  We present a clustering-based language model using word embeddings for text
readability prediction. Presumably, an Euclidean semantic space hypothesis
holds true for word embeddings whose training is done by observing word
co-occurrences. We argue that clustering with word embeddings in the metric
space should yield feature representations in a higher semantic space
appropriate for text regression. Also, by representing features in terms of
histograms, our approach can naturally address documents of varying lengths. An
empirical evaluation using the Common Core Standards corpus reveals that the
features formed on our clustering-based language model significantly improve
the previously known results for the same corpus in readability prediction. We
also evaluate the task of sentence matching based on semantic relatedness using
the Wiki-SimpleWiki corpus and find that our features lead to superior matching
performance.
",computer-science
"  As a counterpart of the classical Yamabe problem, a fractional Yamabe flow
has been introduced by Jin and Xiong (2014) on the sphere. Here we pursue its
study in the context of general compact smooth manifolds with positive
fractional curvature. First, we prove that the flow is locally well posed in
the weak sense on any compact manifold. If the manifold is locally conformally
flat with positive Yamabe invariant, we also prove that the flow is smooth and
converges to a constant scalar curvature metric. We provide different proofs
using extension properties introduced by Chang and González (2011) for the
conformally covariant fractional order operators.
",mathematics
"  Pemantle and Steif provided a sharp threshold for the existence of a RPT
(robust phase transition) for the continuous rotator model and the Potts model
in terms of the branching number and the second eigenvalue of the transfer
operator, where a robust phase transition is said to occur if an arbitrarily
weak coupling with symmetry-breaking boundary conditions suffices to induce
symmetry breaking in the bulk. They further showed that for the Potts model RPT
occurs at a different threshold than PT (phase transition in the sense of
multiple Gibbs measures), and conjectured that RPT and PT should occur at the
same threshold in the continuous rotator model. We consider the class of 4- and
5-state rotation-invariant spin models with reflection symmetry on general
trees which contains the Potts model and the clock model with
scalarproduct-interaction as limiting cases. The clock model can be viewed as a
particular discretization which is obtained from the classical rotator model on
the continuous one-dimensional sphere. We analyze the transition between PT=RPT
and PT is unequal to RPT, in terms of the eigenvalues of the transfer matrix of
the model at the critical threshold value for the existence of RPT. The
transition between the two regimes depends sensitively on the third largest
eigenvalue.
",mathematics
"  This document is a response to a report from the University of Melbourne on
the privacy of the Opal dataset release. The Opal dataset was released by
Data61 (CSIRO) in conjunction with the Transport for New South Wales (TfNSW).
The data consists of two separate weeks of ""tap-on/tap-off"" data of individuals
who used any of the four different modes of public transport from TfNSW: buses,
light rail, train and ferries. These taps are recorded through the smart
ticketing system, known as Opal, available in the state of New South Wales,
Australia.
",computer-science
"  We report the synthesis and structural characterisation of the molecular
framework copper(I) hexacyanocobaltate(III), Cu$_3$[Co(CN)$_6$], which we find
to be isostructural to H$_3$[Co(CN)$_6$] and the colossal negative thermal
expansion material Ag$_3$[Co(CN)$_6$]. Using synchrotron X-ray powder
diffraction measurements, we find strong positive and negative thermal
expansion behaviour respectively perpendicular and parallel to the trigonal
crystal axis: $\alpha_a$ = 25.4(5)\,MK$^{-1}$ and $\alpha_c$ =
$-$43.5(8)\,MK$^{-1}$. These opposing effects collectively result in a volume
expansivity $\alpha_V$ = 7.4(11)\,MK$^{-1}$ that is remarkably small for an
anisotropic molecular framework. This thermal response is discussed in the
context of the behaviour of the analogous H- and Ag-containing systems. We make
use of density-functional theory with many-body dispersion interactions
(DFT+MBD) to demonstrate that Cu$\ldots$Cu metallophilic (`cuprophilic')
interactions are significantly weaker in Cu$_3$[Co(CN)$_6$] than Ag$\ldots$Ag
interactions in Ag$_3$[Co(CN)$_6$], but that this lowering of energy scale
counterintuitively translates to a more moderate---rather than
enhanced---degree of structural flexibility. The same conclusion is drawn from
consideration of a simple lattice dynamical model, which we also present here.
Our results demonstrate that strong interactions can actually be exploited in
the design of ultra-responsive materials if those interactions are set up to
act in tension.
",physics
"  We use Monte Carlo simulations to explore the statistical challenges of
constraining the characteristic mass ($m_c$) and width ($\sigma$) of a
lognormal sub-solar initial mass function (IMF) in Local Group dwarf galaxies
using direct star counts. For a typical Milky Way (MW) satellite ($M_{V} =
-8$), jointly constraining $m_c$ and $\sigma$ to a precision of $\lesssim 20\%$
requires that observations be complete to $\lesssim 0.2 M_{\odot}$, if the IMF
is similar to the MW IMF. A similar statistical precision can be obtained if
observations are only complete down to $0.4M_{\odot}$, but this requires
measurement of nearly 100$\times$ more stars, and thus, a significantly more
massive satellite ($M_{V} \sim -12$). In the absence of sufficiently deep data
to constrain the low-mass turnover, it is common practice to fit a
single-sloped power law to the low-mass IMF, or to fit $m_c$ for a lognormal
while holding $\sigma$ fixed. We show that the former approximation leads to
best-fit power law slopes that vary with the mass range observed and can
largely explain existing claims of low-mass IMF variations in MW satellites,
even if satellite galaxies have the same IMF as the MW. In addition, fixing
$\sigma$ during fitting leads to substantially underestimated uncertainties in
the recovered value of $m_c$ (by a factor of $\sim 4$ for typical
observations). If the IMFs of nearby dwarf galaxies are lognormal and do vary,
observations must reach down to $\sim m_c$ in order to robustly detect these
variations. The high-sensitivity, near-infrared capabilities of JWST and WFIRST
have the potential to dramatically improve constraints on the low-mass IMF. We
present an efficient observational strategy for using these facilities to
measure the IMFs of Local Group dwarf galaxies.
",physics
"  We develop a theory for non-degenerate parametric resonance in a tunable
superconducting cavity. We focus on nonlinear effects that are caused by
nonlinear Josephson elements connected to the cavity. We analyze parametric
amplification in a strong nonlinear regime at the parametric instability
threshold, and calculate maximum gain values. Above the threshold, in the
parametric oscillator regime the linear cavity response diverges at the
oscillator frequency at all pump strengths. We show that this divergence is
related to the continuous degeneracy of the free oscillator state with respect
to the phase. Applying on-resonance input lifts the degeneracy and removes the
divergence. We also investigate the quantum noise squeezing. It is shown that
in the strong amplification regime the noise undergoes four-mode squeezing, and
that in this regime the output signal to noise ratio can significantly exceed
the input value. We also analyze the intermode frequency conversion and
identify parameters at which full conversion is achieved.
",physics
"  Inviscid computational results are presented on a self-propelled swimmer
modeled as a virtual body combined with a two-dimensional hydrofoil pitching
intermittently about its leading edge. Lighthill (1971) originally proposed
that this burst-and-coast behavior can save fish energy during swimming by
taking advantage of the viscous Bone-Lighthill boundary layer thinning
mechanism. Here, an additional inviscid Garrick mechanism is discovered that
allows swimmers to control the ratio of their added mass thrust-producing
forces to their circulatory drag-inducing forces by decreasing their duty
cycle, DC, of locomotion. This mechanism can save intermittent swimmers as much
as 60% of the energy it takes to swim continuously at the same speed. The
inviscid energy savings are shown to increase with increasing amplitude of
motion, increase with decreasing Lighthill number, Li, and switch to an
energetic cost above continuous swimming for sufficiently low DC. Intermittent
swimmers are observed to shed four vortices per cycle that form into groups
that are self-similar with the DC. In addition, previous thrust and power
scaling laws of continuous self-propelled swimming are further generalized to
include intermittent swimming. The key is that by averaging the thrust and
power coefficients over only the bursting period then the intermittent problem
can be transformed into a continuous one. Furthermore, the intermittent thrust
and power scaling relations are extended to predict the mean speed and cost of
transport of swimmers. By tuning a few coefficients with a handful of
simulations these self-propelled relations can become predictive. In the
current study, the mean speed and cost of transport are predicted to within 3%
and 18% of their full-scale values by using these relations.
",physics
"  We show that the level sets of automorphisms of free groups with respect to
the Lipschitz metric are connected as subsets of Culler-Vogtmann space. In fact
we prove our result in a more general setting of deformation spaces. As
applications, we give metric solutions of the conjugacy problem for irreducible
automorphisms and the detection of reducibility. We additionally prove
technical results that may be of independent interest --- such as the fact that
the set of displacements is well ordered.
",mathematics
"  The Weyl semimetallic compound Eu2Ir2O7 along with its hole doped derivatives
(which is achieved by substituting trivalent Eu by divalent Sr) are
investigated through transport, magnetic and calorimetric studies. The
metal-insulator transition (MIT) temperature is found to get substantially
reduced with hole doping and for 10% Sr doping the composition is metallic down
to temperature as low as 5 K. These doped compounds are found to violate the
Mott-Ioffe-Regel condition for minimum electrical conductivity and show
distinct signature of non-Fermi liquid behavior at low temperature. The MIT in
the doped compounds does not correlate with the magnetic transition point and
Anderson-Mott type disorder induced localization may be attributed to the
ground state insulating phase. The observed non-Fermi liquid behavior can be
understood on the basis of disorder induced distribution of spin orbit coupling
parameter which is markedly different in case of Ir4+ and Ir5+ ions.
",physics
"  In this article, we use the strong law of large numbers to give a proof of
the Herschel-Maxwell theorem, which characterizes the normal distribution as
the distribution of the components of a spherically symmetric random vector,
provided they are independent. We present shorter proofs under additional
moment assumptions, and include a remark, which leads to another strikingly
short proof of Maxwell's characterization using the central limit theorem.
",mathematics
"  Correlated oxide heterostructures pose a challenging problem in condensed
matter research due to their structural complexity interweaved with demanding
electron states beyond the effective single-particle picture. By exploring the
correlated electronic structure of SmTiO$_3$ doped with few layers of SrO, we
provide an insight into the complexity of such systems. Furthermore, it is
shown how the advanced combination of band theory on the level of Kohn-Sham
density functional theory with explicit many-body theory on the level of
dynamical mean-field theory provides an adequate tool to cope with the problem.
Coexistence of band-insulating, metallic and Mott-critical electronic regions
is revealed in individual heterostructures with multi-orbital manifolds.
Intriguing orbital polarizations, that qualitatively vary between the metallic
and the Mott layers are also encountered.
",physics
"  We extend Rubio de Francia's extrapolation theorem for functions valued in
UMD Banach function spaces, leading to short proofs of some new and known
results. In particular we prove Littlewood-Paley-Rubio de Francia-type
estimates and boundedness of variational Carleson operators for Banach function
spaces with UMD concavifications.
",mathematics
"  Face image quality can be defined as a measure of the utility of a face image
to automatic face recognition. In this work, we propose (and compare) two
methods for automatic face image quality based on target face quality values
from (i) human assessments of face image quality (matcher-independent), and
(ii) quality values computed from similarity scores (matcher-dependent). A
support vector regression model trained on face features extracted using a deep
convolutional neural network (ConvNet) is used to predict the quality of a face
image. The proposed methods are evaluated on two unconstrained face image
databases, LFW and IJB-A, which both contain facial variations with multiple
quality factors. Evaluation of the proposed automatic face image quality
measures shows we are able to reduce the FNMR at 1% FMR by at least 13% for two
face matchers (a COTS matcher and a ConvNet matcher) by using the proposed face
quality to select subsets of face images and video frames for matching
templates (i.e., multiple faces per subject) in the IJB-A protocol. To our
knowledge, this is the first work to utilize human assessments of face image
quality in designing a predictor of unconstrained face quality that is shown to
be effective in cross-database evaluation.
",computer-science
"  Multi-armed bandit (MAB) is a class of online learning problems where a
learning agent aims to maximize its expected cumulative reward while repeatedly
selecting to pull arms with unknown reward distributions. We consider a
scenario where the reward distributions may change in a piecewise-stationary
fashion at unknown time steps. We show that by incorporating a simple
change-detection component with classic UCB algorithms to detect and adapt to
changes, our so-called M-UCB algorithm can achieve nearly optimal regret bound
on the order of $O(\sqrt{MKT\log T})$, where $T$ is the number of time steps,
$K$ is the number of arms, and $M$ is the number of stationary segments.
Comparison with the best available lower bound shows that our M-UCB is nearly
optimal in $T$ up to a logarithmic factor. We also compare M-UCB with the
state-of-the-art algorithms in numerical experiments using a public Yahoo!
dataset to demonstrate its superior performance.
",statistics
"  In this paper, we focus on developing driver-in-the loop fuel economic
control strategy for multiple connected vehicles. The control strategy is
considered to work in a driver assistance framework where the controller gives
command to a driver to follow while considering the ability of the driver in
following control commands. Our proposed method uses vehicle-to-vehicle (V2V)
communication, exploits traffic lights' Signal Phase and Timing (SPAT)
information, models driver error injection with Markov chain, and employs
scenario tree based stochastic model predictive control to improve vehicle fuel
economy and traffic mobility. The proposed strategy is decentralized in nature
as every vehicle evaluates its own strategy using only local information.
Simulation results show the effect of consideration of driver error injection
when synthesizing fuel economic controllers in a driver assistance fashion.
",computer-science
"  We study existence and multiplicity of semi-classical states for the
nonlinear Choquard equation:
$$ -\varepsilon^2\Delta v+V(x)v =
\frac{1}{\varepsilon^\alpha}(I_\alpha*F(v))f(v) \quad \hbox{in}\ \mathbb{R}^N,
$$ where $N\geq 3$, $\alpha\in (0,N)$, $I_\alpha(x)={A_\alpha\over
|x|^{N-\alpha}}$ is the Riesz potential, $F\in C^1(\mathbb{R},\mathbb{R})$,
$F'(s) = f(s)$ and $\varepsilon>0$ is a small parameter.
We develop a new variational approach and we show the existence of a family
of solutions concentrating, as $\varepsilon\to 0$, to a local minima of $V(x)$
under general conditions on $F(s)$. Our result is new also for
$f(s)=|s|^{p-2}s$ and applicable for $p\in (\frac{N+\alpha}{N},
\frac{N+\alpha}{N-2})$. Especially, we can give the existence result for
locally sublinear case $p\in (\frac{N+\alpha}{N}, 2)$, which gives a positive
answer to an open problem arisen in recent works of Moroz and Van Schaftingen.
We also study the multiplicity of positive single-peak solutions and we show
the existence of at least $\hbox{cupl}(K)+1$ solutions concentrating around $K$
as $\varepsilon\to 0$, where $K\subset \Omega$ is the set of minima of $V(x)$
in a bounded potential well $\Omega$, that is, $m_0 \equiv \inf_{x\in \Omega}
V(x) < \inf_{x\in \partial\Omega}V(x)$ and $K=\{x\in\Omega;\, V(x)=m_0\}$.
",mathematics
"  We construct embedded minimal surfaces which are $n$-periodic in
$\mathbb{R}^n$. They are new for codimension $n-2\ge 2$. We start with a Jordan
curve of edges of the $n$-dimensional cube. It bounds a Plateau minimal disk
which Schwarz reflection extends to a complete minimal surface. Studying the
group of Schwarz reflections, we can characterize those Jordan curves for which
the complete surface is embedded. For example, for $n=4$ exactly five such
Jordan curves generate embedded surfaces. Our results apply to surface classes
other than minimal as well, for instance polygonal surfaces.
",mathematics
"  The new index of the author's popularity estimation is represented in the
paper. The index is calculated on the basis of Wikipedia encyclopedia analysis
(Wikipedia Index - WI). Unlike the conventional existed citation indices, the
suggested mark allows to evaluate not only the popularity of the author, as it
can be done by means of calculating the general citation number or by the
Hirsch index, which is often used to measure the author's research rate. The
index gives an opportunity to estimate the author's popularity, his/her
influence within the sought-after area ""knowledge area"" in the Internet - in
the Wikipedia. The suggested index is supposed to be calculated in frames of
the subject domain, and it, on the one hand, avoids the mistaken computation of
the homonyms, and on the other hand - provides the entirety of the subject
area. There are proposed algorithms and the technique of the Wikipedia Index
calculation through the network encyclopedia sounding, the exemplified
calculations of the index for the prominent researchers, and also the methods
of the information networks formation - models of the subject domains by the
automatic monitoring and networks information reference resources analysis. The
considered in the paper notion network corresponds the terms-heads of the
Wikipedia articles.
",computer-science
"  Current tools for exploratory data analysis (EDA) require users to manually
select data attributes, statistical computations and visual encodings. This can
be daunting for large-scale, complex data. We introduce Foresight, a system
that helps the user rapidly discover visual insights from large
high-dimensional datasets. Formally, an ""insight"" is a strong manifestation of
a statistical property of the data, e.g., high correlation between two
attributes, high skewness or concentration about the mean of a single
attribute, a strong clustering of values, and so on. For each insight type,
Foresight initially presents visualizations of the top k instances in the data,
based on an appropriate ranking metric. The user can then look at ""nearby""
insights by issuing ""insight queries"" containing constraints on insight
strengths and data attributes. Thus the user can directly explore the space of
insights, rather than the space of data dimensions and visual encodings as in
other visual recommender systems. Foresight also provides ""global"" views of
insight space to help orient the user and ensure a thorough exploration
process. Furthermore, Foresight facilitates interactive exploration of large
datasets through fast, approximate sketching.
",computer-science
"  We propose a method for dual-arm manipulation of rigid objects, subject to
external disturbance. The problem is formulated as a Cartesian impedance
controller within a projected inverse dynamics framework. We use the
constrained component of the controller to enforce contact and the
unconstrained controller to accomplish the task with a desired 6-DOF impedance
behaviour. Furthermore, the proposed method optimises the torque required to
maintain contact, subject to unknown disturbances, and can do so without direct
measurement of external force. The techniques are evaluated on a single-arm
wiping a table and a dual-arm platform manipulating a rigid object of unknown
mass and with human interaction.
",computer-science
"  This note deals with certain properties of convex functions. We provide
results on the convexity of the set of minima of these functions, the behaviour
of their subgradient set under restriction, and optimization of these functions
over an affine subspace.
",mathematics
"  Many real-world networks known as attributed networks contain two types of
information: topology information and node attributes. It is a challenging task
on how to use these two types of information to explore structural
regularities. In this paper, by characterizing potential relationship between
link communities and node attributes, a principled statistical model named
PSB_PG that generates link topology and node attributes is proposed. This model
for generating links is based on the stochastic blockmodels following a Poisson
distribution. Therefore, it is capable of detecting a wide range of network
structures including community structures, bipartite structures and other
mixture structures. The model for generating node attributes assumes that node
attributes are high dimensional and sparse and also follow a Poisson
distribution. This makes the model be uniform and the model parameters can be
directly estimated by expectation-maximization (EM) algorithm. Experimental
results on artificial networks and real networks containing various structures
have shown that the proposed model PSB_PG is not only competitive with the
state-of-the-art models, but also provides good semantic interpretation for
each community via the learned relationship between the community and its
related attributes.
",computer-science
"  We consider restless multi-armed bandit (RMAB) with a finite horizon and
multiple pulls per period. Leveraging the Lagrangian relaxation, we approximate
the problem with a collection of single arm problems. We then propose an
index-based policy that uses optimal solutions of the single arm problems to
index individual arms, and offer a proof that it is asymptotically optimal as
the number of arms tends to infinity. We also use simulation to show that this
index-based policy performs better than the state-of-art heuristics in various
problem settings.
",mathematics
"  We define a new method to estimate centroid for text classification based on
the symmetric KL-divergence between the distribution of words in training
documents and their class centroids. Experiments on several standard data sets
indicate that the new method achieves substantial improvements over the
traditional classifiers.
",statistics
"  We introduce a family of tensor network states that we term semi-injective
Projected Entangled-Pair States (PEPS). They extend the class of injective PEPS
and include other states, like the ground states of the AKLT and the CZX models
in square lattices. We construct parent Hamiltonians for which semi-injective
PEPS are unique ground states. We also determine the necessary and sufficient
conditions for two tensors to generate the same family of such states in two
spatial dimensions. Using this result, we show that the third cohomology
labeling of Symmetry Protected Topological phases extends to semi-injective
PEPS.
",physics
"  We study the nonsymmetric Macdonald polynomials specialized at infinity from
various points of view. First, we define a family of modules of the Iwahori
algebra whose characters are equal to the nonsymmetric Macdonald polynomials
specialized at infinity. Second, we show that these modules are isomorphic to
the dual spaces of sections of certain sheaves on the semi-infinite Schubert
varieties. Third, we prove that the global versions of these modules are
homologically dual to the level one affine Demazure modules.
",mathematics
"  In this paper, we study further properties and applications of weighted
homology and persistent homology. We introduce the Mayer-Vietoris sequence and
generalized Bockstein spectral sequence for weighted homology. For
applications, we show an algorithm to construct a filtration of weighted
simplicial complexes from a weighted network. We also prove a theorem that
allows us to calculate the mod $p^2$ weighted persistent homology given some
information on the mod $p$ weighted persistent homology.
",mathematics
"  We prove Cherlin's conjecture, concerning binary primitive permutation
groups, for those groups with socle isomorphic to $\mathrm{PSL}_2(q)$,
${^2\mathrm{B}_2}(q)$, ${^2\mathrm{G}_2}(q)$ or $\mathrm{PSU}_3(q)$. Our method
uses the notion of a ""strongly non-binary action"".
",mathematics
"  For every $q\in \mathbb N$ let $\textrm{FO}_q$ denote the class of sentences
of first-order logic FO of quantifier rank at most $q$. If a graph property can
be defined in $\textrm{FO}_q$, then it can be decided in time $O(n^q)$. Thus,
minimizing $q$ has favorable algorithmic consequences. Many graph properties
amount to the existence of a certain set of vertices of size $k$. Usually this
can only be expressed by a sentence of quantifier rank at least $k$. We use the
color-coding method to demonstrate that some (hyper)graph problems can be
defined in $\textrm{FO}_q$ where $q$ is independent of $k$. This property of a
graph problem is equivalent to the question of whether the corresponding
parameterized problem is in the class $\textrm{para-AC}^0$.
It is crucial for our results that the FO-sentences have access to built-in
addition and multiplication. It is known that then FO corresponds to the
circuit complexity class uniform $\textrm{AC}^0$. We explore the connection
between the quantifier rank of FO-sentences and the depth of
$\textrm{AC}^0$-circuits, and prove that $\textrm{FO}_q \subsetneq
\textrm{FO}_{q+1}$ for structures with built-in addition and multiplication.
",computer-science
"  Draft of textbook chapter on neural machine translation. a comprehensive
treatment of the topic, ranging from introduction to neural networks,
computation graphs, description of the currently dominant attentional
sequence-to-sequence model, recent refinements, alternative architectures and
challenges. Written as chapter for the textbook Statistical Machine
Translation. Used in the JHU Fall 2017 class on machine translation.
",computer-science
"  The paper analyzes special cyclic Jacobi methods for symmetric matrices of
order $4$. Only those cyclic pivot strategies that enable full parallelization
of the method are considered. These strategies, unlike the serial pivot
strategies, can force the method to be very slow or very fast within one cycle,
depending on the underlying matrix. Hence, for the global convergence proof one
has to consider two or three adjacent cycles. It is proved that for any
symmetric matrix $A$ of order~$4$ the inequality
$S(A^{[2]})\leq(1-10^{-5})S(A)$ holds, where $A^{[2]}$ results from $A$ by
applying two cycles of a particular parallel method. Here $S(A)$ stands for the
Frobenius norm of the strictly upper-triangular part of $A$. The result holds
for two special parallel strategies and implies the global convergence of the
method under all possible fully parallel strategies. It is also proved that for
every $\epsilon>0$ and $n\geq4$ there exist a symmetric matrix $A(\epsilon)$ of
order $n$ and a cyclic strategy, such that upon completion of the first cycle
of the appropriate Jacobi method the inequality $S(A^{[1]})>
(1-\epsilon)S(A(\epsilon))$ holds.
",mathematics
"  AA Tau is the archetype for a class of stars with a peculiar periodic
photometric variability thought to be related to a warped inner disk structure
with a nearly edge-on viewing geometry. We present high resolution ($\sim$0.2"")
ALMA observations of the 0.87 and 1.3~mm dust continuum emission from the disk
around AA Tau. These data reveal an evenly spaced three-ringed emission
structure, with distinct peaks at 0.34"", 0.66"", and 0.99"", all viewed at a
modest inclination of 59.1$^{\circ}\pm$0.3$^{\circ}$ (decidedly not edge-on).
In addition to this ringed substructure, we find non-axisymmetric features
including a `bridge' of emission that connects opposite sides of the innermost
ring. We speculate on the nature of this `bridge' in light of accompanying
observations of HCO$^+$ and $^{13}$CO (J=3--2) line emission. The HCO$^+$
emission is bright interior to the innermost dust ring, with a projected
velocity field that appears rotated with respect to the resolved disk geometry,
indicating the presence of a warp or inward radial flow. We suggest that the
continuum bridge and HCO$^+$ line kinematics could originate from gap-crossing
accretion streams, which may be responsible for the long-duration dimming of
optical light from AA Tau.
",physics
"  We present a simple categorical framework for the treatment of probabilistic
theories, with the aim of reconciling the fields of Categorical Quantum
Mechanics (CQM) and Operational Probabilistic Theories (OPTs). In recent years,
both CQM and OPTs have found successful application to a number of areas in
quantum foundations and information theory: they present many similarities,
both in spirit and in formalism, but they remain separated by a number of
subtle yet important differences. We attempt to bridge this gap, by adopting a
minimal number of operationally motivated axioms which provide clean
categorical foundations, in the style of CQM, for the treatment of the problems
that OPTs are concerned with.
",mathematics
"  The nonnegative inverse eigenvalue problem (NIEP) asks which lists of $n$
complex numbers (counting multiplicity) occur as the eigenvalues of some
$n$-by-$n$ entry-wise nonnegative matrix. The NIEP has a long history and is a
known hard (perhaps the hardest in matrix analysis?) and sought after problem.
Thus, there are many subproblems and relevant results in a variety of
directions. We survey most work on the problem and its several variants, with
an emphasis on recent results, and include 130 references. The survey is
divided into: a) the single eigenvalue problems; b) necessary conditions; c)
low dimensional results; d) sufficient conditions; e) appending 0's to achieve
realizability; f) the graph NIEP's; g) Perron similarities; and h) the
relevance of Jordan structure.
",mathematics
"  Most complex networks are not static, but evolve along time. Given a specific
configuration of one such changing network, it becomes a particularly
interesting issue to quantify the diversity of possible unfoldings of its
topology. In this work, we suggest the concept of malleability of a network,
which is defined as the exponential of the entropy of the probabilities of each
possible unfolding with respect to a given configuration. We calculate the
malleability with respect to specific measurements of the involved topologies.
More specifically, we identify the possible topologies derivable from a given
configuration and calculate some topological measurement of them (e.g.
clustering coefficient, shortest path length, assortativity, etc.), leading to
respective probabilities being associated to each possible measurement value.
Though this approach implies some level of degeneracy in the mapping from
topology to measurement space, it still paves the way to inferring the
malleability of specific network types with respect to given topological
measurements. We report that the malleability, in general, depends on each
specific measurement, with the average shortest path length and degree
assortativity typically leading to large malleability values. The maximum
malleability was observed for the Wikipedia network and the minimum for the
Watts-Strogatz model.
",computer-science
"  WTe2 and its sister alloys have attracted tremendous attentions recent years
due to the large non-saturating magnetoresistance and topological non-trivial
properties. Herein, we briefly review the electrical property studies on this
new quantum material.
",physics
"  Tensor train (TT) decomposition provides a space-efficient representation for
higher-order tensors. Despite its advantage, we face two crucial limitations
when we apply the TT decomposition to machine learning problems: the lack of
statistical theory and of scalable algorithms. In this paper, we address the
limitations. First, we introduce a convex relaxation of the TT decomposition
problem and derive its error bound for the tensor completion task. Next, we
develop an alternating optimization method with a randomization technique, in
which the time complexity is as efficient as the space complexity is. In
experiments, we numerically confirm the derived bounds and empirically
demonstrate the performance of our method with a real higher-order tensor.
",statistics
"  We measure the Planck cluster mass bias using dynamical mass measurements
based on velocity dispersions of a subsample of 17 Planck-detected clusters.
The velocity dispersions were calculated using redshifts determined from
spectra obtained at Gemini observatory with the GMOS multi-object spectrograph.
We correct our estimates for effects due to finite aperture, Eddington bias and
correlated scatter between velocity dispersion and the Planck mass proxy. The
result for the mass bias parameter, $(1-b)$, depends on the value of the galaxy
velocity bias $b_v$ adopted from simulations: $(1-b)=(0.51\pm0.09) b_v^3$.
Using a velocity bias of $b_v=1.08$ from Munari et al., we obtain
$(1-b)=0.64\pm 0.11$, i.e, an error of 17% on the mass bias measurement with 17
clusters. This mass bias value is consistent with most previous weak lensing
determinations. It lies within $1\sigma$ of the value needed to reconcile the
Planck cluster counts with the Planck primary CMB constraints. We emphasize
that uncertainty in the velocity bias severely hampers precision measurements
of the mass bias using velocity dispersions. On the other hand, when we fix the
Planck mass bias using the constraints from Penna-Lima et al., based on weak
lensing measurements, we obtain a positive velocity bias $b_v \gtrsim 0.9$ at
$3\sigma$.
",physics
"  In this study, we developed a method to estimate the relationship between
stimulation current and volatility during isometric contraction. In functional
electrical stimulation (FES), joints are driven by applying voltage to muscles.
This technology has been used for a long time in the field of rehabilitation,
and recently application oriented research has been reported. However,
estimation of the relationship between stimulus value and exercise capacity has
not been discussed to a great extent. Therefore, in this study, a human muscle
model was estimated using the transfer function estimation method with fast
Fourier transform. It was found that the relationship between stimulation
current and force exerted could be expressed by a first-order lag system. In
verification of the force estimate, the ability of the proposed model to
estimate the exerted force under steady state response was found to be good.
",quantitative-biology
"  Evaluating the computational reproducibility of data analysis pipelines has
become a critical issue. It is, however, a cumbersome process for analyses that
involve data from large populations of subjects, due to their computational and
storage requirements. We present a method to predict the computational
reproducibility of data analysis pipelines in large population studies. We
formulate the problem as a collaborative filtering process, with constraints on
the construction of the training set. We propose 6 different strategies to
build the training set, which we evaluate on 2 datasets, a synthetic one
modeling a population with a growing number of subject types, and a real one
obtained with neuroinformatics pipelines. Results show that one sampling
method, ""Random File Numbers (Uniform)"" is able to predict computational
reproducibility with a good accuracy. We also analyze the relevance of
including file and subject biases in the collaborative filtering model. We
conclude that the proposed method is able to speedup reproducibility
evaluations substantially, with a reduced accuracy loss.
",statistics
"  Self-organization is a natural phenomenon that emerges in systems with a
large number of interacting components. Self-organized systems show robustness,
scalability, and flexibility, which are essential properties when handling
real-world problems. Swarm intelligence seeks to design nature-inspired
algorithms with a high degree of self-organization. Yet, we do not know why
swarm-based algorithms work well and neither we can compare the different
approaches in the literature. The lack of a common framework capable of
characterizing these several swarm-based algorithms, transcending their
particularities, has led to a stream of publications inspired by different
aspects of nature without much regard as to whether they are similar to already
existing approaches. We address this gap by introducing a network-based
framework$-$the interaction network$-$to examine computational swarm-based
systems via the optics of social dynamics. We discuss the social dimension of
several swarm classes and provide a case study of the Particle Swarm
Optimization. The interaction network enables a better understanding of the
plethora of approaches currently available by looking at them from a general
perspective focusing on the structure of the social interactions.
",computer-science
"  A featured transition system is a transition system in which the transitions
are annotated with feature expressions: Boolean expressions on a finite number
of given features. Depending on its feature expression, each individual
transition can be enabled when some features are present, and disabled for
other sets of features. The behavior of a featured transition system hence
depends on a given set of features. There are algorithms for featured
transition systems which can check their properties for all sets of features at
once, for example for LTL or CTL properties.
Here we introduce a model of featured weighted automata which combines
featured transition systems and (semiring-) weighted automata. We show that
methods and techniques from weighted automata extend to featured weighted
automata and devise algorithms to compute quantitative properties of featured
weighted automata for all sets of features at once. We show applications to
minimum reachability and to energy properties.
",computer-science
"  We present a novel methodology to enable control of a neuromorphic circuit in
close analogy with the physiological neuromodulation of a single neuron. The
methodology is general in that it only relies on a parallel interconnection of
elementary voltage-controlled current sources. In contrast to controlling a
nonlinear circuit through the parameter tuning of a state-space model, our
approach is purely input-output. The circuit elements are controlled and
interconnected to shape the current-voltage characteristics (I-V curves) of the
circuit in prescribed timescales. In turn, shaping those I-V curves determines
the excitability properties of the circuit. We show that this methodology
enables both robust and accurate control of the circuit behavior and resembles
the biophysical mechanisms of neuromodulation. As a proof of concept, we
simulate a SPICE model composed of MOSFET transconductance amplifiers operating
in the weak inversion regime.
",quantitative-biology
"  Vehicle-to-vehicle communications can change the driving behavior of drivers
significantly by providing them rich information on downstream traffic flow
conditions. This study seeks to model the varying car-following behaviors
involving connected vehicles and human-driving vehicles in mixed traffic flow.
A revised car-following model is developed using an intelligent driver model
(IDM) to capture drivers' perceptions of their preceding traffic conditions
through vehicle-to-vehicle communications. Stability analysis of the mixed
traffic flow is conducted for a specific case. Numerical results show that the
stable region is apparently enlarged compared with the IDM.
",computer-science
"  We present a clustering comparison of 12 galaxy formation models (including
Semi-Analytic Models (SAMs) and Halo Occupation Distribution (HOD) models) all
run on halo catalogues and merger trees extracted from a single {\Lambda}CDM
N-body simulation. We compare the results of the measurements of the mean halo
occupation numbers, the radial distribution of galaxies in haloes and the
2-Point Correlation Functions (2PCF). We also study the implications of the
different treatments of orphan (galaxies not assigned to any dark matter
subhalo) and non-orphan galaxies in these measurements. Our main result is that
the galaxy formation models generally agree in their clustering predictions but
they disagree significantly between HOD and SAMs for the orphan satellites.
Although there is a very good agreement between the models on the 2PCF of
central galaxies, the scatter between the models when orphan satellites are
included can be larger than a factor of 2 for scales smaller than 1 Mpc/h. We
also show that galaxy formation models that do not include orphan satellite
galaxies have a significantly lower 2PCF on small scales, consistent with
previous studies. Finally, we show that the 2PCF of orphan satellites is
remarkably different between SAMs and HOD models. Orphan satellites in SAMs
present a higher clustering than in HOD models because they tend to occupy more
massive haloes. We conclude that orphan satellites have an important role on
galaxy clustering and they are the main cause of the differences in the
clustering between HOD models and SAMs.
",physics
"  In a reversible language, any forward computation can be undone by a finite
sequence of backward steps. Reversible computing has been studied in the
context of different programming languages and formalisms, where it has been
used for testing and verification, among others. In this paper, we consider a
subset of Erlang, a functional and concurrent programming language based on the
actor model. We present a formal semantics for reversible computation in this
language and prove its main properties, including its causal consistency. We
also build on top of it a rollback operator that can be used to undo the
actions of a process up to a given checkpoint.
",computer-science
"  The way Quantum Mechanics (QM) is introduced to people used to Classical
Mechanics (CM) is by a complete change of the general methodology) despite QM
historically stemming from CM as a means to explain experimental results.
Therefore, it is desirable to build a bridge from CM to QM.
This paper presents a generalization of CM to QM. It starts from the
generalization of a point-like object and naturally arrives at the quantum
state vector of quantum systems in the complex valued Hilbert space, its time
evolution and quantum representation of a measurement apparatus of any size.
Each time, when generalization is performed, there is a possibility to develop
new theory giving up most simple generalizations. It is shown that a
measurement apparatus is a special case of a general quantum object. An example
of a measurement apparatus of an intermediate size is considered in the end.
",physics
"  Safe interaction with human drivers is one of the primary challenges for
autonomous vehicles. In order to plan driving maneuvers effectively, the
vehicle's control system must infer and predict how humans will behave based on
their latent internal state (e.g., intentions and aggressiveness). This
research uses a simple model for human behavior with unknown parameters that
make up the internal states of the traffic participants and presents a method
for quantifying the value of estimating these states and planning with their
uncertainty explicitly modeled. An upper performance bound is established by an
omniscient Monte Carlo Tree Search (MCTS) planner that has perfect knowledge of
the internal states. A baseline lower bound is established by planning with
MCTS assuming that all drivers have the same internal state. MCTS variants are
then used to solve a partially observable Markov decision process (POMDP) that
models the internal state uncertainty to determine whether inferring the
internal state offers an advantage over the baseline. Applying this method to a
freeway lane changing scenario reveals that there is a significant performance
gap between the upper bound and baseline. POMDP planning techniques come close
to closing this gap, especially when important hidden model parameters are
correlated with measurable parameters.
",computer-science
"  Defining the $m$-th stratum of a closed subset of an $n$ dimensional
Euclidean space to consist of those points, where it can be touched by a ball
from at least $n-m$ linearly independent directions, we establish that the
$m$-th stratum is second-order rectifiable of dimension $m$ and a Borel set.
This was known for convex sets, but is new even for sets of positive reach. The
result is based on a new criterion for second-order rectifiability.
",mathematics
"  Contributions of the CODALEMA/EXTASIS experiment to the 35th International
Cosmic Ray Conference, 12-20 July 2017, Busan, South Korea.
",physics
"  Software as a Service cloud computing model favorites the Multi-Tenancy as a
key factor to exploit economies of scale. However Multi-Tenancy present several
disadvantages. Therein, our approach comes to assign instances to multi-tenants
with an optimal solution while ensuring more economies of scale and avoiding
tenants hesitation to share resources. The present paper present the
architecture of our user-aware multi-tenancy SaaS approach based on the use of
rich-variant components. The proposed approach seek to model services
functional customization as well as automation of computing the optimal
distribution of instances by tenants. The proposed model takes into
consideration tenants functional requirements and tenants deployment
requirements to deduce an optimal distribution using essentially a specific
variability engine and a graph-based execution framework.
",computer-science
"  We study the Kitaev chain under generalized twisted boundary conditions, for
which both the amplitudes and the phases of the boundary couplings can be tuned
at will. We explicitly show the presence of exact zero modes for large chains
belonging to the topological phase in the most general case, in spite of the
absence of ""edges"" in the system. For specific values of the phase parameters,
we rigorously obtain the condition for the presence of the exact zero modes in
finite chains, and show that the zero modes obtained are indeed localized. The
full spectrum of the twisted chains with zero chemical potential is
analytically presented. Finally, we demonstrate the persistence of zero modes
(level crossing) even in the presence of disorder or interactions.
",physics
"  Object detection in wide area motion imagery (WAMI) has drawn the attention
of the computer vision research community for a number of years. WAMI proposes
a number of unique challenges including extremely small object sizes, both
sparse and densely-packed objects, and extremely large search spaces (large
video frames). Nearly all state-of-the-art methods in WAMI object detection
report that appearance-based classifiers fail in this challenging data and
instead rely almost entirely on motion information in the form of background
subtraction or frame-differencing. In this work, we experimentally verify the
failure of appearance-based classifiers in WAMI, such as Faster R-CNN and a
heatmap-based fully convolutional neural network (CNN), and propose a novel
two-stage spatio-temporal CNN which effectively and efficiently combines both
appearance and motion information to significantly surpass the state-of-the-art
in WAMI object detection. To reduce the large search space, the first stage
(ClusterNet) takes in a set of extremely large video frames, combines the
motion and appearance information within the convolutional architecture, and
proposes regions of objects of interest (ROOBI). These ROOBI can contain from
one to clusters of several hundred objects due to the large video frame size
and varying object density in WAMI. The second stage (FoveaNet) then estimates
the centroid location of all objects in that given ROOBI simultaneously via
heatmap estimation. The proposed method exceeds state-of-the-art results on the
WPAFB 2009 dataset by 5-16% for moving objects and nearly 50% for stopped
objects, as well as being the first proposed method in wide area motion imagery
to detect completely stationary objects.
",computer-science
"  We introduce the abstract notion of a closed necklical set in order to
describe a functorial combinatorial model of the free loop fibration $\Omega
Y\rightarrow \Lambda Y\rightarrow Y$ over the geometric realization $Y=|X|$ of
a path connected simplicial set $X.$ In particular, to any path connected
simplicial set $X$ we associate a closed necklical set
$\widehat{\mathbf{\Lambda}}X$ such that its geometric realization
$|\widehat{\mathbf{\Lambda}}X|$, a space built out of gluing ""freehedrical"" and
""cubical"" cells, is homotopy equivalent to the free loop space $\Lambda Y$ and
the differential graded module of chains $C_*(\widehat{\mathbf{\Lambda}}X)$
generalizes the coHochschild chain complex of the chain coalgebra $C_\ast(X).$
",mathematics
"  In this note we show that a mutation theory of species with potential can be
defined so that a certain class of skew-symmetrizable integer matrices have a
species realization admitting a non-degenerate potential. This gives a partial
affirmative answer to a question raised by Jan Geuenich and Daniel
Labardini-Fragoso. We also provide an example of a class of skew-symmetrizable
$4 \times 4$ integer matrices, which are not globally unfoldable nor strongly
primitive, and that have a species realization admitting a non-degenerate
potential.
",mathematics
"  Bike sharing is a vital component of a modern multi-modal transportation
system. However, its implementation can lead to bike supply-demand imbalance
due to fluctuating spatial and temporal demands. This study proposes a
comprehensive framework to develop optimal dynamic bike rebalancing strategies
in a large bike sharing network. It consists of three components, including a
station-level pick-up/drop-off prediction model, station clustering model, and
capacitated location-routing optimization model. For the first component, we
propose a powerful deep learning model called graph convolution neural network
model (GCNN) with data-driven graph filter (DDGF), which can automatically
learn the hidden spatial-temporal correlations among stations to provide more
accurate predictions; for the second component, we apply a graph clustering
algorithm labeled the Community Detection algorithm to cluster stations that
locate geographically close to each other and have a small net demand gap;
last, a capacitated location-routing problem (CLRP) is solved to deal with the
combination of two types of decision variables: the locations of bike
distribution centers and the design of distribution routes for each cluster.
",statistics
"  We consider time-dependent viscous Mean-Field Games systems in the case of
local, decreasing and unbounded coupling. These systems arise in mean-field
game theory, and describe Nash equilibria of games with a large number of
agents aiming at aggregation. We prove the existence of weak solutions that are
minimisers of an associated non-convex functional, by rephrasing the problem in
a convex framework. Under additional assumptions involving the growth at
infinity of the coupling, the Hamiltonian, and the space dimension, we show
that such minimisers are indeed classical solutions by a blow-up argument and
additional Sobolev regularity for the Fokker-Planck equation. We exhibit an
example of non-uniqueness of solutions. Finally, by means of a contraction
principle, we observe that classical solutions exist just by local regularity
of the coupling if the time horizon is short.
",mathematics
"  There have been many discriminative learning methods using convolutional
neural networks (CNN) for several image restoration problems, which learn the
mapping function from a degraded input to the clean output. In this letter, we
propose a self-committee method that can find enhanced restoration results from
the multiple trial of a trained CNN with different but related inputs.
Specifically, it is noted that the CNN sometimes finds different mapping
functions when the input is transformed by a reversible transform and thus
produces different but related outputs with the original. Hence averaging the
outputs for several different transformed inputs can enhance the results as
evidenced by the network committee methods. Unlike the conventional committee
approaches that require several networks, the proposed method needs only a
single network. Experimental results show that adding an additional transform
as a committee always brings additional gain on image denoising and single
image supre-resolution problems.
",computer-science
"  Many scenarios require a robot to be able to explore its 3D environment
online without human supervision. This is especially relevant for inspection
tasks and search and rescue missions. To solve this high-dimensional path
planning problem, sampling-based exploration algorithms have proven successful.
However, these do not necessarily scale well to larger environments or spaces
with narrow openings. This paper presents a 3D exploration planner based on the
principles of Next-Best Views (NBVs). In this approach, a Micro-Aerial Vehicle
(MAV) equipped with a limited field-of-view depth sensor randomly samples its
configuration space to find promising future viewpoints. In order to obtain
high sampling efficiency, our planner maintains and uses a history of visited
places, and locally optimizes the robot's orientation with respect to
unobserved space. We evaluate our method in several simulated scenarios, and
compare it against a state-of-the-art exploration algorithm. The experiments
show substantial improvements in exploration time ($2\times$ faster),
computation time, and path length, and advantages in handling difficult
situations such as escaping dead-ends (up to $20\times$ faster). Finally, we
validate the on-line capability of our algorithm on a computational constrained
real world MAV.
",computer-science
"  We study the stability of a recently proposed model of scalar-field matter
called mimetic dark matter or imperfect dark matter. It has been known that
mimetic matter with higher derivative terms suffers from gradient instabilities
in scalar perturbations. To seek for an instability-free extension of imperfect
dark matter, we develop an effective theory of cosmological perturbations
subject to the constraint on the scalar field's kinetic term. This is done by
using the unifying framework of general scalar-tensor theories based on the ADM
formalism. We demonstrate that it is indeed possible to construct a model of
imperfect dark matter which is free from ghost and gradient instabilities. As a
side remark, we also show that mimetic $F({\cal R})$ theory is plagued with the
Ostrogradsky instability.
",physics
"  Deep convolutional neural network (CNN) inference requires significant amount
of memory and computation, which limits its deployment on embedded devices. To
alleviate these problems to some extent, prior research utilize low precision
fixed-point numbers to represent the CNN weights and activations. However, the
minimum required data precision of fixed-point weights varies across different
networks and also across different layers of the same network. In this work, we
propose using floating-point numbers for representing the weights and
fixed-point numbers for representing the activations. We show that using
floating-point representation for weights is more efficient than fixed-point
representation for the same bit-width and demonstrate it on popular large-scale
CNNs such as AlexNet, SqueezeNet, GoogLeNet and VGG-16. We also show that such
a representation scheme enables compact hardware multiply-and-accumulate (MAC)
unit design. Experimental results show that the proposed scheme reduces the
weight storage by up to 36% and power consumption of the hardware multiplier by
up to 50%.
",computer-science
"  We analyze performance of a class of time-delay first-order consensus
networks from a graph topological perspective and present methods to improve
it. The performance is measured by network's square of H-2 norm and it is shown
that it is a convex function of Laplacian eigenvalues and the coupling weights
of the underlying graph of the network. First, we propose a tight convex, but
simple, approximation of the performance measure in order to achieve lower
complexity in our design problems by eliminating the need for
eigen-decomposition. The effect of time-delay reincarnates itself in the form
of non-monotonicity, which results in nonintuitive behaviors of the performance
as a function of graph topology. Next, we present three methods to improve the
performance by growing, re-weighting, or sparsifying the underlying graph of
the network. It is shown that our suggested algorithms provide near-optimal
solutions with lower complexity with respect to existing methods in literature.
",computer-science
"  It was recently shown that architectural, regularization and rehearsal
strategies can be used to train deep models sequentially on a number of
disjoint tasks without forgetting previously acquired knowledge. However, these
strategies are still unsatisfactory if the tasks are not disjoint but
constitute a single incremental task (e.g., class-incremental learning). In
this paper we point out the differences between multi-task and
single-incremental-task scenarios and show that well-known approaches such as
LWF, EWC and SI are not ideal for incremental task scenarios. A new approach,
denoted as AR1, combining architectural and regularization strategies is then
specifically proposed. AR1 overhead (in term of memory and computation) is very
small thus making it suitable for online learning. When tested on CORe50 and
iCIFAR-100, AR1 outperformed existing regularization strategies by a good
margin.
",statistics
"  We present a parameterized approach to produce personalized variable length
summaries of soccer matches. Our approach is based on temporally segmenting the
soccer video into 'plays', associating a user-specifiable 'utility' for each
type of play and using 'bin-packing' to select a subset of the plays that add
up to the desired length while maximizing the overall utility (volume in
bin-packing terms). Our approach systematically allows a user to override the
default weights assigned to each type of play with individual preferences and
thus see a highly personalized variable length summarization of soccer matches.
We demonstrate our approach based on the output of an end-to-end pipeline that
we are building to produce such summaries. Though aspects of the overall
end-to-end pipeline are human assisted at present, the results clearly show
that the proposed approach is capable of producing semantically meaningful and
compelling summaries. Besides the obvious use of producing summaries of
superior league matches for news broadcasts, we anticipate our work to promote
greater awareness of the local matches and junior leagues by producing
consumable summaries of them.
",computer-science
"  Mars' surface bears the imprint of valley networks formed billions of years
ago and their relicts can still be observed today. However, whether these
networks were formed by groundwater sapping, ice melt, or fluvial runoff has
been continuously debated. These different scenarios have profoundly different
implications for Mars' climatic history, and thus for its habitability in the
distant past. Recent studies on Earth revealed that channel networks in arid
landscapes with more surface runoff branch at narrower angles, while in humid
environments with more groundwater flow, branching angles are much wider. We
find that valley networks on Mars generally tend to branch at narrow angles
similar to those found in arid landscapes on Earth. This result supports the
inference that Mars once had an active hydrologic cycle and that Mars' valley
networks were formed primarily by overland flow erosion with groundwater
seepage playing only a minor role.
",physics
"  We investigate how star formation efficiency can be significantly decreased
by the removal of a molecular cloud's envelope by feedback from an external
source. Feedback from star formation has difficulties halting the process in
dense gas but can easily remove the less dense and warmer envelopes where star
formation does not occur. However, the envelopes can play an important role
keeping their host clouds bound by deepening the gravitational potential and
providing a constraining pressure boundary. We use numerical simulations to
show that removal of the cloud envelopes results in all cases in a fall in the
star formation efficiency (SFE). At 1.38 free-fall times our 4 pc cloud
simulation experienced a drop in the SFE from 16 to six percent, while our 5 pc
cloud fell from 27 to 16 per cent. At the same time, our 3 pc cloud (the least
bound) fell from an SFE of 5.67 per cent to zero when the envelope was lost.
The star formation efficiency per free-fall time varied from zero to $\approx$
0.25 according to $\alpha$, defined to be the ratio of the kinetic plus thermal
to gravitational energy, and irrespective of the absolute star forming mass
available. Furthermore the fall in SFE associated with the loss of the envelope
is found to even occur at later times. We conclude that the SFE will always
fall should a star forming cloud lose its envelope due to stellar feedback,
with less bound clouds suffering the greatest decrease.
",physics
"  This paper presents an estimator for semiparametric models that uses a
feed-forward neural network to fit the nonparametric component. Unlike many
methodologies from the machine learning literature, this approach is suitable
for longitudinal/panel data. It provides unbiased estimation of the parametric
component of the model, with associated confidence intervals that have
near-nominal coverage rates. Simulations demonstrate (1) efficiency, (2) that
parametric estimates are unbiased, and (3) coverage properties of estimated
intervals. An application section demonstrates the method by predicting
county-level corn yield using daily weather data from the period 1981-2015,
along with parametric time trends representing technological change. The method
is shown to out-perform linear methods such as OLS and ridge/lasso, as well as
random forest. The procedures described in this paper are implemented in the R
package panelNNET.
",statistics
"  Big data problems frequently require processing datasets in a streaming
fashion, either because all data are available at once but collectively are
larger than available memory or because the data intrinsically arrive one data
point at a time and must be processed online. Here, we introduce a
computationally efficient version of similarity matching, a framework for
online dimensionality reduction that incrementally estimates the top
K-dimensional principal subspace of streamed data while keeping in memory only
the last sample and the current iterate. To assess the performance of our
approach, we construct and make public a test suite containing both a synthetic
data generator and the infrastructure to test online dimensionality reduction
algorithms on real datasets, as well as performant implementations of our
algorithm and competing algorithms with similar aims. Among the algorithms
considered we find our approach to be competitive, performing among the best on
both synthetic and real data.
",statistics
"  We introduce a novel formulation of motion planning, for continuous-time
trajectories, as probabilistic inference. We first show how smooth
continuous-time trajectories can be represented by a small number of states
using sparse Gaussian process (GP) models. We next develop an efficient
gradient-based optimization algorithm that exploits this sparsity and GP
interpolation. We call this algorithm the Gaussian Process Motion Planner
(GPMP). We then detail how motion planning problems can be formulated as
probabilistic inference on a factor graph. This forms the basis for GPMP2, a
very efficient algorithm that combines GP representations of trajectories with
fast, structure-exploiting inference via numerical optimization. Finally, we
extend GPMP2 to an incremental algorithm, iGPMP2, that can efficiently replan
when conditions change. We benchmark our algorithms against several
sampling-based and trajectory optimization-based motion planning algorithms on
planning problems in multiple environments. Our evaluation reveals that GPMP2
is several times faster than previous algorithms while retaining robustness. We
also benchmark iGPMP2 on replanning problems, and show that it can find
successful solutions in a fraction of the time required by GPMP2 to replan from
scratch.
",computer-science
"  Spiking neural networks (SNNs) enable power-efficient implementations due to
their sparse, spike-based coding scheme. This paper develops a bio-inspired SNN
that uses unsupervised learning to extract discriminative features from speech
signals, which can subsequently be used in a classifier. The architecture
consists of a spiking convolutional/pooling layer followed by a fully connected
spiking layer for feature discovery. The convolutional layer of leaky,
integrate-and-fire (LIF) neurons represents primary acoustic features. The
fully connected layer is equipped with a probabilistic spike-timing-dependent
plasticity learning rule. This layer represents the discriminative features
through probabilistic, LIF neurons. To assess the discriminative power of the
learned features, they are used in a hidden Markov model (HMM) for spoken digit
recognition. The experimental results show performance above 96% that compares
favorably with popular statistical feature extraction methods. Our results
provide a novel demonstration of unsupervised feature acquisition in an SNN.
",computer-science
"  We resolve the thermal motion of a high-stress silicon nitride nanobeam at
frequencies far below its fundamental flexural resonance (3.4 MHz) using
cavity-enhanced optical interferometry. Over two decades, the displacement
spectrum is well-modeled by that of a damped harmonic oscillator driven by a
$1/f$ thermal force, suggesting that the loss angle of the beam material is
frequency-independent. The inferred loss angle at 3.4 MHz, $\phi = 4.5\cdot
10^{-6}$, agrees well with the quality factor ($Q$) of the fundamental beam
mode ($\phi = Q^{-1}$). In conjunction with $Q$ measurements made on higher
order flexural modes, and accounting for the mode dependence of stress-induced
loss dilution, we find that the intrinsic (undiluted) loss angle of the beam
changes by less than a factor of 2 between 50 kHz and 50 MHz. We discuss the
impact of such ""structural damping"" on experiments in quantum optomechanics, in
which the thermal force acting on a mechanical oscillator coupled to an optical
cavity is overwhelmed by radiation pressure shot noise. As an illustration, we
show that structural damping reduces the bandwidth of ponderomotive squeezing.
",physics
"  We study the complexity of approximating the independent set polynomial
$Z_G(\lambda)$ of a graph $G$ with maximum degree $\Delta$ when the activity
$\lambda$ is a complex number.
This problem is already well understood when $\lambda$ is real using
connections to the $\Delta$-regular tree $T$. The key concept in that case is
the ""occupation ratio"" of the tree $T$. This ratio is the contribution to
$Z_T(\lambda)$ from independent sets containing the root of the tree, divided
by $Z_T(\lambda)$ itself. If $\lambda$ is such that the occupation ratio
converges to a limit, as the height of $T$ grows, then there is an FPTAS for
approximating $Z_G(\lambda)$ on a graph $G$ with maximum degree $\Delta$.
Otherwise, the approximation problem is NP-hard.
Unsurprisingly, the case where $\lambda$ is complex is more challenging.
Peters and Regts identified the complex values of $\lambda$ for which the
occupation ratio of the $\Delta$-regular tree converges. These values carve a
cardioid-shaped region $\Lambda_\Delta$ in the complex plane. Motivated by the
picture in the real case, they asked whether $\Lambda_\Delta$ marks the true
approximability threshold for general complex values $\lambda$.
Our main result shows that for every $\lambda$ outside of $\Lambda_\Delta$,
the problem of approximating $Z_G(\lambda)$ on graphs $G$ with maximum degree
at most $\Delta$ is indeed NP-hard. In fact, when $\lambda$ is outside of
$\Lambda_\Delta$ and is not a positive real number, we give the stronger result
that approximating $Z_G(\lambda)$ is actually #P-hard. If $\lambda$ is a
negative real number outside of $\Lambda_\Delta$, we show that it is #P-hard to
even decide whether $Z_G(\lambda)>0$, resolving in the affirmative a conjecture
of Harvey, Srivastava and Vondrak.
Our proof techniques are based around tools from complex analysis -
specifically the study of iterative multivariate rational maps.
",computer-science
"  In this paper, we propose a novel splitting receiver, which involves joint
processing of coherently and non-coherently received signals. Using a passive
RF power splitter, the received signal at each receiver antenna is split into
two streams which are then processed by a conventional coherent detection (CD)
circuit and a power-detection (PD) circuit, respectively. The streams of the
signals from all the receiver antennas are then jointly used for information
detection. We show that the splitting receiver creates a three-dimensional
received signal space, due to the joint coherent and non-coherent processing.
We analyze the achievable rate of a splitting receiver, which shows that the
splitting receiver provides a rate gain of $3/2$ compared to either the
conventional (CD-based) coherent receiver or the PD-based non-coherent receiver
in the high SNR regime. We also analyze the symbol error rate (SER) for
practical modulation schemes, which shows that the splitting receiver achieves
asymptotic SER reduction by a factor of at least $\sqrt{M}-1$ for $M$-QAM
compared to either the conventional (CD-based) coherent receiver or the
PD-based non-coherent receiver.
",computer-science
"  According to the Wiener-Hopf factorization, the characteristic function
$\varphi$ of any probability distribution $\mu$ on $\mathbb{R}$ can be
decomposed in a unique way as
\[1-s\varphi(t)=[1-\chi_-(s,it)][1-\chi_+(s,it)]\,,\;\;\;|s|\le1,\,t\in\mathbb{R}\,,\]
where $\chi_-(e^{iu},it)$ and $\chi_+(e^{iu},it)$ are the characteristic
functions of possibly defective distributions in
$\mathbb{Z}_+\times(-\infty,0)$ and $\mathbb{Z}_+\times[0,\infty)$,
respectively.
We prove that $\mu$ can be characterized by the sole data of the upward
factor $\chi_+(s,it)$, $s\in[0,1)$, $t\in\mathbb{R}$ in many cases including
the cases where:
1) $\mu$ has some exponential moments;
2) the function $t\mapsto\mu(t,\infty)$ is completely monotone on
$(0,\infty)$;
3) the density of $\mu$ on $[0,\infty)$ admits an analytic continuation on
$\mathbb{R}$.
We conjecture that any probability distribution is actually characterized by
its upward factor. This conjecture is equivalent to the following: {\it Any
probability measure $\mu$ on $\mathbb{R}$ whose support is not included in
$(-\infty,0)$ is determined by its convolution powers $\mu^{*n}$, $n\ge1$
restricted to $[0,\infty)$}. We show that in many instances, the sole knowledge
of $\mu$ and $\mu^{*2}$ restricted to $[0,\infty)$ is actually sufficient to
determine $\mu$. Then we investigate the analogous problem in the framework of
infinitely divisible distributions.
",mathematics
"  English to Indian language machine translation poses the challenge of
structural and morphological divergence. This paper describes English to Indian
language statistical machine translation using pre-ordering and suffix
separation. The pre-ordering uses rules to transfer the structure of the source
sentences prior to training and translation. This syntactic restructuring helps
statistical machine translation to tackle the structural divergence and hence
better translation quality. The suffix separation is used to tackle the
morphological divergence between English and highly agglutinative Indian
languages. We demonstrate that the use of pre-ordering and suffix separation
helps in improving the quality of English to Indian Language machine
translation.
",computer-science
"  Quantifying image distortions caused by strong gravitational lensing and
estimating the corresponding matter distribution in lensing galaxies has been
primarily performed by maximum likelihood modeling of observations. This is
typically a time and resource-consuming procedure, requiring sophisticated
lensing codes, several data preparation steps, and finding the maximum
likelihood model parameters in a computationally expensive process with
downhill optimizers. Accurate analysis of a single lens can take up to a few
weeks and requires the attention of dedicated experts. Tens of thousands of new
lenses are expected to be discovered with the upcoming generation of ground and
space surveys, the analysis of which can be a challenging task. Here we report
the use of deep convolutional neural networks to accurately estimate lensing
parameters in an extremely fast and automated way, circumventing the
difficulties faced by maximum likelihood methods. We also show that lens
removal can be made fast and automated using Independent Component Analysis of
multi-filter imaging data. Our networks can recover the parameters of the
Singular Isothermal Ellipsoid density profile, commonly used to model strong
lensing systems, with an accuracy comparable to the uncertainties of
sophisticated models, but about ten million times faster: 100 systems in
approximately 1s on a single graphics processing unit. These networks can
provide a way for non-experts to obtain lensing parameter estimates for large
samples of data. Our results suggest that neural networks can be a powerful and
fast alternative to maximum likelihood procedures commonly used in
astrophysics, radically transforming the traditional methods of data reduction
and analysis.
",physics
"  In a market with a rough or Markovian mean-reverting stochastic volatility
there is no perfect hedge. Here it is shown how various delta-type hedging
strategies perform and can be evaluated in such markets. A precise
characterization of the hedging cost, the replication cost caused by the
volatility fluctuations, is presented in an asymptotic regime of rapid mean
reversion for the volatility fluctuations. The optimal dynamic asset based
hedging strategy in the considered regime is identified as the so-called
`practitioners' delta hedging scheme. It is moreover shown that the
performances of the delta-type hedging schemes are essentially independent of
the regularity of the volatility paths in the considered regime and that the
hedging costs are related to a vega risk martingale whose magnitude is
proportional to a new market risk parameter.
",quantitative-finance
"  In recent years, several powerful techniques have been developed to design
{\em randomized} polynomial-space parameterized algorithms. In this paper, we
introduce an enhancement of color coding to design deterministic
polynomial-space parameterized algorithms. Our approach aims at reducing the
number of random choices by exploiting the special structure of a solution.
Using our approach, we derive the following deterministic algorithms (see
Introduction for problem definitions).
1. Polynomial-space $O^*(3.86^k)$-time (exponential-space $O^*(3.41^k)$-time)
algorithm for {\sc $k$-Internal Out-Branching}, improving upon the previously
fastest {\em exponential-space} $O^*(5.14^k)$-time algorithm for this problem.
2. Polynomial-space $O^*((2e)^{k+o(k)})$-time (exponential-space
$O^*(4.32^k)$-time) algorithm for {\sc $k$-Colorful Out-Branching} on
arc-colored digraphs and {\sc $k$-Colorful Perfect Matching} on planar
edge-colored graphs.
To obtain our polynomial space algorithms, we show that $(n,k,\alpha
k)$-splitters ($\alpha\ge 1$) and in particular $(n,k)$-perfect hash families
can be enumerated one by one with polynomial delay.
",computer-science
"  We propose a Topic Compositional Neural Language Model (TCNLM), a novel
method designed to simultaneously capture both the global semantic meaning and
the local word ordering structure in a document. The TCNLM learns the global
semantic coherence of a document via a neural topic model, and the probability
of each learned latent topic is further used to build a Mixture-of-Experts
(MoE) language model, where each expert (corresponding to one topic) is a
recurrent neural network (RNN) that accounts for learning the local structure
of a word sequence. In order to train the MoE model efficiently, a matrix
factorization method is applied, by extending each weight matrix of the RNN to
be an ensemble of topic-dependent weight matrices. The degree to which each
member of the ensemble is used is tied to the document-dependent probability of
the corresponding topics. Experimental results on several corpora show that the
proposed approach outperforms both a pure RNN-based model and other
topic-guided language models. Further, our model yields sensible topics, and
also has the capacity to generate meaningful sentences conditioned on given
topics.
",computer-science
"  We investigate the integration of a planning mechanism into an
encoder-decoder architecture with an explicit alignment for character-level
machine translation. We develop a model that plans ahead when it computes
alignments between the source and target sequences, constructing a matrix of
proposed future alignments and a commitment vector that governs whether to
follow or recompute the plan. This mechanism is inspired by the strategic
attentive reader and writer (STRAW) model. Our proposed model is end-to-end
trainable with fully differentiable operations. We show that it outperforms a
strong baseline on three character-level decoder neural machine translation on
WMT'15 corpus. Our analysis demonstrates that our model can compute
qualitatively intuitive alignments and achieves superior performance with fewer
parameters.
",computer-science
"  In this paper, the notion of $(L,M)$-fuzzy convex structures is introduced.
It is a generalization of $L$-convex structures and $M$-fuzzifying convex
structures. In our definition of $(L,M)$-fuzzy convex structures, each
$L$-fuzzy subset can be regarded as an $L$-convex set to some degree. The
notion of convexity preserving functions is also generalized to lattice-valued
case. Moreover, under the framework of $(L,M)$-fuzzy convex structures, the
concepts of quotient structures, substructures and products are presented and
their fundamental properties are discussed. Finally, we create a functor
$\omega$ from $\mathbf{MYCS}$ to $\mathbf{LMCS}$ and show that there exists an
adjunction between $\mathbf{MYCS}$ and $\mathbf{LMCS}$, where $\mathbf{MYCS}$
and $\mathbf{LMCS}$ denote the category of $M$-fuzzifying convex structures,
and the category of $(L,M)$-fuzzy convex structures, respectively.
",mathematics
"  Building large-scale, globally consistent maps is a challenging problem, made
more difficult in environments with limited access, sparse features, or when
using data collected by novice users. For such scenarios, where
state-of-the-art mapping algorithms produce globally inconsistent maps, we
introduce a systematic approach to incorporating sparse human corrections,
which we term Human-in-the-Loop Simultaneous Localization and Mapping
(HitL-SLAM). Given an initial factor graph for pose graph SLAM, HitL-SLAM
accepts approximate, potentially erroneous, and rank-deficient human input,
infers the intended correction via expectation maximization (EM),
back-propagates the extracted corrections over the pose graph, and finally
jointly optimizes the factor graph including the human inputs as human
correction factor terms, to yield globally consistent large-scale maps. We thus
contribute an EM formulation for inferring potentially rank-deficient human
corrections to mapping, and human correction factor extensions to the factor
graphs for pose graph SLAM that result in a principled approach to joint
optimization of the pose graph while simultaneously accounting for multiple
forms of human correction. We present empirical results showing the
effectiveness of HitL-SLAM at generating globally accurate and consistent maps
even when given poor initial estimates of the map.
",computer-science
"  Over recent years, emerging interest has occurred in integrating computer
vision technology into the retail industry. Automatic checkout (ACO) is one of
the critical problems in this area which aims to automatically generate the
shopping list from the images of the products to purchase. The main challenge
of this problem comes from the large scale and the fine-grained nature of the
product categories as well as the difficulty for collecting training images
that reflect the realistic checkout scenarios due to continuous update of the
products. Despite its significant practical and research value, this problem is
not extensively studied in the computer vision community, largely due to the
lack of a high-quality dataset. To fill this gap, in this work we propose a new
dataset to facilitate relevant research. Our dataset enjoys the following
characteristics: (1) It is by far the largest dataset in terms of both product
image quantity and product categories. (2) It includes single-product images
taken in a controlled environment and multi-product images taken by the
checkout system. (3) It provides different levels of annotations for the
check-out images. Comparing with the existing datasets, ours is closer to the
realistic setting and can derive a variety of research problems. Besides the
dataset, we also benchmark the performance on this dataset with various
approaches. The dataset and related resources can be found at
\url{this https URL}.
",computer-science
"  We show that a conformal anomaly in Weyl/Dirac semimetals generates a bulk
electric current perpendicular to a temperature gradient and the direction of a
background magnetic field. The associated conductivity of this novel
contribution to the Nernst effect is fixed by a beta function associated with
the electric charge renormalization in the material.
",physics
"  Emission from the molecular ion H$_3^+$ is a powerful diagnostic of the upper
atmosphere of Jupiter, Saturn, and Uranus, but it remains undetected at
Neptune. In search of this emission, we present near-infrared spectral
observations of Neptune between 3.93 and 4.00 $\mu$m taken with the newly
commissioned iSHELL instrument on the NASA Infrared Telescope Facility in
Hawaii, obtained 17-20 August 2017. We spent 15.4 h integrating across the disk
of the planet, yet were unable to unambiguously identify any H$_3^+$ line
emissions. Assuming a temperature of 550 K, we derive an upper limit on the
column integrated density of $1.0^{+1.2}_{-0.8}\times10^{13}$ m$^{-2}$, which
is an improvement of 30\% on the best previous observational constraint. This
result means that models are over-estimating the density by at least a factor
of 5, highlighting the need for renewed modelling efforts. A potential solution
is strong vertical mixing of polyatomic neutral species from Neptune's upper
stratosphere to the thermosphere, reacting with H$_3^+$, thus greatly reducing
the column integrated H$_3^+$ densities. This upper limit also provide
constraints on future attempts at detecting H$_3^+$ using the James Webb Space
Telescope.
",physics
"  Forwarding data by name has been assumed to be a necessary aspect of an
information-centric redesign of the current Internet architecture that makes
content access, dissemination, and storage more efficient. The Named Data
Networking (NDN) and Content-Centric Networking (CCNx) architectures are the
leading examples of such an approach. However, forwarding data by name incurs
storage and communication complexities that are orders of magnitude larger than
solutions based on forwarding data using addresses. Furthermore, the specific
algorithms used in NDN and CCNx have been shown to have a number of
limitations. The Addressable Data Networking (ADN) architecture is introduced
as an alternative to NDN and CCNx. ADN is particularly attractive for
large-scale deployments of the Internet of Things (IoT), because it requires
far less storage and processing in relaying nodes than NDN. ADN allows things
and data to be denoted by names, just like NDN and CCNx do. However, instead of
replacing the waist of the Internet with named-data forwarding, ADN uses an
address-based forwarding plane and introduces an information plane that
seamlessly maps names to addresses without the involvement of end-user
applications. Simulation results illustrate the order of magnitude savings in
complexity that can be attained with ADN compared to NDN.
",computer-science
"  Galaxy clusters are thought to grow by accreting mass through large-scale,
strong, yet elusive, virial shocks. Such a shock is expected to accelerate
relativistic electrons, thus generating a spectrally-flat leptonic virial-ring.
However, until now, only the nearby Coma cluster has shown evidence for a
$\gamma$-ray virial ring. We stack Fermi-LAT data for the 112 most massive,
high latitude, extended clusters, enhancing the ring sensitivity by rescaling
clusters to their virial radii and utilizing the expected flat energy spectrum.
In addition to a central unresolved, hard signal (detected at the $\sim
5.8\sigma$ confidence level), probably dominated by AGN, we identify (at the
$5.8\sigma$ confidence level) a bright, spectrally-flat $\gamma$-ray ring at
the expected virial shock position. The ring signal implies that the shock
deposits $\sim 0.6\%$ (with an interpretation uncertainty factor $\sim2$) of
the thermal energy in relativistic electrons over a Hubble time. This result,
consistent with the Coma signal, validates and calibrates the virial shock
model, and indicates that the cumulative emission from such shocks
significantly contributes to the diffuse extragalactic $\gamma$-ray and
low-frequency radio backgrounds.
",physics
"  In this paper, we investigate the Hawking radiation process as a
semiclassical quantum tunneling phenomenon from black ring and Myers-Perry
black holes in 5-dimensional (5D) spaces. Using Lagrangian of
Glashow-Weinberg-Salam model with background electromagnetic field (for charged
W-bosons) and the WKB approximation, we will evaluate the tunneling
rate/probability of charged vector particles through horizons by taking into
account the electromagnetic vector potential. Moreover, we investigate the
corresponding Hawking temperature values by considering Boltzmann factor for
both cases and analyze the whole spectrum generally.
",physics
"  In today's databases, previous query answers rarely benefit answering future
queries. For the first time, to the best of our knowledge, we change this
paradigm in an approximate query processing (AQP) context. We make the
following observation: the answer to each query reveals some degree of
knowledge about the answer to another query because their answers stem from the
same underlying distribution that has produced the entire dataset. Exploiting
and refining this knowledge should allow us to answer queries more
analytically, rather than by reading enormous amounts of raw data. Also,
processing more queries should continuously enhance our knowledge of the
underlying distribution, and hence lead to increasingly faster response times
for future queries.
We call this novel idea---learning from past query answers---Database
Learning. We exploit the principle of maximum entropy to produce answers, which
are in expectation guaranteed to be more accurate than existing sample-based
approximations. Empowered by this idea, we build a query engine on top of Spark
SQL, called Verdict. We conduct extensive experiments on real-world query
traces from a large customer of a major database vendor. Our results
demonstrate that Verdict supports 73.7% of these queries, speeding them up by
up to 23.0x for the same accuracy level compared to existing AQP systems.
",computer-science
"  We present spectroscopic observations of the C II $\lambda$6578 permitted
line for 83 lines of sight in 76 planetary nebulae at high spectral resolution,
most of them obtained with the Manchester Echelle Spectrograph on the 2.1\,m
telescope at the Observatorio Astronómico Nacional on the Sierra San Pedro
Mártir. We study the kinematics of the C II $\lambda$6578 permitted line with
respect to other permitted and collisionally-excited lines. Statistically, we
find that the kinematics of the C II $\lambda$6578 line are not those expected
if this line arises from the recombination of C$^{2+}$ ions or the fluorescence
of C$^+$ ions in ionization equilibrium in a chemically-homogeneous nebular
plasma, but instead its kinematics are those appropriate for a volume more
internal than expected. The planetary nebulae in this sample have well-defined
morphology and are restricted to a limited range in H$\alpha$ line widths (no
large values) compared to their counterparts in the Milky Way bulge, both of
which could be interpreted as the result of young nebular shells, an inference
that is also supported by nebular modeling. Concerning the long-standing
discrepancy between chemical abundances inferred from permitted and
collisionally-excited emission lines in photoionized nebulae, our results imply
that multiple plasma components occur commonly in planetary nebulae.
",physics
"  Motivated by the study of Nishinou-Nohara-Ueda on the Floer thoery of
Gelfand-Cetlin systems over complex partial flag manifolds, we provide a
complete description of the topology of Gelfand-Cetlin fibers. We prove that
all fibers are \emph{smooth} isotropic submanifolds and give a complete
description of the fiber to be Lagrangian in terms of combinatorics of
Gelfand-Cetlin polytope. Then we study (non-)displaceability of Lagrangian
fibers. After a few combinatorial and numercal tests for the displaceability,
using the bulk-deformation of Floer cohomology by Schubert cycles, we prove
that every full flag manifold $\mathcal{F}(n)$ ($n \geq 3$) with a monotone
Kirillov-Kostant-Souriau symplectic form carries a continuum of
non-displaceable Lagrangian tori which degenerates to a non-torus fiber in the
Hausdorff limit. In particular, the Lagrangian $S^3$-fiber in $\mathcal{F}(3)$
is non-displaceable the question of which was raised by Nohara-Ueda who
computed its Floer cohomology to be vanishing.
",mathematics
"  Recent studies have shown that deep neural networks (DNN) are vulnerable to
adversarial samples: maliciously-perturbed samples crafted to yield incorrect
model outputs. Such attacks can severely undermine DNN systems, particularly in
security-sensitive settings. It was observed that an adversary could easily
generate adversarial samples by making a small perturbation on irrelevant
feature dimensions that are unnecessary for the current classification task. To
overcome this problem, we introduce a defensive mechanism called DeepCloak. By
identifying and removing unnecessary features in a DNN model, DeepCloak limits
the capacity an attacker can use generating adversarial samples and therefore
increase the robustness against such inputs. Comparing with other defensive
approaches, DeepCloak is easy to implement and computationally efficient.
Experimental results show that DeepCloak can increase the performance of
state-of-the-art DNN models against adversarial samples.
",computer-science
"  The anisotropy of magnetic properties commonly is introduced in textbooks
using the case of an antiferromagnetic system with Ising type anisotropy. This
model presents huge anisotropic magnetization and a pronounced metamagnetic
transition and is well-known and well-documented both, in experiments and
theory. In contrast, the case of an antiferromagnetic $X$-$Y$ system with weak
in-plane anisotropy is only poorly documented. We studied the anisotropic
magnetization of the compound GdRh$_2$Si$_2$ and found that it is a perfect
model system for such a weak-anisotropy setting because the Gd$^{3+}$ ions in
GdRh$_2$Si$_2$ have a pure spin moment of S=7/2 which orders in a simple AFM
structure with ${\bf Q} = (001)$. We observed experimentally in $M(B)$ a
continuous spin-flop transition and domain effects for field applied along the
$[100]$- and the $[110]$-direction, respectively. We applied a mean field model
for the free energy to describe our data and combine it with an Ising chain
model to account for domain effects. Our calculations reproduce the
experimental data very well. In addition, we performed magnetic X-ray
scattering and X-ray magnetic circular dichroism measurements, which confirm
the AFM propagation vector to be ${\bf Q} = (001)$ and indicate the absence of
polarization on the rhodium atoms.
",physics
"  We prove Zagier's conjecture regarding the 2-adic valuation of the
coefficients $\{b_m\}$ that appear in Ewing and Schober's series formula for
the area of the Mandelbrot set in the case where $m\equiv 2 \mod 4$.
",mathematics
"  The Griffiths conjecture asserts that every ample vector bundle $E$ over a
compact complex manifold $S$ admits a hermitian metric with positive curvature
in the sense of Griffiths. In this article we give a sufficient condition for a
positive hermitian metric on $\mathcal{O}_{\mathbb{P}(E^*)}(1)$ to induce a
Griffiths positive $L^2$-metric on the vector bundle $E$. This result suggests
to study the relative Kähler-Ricci flow on $\mathcal{O}_{\mathbb{P}(E^*)}(1)$
for the fibration $\mathbb{P}(E^*)\to S$. We define a flow and give arguments
for the convergence.
",mathematics
"  We report on the heterogeneous nucleation of catalyst-free InAs nanowires on
Si (111) substrates by chemical beam epitaxy. We show that nanowire nucleation
is enhanced by sputtering the silicon substrate with energetic particles. We
argue that particle bombardment introduces lattice defects on the silicon
surface that serve as preferential nucleation sites. The formation of these
nucleation sites can be controlled by the sputtering parameters, allowing the
control of nanowire density in a wide range. Nanowire nucleation is accompanied
by unwanted parasitic islands, but by careful choice of annealing and growth
temperature allows to strongly reduce the relative density of these islands and
to realize samples with high nanowire yield.
",physics
"  In this paper, we present a methodology to estimate the parameters of
stochastically contaminated models under two contamination regimes. In both
regimes, we assume that the original process is a variable length Markov chain
that is contaminated by a random noise. In the first regime we consider that
the random noise is added to the original source and in the second regime, the
random noise is multiplied by the original source. Given a contaminated sample
of these models, the original process is hidden. Then we propose a two steps
estimator for the parameters of these models, that is, the probability
transitions and the noise parameter, and prove its consistency. The first step
is an adaptation of the Baum-Welch algorithm for Hidden Markov Models. This
step provides an estimate of a complete order $k$ Markov chain, where $k$ is
bigger than the order of the variable length Markov chain if it has finite
order and is a constant depending on the sample size if the hidden process has
infinite order. In the second estimation step, we propose a bootstrap Bayesian
Information Criterion, given a sample of the Markov chain estimated in the
first step, to obtain the variable length time dependence structure associated
with the hidden process. We present a simulation study showing that our
methodology is able to accurately recover the parameters of the models for a
reasonable interval of random noises.
",statistics
"  In portfolio analysis, the traditional approach of replacing population
moments with sample counterparts may lead to suboptimal portfolio choices. I
show that optimal portfolio weights can be estimated using a machine learning
(ML) framework, where the outcome to be predicted is a constant and the vector
of explanatory variables is the asset returns. It follows that ML specifically
targets estimation risk when estimating portfolio weights, and that
""off-the-shelf"" ML algorithms can be used to estimate the optimal portfolio in
the presence of parameter uncertainty. The framework nests the traditional
approach and recently proposed shrinkage approaches as special cases. By
relying on results from the ML literature, I derive new insights for existing
approaches and propose new estimation methods. Based on simulation studies and
several datasets, I find that ML significantly reduces estimation risk compared
to both the traditional approach and the equal weight strategy.
",quantitative-finance
"  The quantity and distribution of land which is eligible for renewable energy
sources is fundamental to the role these technologies will play in future
energy systems. As it stands, however, the current state of land eligibility
investigation is found to be insufficient to meet the demands of the future
energy modelling community. Three key areas are identified as the predominate
causes of this; inconsistent criteria definitions, inconsistent or unclear
methodologies, and inconsistent dataset usage. To combat these issues, a land
eligibility framework is developed and described in detail. The validity of
this framework is then shown via the recreation of land eligibility results
found in the literature, showing strong agreement in the majority of cases.
Following this, the framework is used to perform an evaluation of land
eligibility criteria within the European context whereby the relative
importance of commonly considered criteria are compared.
",computer-science
"  Completely positive and completely bounded mutlipliers on rigid
$C^{\ast}$-tensor categories were introduced by Popa and Vaes. Using these
notions, we define and study the Fourier-Stieltjes algebra, the Fourier algebra
and the algebra of completely bounded multipliers of a rigid $C^{\ast}$-tensor
category. The rich structure that these algebras have in the setting of locally
compact groups is still present in the setting of rigid $C^{\ast}$-tensor
categories. We also prove that Leptin's characterization of amenability still
holds in this setting, and we collect some natural observations on property
(T).
",mathematics
"  We present convolutional neural network (CNN) based approaches for
unsupervised multimodal subspace clustering. The proposed framework consists of
three main stages - multimodal encoder, self-expressive layer, and multimodal
decoder. The encoder takes multimodal data as input and fuses them to a latent
space representation. The self-expressive layer is responsible for enforcing
the self-expressiveness property and acquiring an affinity matrix corresponding
to the data points. The decoder reconstructs the original input data. The
network uses the distance between the decoder's reconstruction and the original
input in its training. We investigate early, late and intermediate fusion
techniques and propose three different encoders corresponding to them for
spatial fusion. The self-expressive layers and multimodal decoders are
essentially the same for different spatial fusion-based approaches. In addition
to various spatial fusion-based methods, an affinity fusion-based network is
also proposed in which the self-expressive layer corresponding to different
modalities is enforced to be the same. Extensive experiments on three datasets
show that the proposed methods significantly outperform the state-of-the-art
multimodal subspace clustering methods.
",statistics
"  Road networks in cities are massive and is a critical component of mobility.
Fast response to defects, that can occur not only due to regular wear and tear
but also because of extreme events like storms, is essential. Hence there is a
need for an automated system that is quick, scalable and cost-effective for
gathering information about defects. We propose a system for city-scale road
audit, using some of the most recent developments in deep learning and semantic
segmentation. For building and benchmarking the system, we curated a dataset
which has annotations required for road defects. However, many of the labels
required for road audit have high ambiguity which we overcome by proposing a
label hierarchy. We also propose a multi-step deep learning model that segments
the road, subdivide the road further into defects, tags the frame for each
defect and finally localizes the defects on a map gathered using GPS. We
analyze and evaluate the models on image tagging as well as segmentation at
different levels of the label hierarchy.
",computer-science
"  Zebrafish pretectal neurons exhibit specificities for large-field optic flow
patterns associated with rotatory or translatory body motion. We investigate
the hypothesis that these specificities reflect the input statistics of natural
optic flow. Realistic motion sequences were generated using computer graphics
simulating self-motion in an underwater scene. Local retinal motion was
estimated with a motion detector and encoded in four populations of
directionally tuned retinal ganglion cells, represented as two signed input
variables. This activity was then used as input into one of two learning
networks: a sparse coding network (competitive learning) and backpropagation
network (supervised learning). Both simulations develop specificities for optic
flow which are comparable to those found in a neurophysiological study (Kubo et
al. 2014), and relative frequencies of the various neuronal responses are best
modeled by the sparse coding approach. We conclude that the optic flow neurons
in the zebrafish pretectum do reflect the optic flow statistics. The predicted
vectorial receptive fields show typical optic flow fields but also ""Gabor"" and
dipole-shaped patterns that likely reflect difference fields needed for
reconstruction by linear superposition.
",quantitative-biology
"  In this paper, we prove a functorial aspect of the formal geometric
quantization procedure of non-compact spin-c manifolds.
",mathematics
"  The classical involutive division theory by Janet decomposes in the same way
both the ideal and the escalier. The aim of this paper, following Janet's
approach, is to discuss the combinatorial properties of involutive divisions,
when defined on the set of all terms in a fixed degree D, postponing the
discussion of ideal membership and related test. We adapt the theory by Gerdt
and Blinkov, introducing relative involutive divisions and then, given a
complete description of the combinatorial structure of a relative involutive
division, we turn our attention to the problem of membership. In order to deal
with this problem, we introduce two graphs as tools, one is strictly related to
Seiler's L-graph, whereas the second generalizes it, to cover the case of
""non-continuous"" (in the sense of Gerdt-Blinkov) relative involutive divisions.
Indeed, given an element in the ideal (resp. escalier), walking backwards
(resp. forward) in the graph, we can identify all the other generators of the
ideal (resp. elements of degree D in the escalier).
",mathematics
"  Given a relatively projective birational morphism $f\colon X\to Y$ of smooth
algebraic spaces with dimension of fibers bounded by 1, we construct tilting
relative (over $Y$) generators $T_{X,f}$ and $S_{X,f}$ in $\mathcal{D}^b(X)$.
We develop a piece of general theory of strict admissible lattice filtrations
in triangulated categories and show that $\mathcal{D}^b(X)$ has such a
filtration $\mathcal{L}$ where the lattice is the set of all birational
decompositions $f \colon X \xrightarrow{g} Z \xrightarrow{h} Y$ with smooth
$Z$. The $t$-structures related to $T_{X,f}$ and $S_{X,f}$ are proved to be
glued via filtrations left and right dual to $\mathcal{L}$. We realise all such
$Z$ as the fine moduli spaces of simple quotients of $\mathcal{O}_X$ in the
heart of the $t$-structure for which $S_{X,g}$ is a relative projective
generator over $Y$. This implements the program of interpreting relevant smooth
contractions of $X$ in terms of a suitable system of $t$-structures on
$\mathcal{D}^b(X)$.
",mathematics
"  We give an asymptotic formula for the number of biquadratic extensions of the
rationals of bounded discriminant that fail the Hasse norm principle.
",mathematics
"  Mission critical data dissemination in massive Internet of things (IoT)
networks imposes constraints on the message transfer delay between devices. Due
to low power and communication range of IoT devices, data is foreseen to be
relayed over multiple device-to-device (D2D) links before reaching the
destination. The coexistence of a massive number of IoT devices poses a
challenge in maximizing the successful transmission capacity of the overall
network alongside reducing the multi-hop transmission delay in order to support
mission critical applications. There is a delicate interplay between the
carrier sensing threshold of the contention based medium access protocol and
the choice of packet forwarding strategy selected at each hop by the devices.
The fundamental problem in optimizing the performance of such networks is to
balance the tradeoff between conflicting performance objectives such as the
spatial frequency reuse, transmission quality, and packet progress towards the
destination. In this paper, we use a stochastic geometry approach to quantify
the performance of multi-hop massive IoT networks in terms of the spatial
frequency reuse and the transmission quality under different packet forwarding
schemes. We also develop a comprehensive performance metric that can be used to
optimize the system to achieve the best performance. The results can be used to
select the best forwarding scheme and tune the carrier sensing threshold to
optimize the performance of the network according to the delay constraints and
transmission quality requirements.
",computer-science
"  The evolution of structure in biology is driven by accretion and change.
Accretion brings together disparate parts to form bigger wholes. Change
provides opportunities for growth and innovation. Here we review patterns and
processes that are responsible for a 'double tale' of evolutionary accretion at
various levels of complexity, from proteins and nucleic acids to high-rise
building structures in cities. Parts are at first weakly linked and associate
variously. As they diversify, they compete with each other and are selected for
performance. The emerging interactions constrain their structure and
associations. This causes parts to self-organize into modules with tight
linkage. In a second phase, variants of the modules evolve and become new parts
for a new generative cycle of higher-level organization. Evolutionary genomics
and network biology support the 'double tale' of structural module creation and
validate an evolutionary principle of maximum abundance that drives the gain
and loss of modules.
",quantitative-biology
"  We establish the sharp growth rate, in terms of cardinality, of the $L^p$
norms of the maximal Hilbert transform $H_\Omega$ along finite subsets of a
finite order lacunary set of directions $\Omega \subset \mathbb R^3$, answering
a question of Parcet and Rogers in dimension $n=3$. Our result is the first
sharp estimate for maximal directional singular integrals in dimensions greater
than 2.
The proof relies on a representation of the maximal directional Hilbert
transform in terms of a model maximal operator associated to compositions of
two-dimensional angular multipliers, as well as on the usage of weighted norm
inequalities, and their extrapolation, in the directional setting.
",mathematics
"  This paper describes the development of a magnetic attitude control subsystem
for a 2U cubesat. Due to the presence of gravity gradient torques, the
satellite dynamics are open-loop unstable near the desired pointing
configuration. Nevertheless the linearized time-varying system is completely
controllable, under easily verifiable conditions, and the system's disturbance
rejection capabilities can be enhanced by adding air drag panels exemplifying a
beneficial interplay between hardware design and control. In the paper,
conditions for the complete controllability for the case of a magnetically
controlled satellite with passive air drag panels are developed, and simulation
case studies with the LQR and MPC control designs applied in combination with a
nonlinear time-varying input transformation are presented to demonstrate the
ability of the closed-loop system to satisfy mission objectives despite
disturbance torques.
",mathematics
"  The degree splitting problem requires coloring the edges of a graph red or
blue such that each node has almost the same number of edges in each color, up
to a small additive discrepancy. The directed variant of the problem requires
orienting the edges such that each node has almost the same number of incoming
and outgoing edges, again up to a small additive discrepancy.
We present deterministic distributed algorithms for both variants, which
improve on their counterparts presented by Ghaffari and Su [SODA'17]: our
algorithms are significantly simpler and faster, and have a much smaller
discrepancy. This also leads to a faster and simpler deterministic algorithm
for $(2+o(1))\Delta$-edge-coloring, improving on that of Ghaffari and Su.
",computer-science
"  Learning network representations has a variety of applications, such as
network classification. Most existing work in this area focuses on static
undirected networks and do not account for presence of directed edges or
temporarily changes. Furthermore, most work focuses on node representations
that do poorly on tasks like network classification. In this paper, we propose
a novel, flexible and scalable network embedding methodology, \emph{gl2vec},
for network classification in both static and temporal directed networks.
\emph{gl2vec} constructs vectors for feature representation using static or
temporal network graphlet distributions and a null model for comparing them
against random graphs. We argue that \emph{gl2vec} can be used to classify and
compare networks of varying sizes and time period with high accuracy. We
demonstrate the efficacy and usability of \emph{gl2vec} over existing
state-of-the-art methods on network classification tasks such as network type
classification and subgraph identification in several real-world static and
temporal directed networks. Experimental results further show that
\emph{gl2vec}, concatenated with a wide range of state-of-the-art methods,
improves classification accuracy by up to $10\%$ in real-world applications
such as detecting departments for subgraphs in an email network or identifying
mobile users given their app switching behaviors represented as static or
temporal directed networks.
",computer-science
"  We consider the Gierer-Meinhardt system with small inhibitor diffusivity,
very small activator diffusivity and a precursor inhomogeneity.
For any given positive integer k we construct a spike cluster consisting of
$k$ spikes which all approach the same nondegenerate local minimum point of the
precursor inhomogeneity. We show that this spike cluster can be linearly
stable. In particular, we show the existence of spike clusters for spikes
located at the vertices of a polygon with or without centre. Further, the
cluster without centre is stable for up to three spikes, whereas the cluster
with centre is stable for up to six spikes.
The main idea underpinning these stable spike clusters is the following: due
to the small inhibitor diffusivity the interaction between spikes is repulsive,
and the spikes are attracted towards the local minimum point of the precursor
inhomogeneity. Combining these two effects can lead to an equilibrium of spike
positions within the cluster such that the cluster is linearly stable.
",mathematics
"  Expectation for the emergence of higher functions is getting larger in the
framework of end-to-end reinforcement learning using a recurrent neural
network. However, the emergence of ""thinking"" that is a typical higher function
is difficult to realize because ""thinking"" needs non fixed-point, flow-type
attractors with both convergence and transition dynamics. Furthermore, in order
to introduce ""inspiration"" or ""discovery"" in ""thinking"", not completely random
but unexpected transition should be also required.
By analogy to ""chaotic itinerancy"", we have hypothesized that ""exploration""
grows into ""thinking"" through learning by forming flow-type attractors on
chaotic random-like dynamics. It is expected that if rational dynamics are
learned in a chaotic neural network (ChNN), coexistence of rational state
transition, inspiration-like state transition and also random-like exploration
for unknown situation can be realized.
Based on the above idea, we have proposed new reinforcement learning using a
ChNN as an actor. The positioning of exploration is completely different from
the conventional one. The chaotic dynamics inside the ChNN produces exploration
factors by itself. Since external random numbers for stochastic action
selection are not used, exploration factors cannot be isolated from the output.
Therefore, the learning method is also completely different from the
conventional one.
At each non-feedback connection, one variable named causality trace takes in
and maintains the input through the connection according to the change in its
output. Using the trace and TD error, the weight is updated.
In this paper, as the result of a recent simple task to see whether the new
learning works or not, it is shown that a robot with two wheels and two visual
sensors reaches a target while avoiding an obstacle after learning though there
are still many rooms for improvement.
",computer-science
"  Recent research has shown the usefulness of using collective user interaction
data (e.g., query logs) to recommend query modification suggestions for
Intranet search. However, most of the query suggestion approaches for Intranet
search follow an ""one size fits all"" strategy, whereby different users who
submit an identical query would get the same query suggestion list. This is
problematic, as even with the same query, different users may have different
topics of interest, which may change over time in response to the user's
interaction with the system. We address the problem by proposing a personalised
query suggestion framework for Intranet search. For each search session, we
construct two temporal user profiles: a click user profile using the user's
clicked documents and a query user profile using the user's submitted queries.
We then use the two profiles to re-rank the non-personalised query suggestion
list returned by a state-of-the-art query suggestion method for Intranet
search. Experimental results on a large-scale query logs collection show that
our personalised framework significantly improves the quality of suggested
queries.
",computer-science
"  Marshall and Olkin (1997, Biometrika, 84, 641 - 652) introduced a very
powerful method to introduce an additional parameter to a class of continuous
distribution functions and hence it brings more flexibility to the model. They
have demonstrated their method for the exponential and Weibull classes. In the
same paper they have briefly indicated regarding its bivariate extension. The
main aim of this paper is to introduce the same method, for the first time, to
the class of discrete generalized exponential distributions both for the
univariate and bivariate cases. We investigate several properties of the
proposed univariate and bivariate classes. The univariate class has three
parameters, whereas the bivariate class has five parameters. It is observed
that depending on the parameter values the univariate class can be both zero
inflated as well as heavy tailed. We propose to use EM algorithm to estimate
the unknown parameters. Small simulation experiments have been performed to see
the effectiveness of the proposed EM algorithm, and a bivariate data set has
been analyzed and it is observed that the proposed models and the EM algorithm
work quite well in practice.
",statistics
"  We introduce a regression model for data on non-linear manifolds. The model
describes the relation between a set of manifold valued observations, such as
shapes of anatomical objects, and Euclidean explanatory variables. The approach
is based on stochastic development of Euclidean diffusion processes to the
manifold. Defining the data distribution as the transition distribution of the
mapped stochastic process, parameters of the model, the non-linear analogue of
design matrix and intercept, are found via maximum likelihood. The model is
intrinsically related to the geometry encoded in the connection of the
manifold. We propose an estimation procedure which applies the Laplace
approximation of the likelihood function. A simulation study of the performance
of the model is performed and the model is applied to a real dataset of Corpus
Callosum shapes.
",computer-science
"  We report on the precise measurement of the atomic mass of a single proton
with a purpose-built Penning-trap system. With a precision of 32
parts-per-trillion our result not only improves on the current CODATA
literature value by a factor of three, but also disagrees with it at a level of
about 3 standard deviations.
",physics
"  $N$-body simulations study the dynamics of $N$ particles under the influence
of mutual long-distant forces such as gravity. In practice, $N$-body codes will
violate Newton's third law if they use either an approximate Poisson solver or
individual timesteps. In this study, we construct a novel $N$-body scheme by
combining a fast multipole method (FMM) based Poisson solver and a time
integrator using a hierarchical Hamiltonian splitting (HHS) technique. We test
our implementation for collision-less systems using several problems in
galactic dynamics. As a result of the momentum conserving nature of these two
key components, the new $N$-body scheme is also momentum conserving. Moreover,
we can fully utilize the $\mathcal O(\textit N)$ complexity of FMM with the
integrator. With the restored force symmetry, we can improve both angular
momentum conservation and energy conservation substantially. The new scheme
will be suitable for many applications in galactic dynamics and structure
formation. Our implementation, in the code Taichi, is publicly available at
this https URL.
",physics
"  Twinning is an important deformation mode of hexagonal close-packed metals.
The crystallographic theory is based on the 150-years old concept of simple
shear. The habit plane of the twin is the shear plane, it is invariant. Here we
present Electron BackScatter Diffraction observations and crystallographic
analysis of a millimeter size twin in a magnesium single crystal whose straight
habit plane, unambiguously determined both the parent crystal and in its twin,
is not an invariant plane. This experimental evidence demonstrates that
macroscopic deformation twinning can be obtained by a mechanism that is not a
simple shear. Beside, this unconventional twin is often co-formed with a new
conventional twin that exhibits the lowest shear magnitude ever reported in
metals. The existence of unconventional twinning introduces a shift of paradigm
and calls for the development of a new theory for the displacive
transformations
",physics
"  Security is a critical and vital task in wireless sensor networks, therefore
different key management systems have been proposed, many of which are based on
symmetric cryptography. Such systems are very energy efficient, but they lack
some other desirable characteristics. On the other hand, systems based on
public key cryptography have those desirable characteristics, but they consume
more energy. Recently based on authenticated messages from base station a new
PKC based key agreement protocol was proposed. We show this method is
susceptible to a form of denial of service attack where resources of the
network can be exhausted with bogus messages. Then, we propose two different
improvements to solve this vulnerability. Simulation results show that these
new protocols retain desirable characteristics of the basic method and solve
its deficiencies.
",computer-science
"  Inspired by recent work of I. Baoulina, we give a simultaneous generalization
of the theorems of Chevalley-Warning and Morlaye.
",mathematics
"  Numerical QCD is often extremely resource demanding and it is not rare to run
hundreds of simulations at the same time. Each of these can last for days or
even months and it typically requires a job-script file as well as an input
file with the physical parameters for the application to be run. Moreover, some
monitoring operations (i.e. copying, moving, deleting or modifying files,
resume crashed jobs, etc.) are often required to guarantee that the final
statistics is correctly accumulated. Proceeding manually in handling
simulations is probably the most error-prone way and it is deadly uncomfortable
and inefficient! BaHaMAS was developed and successfully used in the last years
as a tool to automatically monitor and administrate simulations.
",physics
"  Automation and computer intelligence to support complex human decisions
becomes essential to manage large and distributed systems in the Cloud and IoT
era. Understanding the root cause of an observed symptom in a complex system
has been a major problem for decades. As industry dives into the IoT world and
the amount of data generated per year grows at an amazing speed, an important
question is how to find appropriate mechanisms to determine root causes that
can handle huge amounts of data or may provide valuable feedback in real-time.
While many survey papers aim at summarizing the landscape of techniques for
modelling system behavior and infering the root cause of a problem based in the
resulting models, none of those focuses on analyzing how the different
techniques in the literature fit growing requirements in terms of performance
and scalability. In this survey, we provide a review of root-cause analysis,
focusing on these particular aspects. We also provide guidance to choose the
best root-cause analysis strategy depending on the requirements of a particular
system and application.
",computer-science
"  We derive in a direct way the exact controllability of the 1D free
Schrödinger equation with Dirichlet boundary control. We use the so-called
flatness approach, which consists in parametrizing the solution and the control
by the derivatives of a ""flat output"". This provides an explicit and very
regular control achieving the exact controllability in the energy space.
",mathematics
"  This paper deals with cellular (e.g. LTE) networks that selectively offload
the mobile data traffic onto WiFi (IEEE 802.11) networks to improve network
performance. We propose the Seamless Internetwork Flow Mobility (SIFM)
architecture that provides seamless flow-mobility support using concepts of
Software Defined Networking (SDN). The SDN paradigm decouples the control and
data plane, leading to a centralized network intelligence and state. The SIFM
architecture utilizes this aspect of SDN and moves the mobility decisions to a
centralized Flow Controller (FC). This provides a global network view while
making mobility decisions and also reduces the complexity at the PGW. We
implement and evaluate both basic PMIPv6 and the SIFM architectures by
incorporating salient LTE and WiFi network features in the ns-3 simulator.
Performance experiments validate that seamless mobility is achieved. Also, the
SIFM architecture shows an improved network performance when compared to the
base PMIPv6 architecture. A proof-of-concept prototype of the SIFM architecture
has been implemented on an experimental testbed. The LTE network is emulated by
integrating USRP B210x with the OpenLTE eNodeB and OpenLTE EPC. The WiFi
network is emulated using hostapd and dnsmasq daemons running on Ubuntu 12.04.
An off-the-shelf LG G2 mobile phone running Android 4.2.2 is used as the user
equipment. We demonstrate seamless mobility between the LTE network and the
WiFi network with the help of ICMP ping and a TCP chat application.
",computer-science
"  We introduce the concept of $r$-equilateral $m$-gons. We prove the existence
of $r$-equilateral $p$-gons in $\mathbb R^d$ if $r<d$ and the existence of
equilateral $p$-gons in the image of continuous injective maps $f:S^d\to
\mathbb R^{d+1}$. Our ideas are based mainly in the paper of Y. Soibelman
\cite{soibelman}, in which the topological Borsuk number of $\mathbb{R}^2$ is
calculated by means of topological methods and the paper of P. Blagojević and
G. Ziegler \cite{blagojevictetrahedra} where Fadell-Husseini index is used for
solving a problem related to the topological Borsuk problem for $\mathbb{R}^3$.
",mathematics
"  We report extensive theoretical calculations on the rotation-inversion
excitation of interstellar ammonia (NH3) due to collisions with atomic and
molecular hydrogen (both para- and ortho-H2). Close-coupling calculations are
performed for total energies in the range 1-2000 cm-1 and rotational cross
sections are obtained for all transitions among the lowest 17 and 34
rotation-inversion levels of ortho- and para-NH3, respectively. Rate
coefficients are deduced for kinetic temperatures up to 200 K. Propensity rules
for the three colliding partners are discussed and we also compare the new
results to previous calculations for the spherically symmetrical He and para-H2
projectiles. Significant differences are found between the different sets of
calculations. Finally, we test the impact of the new rate coefficients on the
calibration of the ammonia thermometer. We find that the calibration curve is
only weakly sensitive to the colliding partner and we confirm that the ammonia
thermometer is robust.
",physics
"  Motivated by the task of clustering either $d$ variables or $d$ points into
$K$ groups, we investigate efficient algorithms to solve the Peng-Wei (P-W)
$K$-means semi-definite programming (SDP) relaxation. The P-W SDP has been
shown in the literature to have good statistical properties in a variety of
settings, but remains intractable to solve in practice. To this end we propose
FORCE, a new algorithm to solve this SDP relaxation. Compared to the naive
interior point method, our method reduces the computational complexity of
solving the SDP from $\tilde{O}(d^7\log\epsilon^{-1})$ to
$\tilde{O}(d^{6}K^{-2}\epsilon^{-1})$ arithmetic operations for an
$\epsilon$-optimal solution. Our method combines a primal first-order method
with a dual optimality certificate search, which when successful, allows for
early termination of the primal method. We show for certain variable clustering
problems that, with high probability, FORCE is guaranteed to find the optimal
solution to the SDP relaxation and provide a certificate of exact optimality.
As verified by our numerical experiments, this allows FORCE to solve the P-W
SDP with dimensions in the hundreds in only tens of seconds. For a variation of
the P-W SDP where $K$ is not known a priori a slight modification of FORCE
reduces the computational complexity of solving this problem as well: from
$\tilde{O}(d^7\log\epsilon^{-1})$ using a standard SDP solver to
$\tilde{O}(d^{4}\epsilon^{-1})$.
",statistics
"  We mine the Tycho-{\it Gaia} astrometric solution (TGAS) catalog for wide
stellar binaries by matching positions, proper motions, and astrometric
parallaxes. We separate genuine binaries from unassociated stellar pairs
through a Bayesian formulation that includes correlated uncertainties in the
proper motions and parallaxes. Rather than relying on assumptions about the
structure of the Galaxy, we calculate Bayesian priors and likelihoods based on
the nature of Keplerian orbits and the TGAS catalog itself. We calibrate our
method using radial velocity measurements and obtain 6196 high-confidence
candidate wide binaries with projected separations $s\lesssim1$ pc. The
normalization of this distribution suggests that at least 0.6\% of TGAS stars
have an associated, distant TGAS companion in a wide binary. We demonstrate
that {\it Gaia}'s astrometry is precise enough that it can detect projected
orbital velocities in wide binaries with orbital periods as large as 10$^6$ yr.
For pairs with $s\ \lesssim\ 4\times10^4$~AU, characterization of random
alignments indicate our contamination to be $\approx$5\%. For $s \lesssim
5\times10^3$~AU, our distribution is consistent with Öpik's Law. At larger
separations, the distribution is steeper and consistent with a power-law
$P(s)\propto s^{-1.6}$; there is no evidence in our data of any bimodality in
this distribution for $s \lesssim$ 1 pc. Using radial velocities, we
demonstrate that at large separations, i.e., of order $s \sim$ 1 pc and beyond,
any potential sample of genuine wide binaries in TGAS cannot be easily
distinguished from ionized former wide binaries, moving groups, or
contamination from randomly aligned stars.
",physics
"  Quantum transport is studied for the nonequilibrium Anderson impurity model
at zero temperature employing the multilayer multiconfiguration time-dependent
Hartree theory within the second quantization representation (ML-MCTDH-SQR) of
Fock space. To adress both linear and nonlinear conductance in the Kondo
regime, two new techniques of the ML-MCTDH-SQR simulation methodology are
introduced: (i) the use of correlated initial states, which is achieved by
imaginary time propagation of the overall Hamiltonian at zero voltage and (ii)
the adoption of the logarithmic discretization of the electronic continuum.
Employing the improved methodology, the signature of the Kondo effect is
analyzed.
",physics
"  Properties of the cold interstellar medium of low-metallicity galaxies are
not well-known due to the faintness and extremely small scale on which emission
is expected. We present deep ALMA band 6 (230GHz) observations of the nearby,
low-metallicity (12 + log(O/H) = 7.25) blue compact dwarf galaxy SBS0335-052 at
an unprecedented resolution of 0.2 arcsec (52 pc). The 12CO J=2-1 line is not
detected and we report a 3-sigma upper limit of LCO(2-1) = 3.6x10^4 K km/s
pc^2. Assuming that molecular gas is converted into stars with a given
depletion time, ranging from 0.02 to 2 Gyr, we find lower limits on the
CO-to-H2 conversion factor alpha_CO in the range 10^2-10^4 Msun pc^-2 (K
km/s)^-1. The continuum emission is detected and resolved over the two main
super star clusters. Re-analysis of the IR-radio spectral energy distribution
suggests that the mm-fluxes are not only free-free emission but are most likely
also associated with a cold dust component coincident with the position of the
brightest cluster. With standard dust properties, we estimate its mass to be as
large as 10^5 Msun. Both line and continuum results suggest the presence of a
large cold gas reservoir unseen in CO even with ALMA.
",physics
"  Considering the problem of color distortion caused by the defogging algorithm
based on dark channel prior, an improved algorithm was proposed to calculate
the transmittance of all channels respectively. First, incident light
frequency's effect on the transmittance of various color channels was analyzed
according to the Beer-Lambert's Law, from which a proportion among various
channel transmittances was derived; afterwards, images were preprocessed by
down-sampling to refine transmittance, and then the original size was restored
to enhance the operational efficiency of the algorithm; finally, the
transmittance of all color channels was acquired in accordance with the
proportion, and then the corresponding transmittance was used for image
restoration in each channel. The experimental results show that compared with
the existing algorithm, this improved image defogging algorithm could make
image colors more natural, solve the problem of slightly higher color
saturation caused by the existing algorithm, and shorten the operation time by
four to nine times.
",computer-science
"  This study presents a smoothed particle hydrodynamics (SPH) method with
Peng-Robinson equation of state for simulating drop vaporization and drop
impact on a hot surface. The conservation equations of momentum and energy and
Peng-Robinson equation of state are applied to describe both the liquid and gas
phases. The governing equations are solved numerically by the SPH method. The
phase change between the liquid and gas phases are simulated directly without
using any phase change models. The numerical method is validated by comparing
numerical results with analytical solutions for the vaporization of n-heptane
drops at different temperatures. Using the SPH method, the processes of
n-heptane drops impacting on a solid wall with different temperatures are
studied numerically. The results show that the size of the film formed by drop
impact decreases when temperature increases. When the temperature is high
enough, the drop will rebound.
",physics
"  The concepts of Gross Domestic Product (GDP), GDP per capita, and population
are central to the study of political science and economics. However, a growing
literature suggests that existing measures of these concepts contain
considerable error or are based on overly simplistic modeling choices. We
address these problems by creating a dynamic, three-dimensional latent trait
model, which uses observed information about GDP, GDP per capita, and
population to estimate posterior prediction intervals for each of these
important concepts. By combining historical and contemporary sources of
information, we are able to extend the temporal and spatial coverage of
existing datasets for country-year units back to 1500 A.D through 2015 A.D.
and, because the model makes use of multiple indicators of the underlying
concepts, we are able to estimate the relative precision of the different
country-year estimates. Overall, our latent variable model offers a principled
method for incorporating information from different historic and contemporary
data sources. It can be expanded or refined as researchers discover new or
alternative sources of information about these concepts.
",statistics
"  Argument component detection (ACD) is an important sub-task in argumentation
mining. ACD aims at detecting and classifying different argument components in
natural language texts. Historical annotations (HAs) are important features the
human annotators consider when they manually perform the ACD task. However, HAs
are largely ignored by existing automatic ACD techniques. Reinforcement
learning (RL) has proven to be an effective method for using HAs in some
natural language processing tasks. In this work, we propose a RL-based ACD
technique, and evaluate its performance on two well-annotated corpora. Results
suggest that, in terms of classification accuracy, HAs-augmented RL outperforms
plain RL by at most 17.85%, and outperforms the state-of-the-art supervised
learning algorithm by at most 11.94%.
",computer-science
"  Questions that require counting a variety of objects in images remain a major
challenge in visual question answering (VQA). The most common approaches to VQA
involve either classifying answers based on fixed length representations of
both the image and question or summing fractional counts estimated from each
section of the image. In contrast, we treat counting as a sequential decision
process and force our model to make discrete choices of what to count.
Specifically, the model sequentially selects from detected objects and learns
interactions between objects that influence subsequent selections. A
distinction of our approach is its intuitive and interpretable output, as
discrete counts are automatically grounded in the image. Furthermore, our
method outperforms the state of the art architecture for VQA on multiple
metrics that evaluate counting.
",computer-science
"  We show that NSOP$_{1}$ theories are exactly the theories in which
Kim-independence satisfies a form of local character. In particular, we show
that if $T$ is NSOP$_{1}$, $M\models T$, and $p$ is a type over $M$, then the
collection of elementary substructures of size $\left|T\right|$ over which $p$
does not Kim-fork is a club of $\left[M\right]^{\left|T\right|}$ and that this
characterizes NSOP$_{1}$.
We also present a new phenomenon we call dual local-character for
Kim-independence in NSOP$_{1}$-theories.
",mathematics
"  The importance of microscopic details on cooperation level is an intensively
studied aspect of evolutionary game theory. Interestingly, these details become
crucial on heterogeneous populations where individuals may possess diverse
traits. By introducing a coevolutionary model in which not only strategies but
also individual dynamical features may evolve we revealed that the formerly
established conclusion is not necessarily true when different updating rules
are on stage. In particular, we apply two strategy updating rules, imitation
and Death-Birth rule, which allow local selection in a spatial system. Our
observation highlights that the microscopic feature of dynamics, like the level
of learning activity, could be a fundamental factor even if all players share
the same trait uniformly.
",quantitative-biology
"  We present GALARIO, a computational library that exploits the power of modern
graphical processing units (GPUs) to accelerate the analysis of observations
from radio interferometers like ALMA or the VLA. GALARIO speeds up the
computation of synthetic visibilities from a generic 2D model image or a radial
brightness profile (for axisymmetric sources). On a GPU, GALARIO is 150 faster
than standard Python and 10 times faster than serial C++ code on a CPU. Highly
modular, easy to use and to adopt in existing code, GALARIO comes as two
compiled libraries, one for Nvidia GPUs and one for multicore CPUs, where both
have the same functions with identical interfaces. GALARIO comes with Python
bindings but can also be directly used in C or C++. The versatility and the
speed of GALARIO open new analysis pathways that otherwise would be
prohibitively time consuming, e.g. fitting high resolution observations of
large number of objects, or entire spectral cubes of molecular gas emission. It
is a general tool that can be applied to any field that uses radio
interferometer observations. The source code is available online at
this https URL under the open source GNU Lesser General
Public License v3.
",physics
"  In this paper we classify the isomorphism classes of four dimensional
nilpotent associative algebras over a field F, studying regular subgroups of
the affine group AGL_4(F). In particular we provide explicit representatives
for such classes when F is a finite field, the real field R or an algebraically
closed field.
",mathematics
"  Bacterial DNA gyrase introduces negative supercoils into chromosomal DNA and
relaxes positive supercoils introduced by replication and transiently by
transcription. Removal of these positive supercoils is essential for
replication fork progression and for the overall unlinking of the two duplex
DNA strands, as well as for ongoing transcription. To address how gyrase copes
with these topological challenges, we used high-speed single-molecule
fluorescence imaging in live Escherichia coli cells. We demonstrate that at
least 300 gyrase molecules are stably bound to the chromosome at any time, with
~12 enzymes enriched near each replication fork. Trapping of reaction
intermediates with ciprofloxacin revealed complexes undergoing catalysis. Dwell
times of ~2 s were observed for the dispersed gyrase molecules, which we
propose maintain steady-state levels of negative supercoiling of the
chromosome. In contrast, the dwell time of replisome-proximal molecules was ~8
s, consistent with these catalyzing processive positive supercoil relaxation in
front of the progressing replisome.
",quantitative-biology
"  In the last decade, many business applications have moved into the cloud. In
particular, the ""database-as-a-service"" paradigm has become mainstream. While
existing multi-tenant data management systems focus on single-tenant query
processing, we believe that it is time to rethink how queries can be processed
across multiple tenants in such a way that we do not only gain more valuable
insights, but also at minimal cost. As we will argue in this paper, standard
SQL semantics are insufficient to process cross-tenant queries in an
unambiguous way, which is why existing systems use other, expensive means like
ETL or data integration. We first propose MTSQL, a set of extensions to
standard SQL, which fixes the ambiguity problem. Next, we present MTBase, a
query processing middleware that efficiently processes MTSQL on top of SQL. As
we will see, there is a canonical, provably correct, rewrite algorithm from
MTSQL to SQL, which may however result in poor query execution performance,
even on high-performance database products. We further show that with
carefully-designed optimizations, execution times can be reduced in such ways
that the difference to single-tenant queries becomes marginal.
",computer-science
"  Magnetic oxyselenides have been the topic of research for several decades
being first of interest in the context of photoconductivity and
thermoelectricity owing to their intrinsic semiconducting properties and
ability to tune the energy gap through metal ion substitution. More recently,
interest in the oxyselenides has experienced a resurgence owing to the possible
relation to strongly correlated phenomena given the fact that many oxyslenides
share a similar structure to unconventional superconducting pnictides and
chalcogenides. The two dimensional nature of many oxyselenide systems also
draws an analogy to cuprate physics where a strong interplay between
unconventional electronic phases and localised magnetism has been studied for
several decades. It is therefore timely to review the physics of the
oxyselenides in the context of the broader field of strongly correlated
magnetism and electronic phenomena. Here we review the current status and
progress in this area of research with the focus on the influence of
lanthanides and transition metal ions on the intertwined magnetic and
electronic properties of oxyselenides. The emphasis of the review is on the
magnetic properties and comparisons are made with iron based pnictide and
chalcogenide systems.
",physics
"  The low energy optical conductivity of conventional superconductors is
usually well described by Mattis-Bardeen (MB) theory which predicts the onset
of absorption above an energy corresponding to twice the superconducing (SC)
gap parameter Delta. Recent experiments on strongly disordered superconductors
have challenged the application of the MB formulas due to the occurrence of
additional spectral weight at low energies below 2Delta. Here we identify three
crucial items which have to be included in the analysis of optical-conductivity
data for these systems: (a) the correct identification of the optical threshold
in the Mattis-Bardeen theory, and its relation with the gap value extracted
from the measured density of states, (b) the gauge-invariant evaluation of the
current-current response function, needed to account for the optical absorption
by SC collective modes, and (c) the inclusion into the MB formula of the energy
dependence of the density of states present already above Tc. By computing the
optical conductvity in the disordered attractive Hubbard model we analyze the
relevance of all these items, and we provide a compelling scheme for the
analysis and interpretation of the optical data in real materials.
",physics
"  The convergence speed of stochastic gradient descent (SGD) can be improved by
actively selecting mini-batches. We explore sampling schemes where similar data
points are less likely to be selected in the same mini-batch. In particular, we
prove that such repulsive sampling schemes lowers the variance of the gradient
estimator. This generalizes recent work on using Determinantal Point Processes
(DPPs) for mini-batch diversification (Zhang et al., 2017) to the broader class
of repulsive point processes. We first show that the phenomenon of variance
reduction by diversified sampling generalizes in particular to non-stationary
point processes. We then show that other point processes may be computationally
much more efficient than DPPs. In particular, we propose and investigate
Poisson Disk sampling---frequently encountered in the computer graphics
community---for this task. We show empirically that our approach improves over
standard SGD both in terms of convergence speed as well as final model
performance.
",statistics
"  The optical absorption of CdWO$_4$ is reported at high pressures up to 23
GPa. The onset of a phase transition was detected at 19.5 GPa, in good
agreement with a previous Raman spectroscopy study. The crystal structure of
the high-pressure phase of CdWO$_4$ was solved at 22 GPa employing
single-crystal synchrotron x-ray diffraction. The symmetry changes from space
group $P$2/$c$ in the low-pressure wolframite phase to $P2_1/c$ in the
high-pressure post-wolframite phase accompanied by a doubling of the unit-cell
volume. The octahedral oxygen coordination of the tungsten and cadmium ions is
increased to [7]-fold and [6+1]-fold, respectively, at the phase transition.
The compressibility of the low-pressure phase of CdWO$_4$ has been reevaluated
with powder x-ray diffraction up to 15 GPa finding a bulk modulus of $B_0$ =
123 GPa. The direct band gap of the low-pressure phase increases with
compression up to 16.9 GPa at 12 meV/GPa. At this point an indirect band gap
crosses the direct band gap and decreases at -2 meV/GPa up to 19.5 GPa where
the phase transition starts. At the phase transition the band gap collapses by
0.7 eV and another direct band gap decreases at -50 meV/GPa up to the maximum
measured pressure. The structural stability of the post-wolframite structure is
confirmed by \textit{ab initio} calculations finding the post-wolframite-type
phase to be more stable than the wolframite at 18 GPa. Lattice dynamic
calculations based on space group $P2_1/c$ explain well the Raman-active modes
previously measured in the high-pressure post-wolframite phase. The
pressure-induced band gap crossing in the wolframite phase as well as the
pressure dependence of the direct band gap in the high-pressure phase are
further discussed with respect to the calculations.
",physics
"  Playing the game of heads or tails in zero gravity demonstrates that there
exists a contextual ""measurement"" in classical mechanics. When the coin is
flipped, its orientation is a continuous variable. However, the ""measurement""
that occurs when the coin is caught by clapping two hands together gives a
discrete value (heads or tails) that depends on the context (orientation of the
hands). It is then shown that there is a strong analogy with the spin
measurement of the Stern-Gerlach experiment, and in particular with Stern and
Gerlach's sequential measurements. Finally, we clarify the analogy by recalling
how the de Broglie-Bohm interpretation simply explains the spin ""measurement"".
",physics
"  Power grids are critical infrastructure assets that face non-technical losses
(NTL) such as electricity theft or faulty meters. NTL may range up to 40% of
the total electricity distributed in emerging countries. Industrial NTL
detection systems are still largely based on expert knowledge when deciding
whether to carry out costly on-site inspections of customers. Electricity
providers are reluctant to move to large-scale deployments of automated systems
that learn NTL profiles from data due to the latter's propensity to suggest a
large number of unnecessary inspections. In this paper, we propose a novel
system that combines automated statistical decision making with expert
knowledge. First, we propose a machine learning framework that classifies
customers into NTL or non-NTL using a variety of features derived from the
customers' consumption data. The methodology used is specifically tailored to
the level of noise in the data. Second, in order to allow human experts to feed
their knowledge in the decision loop, we propose a method for visualizing
prediction results at various granularity levels in a spatial hologram. Our
approach allows domain experts to put the classification results into the
context of the data and to incorporate their knowledge for making the final
decisions of which customers to inspect. This work has resulted in appreciable
results on a real-world data set of 3.6M customers. Our system is being
deployed in a commercial NTL detection software.
",computer-science
"  All-goals updating exploits the off-policy nature of Q-learning to update all
possible goals an agent could have from each transition in the world, and was
introduced into Reinforcement Learning (RL) by Kaelbling (1993). In prior work
this was mostly explored in small-state RL problems that allowed tabular
representations and where all possible goals could be explicitly enumerated and
learned separately. In this paper we empirically explore 3 different extensions
of the idea of updating many (instead of all) goals in the context of RL with
deep neural networks (or DeepRL for short). First, in a direct adaptation of
Kaelbling's approach we explore if many-goals updating can be used to achieve
mastery in non-tabular visual-observation domains. Second, we explore whether
many-goals updating can be used to pre-train a network to subsequently learn
faster and better on a single main task of interest. Third, we explore whether
many-goals updating can be used to provide auxiliary task updates in training a
network to learn faster and better on a single main task of interest. We
provide comparisons to baselines for each of the 3 extensions.
",statistics
"  Threshold-linear networks (TLNs) are models of neural networks that consist
of simple, perceptron-like neurons and exhibit nonlinear dynamics that are
determined by the network's connectivity. The fixed points of a TLN, including
both stable and unstable equilibria, play a critical role in shaping its
emergent dynamics. In this work, we provide two novel characterizations for the
set of fixed points of a competitive TLN: the first is in terms of a simple
sign condition, while the second relies on the concept of domination. We apply
these results to a special family of TLNs, called combinatorial
threshold-linear networks (CTLNs), whose connectivity matrices are defined from
directed graphs. This leads us to prove a series of graph rules that enable one
to determine fixed points of a CTLN by analyzing the underlying graph.
Additionally, we study larger networks composed of smaller ""building block""
subnetworks, and prove several theorems relating the fixed points of the full
network to those of its components. Our results provide the foundation for a
kind of ""graphical calculus"" to infer features of the dynamics from a network's
connectivity.
",quantitative-biology
"  For marketing or power grid management purposes, many studies based on the
analysis of the total electricity consumption curves of groups of customers are
now carried out by electricity companies. Aggregated total or mean load curves
are estimated using individual curves measured at fine time grid and collected
according to some sampling design. Due to the skewness of the distribution of
electricity consumptions, these samples often contain outlying curves which may
have an important impact on the usual estimation procedures. We introduce
several robust estimators of the total consumption curve which are not
sensitive to such outlying curves. These estimators are based on the
conditional bias approach and robust functional methods. We also derive mean
square error estimators of these robust estimators and finally, we evaluate and
compare the performance of the suggested estimators on Irish electricity data.
",statistics
"  The almost sure Hausdorff dimension of the limsup set of randomly distributed
rectangles in a product of Ahlfors regular metric spaces is computed in terms
of the singular value function of the rectangles.
",mathematics
"  For conventional secret sharing, if cheaters can submit possibly forged
shares after observing shares of the honest users in the reconstruction phase
then they cannot only disturb the protocol but also only they may reconstruct
the true secret. To overcome the problem, secret sharing scheme with properties
of cheater-identification have been proposed. Existing protocols for
cheater-identifiable secret sharing assumed non-rushing cheaters or honest
majority. In this paper, we remove both conditions simultaneously, and give its
universal construction from any secret sharing scheme. To resolve this end, we
propose the concepts of ""individual identification"" and ""agreed
identification"".
",computer-science
"  We consider a class of one-dimensional compass models with staggered
Dzyaloshinskii-Moriya exchange interactions in an external transverse magnetic
field. Based on the exact solution derived from Jordan-Wigner approach, we
study the excitation gap, energy spectra, spin correlations and critical
properties at phase transitions. We explore mutual effects of the staggered
Dzyaloshinskii-Moriya interaction and the magnetic field on the energy spectra
and the ground-state phase diagram. Thermodynamic quantities including the
entropy and the specific heat are discussed, and their universal scalings at
low temperature are demonstrated.
",physics
"  Language change involves the competition between alternative linguistic forms
(1). The spontaneous evolution of these forms typically results in monotonic
growths or decays (2, 3) like in winner-take-all attractor behaviors. In the
case of the Spanish past subjunctive, the spontaneous evolution of its two
competing forms (ended in -ra and -se) was perturbed by the appearance of the
Royal Spanish Academy in 1713, which enforced the spelling of both forms as
perfectly interchangeable variants (4), at a moment in which the -ra form was
dominant (5). Time series extracted from a massive corpus of books (6) reveal
that this regulation in fact produced a transient renewed interest for the old
form -se which, once faded, left the -ra again as the dominant form up to the
present day. We show that time series are successfully explained by a
two-dimensional linear model that integrates an imitative and a novelty
component. The model reveals that the temporal scale over which collective
attention fades is in inverse proportion to the verb frequency. The integration
of the two basic mechanisms of imitation and attention to novelty allows to
understand diverse competing objects, with lifetimes that range from hours for
memes and news (7, 8) to decades for verbs, suggesting the existence of a
general mechanism underlying cultural evolution.
",quantitative-biology
"  Many neural systems display avalanche behavior characterized by uninterrupted
sequences of neuronal firing whose distributions of size and durations are
heavy-tailed. Theoretical models of such systems suggest that these dynamics
support optimal information transmission and storage. However, the unknown role
of network structure precludes an understanding of how variations in network
topology manifest in neural dynamics and either support or impinge upon
information processing. Here, using a generalized spiking model, we develop a
mechanistic understanding of how network topology supports information
processing through network dynamics. First, we show how network topology
determines network dynamics by analytically and numerically demonstrating that
network topology can be designed to propagate stimulus patterns for long
durations. We then identify strongly connected cycles as empirically observable
network motifs that are prevalent in such networks. Next, we show that within a
network, mathematical intuitions from network control theory are tightly linked
with dynamics initiated by node-specific stimulation and can identify stimuli
that promote long-lasting cascades. Finally, we use these network-based metrics
and control-based stimuli to demonstrate that long-lasting cascade dynamics
facilitate delayed recovery of stimulus patterns from network activity, as
measured by mutual information. Collectively, our results provide evidence that
cortical networks are structured with architectural motifs that support
long-lasting propagation and recovery of a few crucial patterns of stimulation,
especially those consisting of activity in highly controllable neurons.
Broadly, our results imply that avalanching neural networks could contribute to
cognitive faculties that require persistent activation of neuronal patterns,
such as working memory or attention.
",quantitative-biology
"  The hardness of the learning with errors (LWE) problem is one of the most
fruitful resources of modern cryptography. In particular, it is one of the most
prominent candidates for secure post-quantum cryptography. Understanding its
quantum complexity is therefore an important goal. We show that under quantum
polynomial time reductions, LWE is equivalent to a relaxed version of the
dihedral coset problem (DCP), which we call extrapolated DCP (eDCP). The extent
of extrapolation varies with the LWE noise rate. By considering different
extents of extrapolation, our result generalizes Regev's famous proof that if
DCP is in BQP (quantum poly-time) then so is LWE (FOCS'02). We also discuss a
connection between eDCP and Childs and Van Dam's algorithm for generalized
hidden shift problems (SODA'07). Our result implies that a BQP solution for LWE
might not require the full power of solving DCP, but rather only a solution for
its relaxed version, eDCP, which could be easier.
",computer-science
"  The practical success of Boolean Satisfiability (SAT) solvers stems from the
CDCL (Conflict-Driven Clause Learning) approach to SAT solving. However, from a
propositional proof complexity perspective, CDCL is no more powerful than the
resolution proof system, for which many hard examples exist. This paper
proposes a new problem transformation, which enables reducing the decision
problem for formulas in conjunctive normal form (CNF) to the problem of solving
maximum satisfiability over Horn formulas. Given the new transformation, the
paper proves a polynomial bound on the number of MaxSAT resolution steps for
pigeonhole formulas. This result is in clear contrast with earlier results on
the length of proofs of MaxSAT resolution for pigeonhole formulas. The paper
also establishes the same polynomial bound in the case of modern core-guided
MaxSAT solvers. Experimental results, obtained on CNF formulas known to be hard
for CDCL SAT solvers, show that these can be efficiently solved with modern
MaxSAT solvers.
",computer-science
"  Measuring the full distribution of individual particles is of fundamental
importance to characterize many-body quantum systems through correlation
functions at any order. Here we demonstrate the possibility to reconstruct the
momentum-space distribution of three-dimensional interacting lattice gases
atom-by-atom. This is achieved by detecting individual metastable Helium atoms
in the far-field regime of expansion, when released from an optical lattice. We
benchmark our technique with Quantum Monte-Carlo calculations, demonstrating
the ability to resolve momentum distributions of superfluids occupying $10^5$
lattice sites. It permits a direct measure of the condensed fraction across
phase transitions, as we illustrate on the superfluid-to-normal transition. Our
single-atom-resolved approach opens a new route to investigate interacting
lattice gases through momentum correlations.
",physics
"  We study the supersymmetric partition function on $S^1 \times L(r, 1)$, or
the lens space index of four-dimensional $\mathcal{N}=2$ superconformal field
theories and their connection to two-dimensional chiral algebras. We primarily
focus on free theories as well as Argyres-Douglas theories of type $(A_1, A_k)$
and $(A_1, D_k)$. We observe that in specific limits, the lens space index is
reproduced in terms of the (refined) character of an appropriately twisted
module of the associated two-dimensional chiral algebra or a generalized vertex
operator algebra. The particular twisted module is determined by the choice of
discrete holonomies for the flavor symmetry in four-dimensions.
",mathematics
"  On the probability simplex, we can consider the standard information
geometric structure with the e- and m-affine connections mutually dual with
respect to the Fisher metric. The geometry naturally defines submanifolds
simultaneously autoparallel for the both affine connections, which we call {\em
doubly autoparallel submanifolds}.
In this note we discuss their several interesting common properties. Further,
we algebraically characterize doubly autoparallel submanifolds on the
probability simplex and give their classification.
",mathematics
"  Let $\mathcal{P}_r$ denote an almost-prime with at most $r$ prime factors,
counted according to multiplicity. In this paper, it is proved that, for
$12\leqslant b\leqslant 35$ and for every sufficiently large odd integer $N$,
the equation \begin{equation*}
N=x^2+p_1^3+p_2^3+p_3^3+p_4^3+p_5^4+p_6^b \end{equation*} is solvable with
$x$ being an almost-prime $\mathcal{P}_{r(b)}$ and the other variables primes,
where $r(b)$ is defined in the Theorem. This result constitutes an improvement
upon that of Lü and Mu.
",mathematics
"  Online advertising and product recommendation are important domains of
applications for multi-armed bandit methods. In these fields, the reward that
is immediately available is most often only a proxy for the actual outcome of
interest, which we refer to as a conversion. For instance, in web advertising,
clicks can be observed within a few seconds after an ad display but the
corresponding sale --if any-- will take hours, if not days to happen. This
paper proposes and investigates a new stochas-tic multi-armed bandit model in
the framework proposed by Chapelle (2014) --based on empirical studies in the
field of web advertising-- in which each action may trigger a future reward
that will then happen with a stochas-tic delay. We assume that the probability
of conversion associated with each action is unknown while the distribution of
the conversion delay is known, distinguishing between the (idealized) case
where the conversion events may be observed whatever their delay and the more
realistic setting in which late conversions are censored. We provide
performance lower bounds as well as two simple but efficient algorithms based
on the UCB and KLUCB frameworks. The latter algorithm, which is preferable when
conversion rates are low, is based on a Poissonization argument, of independent
interest in other settings where aggregation of Bernoulli observations with
different success probabilities is required.
",computer-science
"  In the paper ""Formality conjecture"" (1996) Kontsevich designed a universal
flow $\dot{\mathcal{P}}=\mathcal{Q}_{a:b}(\mathcal{P})=a\Gamma_{1}+b\Gamma_{2}$
on the spaces of Poisson structures $\mathcal{P}$ on all affine manifolds of
dimension $n \geqslant 2$. We prove a claim from $\textit{loc. cit.}$ stating
that if $n=2$, the flow $\mathcal{Q}_{1:0}=\Gamma_{1}(\mathcal{P})$ is
Poisson-cohomology trivial: $\Gamma_{1}(\mathcal{P})$ is the Schouten bracket
of $\mathcal{P}$ with $\mathcal{X}$, for some vector field $\mathcal{X}$; we
examine the structure of the space of solutions $\mathcal{X}$. Both the
construction of differential polynomials $\Gamma_{1}(\mathcal{P})$ and
$\Gamma_{2}(\mathcal{P})$ and the technique to study them remain valid in
higher dimensions $n \geqslant 3$, but neither the trivializing vector field
$\mathcal{X}$ nor the setting $b:=0$ survive at $n\geqslant 3$, where the
balance is $a:b=1:6$.
",mathematics
"  We present model for anisotropic compact star under the general theory of
relativity of Einstein. In the study a 4-dimensional spacetime has been
considered which is embedded into the 5-dimensional flat metric so that the
spherically symmetric metric has class 1 when the condition
$e^{\lambda}=\left(\,1+C\,e^{\nu} \,{\nu'}^2\,\right)$ is satisfied ($\lambda$
and $\nu$ being the metric potentials along with a constant $C$). A set of
solutions for the field equations are found depending on the index $n$ involved
in the physical parameters. The interior solutions have been matched smoothly
at the boundary of the spherical distribution to the exterior Schwarzschild
solution which necessarily provides values of the unknown constants. We have
chosen the values of $n$ as $n=2$ and $n$=10 to 20000 for which interesting and
physically viable results can be found out. The numerical values of the
parameters and arbitrary constants for different compact stars are assumed in
the graphical plots and tables as follows: (i) LMC X-4 : $a=0.0075$,
$b=0.000821$ for $n=2$ and $a=0.0075$, $nb=0.00164$ for $n\ge 10$, (ii) SMC
X-1: $a=0.00681$, $b=0.00078$ for $n=2$, and $a=0.00681$, $nb=0.00159$ for $n
\ge 10$. The investigations on the physical features of the model include
several astrophysical issues, like (i) regularity behavior of stars at the
centre, (ii) well behaved condition for velocity of sound, (iii) energy
conditions, (iv) stabilty of the system via the following three techniques -
adiabatic index, Herrera cracking concept and TOV equation, (v) total mass,
effective mass and compactification factor and (vi) surface redshift. Specific
numerical values of the compact star candidates LMC X-4 and SMC X-1 are
calculated for central and surface densities as well as central pressure to
compare the model value with actual observational data.
",physics
"  We propose the existence of a new universality in classical chaotic systems
when the number of degrees of freedom is large: the statistical property of the
Lyapunov spectrum is described by Random Matrix Theory. We demonstrate it by
studying the finite-time Lyapunov exponents of the matrix model of a stringy
black hole and the mass deformed models. The massless limit, which has a dual
string theory interpretation, is special in that the universal behavior can be
seen already at t=0, while in other cases it sets in at late time. The same
pattern is demonstrated also in the product of random matrices.
",physics
"  Manipulation of deformable objects, such as ropes and cloth, is an important
but challenging problem in robotics. We present a learning-based system where a
robot takes as input a sequence of images of a human manipulating a rope from
an initial to goal configuration, and outputs a sequence of actions that can
reproduce the human demonstration, using only monocular images as input. To
perform this task, the robot learns a pixel-level inverse dynamics model of
rope manipulation directly from images in a self-supervised manner, using about
60K interactions with the rope collected autonomously by the robot. The human
demonstration provides a high-level plan of what to do and the low-level
inverse model is used to execute the plan. We show that by combining the high
and low-level plans, the robot can successfully manipulate a rope into a
variety of target shapes using only a sequence of human-provided images for
direction.
",computer-science
"  We present a Character-Word Long Short-Term Memory Language Model which both
reduces the perplexity with respect to a baseline word-level language model and
reduces the number of parameters of the model. Character information can reveal
structural (dis)similarities between words and can even be used when a word is
out-of-vocabulary, thus improving the modeling of infrequent and unknown words.
By concatenating word and character embeddings, we achieve up to 2.77% relative
improvement on English compared to a baseline model with a similar amount of
parameters and 4.57% on Dutch. Moreover, we also outperform baseline word-level
models with a larger number of parameters.
",computer-science
"  Generalize Kobayashi's example for the Noether inequality in dimension three,
we provide examples of n-folds of general type with small volumes.
",mathematics
"  We have synthesized 10 new iron oxyarsenides, K$Ln_2$Fe$_4$As$_4$O$_2$ ($Ln$
= Gd, Tb, Dy, and Ho) and Cs$Ln_2$Fe$_4$As$_4$O$_2$ ($Ln$ = Nd, Sm, Gd, Tb, Dy,
and Ho), with the aid of lattice-match [between $A$Fe$_2$As$_2$ ($A$ = K and
Cs) and $Ln$FeAsO] approach. The resultant compounds possess hole-doped
conducting double FeAs layers, [$A$Fe$_4$As$_4$]$^{2-}$, that are separated by
the insulating [$Ln_2$O$_2$]$^{2+}$ slabs. Measurements of electrical
resistivity and dc magnetic susceptibility demonstrate bulk superconductivity
at $T_\mathrm{c}$ = 33 - 37 K. We find that $T_\mathrm{c}$ correlates with the
axis ratio $c/a$ for all 12442-type superconductors discovered. Also,
$T_\mathrm{c}$ tends to increase with the lattice mismatch, implying a role of
lattice instability for the enhancement of superconductivity.
",physics
"  We report the transverse relaxation rates 1/$T_2$'s of the $^{63}$Cu nuclear
spin-echo envelope for double-layer high-$T_c$ cuprate superconductors
HgBa$_{2}$CaCu$_{2}$O$_{6+d}$ from underdoped to overdoped. The relaxation rate
1/$T_{2L}$ of the exponential function (Lorentzian component) shows a peak at
220$-$240 K in the underdoped ($T_c$ = 103 K) and the optimally doped ($T_c$ =
127 K) samples but no peak in the overdoped ($T_c$ = 93 K) sample. The
enhancement in 1/$T_{2L}$ suggests development of the zero frequency components
of local field fluctuations. Ultraslow fluctuations are hidden in the pseudogap
states.
",physics
"  We introduce the Helsinki Neural Machine Translation system (HNMT) and how it
is applied in the news translation task at WMT 2017, where it ranked first in
both the human and automatic evaluations for English--Finnish. We discuss the
success of English--Finnish translations and the overall advantage of NMT over
a strong SMT baseline. We also discuss our submissions for English--Latvian,
English--Chinese and Chinese--English.
",computer-science
"  Living cells move thanks to assemblies of actin filaments and myosin motors
that range from very organized striated muscle tissue to disordered
intracellular bundles. The mechanisms powering these disordered structures are
debated, and all models studied so far predict that they are contractile. We
reexamine this prediction through a theoretical treatment of the interplay of
three well-characterized internal dynamical processes in actomyosin bundles:
actin treadmilling, the attachement-detachment dynamics of myosin and that of
crosslinking proteins. We show that these processes enable an extensive control
of the bundle's active mechanics, including reversals of the filaments'
apparent velocities and the possibility of generating extension instead of
contraction. These effects offer a new perspective on well-studied in vivo
systems, as well as a robust criterion to experimentally elucidate the
underpinnings of actomyosin activity.
",physics
"  Generating diverse questions for given images is an important task for
computational education, entertainment and AI assistants. Different from many
conventional prediction techniques is the need for algorithms to generate a
diverse set of plausible questions, which we refer to as ""creativity"". In this
paper we propose a creative algorithm for visual question generation which
combines the advantages of variational autoencoders with long short-term memory
networks. We demonstrate that our framework is able to generate a large set of
varying questions given a single input image.
",computer-science
"  KAGRA is a 3-km cryogenic interferometric gravitational wave telescope
located at an underground site in Japan. In order to achieve its target
sensitivity, the relative positions of the mirrors of the interferometer must
be finely adjusted with attached actuators. We have developed a model to
simulate the length control loops of the KAGRA interferometer with realistic
suspension responses and various noises for mirror actuation. Using our model,
we have designed the actuation parameters to have sufficient force range to
acquire lock as well as to control all the length degrees of freedom without
introducing excess noise.
",physics
"  Pseudogap phase in superconductors continues to be an outstanding puzzle that
differentiates unconventional superconductors from the conventional ones
(BCS-superconductors). Employing high resolution photoemission spectroscopy on
a highly dense conventional superconductor, MgB2, we discover an interesting
scenario. While the spectral evolution close to the Fermi energy is
commensurate to BCS descriptions as expected, the spectra in the wider energy
range reveal emergence of a pseudogap much above the superconducting transition
temperature indicating apparent departure from the BCS scenario. The energy
scale of the pseudogap is comparable to the energy of E2g phonon mode
responsible for superconductivity in MgB2 and the pseudogap can be attributed
to the effect of electron-phonon coupling on the electronic structure. These
results reveal a scenario of the emergence of the superconducting gap within an
electron-phonon coupling induced pseudogap.
",physics
"  Membrane proteins constitute a large portion of the human proteome and
perform a variety of important functions as membrane receptors, transport
proteins, enzymes, signaling proteins, and more. The computational studies of
membrane proteins are usually much more complicated than those of globular
proteins. Here we propose a new continuum model for Poisson-Boltzmann
calculations of membrane channel proteins. Major improvements over the existing
continuum slab model are as follows: 1) The location and thickness of the slab
model are fine-tuned based on explicit-solvent MD simulations. 2) The highly
different accessibility in the membrane and water regions are addressed with a
two-step, two-probe grid labeling procedure, and 3) The water pores/channels
are automatically identified. The new continuum membrane model is optimized (by
adjusting the membrane probe, as well as the slab thickness and center) to best
reproduce the distributions of buried water molecules in the membrane region as
sampled in explicit water simulations. Our optimization also shows that the
widely adopted water probe of 1.4 {\AA} for globular proteins is a very
reasonable default value for membrane protein simulations. It gives an overall
minimum number of inconsistencies between the continuum and explicit
representations of water distributions in membrane channel proteins, at least
in the water accessible pore/channel regions that we focus on. Finally, we
validate the new membrane model by carrying out binding affinity calculations
for a potassium channel, and we observe a good agreement with experiment
results.
",physics
"  Despite their overwhelming capacity to overfit, deep learning architectures
tend to generalize relatively well to unseen data, allowing them to be deployed
in practice. However, explaining why this is the case is still an open area of
research. One standing hypothesis that is gaining popularity, e.g. Hochreiter &
Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the
loss function found by stochastic gradient based methods results in good
generalization. This paper argues that most notions of flatness are problematic
for deep models and can not be directly applied to explain generalization.
Specifically, when focusing on deep networks with rectifier units, we can
exploit the particular geometry of parameter space induced by the inherent
symmetries that these architectures exhibit to build equivalent models
corresponding to arbitrarily sharper minima. Furthermore, if we allow to
reparametrize a function, the geometry of its parameters can change drastically
without affecting its generalization properties.
",computer-science
"  Many applied settings in empirical economics involve simultaneous estimation
of a large number of parameters. In particular, applied economists are often
interested in estimating the effects of many-valued treatments (like teacher
effects or location effects), treatment effects for many groups, and prediction
models with many regressors. In these settings, machine learning methods that
combine regularized estimation and data-driven choices of regularization
parameters are useful to avoid over-fitting. In this article, we analyze the
performance of a class of machine learning estimators that includes ridge,
lasso and pretest in contexts that require simultaneous estimation of many
parameters. Our analysis aims to provide guidance to applied researchers on (i)
the choice between regularized estimators in practice and (ii) data-driven
selection of regularization parameters. To address (i), we characterize the
risk (mean squared error) of regularized estimators and derive their relative
performance as a function of simple features of the data generating process. To
address (ii), we show that data-driven choices of regularization parameters,
based on Stein's unbiased risk estimate or on cross-validation, yield
estimators with risk uniformly close to the risk attained under the optimal
(unfeasible) choice of regularization parameters. We use data from recent
examples in the empirical economics literature to illustrate the practical
applicability of our results.
",statistics
"  The Boltzmann equation is an integro-differential equation which describes
the density function of the distribution of the velocities of the molecules of
dilute monoatomic gases under the assumption that the energy is only
transferred via collisions between the molecules. In 1956 Kac studied the
Boltzmann equation and defined a property of the density function that he
called the ""Boltzmann property"" which describes the behavior of the density
function at a given fixed time as the number of particles tends to infinity.
The Boltzmann property has been studied extensively since then, and now it is
simply called chaos, or Kac's chaos. On the other hand, in ergodic theory,
chaos usually refers to the mixing properties of a dynamical system as time
tends to infinity. A relationship is derived between Kac's chaos and the notion
of mixing.
",mathematics
"  Interpretability has become an important issue in the machine learning field,
along with the success of layered neural networks in various practical tasks.
Since a trained layered neural network consists of a complex nonlinear
relationship between large number of parameters, we failed to understand how
they could achieve input-output mappings with a given data set. In this paper,
we propose the non-negative task decomposition method, which applies
non-negative matrix factorization to a trained layered neural network. This
enables us to decompose the inference mechanism of a trained layered neural
network into multiple principal tasks of input-output mapping, and reveal the
roles of hidden units in terms of their contribution to each principal task.
",statistics
"  We present a general formalism of multipole descriptions under the space-time
inversion group. We elucidate that two types of atomic toroidal multipoles,
i.e., electric and magnetic, are fundamental pieces to express electronic order
parameters in addition to ordinary electric and magnetic multipoles. By
deriving quantum-mechanical operators for both toroidal multipoles, we show
that electric (magnetic) toroidal multipole higher than dipole (monopole) can
become a primary order parameter in a hybridized-orbital system. We also
demonstrate emergent cross-correlated couplings between electric, magnetic, and
elastic degrees of freedom, such as magneto-electric and
magneto(electro)-elastic couplings, under toroidal multipole orders.
",physics
"  We resolve a number of long-standing open problems in online graph coloring.
More specifically, we develop tight lower bounds on the performance of online
algorithms for fundamental graph classes. An important contribution is that our
bounds also hold for randomized online algorithms, for which hardly any results
were known. Technically, we construct lower bounds for chordal graphs. The
constructions then allow us to derive results on the performance of randomized
online algorithms for the following further graph classes: trees, planar,
bipartite, inductive, bounded-treewidth and disk graphs. It shows that the best
competitive ratio of both deterministic and randomized online algorithms is
$\Theta(\log n)$, where $n$ is the number of vertices of a graph. Furthermore,
we prove that this guarantee cannot be improved if an online algorithm has a
lookahead of size $O(n/\log n)$ or access to a reordering buffer of size
$n^{1-\epsilon}$, for any $0<\epsilon\leq 1$. A consequence of our results is
that, for all of the above mentioned graph classes except bipartite graphs, the
natural $\textit{First Fit}$ coloring algorithm achieves an optimal
performance, up to constant factors, among deterministic and randomized online
algorithms.
",computer-science
"  Knowledge base completion (KBC) aims to predict missing information in a
knowledge base.In this paper, we address the out-of-knowledge-base (OOKB)
entity problem in KBC:how to answer queries concerning test entities not
observed at training time. Existing embedding-based KBC models assume that all
test entities are available at training time, making it unclear how to obtain
embeddings for new entities without costly retraining. To solve the OOKB entity
problem without retraining, we use graph neural networks (Graph-NNs) to compute
the embeddings of OOKB entities, exploiting the limited auxiliary knowledge
provided at test time.The experimental results show the effectiveness of our
proposed model in the OOKB setting.Additionally, in the standard KBC setting in
which OOKB entities are not involved, our model achieves state-of-the-art
performance on the WordNet dataset. The code and dataset are available at
this https URL
",computer-science
"  In order to understand underlying processes governing environmental and
physical processes, and predict future outcomes, a complex computer model is
frequently required to simulate these dynamics. However there is inevitably
uncertainty related to the exact parametric form or the values of such
parameters to be used when developing these simulators, with \emph{ranges} of
plausible values prevalent in the literature. Systematic errors introduced by
failing to account for these uncertainties have the potential to have a large
effect on resulting estimates in unknown quantities of interest. Due to the
complexity of these types of models, it is often unfeasible to run large
numbers of training runs that are usually required for full statistical
emulators of the environmental processes. We therefore present a method for
accounting for uncertainties in complex environmental simulators without the
need for very large numbers of training runs and illustrate the method through
an application to the Met Office's atmospheric transport model NAME. We
conclude that there are two principle parameters that are linked with
variability in NAME outputs, namely the free tropospheric turbulence parameter
and particle release height. Our results suggest the former should be
significantly larger than is currently implemented as a default in NAME, whilst
changes in the latter most likely stem from inconsistencies between the model
specified ground height at the observation locations and the true height at
this location. Estimated discrepancies from independent data are consistent
with the discrepancy between modelled and true ground height.
",statistics
"  Single atoms form a model system for understanding the limits of single
photon detection. Here, we develop a non-Markov theory of single-photon
absorption by a two-level atom to place limits on the absorption (transduction)
time. We show the existence of a finite rise time in the probability of
excitation of the atom during the absorption event which is infinitely fast in
previous Markov theories. This rise time is governed by the bandwidth of the
atom-field interaction spectrum and leads to a fundamental jitter in
time-stamping the absorption event. Our theoretical framework captures both the
weak and strong atom-field coupling regimes and sheds light on the spectral
matching between the interaction bandwidth and single photon Fock state pulse
spectrum. Our work opens questions whether such jitter in the absorption event
can be observed in a multi-mode realistic single photon detector. Finally, we
also shed light on the fundamental differences between linear and nonlinear
detector outputs for single photon Fock state vs. coherent state pulses.
",physics
"  We study the entanglement entropy of gapped phases of matter in three spatial
dimensions. We focus in particular on size-independent contributions to the
entropy across entanglement surfaces of arbitrary topologies. We show that for
low energy fixed-point theories, the constant part of the entanglement entropy
across any surface can be reduced to a linear combination of the entropies
across a sphere and a torus. We first derive our results using strong
sub-additivity inequalities along with assumptions about the entanglement
entropy of fixed-point models, and identify the topological contribution by
considering the renormalization group flow; in this way we give an explicit
definition of topological entanglement entropy $S_{\mathrm{topo}}$ in (3+1)D,
which sharpens previous results. We illustrate our results using several
concrete examples and independent calculations, and show adding ""twist"" terms
to the Lagrangian can change $S_{\mathrm{topo}}$ in (3+1)D. For the generalized
Walker-Wang models, we find that the ground state degeneracy on a 3-torus is
given by $\exp(-3S_{\mathrm{topo}}[T^2])$ in terms of the topological
entanglement entropy across a 2-torus. We conjecture that a similar
relationship holds for Abelian theories in $(d+1)$ dimensional spacetime, with
the ground state degeneracy on the $d$-torus given by
$\exp(-dS_{\mathrm{topo}}[T^{d-1}])$.
",physics
"  Using the representation theory of Cherednik algebras at $t=0$ and a Galois
covering of the Calogero-Moser space, we define the notions of left, right and
two-sided Calogero-Moser cells for any finite complex reflection group. To each
Caloger-Moser two-sided cell is associated a Calogero-Moser family, while to
each Calogero-Moser left cell is associated a Calogero-Moser cellular
representation. We study properties of these objects and we conjecture that,
whenever the reflection group is real (i.e. is a Coxeter group), these notions
coincide with the one of Kazhdan-Lusztig left, right and two-sided cells,
Kazhdan-Lusztig families and Kazhdan-Lusztig cellular representations.
",mathematics
"  The SRv6 architecture (Segment Routing based on IPv6 data plane) is a
promising solution to support services like Traffic Engineering, Service
Function Chaining and Virtual Private Networks in IPv6 backbones and
datacenters. The SRv6 architecture has interesting scalability properties as it
reduces the amount of state information that needs to be configured in the
nodes to support the network services. In this paper, we describe the
advantages of complementing the SRv6 technology with an SDN based approach in
backbone networks. We discuss the architecture of a SRv6 enabled network based
on Linux nodes. In addition, we present the design and implementation of the
Southbound API between the SDN controller and the SRv6 device. We have defined
a data-model and four different implementations of the API, respectively based
on gRPC, REST, NETCONF and remote Command Line Interface (CLI). Since it is
important to support both the development and testing aspects we have realized
an Intent based emulation system to build realistic and reproducible
experiments. This collection of tools automate most of the configuration
aspects relieving the experimenter from a significant effort. Finally, we have
realized an evaluation of some performance aspects of our architecture and of
the different variants of the Southbound APIs and we have analyzed the effects
of the configuration updates in the SRv6 enabled nodes.
",computer-science
"  Traditional data cleaning identifies dirty data by classifying original data
sequences, which is a class$-$imbalanced problem since the proportion of
incorrect data is much less than the proportion of correct ones for most
diagnostic systems in Magnetic Confinement Fusion (MCF) devices. When using
machine learning algorithms to classify diagnostic data based on
class$-$imbalanced training set, most classifiers are biased towards the major
class and show very poor classification rates on the minor class. By
transforming the direct classification problem about original data sequences
into a classification problem about the physical similarity between data
sequences, the class$-$balanced effect of Time$-$Domain Global Similarity
(TDGS) method on training set structure is investigated in this paper.
Meanwhile, the impact of improved training set structure on data cleaning
performance of TDGS method is demonstrated with an application example in EAST
POlarimetry$-$INTerferometry (POINT) system.
",computer-science
"  Objective: A model is presented to evaluate the viability of using electrical
impedance tomography (EIT) with a nerve cuff to record neural activity in
peripheral nerves. Approach: Established modelling approaches in neural-EIT are
expanded on to be used, for the first time, on myelinated fibres which are
abundant in mammalian peripheral nerves and transmit motor commands. Main
results: Fibre impedance models indicate activity in unmyelinated fibres can be
screened out using operating frequencies above 100 Hz. At 1 kHz and 10 mm
electrode spacing, impedance magnitude of inactive intra-fascicle tissue and
the fraction changes during neural activity are estimated to be 1,142
{\Omega}.cm and -8.8x10-4, respectively, with a transverse current, and 328
{\Omega}.cm & -0.30, respectively with a longitudinal current. We show that a
novel EIT drive and measurement electrode pattern which utilises longitudinal
current and longitudinal differential boundary voltage measurements could
distinguish activity in different fascicles of a three-fascicle mammalian nerve
using pseudo-experimental data synthesised to replicate real operating
conditions. Significance: The results of this study provide an estimate of the
transient change in impedance of intra-fascicle tissue during neural activity
in mammalian nerve, and present a viable EIT electrode pattern, both of which
are critical steps towards implementing EIT in a nerve cuff for neural
prosthetics interfaces.
",physics
"  The paper deals with a construction of a separating system of rational
invariants for finite dimensional generic algebras. In the process of dealing
an approach to a rough classification of finite dimensional algebras is offered
by attaching them some quadratic forms.
",mathematics
"  Risk diversification is one of the dominant concerns for portfolio managers.
Various portfolio constructions have been proposed to minimize the risk of the
portfolio under some constrains including expected returns. We propose a
portfolio construction method that incorporates the complex valued principal
component analysis into the risk diversification portfolio construction. The
proposed method is verified to outperform the conventional risk parity and risk
diversification portfolio constructions.
",quantitative-finance
"  Several studies have shown that the network traffic that is generated by a
visit to a website over Tor reveals information specific to the website through
the timing and sizes of network packets. By capturing traffic traces between
users and their Tor entry guard, a network eavesdropper can leverage this
meta-data to reveal which website Tor users are visiting. The success of such
attacks heavily depends on the particular set of traffic features that are used
to construct the fingerprint. Typically, these features are manually engineered
and, as such, any change introduced to the Tor network can render these
carefully constructed features ineffective. In this paper, we show that an
adversary can automate the feature engineering process, and thus automatically
deanonymize Tor traffic by applying our novel method based on deep learning. We
collect a dataset comprised of more than three million network traces, which is
the largest dataset of web traffic ever used for website fingerprinting, and
find that the performance achieved by our deep learning approaches is
comparable to known methods which include various research efforts spanning
over multiple years. The obtained success rate exceeds 96% for a closed world
of 100 websites and 94% for our biggest closed world of 900 classes. In our
open world evaluation, the most performant deep learning model is 2% more
accurate than the state-of-the-art attack. Furthermore, we show that the
implicit features automatically learned by our approach are far more resilient
to dynamic changes of web content over time. We conclude that the ability to
automatically construct the most relevant traffic features and perform accurate
traffic recognition makes our deep learning based approach an efficient,
flexible and robust technique for website fingerprinting.
",computer-science
"  The ECIR half-day workshop on Task-Based and Aggregated Search (TBAS) was
held in Barcelona, Spain on 1 April 2012. The program included a keynote talk
by Professor Jarvelin, six full paper presentations, two poster presentations,
and an interactive discussion among the approximately 25 participants. This
report overviews the aims and contents of the workshop and outlines the major
outcomes.
",computer-science
"  It is likely that most protostellar systems undergo a brief phase where the
protostellar disc is self-gravitating. If these discs are prone to
fragmentation, then they are able to rapidly form objects that are initially of
several Jupiter masses and larger. The fate of these disc fragments (and the
fate of planetary bodies formed afterwards via core accretion) depends
sensitively not only on the fragment's interaction with the disc, but with its
neighbouring fragments.
We return to and revise our population synthesis model of self-gravitating
disc fragmentation and tidal downsizing. Amongst other improvements, the model
now directly incorporates fragment-fragment interactions while the disc is
still present. We find that fragment-fragment scattering dominates the orbital
evolution, even when we enforce rapid migration and inefficient gap formation.
Compared to our previous model, we see a small increase in the number of
terrestrial-type objects being formed, although their survival under tidal
evolution is at best unclear. We also see evidence for disrupted fragments with
evolved grain populations - this is circumstantial evidence for the formation
of planetesimal belts, a phenomenon not seen in runs where fragment-fragment
interactions are ignored.
In spite of intense dynamical evolution, our population is dominated by
massive giant planets and brown dwarfs at large semimajor axis, which direct
imaging surveys should, but only rarely, detect. Finally, disc fragmentation is
shown to be an efficient manufacturer of free floating planetary mass objects,
and the typical multiplicity of systems formed via gravitational instability
will be low.
",physics
"  Janus type Water-Splitting Catalysts have attracted highest attention as a
tool of choice for solar to fuel conversion. AISI Ni 42 steel was upon harsh
anodization converted in a bifunctional electrocatalyst. Oxygen evolution
reaction- (OER) and hydrogen evolution reaction (HER) are highly efficiently
and steadfast catalyzed at pH 7, 13, 14, 14.6 (OER) respectively at pH 0, 1,
13, 14, 14.6 (HER). The current density taken from long-term OER measurements
in pH 7 buffer solution upon the electro activated steel at 491 mV
overpotential was around 4 times higher (4 mA/cm2) in comparison with recently
developed OER electrocatalysts. The very strong voltage-current behavior of the
catalyst shown in OER polarization experiments at both pH 7 and at pH 13 were
even superior to those known for IrO2-RuO2. No degradation of the catalyst was
detected even when conditions close to standard industrial operations were
applied to the catalyst. A stable Ni-, Fe- oxide based passivating layer
sufficiently protected the bare metal for further oxidation. Quantitative
charge to oxygen- (OER) and charge to hydrogen (HER) conversion was confirmed.
High resolution XPS spectra showed that most likely gamma-NiO(OH) and FeO(OH)
are the catalytic active OER and NiO is the catalytic active HER species.
",physics
"  Sparse subspace clustering (SSC) is one of the current state-of-the-art
methods for partitioning data points into the union of subspaces, with strong
theoretical guarantees. However, it is not practical for large data sets as it
requires solving a LASSO problem for each data point, where the number of
variables in each LASSO problem is the number of data points. To improve the
scalability of SSC, we propose to select a few sets of anchor points using a
randomized hierarchical clustering method, and, for each set of anchor points,
solve the LASSO problems for each data point allowing only anchor points to
have a non-zero weight (this reduces drastically the number of variables). This
generates a multilayer graph where each layer corresponds to a different set of
anchor points. Using the Grassmann manifold of orthogonal matrices, the shared
connectivity among the layers is summarized within a single subspace. Finally,
we use $k$-means clustering within that subspace to cluster the data points,
similarly as done by spectral clustering in SSC. We show on both synthetic and
real-world data sets that the proposed method not only allows SSC to scale to
large-scale data sets, but that it is also much more robust as it performs
significantly better on noisy data and on data with close susbspaces and
outliers, while it is not prone to oversegmentation.
",statistics
"  For analysing text algorithms, for computing superstrings, or for testing
random number generators, one needs to compute all overlaps between any pairs
of words in a given set. The positions of overlaps of a word onto itself, or of
two words, are needed to compute the absence probability of a word in a random
text, or the numbers of common words shared by two random texts. In all these
contexts, one needs to compute or to query overlaps between pairs of words in a
given set. For this sake, we designed COvI, a compressed overlap index that
supports multiple queries on overlaps: like computing the correlation of two
words, or listing pairs of words whose longest overlap is maximal among all
possible pairs. COvI stores overlaps in a hierarchical and non-redundant
manner. We propose an implementation that can handle datasets of millions of
words and still answer queries efficiently. Comparison with a baseline solution
- called FullAC - relying on the Aho-Corasick automaton shows that COvI
provides significant advantages. For similar construction times, COvI requires
half the memory FullAC, and still solves complex queries much faster.
",computer-science
"  We study finite-size fluctuations in a network of spiking deterministic
neurons coupled with non-uniform synaptic coupling. We generalize a previously
developed theory of finite size effects for uniform globally coupled neurons.
In the uniform case, mean field theory is well defined by averaging over the
network as the number of neurons in the network goes to infinity. However, for
nonuniform coupling it is no longer possible to average over the entire network
if we are interested in fluctuations at a particular location within the
network. We show that if the coupling function approaches a continuous function
in the infinite system size limit then an average over a local neighborhood can
be defined such that mean field theory is well defined for a spatially
dependent field. We then derive a perturbation expansion in the inverse system
size around the mean field limit for the covariance of the input to a neuron
(synaptic drive) and firing rate fluctuations due to dynamical deterministic
finite-size effects.
",quantitative-biology
"  Representation learning is a fundamental but challenging problem, especially
when the distribution of data is unknown. We propose a new representation
learning method, termed Structure Transfer Machine (STM), which enables feature
learning process to converge at the representation expectation in a
probabilistic way. We theoretically show that such an expected value of the
representation (mean) is achievable if the manifold structure can be
transferred from the data space to the feature space. The resulting structure
regularization term, named manifold loss, is incorporated into the loss
function of the typical deep learning pipeline. The STM architecture is
constructed to enforce the learned deep representation to satisfy the intrinsic
manifold structure from the data, which results in robust features that suit
various application scenarios, such as digit recognition, image classification
and object tracking. Compared to state-of-the-art CNN architectures, we achieve
the better results on several commonly used benchmarks\footnote{The source code
is available. this https URL }.
",statistics
"  Van der Waals (vdW) heterostructures are receiving great attentions due to
their intriguing properties and potentials in many research fields. The flow of
charge carriers in vdW heterostructures can be efficiently rectified by the
inter-layer coupling between neighboring layers, offering a rich collection of
functionalities and a mechanism for designing atomically thin devices.
Nevertheless, non-uniform contact in larger-area heterostructures reduces the
device efficiency. In this work, ion irradiation had been verified as an
efficient technique to enhance the contact and interlayer coupling in the newly
developed graphene/WSe2 hetero-structure with a large area of 10 mm x 10 mm.
During the ion irradiation process, the morphology of monolayer graphene had
been modified, promoting the contact with WSe2. Experimental evidences of the
tunable interlayer electron transfer are displayed by investigation of
photoluminescence and ultrafast absorption of the irradiated heterostructure.
Besides, we have found that in graphene/WSe2 heterostructure, graphene serves
as a fast channel for the photo-excited carriers to relax in WSe2, and the
nonlinear absorption of WSe2 could be effectively tuned by the carrier transfer
process in graphene, enabling specific optical absorption of the
heterostructure in comparison with separated graphene or WSe2. On the basis of
these new findings, by applying the ion beam modified graphene/WSe2
heterostructure as a saturable absorber, Q-switched pulsed lasing with
optimized performance has been realized in a Nd:YAG waveguide cavity. This work
paves the way towards developing novel devices based on large-area
heterostructures by using ion beam irradiation.
",physics
"  Certain analytical expressions which ""feel"" the divisors of natural numbers
are investigated. We show that these expressions encode to some extent the
well-known algorithm of the sieve of Eratosthenes. Most part of the text is
written in pedagogical style, however some formulas are new.
",mathematics
"  The cancellation theorem for Grothendieck-Witt-correspondences and
Witt-correspondences between smooth varieties over an infinite prefect field
$k$, $char k \neq 2$, is proved, the isomorphism
$$Hom_{\mathbf{DM}^\mathrm{GW}_\mathrm{eff}}(A^\bullet,B^\bullet) \simeq
Hom_{\mathbf{DM}^\mathrm{GW}_\mathrm{eff}}(A^\bullet(1),B^\bullet(1)),$$ for
$A^\bullet,B^\bullet\in \mathbf{DM}^\mathrm{GW}_\mathrm{eff}(k)$ in the
category of effective Grothendieck-Witt-motives constructed in
\cite{AD_DMGWeff} is obtained (and similarly for Witt-motives).
This implies that the canonical functor $\Sigma_{\mathbb G_m^{\wedge
1}}^\infty\colon \mathbf{DM}^\mathrm{GW}_\mathrm{eff}(k)\to
\mathbf{DM}^\mathrm{GW}(k)$ is fully faithful, where
$\mathbf{DM}^\mathrm{GW}(k)$ is the category of non-effective GW-motives
(defined by stabilization of $\mathbf{DM}^\mathrm{GW}_\mathrm{eff}(k)$ along
$\mathbb G_m^{\wedge 1}$) and yields the main property of motives of smooth
varieties in the category $\mathbf{DM}^\mathrm{GW}(k)$: $$
Hom_{\mathbf{DM}^\mathrm{GW}(k)}(M^{GW}(X), \Sigma_{\mathbb G_m^{\wedge
1}}^\infty\mathcal F[i]) \simeq H^i_{Nis}(X,\mathcal F) ,$$ for any smooth
variety $X$ and homotopy invariant sheave with GW-transfers $\mathcal F$ (and
similarly for $\mathbf{DM}^\mathrm{W}(k)$).
",mathematics
"  Large-scale computational experiments, often running over weeks and over
large datasets, are used extensively in fields such as epidemiology,
meteorology, computational biology, and healthcare to understand phenomena, and
design high-stakes policies affecting everyday health and economy. For
instance, the OpenMalaria framework is a computationally-intensive simulation
used by various non-governmental and governmental agencies to understand
malarial disease spread and effectiveness of intervention strategies, and
subsequently design healthcare policies. Given that such shared results form
the basis of inferences drawn, technological solutions designed, and day-to-day
policies drafted, it is essential that the computations are validated and
trusted. In particular, in a multi-agent environment involving several
independent computing agents, a notion of trust in results generated by peers
is critical in facilitating transparency, accountability, and collaboration.
Using a novel combination of distributed validation of atomic computation
blocks and a blockchain-based immutable audits mechanism, this work proposes a
universal framework for distributed trust in computations. In particular we
address the scalaibility problem by reducing the storage and communication
costs using a lossy compression scheme. This framework guarantees not only
verifiability of final results, but also the validity of local computations,
and its cost-benefit tradeoffs are studied using a synthetic example of
training a neural network.
",computer-science
"  Local graph partitioning is a key graph mining tool that allows researchers
to identify small groups of interrelated nodes (e.g. people) and their
connective edges (e.g. interactions). Because local graph partitioning is
primarily focused on the network structure of the graph (vertices and edges),
it often fails to consider the additional information contained in the
attributes. In this paper we propose---(i) a scalable algorithm to improve
local graph partitioning by taking into account both the network structure of
the graph and the attribute data and (ii) an application of the proposed local
graph partitioning algorithm (AttriPart) to predict the evolution of local
communities (LocalForecasting). Experimental results show that our proposed
AttriPart algorithm finds up to 1.6$\times$ denser local partitions, while
running approximately 43$\times$ faster than traditional local partitioning
techniques (PageRank-Nibble). In addition, our LocalForecasting algorithm shows
a significant improvement in the number of nodes and edges correctly predicted
over baseline methods.
",computer-science
"  Recently, along with the emergence of food scandals, food supply chains have
to face with ever-increasing pressure from compliance with food quality and
safety regulations and standards. This paper aims to explore critical factors
of compliance risk in food supply chain with an illustrated case in Vietnamese
seafood industry. To this end, this study takes advantage of both primary and
secondary data sources through a comprehensive literature research of
industrial and scientific papers, combined with expert interview. Findings
showed that there are three main critical factor groups influencing on
compliance risk including challenges originating from Vietnamese food supply
chain itself, characteristics of regulation and standards, and business
environment. Furthermore, author proposed enablers to eliminate compliance
risks to food supply chain managers as well as recommendations to government
and other influencers and supporters.
",quantitative-finance
"  Formal verification techniques are widely used for detecting design flaws in
software systems. Formal verification can be done by transforming an already
implemented source code to a formal model and attempting to prove certain
properties of the model (e.g. that no erroneous state can occur during
execution). Unfortunately, transformations from source code to a formal model
often yield large and complex models, making the verification process
inefficient and costly. In order to reduce the size of the resulting model,
optimization transformations can be used. Such optimizations include common
algorithms known from compiler design and different program slicing techniques.
Our paper describes a framework for transforming C programs to a formal model,
enhanced by various optimizations for size reduction. We evaluate and compare
several optimization algorithms regarding their effect on the size of the model
and the efficiency of the verification. Results show that different
optimizations are more suitable for certain models, justifying the need for a
framework that includes several algorithms.
",computer-science
"  We study synaptically coupled neuronal networks to identify the role of
coupling delays in network's synchronized behaviors. We consider a network of
excitable, relaxation oscillator neurons where two distinct populations, one
excitatory and one inhibitory, are coupled and interact with each other. The
excitatory population is uncoupled, while the inhibitory population is tightly
coupled. A geometric singular perturbation analysis yields existence and
stability conditions for synchronization states under different firing patterns
between the two populations, along with formulas for the periods of such
synchronous solutions. Our results demonstrate that the presence of coupling
delays in the network promotes synchronization. Numerical simulations are
conducted to supplement and validate analytical results. We show the results
carry over to a model for spindle sleep rhythms in thalamocortical networks,
one of the biological systems which motivated our study. The analysis helps to
explain how coupling delays in either excitatory or inhibitory synapses
contribute to producing synchronized rhythms.
",physics
"  An analysis software was developed for the high aspect ratio optical scanning
system in the Detec- tor Laboratory of the University of Helsinki and the
Helsinki Institute of Physics. The system is used e.g. in the quality assurance
of the GEM-TPC detectors being developed for the beam diagnostics system of the
SuperFRS at future FAIR facility. The software was tested by analyzing five
CERN standard GEM foils scanned with the optical scanning system. The
measurement uncertainty of the diameter of the GEM holes and the pitch of the
hole pattern was found to be 0.5 {\mu}m and 0.3 {\mu}m, respectively. The
software design and the performance are discussed. The correlation between the
GEM hole size distribution and the corresponding gain variation was studied by
comparing them against a detailed gain mapping of a foil and a set of six lower
precision control measurements. It can be seen that a qualitative estimation of
the behavior of the local variation in gain across the GEM foil can be made
based on the measured sizes of the outer and inner holes.
",physics
"  A sparse modeling approach is proposed for analyzing scanning tunneling
microscopy topography data, which contains numerous peaks corresponding to
surface atoms. The method, based on the relevance vector machine with
$\mathrm{L}_1$ regularization and $k$-means clustering, enables separation of
the peaks and atomic center positioning with accuracy beyond the resolution of
the measurement grid. The validity and efficiency of the proposed method are
demonstrated using synthetic data in comparison to the conventional
least-square method. An application of the proposed method to experimental data
of a metallic oxide thin film clearly indicates the existence of defects and
corresponding local lattice deformations.
",physics
"  To investigate the role of tachysterol in the photophysical/chemical
regulation of vitamin D photosynthesis, we studied its electronic absorption
properties and excited state dynamics using time-dependent density functional
theory (TDDFT), coupled cluster theory (CC2), and non-adiabatic molecular
dynamics. In excellent agreement with experiments, the simulated electronic
spectrum shows a broad absorption band covering the spectra of the other
vitamin D photoisomers. The broad band stems from the spectral overlap of four
different ground state rotamers. After photoexcitation, the first excited
singlet state (S1) decays within 882 fs. The S1 dynamics is characterized by a
strong twisting of the central double bond. 96% of all trajectories relax
without chemical transformation to the ground state. In 2.3 % of the
trajectories we observed [1,5]-sigmatropic hydrogen shift forming the partly
deconjugated toxisterol D1. 1.4 % previtamin D formation is observed via
hula-twist double bond isomerization. We find a strong dependence between
photoreactivity and dihedral angle conformation: hydrogen shift only occurs in
cEc and cEt rotamers and double bond isomerization occurs mainly in cEc
rotamers. Our study confirms the hypothesis that cEc rotamers are more prone to
previtamin D formation than other isomers. We also observe the formation of a
cyclobutene-toxisterol in the hot ground state (0.7 %). Due to its strong
absorption and unreactive behavior, tachysterol acts mainly as a sun shield
suppressing previtamin D formation. Tachysterol shows stronger toxisterol
formation than previtamin D. Absorption of low energy UV light by the cEc
rotamer can lead to previtamin D formation. Our study reinforces a recent
hypothesis that tachysterol can act as a previtamin D source when only low
energy ultraviolet light is available, as it is the case in winter or in the
morning and evening hours of the day.
",physics
"  A Danish computer, GIER, from 1961 played a vital role in the development of
a new method for astrometric measurement. This method, photon counting
astrometry, ultimately led to two satellites with a significant role in the
modern revolution of astronomy. A GIER was installed at the Hamburg Observatory
in 1964 where it was used to implement the entirely new method for the
measurement of stellar positions by means of a meridian circle, then the
fundamental instrument of astrometry. An expedition to Perth in Western
Australia with the instrument and the computer was a success. This method was
also implemented in space in the first ever astrometric satellite Hipparcos
launched by ESA in 1989. The Hipparcos results published in 1997 revolutionized
astrometry with an impact in all branches of astronomy from the solar system
and stellar structure to cosmic distances and the dynamics of the Milky Way. In
turn, the results paved the way for a successor, the one million times more
powerful Gaia astrometry satellite launched by ESA in 2013. Preparations for a
Gaia successor in twenty years are making progress.
",physics
"  In wind farms, wake interaction leads to losses in power capture and
accelerated structural degradation when compared to freestanding turbines. One
method to reduce wake losses is by misaligning the rotor with the incoming flow
using its yaw actuator, thereby laterally deflecting the wake away from
downstream turbines. However, this demands an accurate and computationally
tractable model of the wind farm dynamics. This problem calls for a closed-loop
solution. This tutorial paper fills the scientific gap by demonstrating the
full closed-loop controller synthesis cycle using a steady-state surrogate
model. Furthermore, a novel, computationally efficient and modular
communication interface is presented that enables researchers to
straight-forwardly test their control algorithms in large-eddy simulations.
High-fidelity simulations of a 9-turbine farm show a power production increase
of up to 11% using the proposed closed-loop controller compared to traditional,
greedy wind farm operation.
",computer-science
"  Impervious surface area is a direct consequence of the urbanization, which
also plays an important role in urban planning and environmental management.
With the rapidly technical development of remote sensing, monitoring urban
impervious surface via high spatial resolution (HSR) images has attracted
unprecedented attention recently. Traditional multi-classes models are
inefficient for impervious surface extraction because it requires labeling all
needed and unneeded classes that occur in the image exhaustively. Therefore, we
need to find a reliable one-class model to classify one specific land cover
type without labeling other classes. In this study, we investigate several
one-class classifiers, such as Presence and Background Learning (PBL), Positive
Unlabeled Learning (PUL), OCSVM, BSVM and MAXENT, to extract urban impervious
surface area using high spatial resolution imagery of GF-1, China's new
generation of high spatial remote sensing satellite, and evaluate the
classification accuracy based on artificial interpretation results. Compared to
traditional multi-classes classifiers (ANN and SVM), the experimental results
indicate that PBL and PUL provide higher classification accuracy, which is
similar to the accuracy provided by ANN model. Meanwhile, PBL and PUL
outperforms OCSVM, BSVM, MAXENT and SVM models. Hence, the one-class
classifiers only need a small set of specific samples to train models without
losing predictive accuracy, which is supposed to gain more attention on urban
impervious surface extraction or other one specific land cover type.
",computer-science
"  Linear regression models contaminated by Gaussian noise (inlier) and possibly
unbounded sparse outliers are common in many signal processing applications.
Sparse recovery inspired robust regression (SRIRR) techniques are shown to
deliver high quality estimation performance in such regression models.
Unfortunately, most SRIRR techniques assume \textit{a priori} knowledge of
noise statistics like inlier noise variance or outlier statistics like number
of outliers. Both inlier and outlier noise statistics are rarely known
\textit{a priori} and this limits the efficient operation of many SRIRR
algorithms. This article proposes a novel noise statistics oblivious algorithm
called residual ratio thresholding GARD (RRT-GARD) for robust regression in the
presence of sparse outliers. RRT-GARD is developed by modifying the recently
proposed noise statistics dependent greedy algorithm for robust de-noising
(GARD). Both finite sample and asymptotic analytical results indicate that
RRT-GARD performs nearly similar to GARD with \textit{a priori} knowledge of
noise statistics. Numerical simulations in real and synthetic data sets also
point to the highly competitive performance of RRT-GARD.
",statistics
"  We consider a closed chain of even number of Majorana zero modes with
nearest-neighbour couplings which are different site by site generically, thus
no any crystal symmetry. Instead, we demonstrate the possibility of an emergent
supersymmetry (SUSY), which is accompanied by gapless Fermionic excitations. In
particular, the condition can be easily satisfied by tuning only one coupling,
regardless of how many other couplings are there. Such a system can be realized
by four Majorana modes on two parallel Majorana nanowires with their ends
connected by Josephson junctions and bodies connected by an external
superconducting ring. By tuning the Josephson couplings with a magnetic flux
$\Phi$ through the ring, we get the gapless excitations at $\Phi_{SUSY}=\pm
f\Phi_0$ with $\Phi_0= hc/2e$, which is signaled by a zero-bias conductance
peak in tunneling conductance. We find this $f$ generally a fractional number
and oscillating with increasing Zeeman fields that parallel to the nanowires,
which provide a unique experimental signature for the existence of Majorana
modes.
",physics
"  Evolutionary game dynamics in structured populations are strongly affected by
updating rules. Previous studies usually focus on imitation-based rules, which
rely on payoff information of social peers. Recent behavioral experiments
suggest that whether individuals use such social information for strategy
updating may be crucial to the outcomes of social interactions. This hints at
the importance of considering updating rules without dependence on social
peers' payoff information, which, however, is rarely investigated. Here, we
study aspiration-based self-evaluation rules, with which individuals
self-assess the performance of strategies by comparing own payoffs with an
imaginary value they aspire, called the aspiration level. We explore the fate
of strategies on population structures represented by graphs or networks. Under
weak selection, we analytically derive the condition for strategy dominance,
which is found to coincide with the classical condition of risk-dominance. This
condition holds for all networks and all distributions of aspiration levels,
and for individualized ways of self-evaluation. Our condition can be
intuitively interpreted: one strategy prevails over the other if the strategy
brings more satisfaction to individuals than the other does. Our work thus
sheds light on the intrinsic difference between evolutionary dynamics induced
by aspiration-based and imitation-based rules.
",quantitative-biology
"  The quantile ratio index introduced by Prendergast and Staudte 2017 is a
simple and effective measure of relative inequality for income data that is
resistant to outliers. It measures the average relative distance of a randomly
chosen income from its symmetric quantile. Another useful property of this
index is investigated here: given a partition of the income distribution into a
union of sets of symmetric quantiles, one can find the conditional inequality
for each set as measured by the quantile ratio index and readily combine them
in a weighted average to obtain the index for the entire population. When
applied to data for various years, one can track how these contributions to
inequality vary over time, as illustrated here for Australian Bureau of
Statistics income and wealth data.
",statistics
"  Categorical equivalences between block algebras of finite groups - such as
Morita and derived equivalences - are well-known to induce character bijections
which commute with the Galois groups of field extensions. This is the
motivation for attempting to realise known Morita and derived equivalences over
non splitting fields. This article presents various result on the theme of
descent. We start with the observation that perfect isometries induced by a
virtual Morita equivalence induce isomorphisms of centers in non-split
situations, and explain connections with Navarro's generalisation of the
Alperin-McKay conjecture. We show that Rouquier's splendid Rickard complex for
blocks with cyclic defect groups descends to the non-split case. We also prove
a descent theorem for Morita equivalences with endopermutation source.
",mathematics
"  In this work we consider an association of meromorphic Jacobi forms of
half-integral index to the pure D-type cases of umbral moonshine, and solve the
module problem for four of these cases by constructing vertex operator
superalgebras that realise the corresponding meromorphic Jacobi forms as graded
traces. We also present a general discussion of meromorphic Jacobi forms with
half-integral index and their relationship to mock modular forms.
",mathematics
"  Granular gases as dilute ensembles of particles in random motion are not only
at the basis of elementary structure-forming processes in the universe and
involved in many industrial and natural phenomena, but also excellent models to
study fundamental statistical dynamics. A vast number of theoretical and
numerical investigations have dealt with this apparently simple non-equilibrium
system. The essential difference to molecular gases is the energy dissipation
in particle collisions, a subtle distinction with immense impact on their
global dynamics. Its most striking manifestation is the so-called granular
cooling, the gradual loss of mechanical energy in absence of external
excitation.
We report an experimental study of homogeneous cooling of three-dimensional
(3D) granular gases in microgravity. Surprisingly, the asymptotic scaling
$E(t)\propto t^{-2}$ obtained by Haff's minimal model [J. Fluid Mech. 134, 401
(1983)] proves to be robust, despite the violation of several of its central
assumptions. The shape anisotropy of the grains influences the characteristic
time of energy loss quantitatively, but not qualitatively. We compare kinetic
energies in the individual degrees of freedom, and find a slight predominance
of the translational motions. In addition, we detect a certain preference of
the grains to align with their long axis in flight direction, a feature known
from active matter or animal flocks, and the onset of clustering.
",physics
"  Hot Jupiters receive strong stellar irradiation, producing equilibrium
temperatures of $1000 - 2500 \ \mathrm{Kelvin}$. Incoming irradiation directly
heats just their thin outer layer, down to pressures of $\sim 0.1 \
\mathrm{bars}$. In standard irradiated evolution models of hot Jupiters,
predicted transit radii are too small. Previous studies have shown that deeper
heating -- at a small fraction of the heating rate from irradiation -- can
explain observed radii. Here we present a suite of evolution models for HD
209458b where we systematically vary both the depth and intensity of internal
heating, without specifying the uncertain heating mechanism(s). Our models
start with a hot, high entropy planet whose radius decreases as the convective
interior cools. The applied heating suppresses this cooling. We find that very
shallow heating -- at pressures of $1 - 10 \ \mathrm{bars}$ -- does not
significantly suppress cooling, unless the total heating rate is $\gtrsim 10\%$
of the incident stellar power. Deeper heating, at $100 \ \mathrm{bars}$,
requires heating at only $1\%$ of the stellar irradiation to explain the
observed transit radius of $1.4 R_{\rm Jup}$ after 5 Gyr of cooling. In
general, more intense and deeper heating results in larger hot Jupiter radii.
Surprisingly, we find that heat deposited at $10^4 \ \mathrm{bars}$ -- which is
exterior to $\approx 99\%$ of the planet's mass -- suppresses planetary cooling
as effectively as heating at the center. In summary, we find that relatively
shallow heating is required to explain the radii of most hot Jupiters, provided
that this heat is applied early and persists throughout their evolution.
",physics
"  We study local optima of the Hamiltonian of the Sherrington-Kirkpatrick
model. We compute the exponent of the expected number of local optima and
determine the ""typical"" value of the Hamiltonian.
",computer-science
"  The use of standard robotic platforms can accelerate research and lower the
entry barrier for new research groups. There exist many affordable humanoid
standard platforms in the lower size ranges of up to 60cm, but larger humanoid
robots quickly become less affordable and more difficult to operate, maintain
and modify. The igus Humanoid Open Platform is a new and affordable, fully
open-source humanoid platform. At 92cm in height, the robot is capable of
interacting in an environment meant for humans, and is equipped with enough
sensors, actuators and computing power to support researchers in many fields.
The structure of the robot is entirely 3D printed, leading to a lightweight and
visually appealing design. The main features of the platform are described in
this article.
",computer-science
"  This paper proposes a new actor-critic-style algorithm called Dual
Actor-Critic or Dual-AC. It is derived in a principled way from the Lagrangian
dual form of the Bellman optimality equation, which can be viewed as a
two-player game between the actor and a critic-like function, which is named as
dual critic. Compared to its actor-critic relatives, Dual-AC has the desired
property that the actor and dual critic are updated cooperatively to optimize
the same objective function, providing a more transparent way for learning the
critic that is directly related to the objective function of the actor. We then
provide a concrete algorithm that can effectively solve the minimax
optimization problem, using techniques of multi-step bootstrapping, path
regularization, and stochastic dual ascent algorithm. We demonstrate that the
proposed algorithm achieves the state-of-the-art performances across several
benchmarks.
",computer-science
"  Malware is constantly adapting in order to avoid detection. Model based
malware detectors, such as SVM and neural networks, are vulnerable to so-called
adversarial examples which are modest changes to detectable malware that allows
the resulting malware to evade detection. Continuous-valued methods that are
robust to adversarial examples of images have been developed using saddle-point
optimization formulations. We are inspired by them to develop similar methods
for the discrete, e.g. binary, domain which characterizes the features of
malware. A specific extra challenge of malware is that the adversarial examples
must be generated in a way that preserves their malicious functionality. We
introduce methods capable of generating functionally preserved adversarial
malware examples in the binary domain. Using the saddle-point formulation, we
incorporate the adversarial examples into the training of models that are
robust to them. We evaluate the effectiveness of the methods and others in the
literature on a set of Portable Execution~(PE) files. Comparison prompts our
introduction of an online measure computed during training to assess general
expectation of robustness.
",statistics
"  We demonstrate how non-convex ""time crystal"" Lagrangians arise in the
effective description of conventional, realizable physical systems. Such
embeddings allow for the resolution of dynamical singularities that arise in
the reduced description. Sisyphus dynamics, featuring intervals of forward
motion interrupted by quick resets, is a generic consequence. Near the would-be
singularity of the time crystal, we find striking microstructure.
",physics
"  Strongly coupled quantum fluids are found in different forms, including
ultracold Fermi gases or tiny droplets of extremely hot Quark-Gluon Plasma.
Although the systems differ in temperature by many orders of magnitude, they
exhibit a similar almost inviscid fluid dynamical behavior. In this work, we
summarize some of the recent theoretical developments toward better
understanding this property in cold Fermi gases at and near unitarity.
",physics
"  Increasing proton beam power on neutrino production targets is one of the
major goals of the Fermilab long term accelerator programs. In this effort, the
Fermilab 8 GeV Booster synchrotron plays a critical role for at least the next
two decades. Therefore, understanding the Booster in great detail is important
as we continue to improve its performance. For example, it is important to know
accurately the available RF power in the Booster by carrying out beam-based
measurements in order to specify the needed upgrades to the Booster RF system.
Since the Booster magnetic field is changing continuously measuring/calibrating
the RF voltage is not a trivial task. Here, we present a beam based method for
the RF voltage measurements. Data analysis is carried out using computer
programs developed in Python and MATLAB. The method presented here is
applicable to any RCS which do not have flat-bottom and flat-top in the
acceleration magnetic ramps. We have also carried out longitudinal beam
tomography at injection and extraction energies with the data used for RF
voltage measurements. Beam based RF voltage measurements and beam tomography
were never done before for the Fermilab Booster. The results from these
investigations will be very useful in future intensity upgrades.
",physics
"  We consider a one-dimensional two component extended Fermi-Hubbard model with
nearest neighbor interactions and mass imbalance between the two species. We
study the stability of trimers, various observables for detecting them, and
expansion dynamics. We generalize the definition of the trimer gap to include
the formation of different types of clusters originating from nearest neighbor
interactions. Expansion dynamics reveal rapidly propagating trimers, with
speeds exceeding doublon propagation in strongly interacting regime. We present
a simple model for understanding this unique feature of the movement of the
trimers, and we discuss the potential for experimental realization.
",physics
"  In 2012, JPMorgan accumulated a USD~6.2 billion loss on a credit derivatives
portfolio, the so-called `London Whale', partly as a consequence of
de-correlations of non-perfectly correlated positions that were supposed to
hedge each other. Motivated by this case, we devise a factor model for
correlations that allows for scenario-based stress testing of correlations. We
derive a number of analytical results related to a portfolio of homogeneous
assets. Using the concept of Mahalanobis distance, we show how to identify
adverse scenarios of correlation risk. In addition, we demonstrate how
correlation and volatility stress tests can be combined. As an example, we
apply the factor-model approach to the ""London Whale"" portfolio and determine
the value-at-risk impact from correlation changes. Since our findings are
particularly relevant for large portfolios, where even small correlation
changes can have a large impact, a further application would be to stress test
portfolios of central counterparties, which are of systemically relevant size.
",quantitative-finance
"  Jittered Sampling is a refinement of the classical Monte Carlo sampling
method. Instead of picking $n$ points randomly from $[0,1]^2$, one partitions
the unit square into $n$ regions of equal measure and then chooses a point
randomly from each partition. Currently, no good rules for how to partition the
space are available. In this paper, we present a solution for the special case
of subdividing the unit square by a decreasing function into two regions so as
to minimize the expected squared $\mathcal{L}_2-$discrepancy. The optimal
partitions are given by a \textit{highly} nonlinear integral equation for which
we determine an approximate solution. In particular, there is a break of
symmetry and the optimal partition is not into two sets of equal measure. We
hope this stimulates further interest in the construction of good partitions.
",mathematics
"  We study multi-frequency quasiperiodic Schrödinger operators on
$\mathbb{Z} $. We prove that for a large real analytic potential satisfying
certain restrictions the spectrum consists of a single interval. The result is
a consequence of a criterion for the spectrum to contain an interval at a given
location that we establish non-perturbatively in the regime of positive
Lyapunov exponent.
",mathematics
"  Since its emergence two decades ago, astrophotonics has found broad
application in scientific instruments at many institutions worldwide. The case
for astrophotonics becomes more compelling as telescopes push for AO-assisted,
diffraction-limited performance, a mode of observing that is central to the
next-generation of extremely large telescopes (ELTs). Even AO systems are
beginning to incorporate advanced photonic principles as the community pushes
for higher performance and more complex guide-star configurations. Photonic
instruments like Gravity on the Very Large Telescope achieve milliarcsec
resolution at 2000 nm which would be very difficult to achieve with
conventional optics. While space photonics is not reviewed here, we foresee
that remote sensing platforms will become a major beneficiary of astrophotonic
components in the years ahead. The field has given back with the development of
new technologies (e.g. photonic lantern, large area multi-core fibres) already
finding widespread use in other fields; Google Scholar lists more than 400
research papers making reference to this technology. This short review covers
representative key developments since the 2009 Focus issue on Astrophotonics.
",physics
"  With the advent of modern communications systems, much attention has been put
on developing methods for securely transferring information between
constituents of wireless sensor networks. To this effect, we introduce a
mathematical programming formulation for the key management problem, which
broadly serves as a mechanism for encrypting communications. In particular, an
integer programming model of the q-Composite scheme is proposed and utilized to
distribute keys among nodes of a network whose topology is known. Numerical
experiments demonstrating the effectiveness of the proposed model are conducted
using using a well-known optimization solver package. An illustrative example
depicting an optimal encryption for a small-scale network is also presented.
",computer-science
"  In this paper we propose, design and test a new dual-nuclei RF-coil inspired
by wire metamaterial structures. The coil operates due to resonant excitation
of hybridized eigenmodes in multimode flat periodic structures comprising
several coupled thin metal strips. It was shown that the field distribution of
the coil (i.e. penetration depth) can be controlled independently at two
different Larmor frequencies by selecting a proper eigenmode in each of two
mutually orthogonal periodic structures. The proposed coil requires no lumped
capacitors for tuning and matching. In order to demonstrate the performance of
the new design, an experimental preclinical coil for $^{19}$F/$^{1}$H imaging
of small animals at 7.05T was engineered and tested on a homogeneous liquid
phantom and in-vivo. The presented results demonstrate that the coil was well
tuned and matched simultaneously at two Larmor frequencies and capable of image
acquisition with both the nuclei reaching large homogeneity area along with a
sufficient signal-to-noise ratio. In an in-vivo experiment it has been shown
that without retuning the setup it was possible to obtain anatomical $^{1}$H
images of a mouse under anesthesia consecutively with $^{19}$F images of a tiny
tube filled with a fluorine-containing liquid and attached to the body of the
mouse.
",physics
"  Amyloid precursor with 770 amino acids dimerizes and aggregates, as do its c
terminal 99 amino acids and amyloid 40,42 amino acids fragments. The titled
question has been discussed extensively, and here it is addressed further using
thermodynamic scaling theory to analyze mutational trends in structural factors
and kinetics. Special attention is given to Family Alzheimer's Disease
mutations outside amyloid 42. The scaling analysis is connected to extensive
docking simulations which included membranes, thereby confirming their results
and extending them to Amyloid precursor.
",quantitative-biology
"  Associated varieties of vertex algebras are analogue of the associated
varieties of primitive ideals of the universal enveloping algebras of
semisimple Lie algebras. They not only capture some of the important properties
of vertex algebras but also have interesting relationship with the Higgs
branches of four-dimensional $N=2$ superconformal field theories (SCFTs). As a
consequence, one can deduce the modular invariance of Schur indices of 4d $N=2$
SCFTs from the theory of vertex algebras.
",mathematics
"  Forecasts of mortality provide vital information about future populations,
with implications for pension and health-care policy as well as for decisions
made by private companies about life insurance and annuity pricing. Stochastic
mortality forecasts allow the uncertainty in mortality predictions to be taken
into consideration when making policy decisions and setting product prices.
Longer lifespans imply that forecasts of mortality at ages 90 and above will
become more important in such calculations.
This paper presents a Bayesian approach to the forecasting of mortality that
jointly estimates a Generalised Additive Model (GAM) for mortality for the
majority of the age-range and a parametric model for older ages where the data
are sparser. The GAM allows smooth components to be estimated for age, cohort
and age-specific improvement rates, together with a non-smoothed period effect.
Forecasts for the United Kingdom are produced using data from the Human
Mortality Database spanning the period 1961-2013. A metric that approximates
predictive accuracy under Leave-One-Out cross-validation is used to estimate
weights for the `stacking' of forecasts with different points of transition
between the GAM and parametric elements.
Mortality for males and females are estimated separately at first, but a
joint model allows the asymptotic limit of mortality at old ages to be shared
between sexes, and furthermore provides for forecasts accounting for
correlations in period innovations. The joint and single sex model forecasts
estimated using data from 1961-2003 are compared against observed data from
2004-2013 to facilitate model assessment.
",statistics
"  We theoretically investigate the dispersion and polarization properties of
the electromagnetic waves in a multi-layered structure composed of a
magneto-optic waveguide on dielectric substrate covered by one-dimensional
dielectric photonic crystal. The numerical analysis of such a complex structure
shows polarization filtration of TE- and TM-modes depending on geometrical
parameters of the waveguide and photonic crystal. We consider different regimes
of the modes propagation inside such a structure: when guiding modes propagate
inside the magnetic film and decay in the photonic crystal; when they propagate
in both magnetic film and photonic crystal.
",physics
"  The geometric approach to optimal transport and information theory has
triggered the interpretation of probability densities as an
infinite-dimensional Riemannian manifold. The most studied Riemannian
structures are Otto's metric, yielding the $L^2$-Wasserstein distance of
optimal mass transport, and the Fisher--Rao metric, predominant in the theory
of information geometry. On the space of smooth probability densities, none of
these Riemannian metrics are geodesically complete---a property desirable for
example in imaging applications. That is, the existence interval for solutions
to the geodesic flow equations cannot be extended to the whole real line. Here
we study a class of Hamilton--Jacobi-like partial differential equations
arising as geodesic flow equations for higher-order Sobolev type metrics on the
space of smooth probability densities. We give order conditions for global
existence and uniqueness, thereby providing geodesic completeness. The system
we study is an interesting example of a flow equation with loss of derivatives,
which is well-posed in the smooth category, yet non-parabolic and fully
non-linear. On a more general note, the paper establishes a link between
geometric analysis on the space of probability densities and analysis of
Euler-Arnold equations in topological hydrodynamics.
",mathematics
"  The aim of this paper is to investigate the stability of Prandtl boundary
layers in the vanishing viscosity limit: $\nu \to 0$. In \cite{Grenier}, one of
the authors proved that there exists no asymptotic expansion involving one
Prandtl's boundary layer with thickness of order $\sqrt\nu$, which describes
the inviscid limit of Navier-Stokes equations. The instability gives rise to a
viscous boundary sublayer whose thickness is of order $\nu^{3/4}$. In this
paper, we point out how the stability of the classical Prandtl's layer is
linked to the stability of this sublayer. In particular, we prove that the two
layers cannot both be nonlinearly stable in $L^\infty$. That is, either the
Prandtl's layer or the boundary sublayer is nonlinearly unstable in the sup
norm.
",mathematics
"  Central limit theorems play an important role in the study of statistical
inference for stochastic processes. However, when the nonparametric local
polynomial threshold estimator, especially local linear case, is employed to
estimate the diffusion coefficients of diffusion processes, the adaptive and
predictable structure of the estimator conditionally on the $\sigma-$field
generated by diffusion processes is destroyed, the classical central limit
theorem for martingale difference sequences can not work. In this paper, we
proved the central limit theorems of local polynomial threshold estimators for
the volatility function in diffusion processes with jumps. We believe that our
proof for local polynomial threshold estimators provides a new method in this
fields, especially local linear case.
",mathematics
"  Large batch size training of Neural Networks has been shown to incur accuracy
loss when trained with the current methods. The exact underlying reasons for
this are still not completely understood. Here, we study large batch size
training through the lens of the Hessian operator and robust optimization. In
particular, we perform a Hessian based study to analyze exactly how the
landscape of the loss function changes when training with large batch size. We
compute the true Hessian spectrum, without approximation, by back-propagating
the second derivative. Extensive experiments on multiple networks show that
saddle-points are not the cause for generalization gap of large batch size
training, and the results consistently show that large batch converges to
points with noticeably higher Hessian spectrum. Furthermore, we show that
robust training allows one to favor flat areas, as points with large Hessian
spectrum show poor robustness to adversarial perturbation. We further study
this relationship, and provide empirical and theoretical proof that the inner
loop for robust training is a saddle-free optimization problem \textit{almost
everywhere}. We present detailed experiments with five different network
architectures, including a residual network, tested on MNIST, CIFAR-10, and
CIFAR-100 datasets. We have open sourced our method which can be accessed at
[1].
",statistics
"  Delay Tolerant Networking (DTN) is an approach to networking which handles
network disruptions and high delays that may occur in many kinds of
communication networks. The major reasons for high delay include partial
connectivity of networks as can be seen in many types of ad hoc wireless
networks with frequent network partitions, long propagation time as experienced
in inter-planetary and deep space networks, and frequent link disruptions due
to the mobility of nodes as observed in terrestrial wireless network
environments. Experimenting network architectures, protocols, and mobility
models in such real-world scenarios is difficult due to the complexities
involved in the network environment. Therefore, in this document, we present
the documentation of an Urban Delay Tolerant Network Simulator (UDTNSim)
version 0.1, capable of simulating urban road network environments with DTN
characteristics including mobility models and routing protocols. The mobility
models included in this version of UDTNSim are (i) Stationary Movement, (ii)
Simple Random Movement, (iii) Path Type Based Movememt, (iv) Path Memory Based
Movement, (v) Path Type with Restricted Movement, and (vi) Path Type with Wait
Movement. In addition to mobility models, we also provide three routing and
data hand-off protocols: (i) Epidemic Routing, (ii) Superior Only Handoff, and
(iii) Superior Peer Handoff. UDTNSim v0.1 is designed using object-oriented
programming approach in order to provide flexibility in addition of new
features to the DTN environment. UDTNSim v0.1 is distributed as an open source
simulator for the use of the research community.
",computer-science
"  Substitution of isovalent non-magnetic defects, such as Zn, in CuO2 plane
strongly modifies the magnetic properties of strongly electron correlated hole
doped cuprate superconductors. The reason for enhanced uniform magnetic
susceptibility, \c{hi}, in Zn substituted cuprates is debatable. So far, the
observed magnetic behavior has been analyzed mainly in terms of two somewhat
contrasting scenarios, (a) that due to independent localized moments appearing
in the vicinity of Zn arising because of the strong electronic/magnetic
correlations present in the host compound and (b) that due to transfer of
quasiparticle spectral weight and creation of weakly localized low energy
electronic states associated with each Zn atom in place of an in-plane Cu. If
the second scenario is correct, one should expect a direct correspondence
between Zn induced suppression of superconducting transition temperature, Tc,
and the extent of the enhanced magnetic susceptibility at low temperature. In
this case, the low-T enhancement of \c{hi} would be due to weakly localized
quasiparticle states at low energy and these electronic states will be
precluded from taking part in Cooper pairing. We explore this second
possibility by analyzing the \c{hi}(T) data for La2-xSrxCu1-yZnyO4 with
different hole contents, p (= x), and Zn concentrations (y) in this paper.
Results of our analysis support this scenario.
",physics
"  Most past work on social network link fraud detection tries to separate
genuine users from fraudsters, implicitly assuming that there is only one type
of fraudulent behavior. But is this assumption true? And, in either case, what
are the characteristics of such fraudulent behaviors? In this work, we set up
honeypots (""dummy"" social network accounts), and buy fake followers (after
careful IRB approval). We report the signs of such behaviors including oddities
in local network connectivity, account attributes, and similarities and
differences across fraud providers. Most valuably, we discover and characterize
several types of fraud behaviors. We discuss how to leverage our insights in
practice by engineering strongly performing entropy-based features and
demonstrating high classification accuracy. Our contributions are (a)
instrumentation: we detail our experimental setup and carefully engineered data
collection process to scrape Twitter data while respecting API rate-limits, (b)
observations on fraud multimodality: we analyze our honeypot fraudster
ecosystem and give surprising insights into the multifaceted behaviors of these
fraudster types, and (c) features: we propose novel features that give strong
(>0.95 precision/recall) discriminative power on ground-truth Twitter data.
",computer-science
"  We offer two new Mellin transform evaluations for the Riemann zeta function
in the region $0<\Re(s)<1.$ Some discussion is offered in the way of evaluating
some further Fourier integrals involving the Riemann xi function.
",mathematics
"  Eigenstates of fully many-body localized (FMBL) systems are described by
quasilocal operators $\tau_i^z$ (l-bits), which are conserved exactly under
Hamiltonian time evolution. The algebra of the operators $\tau_i^z$ and
$\tau_i^x$ associated with l-bits ($\boldsymbol{\tau}_i$) completely defines
the eigenstates and the matrix elements of local operators between eigenstates
at all energies. We develop a non-perturbative construction of the full set of
l-bit algebras in the many-body localized phase for the canonical model of MBL.
Our algorithm to construct the Pauli-algebra of l-bits combines exact
diagonalization and a tensor network algorithm developed for efficient
diagonalization of large FMBL Hamiltonians. The distribution of localization
lengths of the l-bits is evaluated in the MBL phase and used to characterize
the MBL-to-thermal transition.
",physics
"  This paper presents a robotic pick-and-place system that is capable of
grasping and recognizing both known and novel objects in cluttered
environments. The key new feature of the system is that it handles a wide range
of object categories without needing any task-specific training data for novel
objects. To achieve this, it first uses a category-agnostic affordance
prediction algorithm to select and execute among four different grasping
primitive behaviors. It then recognizes picked objects with a cross-domain
image classification framework that matches observed images to product images.
Since product images are readily available for a wide range of objects (e.g.,
from the web), the system works out-of-the-box for novel objects without
requiring any additional training data. Exhaustive experimental results
demonstrate that our multi-affordance grasping achieves high success rates for
a wide variety of objects in clutter, and our recognition algorithm achieves
high accuracy for both known and novel grasped objects. The approach was part
of the MIT-Princeton Team system that took 1st place in the stowing task at the
2017 Amazon Robotics Challenge. All code, datasets, and pre-trained models are
available online at this http URL
",computer-science
"  By using, among other things, the Fourier analysis techniques on hyperbolic
and symmetric spaces, we establish the Hardy-Sobolev-Maz'ya inequalities for
higher order derivatives on half spaces. The proof relies on a
Hardy-Littlewood-Sobolev inequality on hyperbolic spaces which is of its
independent interest. We also give an alternative proof of Benguria, Frank and
Loss' work concerning the sharp constant in the Hardy-Sobolev-Maz'ya inequality
in the three dimensional upper half space. Finally, we show the sharp constant
in the Hardy-Sobolev-Maz'ya inequality for bi-Laplacian in the upper half space
of dimension five coincides with the Sobolev constant.
",mathematics
"  We start with an overview of a class of submodular functions called SCMMs
(sums of concave composed with non-negative modular functions plus a final
arbitrary modular). We then define a new class of submodular functions we call
{\em deep submodular functions} or DSFs. We show that DSFs are a flexible
parametric family of submodular functions that share many of the properties and
advantages of deep neural networks (DNNs). DSFs can be motivated by considering
a hierarchy of descriptive concepts over ground elements and where one wishes
to allow submodular interaction throughout this hierarchy. Results in this
paper show that DSFs constitute a strictly larger class of submodular functions
than SCMMs. We show that, for any integer $k>0$, there are $k$-layer DSFs that
cannot be represented by a $k'$-layer DSF for any $k'<k$. This implies that,
like DNNs, there is a utility to depth, but unlike DNNs, the family of DSFs
strictly increase with depth. Despite this, we show (using a ""backpropagation""
like method) that DSFs, even with arbitrarily large $k$, do not comprise all
submodular functions. In offering the above results, we also define the notion
of an antitone superdifferential of a concave function and show how this
relates to submodular functions (in general), DSFs (in particular), negative
second-order partial derivatives, continuous submodularity, and concave
extensions. To further motivate our analysis, we provide various special case
results from matroid theory, comparing DSFs with forms of matroid rank, in
particular the laminar matroid. Lastly, we discuss strategies to learn DSFs,
and define the classes of deep supermodular functions, deep difference of
submodular functions, and deep multivariate submodular functions, and discuss
where these can be useful in applications.
",computer-science
"  Lineage tracing, the joint segmentation and tracking of living cells as they
move and divide in a sequence of light microscopy images, is a challenging
task. Jug et al. have proposed a mathematical abstraction of this task, the
moral lineage tracing problem (MLTP), whose feasible solutions define both a
segmentation of every image and a lineage forest of cells. Their branch-and-cut
algorithm, however, is prone to many cuts and slow convergence for large
instances. To address this problem, we make three contributions: (i) we devise
the first efficient primal feasible local search algorithms for the MLTP, (ii)
we improve the branch-and-cut algorithm by separating tighter cutting planes
and by incorporating our primal algorithms, (iii) we show in experiments that
our algorithms find accurate solutions on the problem instances of Jug et al.
and scale to larger instances, leveraging moral lineage tracing to practical
significance.
",computer-science
"  Let K be a field and denote by K[t], the polynomial ring with coefficients in
K. Set A = K[f1,. .. , fs], with f1,. .. , fs $\in$ K[t]. We give a procedure
to calculate the monoid of degrees of the K algebra M = F1A + $\times$ $\times$
$\times$ + FrA with F1,. .. , Fr $\in$ K[t]. We show some applications to the
problem of the classification of plane polynomial curves (that is, plane
algebraic curves parametrized by polynomials) with respect to some oh their
invariants, using the module of K{ä}hler differentials.
",mathematics
"  The combination of photometry, spectroscopy and spectropolarimetry of the
chemically peculiar stars often aims to study the complex physical phenomena
such as stellar pulsation, chemical inhomogeneity, magnetic field and their
interplay with stellar atmosphere and circumstellar environment. The prime
objective of the present study is to determine the atmospheric parameters of a
set of Am stars to understand their evolutionary status. Atmospheric abundances
and basic parameters are determined using full spectrum fitting technique by
comparing the high-resolution spectra to the synthetic spectra. To know the
evolutionary status we derive the effective temperature and luminosity from
different methods and compare them with the literature. The location of these
stars in the H-R diagram demonstrate that all the sample stars are evolved from
the Zero-Age-Main-Sequence towards Terminal-Age-Main-Sequence and occupy the
region of $\delta$ Sct instability strip. The abundance analysis shows that the
light elements e.g. Ca and Sc are underabundant while iron peak elements such
as Ba, Ce etc. are overabundant and these chemical properties are typical for
Am stars. The results obtained from the spectropolarimetric analysis shows that
the longitudinal magnetic fields in all the studied stars are negligible that
gives further support their Am class of peculiarity.
",physics
"  We study upper bounds on Weierstrass primary factors and discuss their
application in spectral theory. One of the main aims of this note is to draw
attention to works of Blumenthal and Denjoy from 1910, but we also provide some
new results and some numerical computations of our own.
",mathematics
"  We present a memristive device based R$ ^3 $PUF construction achieving highly
desired PUF properties, which are not offered by most current PUF designs: (1)
High reliability, almost 100\% that is crucial for PUF-based cryptographic key
generations, significantly reducing, or even eliminating the expensive overhead
of on-chip error correction logic and the associated helper on-chip data
storage or off-chip storage and transfer. (2) Reconfigurability, while current
PUF designs rarely exhibit such an attractive property. We validate our R$ ^3
$PUF via extensive Monte-Carlo simulations in Cadence based on parameters of
real devices. The R$ ^3 $PUF is simple, cost-effective and easy to manage
compared to other PUF constructions exhibiting high reliability or
reconfigurability. None of previous PUF constructions is able to provide both
desired high reliability and reconfigurability concurrently.
",computer-science
"  Bayesian inverse modeling is important for a better understanding of
hydrological processes. However, this approach can be computationally demanding
as it usually requires a large number of model evaluations. To address this
issue, one can take advantage of surrogate modeling techniques. Nevertheless,
when approximation error of the surrogate model is neglected in inverse
modeling, the inversion result will be biased. In this paper, we develop a
surrogate-based Bayesian inversion framework that explicitly quantifies and
gradually reduces the approximation error of the surrogate. Specifically, two
strategies are proposed and compared. The first strategy works by obtaining an
ensemble of sparse polynomial chaos expansion (PCE) surrogates with Markov
chain Monte Carlo sampling, while the second one uses Gaussian process (GP) to
simulate the approximation error of a single sparse PCE surrogate. The two
strategies can also be applied with other surrogates, thus they have general
applicability. By adaptively refining the surrogate over the posterior
distribution, we can gradually reduce the surrogate approximation error to a
small level. Demonstrated with three case studies involving
high-dimensionality, multi-modality and a real-world application, respectively,
it is found that both strategies can reduce the bias introduced by surrogate
modeling, while the second strategy has a better performance as it integrates
two methods (i.e., sparse PCE and GP) that complement each other.
",statistics
"  The well-known theorem of Eilenberg and Ganea expresses the Lusternik -
Schnirelmann category of an aspherical space as the cohomological dimension of
its fundamental group. In this paper we study a similar problem of determining
algebraically the topological complexity of the Eilenberg-MacLane spaces. One
of our main results states that in the case when the fundamental group is
hyperbolic in the sense of Gromov the topological complexity of an aspherical
space $K(\pi, 1)$ either equals or is by one larger than the cohomological
dimension of $\pi\times \pi$. We approach the problem by studying essential
cohomology classes, i.e. classes which can be obtained from the powers of the
canonical class via coefficient homomorphisms. We describe a spectral sequence
which allows to specify a full set of obstructions for a cohomology class to be
essential. In the case of a hyperbolic group we establish a vanishing property
of this spectral sequence which leads to the main result.
",mathematics
"  A recent stacking analysis of Planck HFI data of galaxy clusters (Hurier
2016) allowed to derive the cluster temperatures by using the relativistic
corrections to the Sunyaev-Zel'dovich effect (SZE). However, the temperatures
of high-temperature clusters, as derived from this analysis, resulted to be
basically higher than the temperatures derived from X-ray measurements, at a
moderate statistical significance of $1.5\sigma$. This discrepancy has been
attributed by Hurier (2016) to calibration issues. In this paper we discuss an
alternative explanation for this discrepancy in terms of a non-thermal SZE
astrophysical component. We find that this explanation can work if non-thermal
electrons in galaxy clusters have a low value of their minimum momentum
($p_1\sim0.5-1$), and if their pressure is of the order of $20-30\%$ of the
thermal gas pressure. Both these conditions are hard to obtain if the
non-thermal electrons are mixed with the hot gas in the intra cluster medium,
but can be possibly obtained if the non-thermal electrons are mainly confined
in bubbles with high content of non-thermal plasma and low content of thermal
plasma, or in giant radio lobes/relics located in the outskirts of clusters. In
order to derive more precise results on the properties of non-thermal electrons
in clusters, and in view of more solid detections of a discrepancy between
X-rays and SZE derived clusters temperatures that cannot be explained in other
ways, it would be necessary to reproduce the full analysis done by Hurier
(2016) by adding systematically the non-thermal component of the SZE.
",physics
"  An irreducible weight module of an affine Kac-Moody algebra $\mathfrak{g}$ is
called dense if its support is equal to a coset in $\mathfrak{h}^{*}/Q$.
Following a conjecture of V. Futorny about affine Kac-Moody algebras
$\mathfrak{g}$, an irreducible weight $\mathfrak{g}$-module is dense if and
only if it is cuspidal (i.e. not a quotient of an induced module). The
conjecture is confirmed for $\mathfrak{g}=A_{2}^{\left(1\right)}$,
$A_{3}^{\left(1\right)}$ and$A_{4}^{\left(1\right)}$ and a classification of
the supports of the irreducible weight $\mathfrak{g}$-modules obtained. For all
$A_{n}^{\left(1\right)}$ the problem is reduced to finding primitive elements
for only finitely many cases, all lying below a certain bound. For the
left-over finitely many cases an algorithm is proposed, which leads to the
solution of Futorny's conjecture for the cases $A_{2}^{\left(1\right)}$ and
$A_{3}^{\left(1\right)}$. Yet, the solution of the case
$A_{4}^{\left(1\right)}$ required additional combinatorics.
For the proofs, a new category of hypoabelian Lie subalgebras,
pre-prosolvable subalgebras, and a subclass thereof, quasicone subalgebras, is
introduced and its tropical matrix algebra structure outlined.
",mathematics
"  Surface-functionalized nanomaterials can act as theranostic agents that
detect disease and track biological processes using hyperpolarized magnetic
resonance imaging (MRI). Candidate materials are sparse however, requiring
spinful nuclei with long spin-lattice relaxation (T1) and spin-dephasing times
(T2), together with a reservoir of electrons to impart hyperpolarization. Here,
we demonstrate the versatility of the nanodiamond material system for
hyperpolarized 13C MRI, making use of its intrinsic paramagnetic defect
centers, hours-long nuclear T1 times, and T2 times suitable for spatially
resolving millimeter-scale structures. Combining these properties, we enable a
new imaging modality that exploits the phase-contrast between spins encoded
with a hyperpolarization that is aligned, or anti-aligned with the external
magnetic field. The use of phase-encoded hyperpolarization allows nanodiamonds
to be tagged and distinguished in an MRI based on their spin-orientation alone,
and could permit the action of specific bio-functionalized complexes to be
directly compared and imaged.
",physics
"  In this paper we present and characterize a nearest-neighbors color-matching
photometric redshift estimator that features a direct relationship between the
precision and accuracy of the input magnitudes and the output photometric
redshifts. This aspect makes our estimator an ideal tool for evaluating the
impact of changes to LSST survey parameters that affect the measurement errors
of the photometry, which is the main motivation of our work (i.e., it is not
intended to provide the ""best"" photometric redshifts for LSST data). We show
how the photometric redshifts will improve with time over the 10-year LSST
survey and confirm that the nominal distribution of visits per filter provides
the most accurate photo-$z$ results. The LSST survey strategy naturally
produces observations over a range of airmass, which offers the opportunity of
using an SED- and $z$-dependent atmospheric affect on the observed photometry
as a color-independent redshift indicator. We show that measuring this airmass
effect and including it as a prior has the potential to improve the photometric
redshifts and can ameliorate extreme outliers, but that it will only be
adequately measured for the brightest galaxies, which limits its overall impact
on LSST photometric redshifts. We furthermore demonstrate how this airmass
effect can induce a bias in the photo-$z$ results, and caution against survey
strategies that prioritize high-airmass observations for the purpose of
improving this prior. Ultimately, we intend for this work to serve as a guide
for the expectations and preparations of the LSST science community with
regards to the minimum quality of photo-$z$ as the survey progresses.
",physics
"  Recently, the separated fragment (SF) of first-order logic has been
introduced. Its defining principle is that universally and existentially
quantified variables may not occur together in atoms. SF properly generalizes
both the Bernays-Schönfinkel-Ramsey (BSR) fragment and the relational monadic
fragment. In this paper the restrictions on variable occurrences in SF
sentences are relaxed such that universally and existentially quantified
variables may occur together in the same atom under certain conditions. Still,
satisfiability can be decided. This result is established in two ways: firstly,
by an effective equivalence-preserving translation into the BSR fragment, and,
secondly, by a model-theoretic argument.
Slight modifications to the described concepts facilitate the definition of
other decidable classes of first-order sentences. The paper presents a second
fragment which is novel, has a decidable satisfiability problem, and properly
contains the Ackermann fragment and---once more---the relational monadic
fragment. The definition is again characterized by restrictions on the
occurrences of variables in atoms. More precisely, after certain
transformations, Skolemization yields only unary functions and constants, and
every atom contains at most one universally quantified variable. An effective
satisfiability-preserving translation into the monadic fragment is devised and
employed to prove decidability of the associated satisfiability problem.
",computer-science
"  We discuss the design and optimisation of two types of junctions between
surface-electrode radiofrequency ion-trap arrays that enable the integration of
experiments with sympathetically cooled molecular ions on a monolithic chip
device. A detailed description of a multi-objective optimisation procedure
applicable to an arbitrary planar junction is presented, and the results for a
cross junction between four quadrupoles as well as a quadrupole-to-octupole
junction are discussed. Based on these optimised functional elements, we
propose a multi-functional ion-trap chip for experiments with translationally
cold molecular ions at temperatures in the millikelvin range. This study opens
the door to extending complex chip-based trapping techniques to
Coulomb-crystallised molecular ions with potential applications in mass
spectrometry, spectroscopy, controlled chemistry and quantum technology.
",physics
"  Expertise of annotators has a major role in crowdsourcing based opinion
aggregation models. In such frameworks, accuracy and biasness of annotators are
occasionally taken as important features and based on them priority of the
annotators are assigned. But instead of relying on a single feature, multiple
features can be considered and separate rankings can be produced to judge the
annotators properly. Finally, the aggregation of those rankings with perfect
weightage can be done with an aim to produce better ground truth prediction.
Here, we propose a novel weighted rank aggregation method and its efficacy with
respect to other existing approaches is shown on artificial dataset. The
effectiveness of weighted rank aggregation to enhance quality prediction is
also shown by applying it on an Amazon Mechanical Turk (AMT) dataset.
",computer-science
"  We investigate the languages recognized by well-structured transition systems
(WSTS) with upward and downward compatibility. Our first result shows that,
under very mild assumptions, every two disjoint WSTS languages are regular
separable: There is a regular language containing one of them and being
disjoint from the other. As a consequence, if a language as well as its
complement are both recognized by WSTS, then they are necessarily regular. In
particular, no subclass of WSTS languages beyond the regular languages is
closed under complement. Our second result shows that for Petri nets, the
complexity of the backwards coverability algorithm yields a bound on the size
of the regular separator. We complement it by a lower bound construction.
",computer-science
"  We investigate the intrinsic Baldwin effect (Beff) of the broad H$\alpha$ and
H$\beta$ emission lines for six Type 1 active galactic nuclei (AGNs) with
different broad line characteristics: two Seyfert 1 (NGC 4151 and NGC 5548),
two AGNs with double-peaked broad line profiles (3C 390.3 and Arp 102B), one
narrow line Seyfert 1 (Ark 564), and one high-luminosity quasar with highly red
asymmetric broad line profiles (E1821+643). We found that a significant
intrinsic Beff was present in all Type 1 AGNs in our sample. Moreover, we do
not see strong difference in intrinsic Beff slopes in different types of AGNs
which probably have different physical properties, such as inclination, broad
line region geometry, or accretion rate. Additionally, we found that the
intrinsic Beff was not connected with the global one, which, instead, could not
be detected in the broad H$\alpha$ or H$\beta$ emission lines. In the case of
NGC 4151, the detected variation of the Beff slope could be due to the change
in the site of line formation in the BLR. Finally, the intrinsic Beff might be
caused by the additional optical continuum component that is not part of the
ionization continuum.
",physics
"  A path (resp. cycle) decomposition of a graph $G$ is a set of edge-disjoint
paths (resp. cycles) of $G$ that covers the edge set of $G$. Gallai (1966)
conjectured that every graph on $n$ vertices admits a path decomposition of
size at most $\lfloor (n+1)/2\rfloor$, and Hajós (1968) conjectured that
every Eulerian graph on $n$ vertices admits a cycle decomposition of size at
most $\lfloor (n-1)/2\rfloor$. Gallai's Conjecture was verified for many
classes of graphs. In particular, Lovász (1968) verified this conjecture for
graphs with at most one vertex of even degree, and Pyber (1996) verified it for
graphs in which every cycle contains a vertex of odd degree. Hajós'
Conjecture, on the other hand, was verified only for graphs with maximum degree
$4$ and for planar graphs. In this paper, we verify Gallai's and Hajós'
Conjectures for graphs with treewidth at most $3$. Moreover, we show that the
only graphs with treewidth at most $3$ that do not admit a path decomposition
of size at most $\lfloor n/2\rfloor$ are isomorphic to $K_3$ or $K_5-e$.
Finally, we use the technique developed in this paper to present new proofs for
Gallai's and Hajós' Conjectures for graphs with maximum degree at most $4$,
and for planar graphs with girth at least $6$.
",computer-science
"  We show that the knowledge of Dirichlet to Neumann map for rough $A$ and $q$
in $(-\Delta)^m +A\cdot D +q$ for $m \geq 2$ for a bounded domain in
$\mathbb{R}^n$, $n \geq 3$ determines $A$ and $q$ uniquely. The unique
identifiability is proved using property of products of functions in Sobolev
spaces and constructing complex geometrical optics solutions with sufficient
decay of remainder terms.
",mathematics
"  Current induced magnetization manipulation is a key issue for spintronic
application. Therefore, deterministic switching of the magnetization at the
picoseconds timescale with a single electronic pulse represents a major step
towards the future developments of ultrafast spintronic. Here, we have studied
the ultrafast magnetization dynamics in engineered Gdx[FeCo]1-x based structure
to compare the effect of femtosecond laser and hot-electrons pulses. We
demonstrate that a single femtosecond hot-electrons pulse allows a
deterministic magnetization reversal in either Gd-rich and FeCo-rich alloys
similarly to a femtosecond laser pulse. In addition, we show that the limiting
factor of such manipulation for perpendicular magnetized films arises from the
multi-domain formation due to dipolar interaction. By performing time resolved
measurements under various field, we demonstrate that the same magnetization
dynamics is observed for both light and hot-electrons excitation and that the
full magnetization reversal take place within 5 ps. The energy efficiency of
the ultra-fast current induced magnetization manipulation is optimized thanks
to the ballistic transport of hot-electrons before reaching the GdFeCo magnetic
layer.
",physics
"  Bizarrely shaped voting districts are frequently lambasted as likely
instances of gerrymandering. In order to systematically identify such
instances, researchers have devised several tests for so-called geographic
compactness (i.e., shape niceness). We demonstrate that under certain
conditions, a party can gerrymander a competitive state into geographically
compact districts to win an average of over 70% of the districts. Our results
suggest that geometric features alone may fail to adequately combat partisan
gerrymandering.
",mathematics
"  Recent advances in ultrafast measurement in cold atoms, as well as pump-probe
spectroscopy of $K_3 C_{60}$ films, have opened the possibility of rapidly
quenching systems of interacting fermions to, and across, a finite temperature
superfluid transition. However, determining that a transient state has
approached a second-order critical point is difficult, as standard equilibrium
techniques are inapplicable. We show that the approach to the superfluid
critical point in a transient state may be detected via time-resolved transport
measurements, such as the optical conductivity. We leverage the fact that
quenching to the vicinity of the critical point produces a highly time
dependent density of superfluid fluctuations, which affect the conductivity in
two ways. First, by inelastic scattering between the fermions and the
fluctuations, and second by direct conduction through the fluctuations, with
the latter providing a lower resistance current carrying channel. The
competition between these two effects leads to nonmonotonic behavior in the
time- resolved optical conductivity, providing a signature of the critical
transient state.
",physics
"  This paper contains two parts: the description of a real electrical system,
with many redundancies, reconfigurations and repairs, then the description of a
reliability model of this system, based on the BDMP (Boolean logic Driven
Markov Processes) formalism and partial results of a reliability and
availability calculation made from this model.
",computer-science
"  Recently, a proposal has been advanced to detect unconstitutional partisan
gerrymandering with a simple formula called the efficiency gap. The efficiency
gap is now working its way towards a possible landmark case in the Supreme
Court. This note explores some of its mathematical properties in light of the
fact that it reduces to a straight proportional comparison of votes to seats.
Though we offer several critiques, we assess that EG can still be a useful
component of a courtroom analysis. But a famous formula can take on a life of
its own and this one will need to be watched closely.
",physics
"  We developed an automated deep learning system to detect hip fractures from
frontal pelvic x-rays, an important and common radiological task. Our system
was trained on a decade of clinical x-rays (~53,000 studies) and can be applied
to clinical data, automatically excluding inappropriate and technically
unsatisfactory studies. We demonstrate diagnostic performance equivalent to a
human radiologist and an area under the ROC curve of 0.994. Translated to
clinical practice, such a system has the potential to increase the efficiency
of diagnosis, reduce the need for expensive additional testing, expand access
to expert level medical image interpretation, and improve overall patient
outcomes.
",statistics
"  Cycloids, hipocycloids and epicycloids have an often forgotten common
property: they are homothetic to their evolutes. But what if use convex
symmetric polygons as unit balls, can we define evolutes and cycloids which are
genuinely discrete? Indeed, we can! We define discrete cycloids as eigenvectors
of a discrete double evolute transform which can be seen as a linear operator
on a vector space we call curvature radius space. We are also able to classify
such cycloids according to the eigenvalues of that transform, and show that the
number of cusps of each cycloid is well determined by the ordering of those
eigenvalues. As an elegant application, we easily establish a version of the
four-vertex theorem for closed convex polygons. The whole theory is developed
using only linear algebra, and concrete examples are given.
",mathematics
"  Large-scale variations still pose a challenge in unconstrained face
detection. To the best of our knowledge, no current face detection algorithm
can detect a face as large as 800 x 800 pixels while simultaneously detecting
another one as small as 8 x 8 pixels within a single image with equally high
accuracy. We propose a two-stage cascaded face detection framework, Multi-Path
Region-based Convolutional Neural Network (MP-RCNN), that seamlessly combines a
deep neural network with a classic learning strategy, to tackle this challenge.
The first stage is a Multi-Path Region Proposal Network (MP-RPN) that proposes
faces at three different scales. It simultaneously utilizes three parallel
outputs of the convolutional feature maps to predict multi-scale candidate face
regions. The ""atrous"" convolution trick (convolution with up-sampled filters)
and a newly proposed sampling layer for ""hard"" examples are embedded in MP-RPN
to further boost its performance. The second stage is a Boosted Forests
classifier, which utilizes deep facial features pooled from inside the
candidate face regions as well as deep contextual features pooled from a larger
region surrounding the candidate face regions. This step is included to further
remove hard negative samples. Experiments show that this approach achieves
state-of-the-art face detection performance on the WIDER FACE dataset ""hard""
partition, outperforming the former best result by 9.6% for the Average
Precision.
",computer-science
"  Quantum magnetic phases near the magnetic saturation of triangular-lattice
antiferromagnets with XXZ anisotropy have been attracting renewed interest
since it has been suggested that a nontrivial coplanar phase, called the
$\pi$-coplanar or $\Psi$ phase, could be stabilized by quantum effects in a
certain range of anisotropy parameter $J/J_z$ besides the well-known 0-coplanar
(known also as $V$) and umbrella phases. Recently, Sellmann $et$ $al$. [Phys.
Rev. B {\bf 91}, 081104(R) (2015)] claimed that the $\pi$-coplanar phase is
absent for $S=1/2$ from an exact-diagonalization analysis in the sector of the
Hilbert space with only three down-spins (three magnons). We first reconsider
and improve this analysis by taking into account several low-lying eigenvalues
and the associated eigenstates as a function of $J/J_z$ and by sensibly
increasing the system sizes (up to 1296 spins). A careful identification
analysis shows that the lowest eigenstate is a chirally antisymmetric
combination of finite-size umbrella states for $J/J_z\gtrsim 2.218$ while it
corresponds to a coplanar phase for $J/J_z\lesssim 2.218$. However, we
demonstrate that the distinction between 0-coplanar and $\pi$-coplanar phases
in the latter region is fundamentally impossible from the symmetry-preserving
finite-size calculations with fixed magnon number.} Therefore, we also perform
a cluster mean-field plus scaling analysis for small spins $S\leq 3/2$. The
obtained results, together with the previous large-$S$ analysis, indicate that
the $\pi$-coplanar phase exists for any $S$ except for the classical limit
($S\rightarrow \infty$) and the existence range in $J/J_z$ is largest in the
most quantum case of $S=1/2$.
",physics
"  Computational procedures to foresee the 3D structure of aptamers are in
continuous progress. They constitute a crucial input to research, mainly when
the crystallographic counterpart of the structures in silico produced is not
present. At now, many codes are able to perform structure and binding
prediction, although their ability in scoring the results remains rather weak.
In this paper, we propose a novel procedure to complement the ranking outcomes
of free docking code, by applying it to a set of anti-angiopoietin aptamers,
whose performances are known. We rank the in silico produced configurations,
adopting a maximum likelihood estimate, based on their topological and
electrical properties. From the analysis, two principal kinds of conformers are
identified, whose ability to mimick the binding features of the natural
receptor is discussed. The procedure is easily generalizable to many biological
biomolecules, useful for increasing chances of success in designing
high-specificity biosensors (aptasensors).
",quantitative-biology
"  The nervous system encodes continuous information from the environment in the
form of discrete spikes, and then decodes these to produce smooth motor
actions. Understanding how spikes integrate, represent, and process information
to produce behavior is one of the greatest challenges in neuroscience.
Information theory has the potential to help us address this challenge.
Informational analyses of deep and feed-forward artificial neural networks
solving static input-output tasks, have led to the proposal of the
\emph{Information Bottleneck} principle, which states that deeper layers encode
more relevant yet minimal information about the inputs. Such an analyses on
networks that are recurrent, spiking, and perform control tasks is relatively
unexplored. Here, we present results from a Mutual Information analysis of a
recurrent spiking neural network that was evolved to perform the classic
pole-balancing task. Our results show that these networks deviate from the
\emph{Information Bottleneck} principle prescribed for feed-forward networks.
",computer-science
"  We address the task of ranking objects (such as people, blogs, or verticals)
that, unlike documents, do not have direct term-based representations. To be
able to match them against keyword queries, evidence needs to be amassed from
documents that are associated with the given object. We present two design
patterns, i.e., general reusable retrieval strategies, which are able to
encompass most existing approaches from the past. One strategy combines
evidence on the term level (early fusion), while the other does it on the
document level (late fusion). We demonstrate the generality of these patterns
by applying them to three different object retrieval tasks: expert finding,
blog distillation, and vertical ranking.
",computer-science
"  In this article we present an idea of using liquid scintillator Cherenkov
neutrino detectors to detect the mantle and K-40 components of geoneutrinos.
Liquid scintillator Cherenkov detectors feature both energy and direction
measurement for charge particles. Geoneutrinos can be detected with the elastic
scattering process of neutrino and electron. With the directionality, the
dominant intrinsic background originated from solar neutrinos in common liquid
scintillator detectors can be suppressed. The mantle geoneutrinos can be
distinguished because they come mainly underneath. The K-40 geoneutrinos can
also be identified, if the detection threshold for direction measurement can be
lower than, for example, 0.8 MeV. According to our calculation, a moderate,
kilo-ton scale, detector can observe tens of candidates, and is a practical
start for an experiment.
",physics
"  The main goal of the paper is the full proof of a cardinal inequality for a
space with points $G_\delta $, obtained with the help of a long version of the
Menger game. This result, which improves a similar one of Scheepers and Tall,
was already established by the authors under the Continuum Hypothesis. The
paper is completed by few remarks on a long version of the tightness game.
",mathematics
"  In this paper, we consider time-inhomogeneous branching processes and
time-inhomogeneous birth-and-death processes, in which the offspring
distribution and birth and death rates (respectively) vary in time. A classical
result of branching processes states that in the critical regime, a process
conditioned on non-extinction and normalized will converge in distribution to a
standard exponential. In a paper of Jagers, time-inhomogeneous branching
processes are shown to exhibit this convergence as well. In this paper, the
hypotheses of Jagers' result are relaxed, further hypotheses are presented for
convergence in moments, and the result is extended to the continuous-time
analogue of time-inhomogeneous birth-and-death processes. In particular, the
new hypotheses suggest a simple characterization of the critical regime.
",mathematics
"  Recent developments in specialized computer hardware have greatly accelerated
atomic level Molecular Dynamics (MD) simulations. A single GPU-attached cluster
is capable of producing microsecond-length trajectories in reasonable amounts
of time. Multiple protein states and a large number of microstates associated
with folding and with the function of the protein can be observed as
conformations sampled in the trajectories. Clustering those conformations,
however, is needed for identifying protein states, evaluating transition rates
and understanding protein behavior. In this paper, we propose a novel
data-driven generative conformation clustering method based on the adversarial
autoencoder (AAE) and provide the associated software implementation Cong. The
method was tested using a 208 microseconds MD simulation of the fast-folding
peptide Trp-Cage (20 residues) obtained from the D.E. Shaw Research Group. The
proposed clustering algorithm identifies many of the salient features of the
folding process by grouping a large number of conformations that share common
features not easily identifiable in the trajectory.
",quantitative-biology
"  We give lower bounds for the degree of multiplicative combinations of
iterates of rational functions (with certain exceptions) over a general field,
establishing the multiplicative independence of said iterates. This leads to a
generalisation of Gao's method for constructing elements in the finite field
$\mathbb{F}_{q^n}$ whose orders are larger than any polynomial in $n$ when $n$
becomes large. Additionally, we discuss the finiteness of polynomials which
translate a given finite set of polynomials to become multiplicatively
dependent.
",mathematics
"  In this article we are interested in the nonlocal regional Schrödinger
equation with critical exponent \begin{eqnarray*} &\epsilon^{2\alpha}
(-\Delta)_{\rho}^{\alpha}u + u = \lambda u^q + u^{2_{\alpha}^{*}-1} \mbox{ in }
\mathbb{R}^{N}, \\ & u \in H^{\alpha}(\mathbb{R}^{N}), \end{eqnarray*} where
$\epsilon$ is a small positive parameter, $\alpha \in (0,1)$, $q\in
(1,2_{\alpha}^{*}-1)$, $2_{\alpha}^{*} = \frac{2N}{N-2\alpha}$ is the critical
Sobolev exponent, $\lambda >0$ is a parameter and $(-\Delta)_{\rho}^{\alpha}$
is a variational version of the regional laplacian, whose range of scope is a
ball with radius $\rho(x)>0$. We study the existence of a ground state and we
analyze the behavior of semi-classical solutions as $\varepsilon\to 0$.
",mathematics
"  A vector bundle E on a projective variety X is called finite if it satisfies
a nontrivial polynomial equation with integral coefficients. A theorem of Nori
implies that E is finite if and only if the pullback of E to some finite etale
Galois covering of X is trivial. We prove the same statement when X is a
compact complex manifold admitting a Gauduchon astheno-Kahler metric.
",mathematics
"  Fixed point iterations play a central role in the design and the analysis of
a large number of optimization algorithms. We study a new iterative scheme in
which the update is obtained by applying a composition of quasinonexpansive
operators to a point in the affine hull of the orbit generated up to the
current iterate. This investigation unifies several algorithmic constructs,
including Mann's mean value method, inertial methods, and multi-layer
memoryless methods. It also provides a framework for the development of new
algorithms, such as those we propose for solving monotone inclusion and
minimization problems.
",mathematics
"  Newton's method for finding an unconstrained minimizer for strictly convex
functions, generally speaking, does not converge from any starting point.
We introduce and study the damped regularized Newton's method (DRNM). It
converges globally for any strictly convex function, which has a minimizer in
$R^n$.
Locally DRNM converges with a quadratic rate. We characterize the
neighborhood of the minimizer, where the quadratic rate occurs. Based on it we
estimate the number of DRNM's steps required for finding an $\varepsilon$-
approximation for the minimizer.
",mathematics
"  Let $G$ be an adjoint quasi-simple group defined and split over a
non-archimedean local field $K$. We prove that the dual of the Steinberg
representation of $G$ is isomorphic to a certain space of harmonic cochains on
the Bruhat-Tits building of $G$. The Steinberg representation is considered
with coefficients in any commutative ring.
",mathematics
"  The crystallographic stacking order in multilayer graphene plays an important
role in determining its electronic structure. In trilayer graphene,
rhombohedral stacking (ABC) is particularly intriguing, exhibiting a flat band
with an electric-field tunable band gap. Such electronic structure is distinct
from simple hexagonal stacking (AAA) or typical Bernal stacking (ABA), and is
promising for nanoscale electronics, optoelectronics applications. So far clean
experimental electronic spectra on the first two stackings are missing because
the samples are usually too small in size (um or nm scale) to be resolved by
conventional angle-resolved photoemission spectroscopy (ARPES). Here by using
ARPES with nanospot beam size (NanoARPES), we provide direct experimental
evidence for the coexistence of three different stackings of trilayer graphene
and reveal their distinctive electronic structures directly. By fitting the
experimental data, we provide important experimental band parameters for
describing the electronic structure of trilayer graphene with different
stackings.
",physics
"  The recent rapid progress in observations of circumstellar disks and
extrasolar planets has reinforced the importance of understanding an intimate
coupling between star and planet formation. Under such a circumstance, it may
be invaluable to attempt to specify when and how planet formation begins in
star-forming regions and to identify what physical processes/quantities are the
most significant to make a link between star and planet formation. To this end,
we have recently developed a couple of projects. These include an observational
project about dust growth in Class 0 YSOs and a theoretical modeling project of
the HL Tauri disk. For the first project, we utilize the archive data of radio
interferometric observations, and examine whether dust growth, a first step of
planet formation, occurs in Class 0 YSOs. We find that while our observational
results can be reproduced by the presence of large ($\sim$ mm) dust grains for
some of YSOs under the single-component modified blackbody formalism, an
interpretation of no dust growth would be possible when a more detailed model
is used. For the second project, we consider an origin of the disk
configuration around HL Tauri, focusing on magnetic fields. We find that
magnetically induced disk winds may play an important role in the HL Tauri
disk. The combination of these attempts may enable us to move towards a
comprehensive understanding of how star and planet formation are intimately
coupled with each other.
",physics
"  The discrete cosine transform (DCT) is a widely-used and important signal
processing tool employed in a plethora of applications. Typical fast algorithms
for nearly-exact computation of DCT require floating point arithmetic, are
multiplier intensive, and accumulate round-off errors. Recently proposed fast
algorithm arithmetic cosine transform (ACT) calculates the DCT exactly using
only additions and integer constant multiplications, with very low area
complexity, for null mean input sequences. The ACT can also be computed
non-exactly for any input sequence, with low area complexity and low power
consumption, utilizing the novel architecture described. However, as a
trade-off, the ACT algorithm requires 10 non-uniformly sampled data points to
calculate the 8-point DCT. This requirement can easily be satisfied for
applications dealing with spatial signals such as image sensors and biomedical
sensor arrays, by placing sensor elements in a non-uniform grid. In this work,
a hardware architecture for the computation of the null mean ACT is proposed,
followed by a novel architectures that extend the ACT for non-null mean
signals. All circuits are physically implemented and tested using the Xilinx
XC6VLX240T FPGA device and synthesized for 45 nm TSMC standard-cell library for
performance assessment.
",computer-science
"  We report on the optical and mechanical characterization of arrays of
parallel micromechanical membranes. Pairs of high-tensile stress, 100 nm-thick
silicon nitride membranes are assembled parallel with each other with
separations ranging from 8.5 to 200 $\mu$m. Their optical properties are
accurately determined using a combination of broadband and monochromatic
illuminations and the lowest vibrational mode frequencies and mechanical
quality factors are determined interferometrically. The results and techniques
demonstrated are promising for investigations of collective phenomena in
optomechanical arrays.
",physics
"  By a labeled graph $C^*$-algebra we mean a $C^*$-algebra associated to a
labeled space $(E,\mathcal L,\mathcal E)$ consisting of a labeled graph
$(E,\mathcal L)$ and the smallest normal accommodating set $\mathcal E$ of
vertex subsets. Every graph $C^*$-algebra $C^*(E)$ is a labeled graph
$C^*$-algebra and it is well known that $C^*(E)$ is simple if and only if the
graph $E$ is cofinal and satisfies Condition (L). Bates and Pask extend these
conditions of graphs $E$ to labeled spaces, and show that if a set-finite and
receiver set-finite labeled space $(E,\mathcal L, \mathcal E)$ is cofinal and
disagreeable, then its $C^*$-algebra $C^*(E,\mathcal L, \mathcal E)$ is simple.
In this paper, we show that the converse is also true.
",mathematics
"  We prove the following superexponential distribution inequality: for any
integrable $g$ on $[0,1)^{d}$ with zero average, and any $\lambda>0$ \[ |\{ x
\in [0,1)^{d} \; :\; g \geq\lambda \}| \leq e^{-
\lambda^{2}/(2^{d}\|S(g)\|_{\infty}^{2})}, \] where $S(g)$ denotes the
classical dyadic square function in $[0,1)^{d}$. The estimate is sharp when
dimension $d$ tends to infinity in the sense that the constant $2^{d}$ in the
denominator cannot be replaced by $C2^{d}$ with $0<C<1$ independent of $d$ when
$d \to \infty$.
For $d=1$ this is a classical result of Chang--Wilson--Wolff [4]; however, in
the case $d>1$ they work with a special square function $S_\infty$, and their
result does not imply the estimates for the classical square function.
Using good $\lambda$ inequalities technique we then obtain unweighted and
weighted $L^p$ lower bounds for $S$; to get the corresponding good $\lambda$
inequalities we need to modify the classical construction.
We also show how to obtain our superexponential distribution inequality
(although with worse constants) from the weighted $L^2$ lower bounds for $S$,
obtained in [5].
",mathematics
"  The use of low-precision fixed-point arithmetic along with stochastic
rounding has been proposed as a promising alternative to the commonly used
32-bit floating point arithmetic to enhance training neural networks training
in terms of performance and energy efficiency. In the first part of this paper,
the behaviour of the 12-bit fixed-point arithmetic when training a
convolutional neural network with the CIFAR-10 dataset is analysed, showing
that such arithmetic is not the most appropriate for the training phase. After
that, the paper presents and evaluates, under the same conditions, alternative
low-precision arithmetics, starting with the 12-bit floating-point arithmetic.
These two representations are then leveraged using local scaling in order to
increase accuracy and get closer to the baseline 32-bit floating-point
arithmetic. Finally, the paper introduces a simplified model in which both the
outputs and the gradients of the neural networks are constrained to
power-of-two values, just using 7 bits for their representation. The evaluation
demonstrates a minimal loss in accuracy for the proposed Power-of-Two neural
network, avoiding the use of multiplications and divisions and thereby,
significantly reducing the training time as well as the energy consumption and
memory requirements during the training and inference phases.
",statistics
"  Numerous institutions and organizations need not only to preserve the
material and publications they produce, but also have as their task (although
it would be desirable it was an obligation) to publish, disseminate and make
publicly available all the results of the research and any other
scientific/academic material. The Open Archives Initiative (OAI) and the
introduction of Open Archives Initiative Protocol for Metadata Harvesting
(OAI-PMH), make this task much easier. The main objective of this work is to
make a comparative and qualitative study of the data -metadata specifically-
contained in the whole set of Argentine repositories listed in the ROAR portal,
focusing on the functional perspective of the quality of this metadata. Another
objective is to offer an overview of the status of these repositories, in an
attempt to detect common failures and errors institutions incur when storing
the metadata of the resources contained in these repositories, and thus be able
to suggest measures to be able to improve the load and further retrieval
processes. It was found that the eight most used Dublin Core fields are:
identifier, type, title, date, subject, creator, language and description. Not
all repositories fill all the fields, and the lack of normalization, or the
excessive use of fields like language, type, format and subject is somewhat
striking, and in some cases even alarming
",computer-science
"  We demonstrate how electric fields with arbitrary time profile can be used to
control the time-dependent parameters of spin and orbital exchange
Hamiltonians. Analytic expressions for the exchange constants are derived from
a time-dependent Schrieffer-Wolff transformation, and the validity of the
resulting effective Hamiltonian is verified for the case of a quarter-filled
two-orbital Hubbard model, by comparing to the results of a full nonequilibrium
dynamical mean-field theory simulation. The ability to manipulate Hamiltonians
with arbitrary time-dependent fields, beyond the paradigm of Floquet
engineering, opens the possibility to control intertwined spin and orbital
order using laser or THz pulses which are tailored to minimize electronic
excitations.
",physics
"  We establish a general connection between ballistic and diffusive transport
in systems where the ballistic contribution in canonical ensemble vanishes. A
lower bound on the Green-Kubo diffusion constant is derived in terms of the
curvature of the ideal transport coefficient, the Drude weight, with respect to
the filling parameter. As an application, we explicitly determine the lower
bound on the high temperature diffusion constant in the anisotropic spin 1/2
Heisenberg chain for anisotropy parameters $\Delta \geq 1$, thus settling the
question whether the transport is sub-diffusive or not. Addi- tionally, the
lower bound is shown to saturate the diffusion constant for a certain classical
integrable model.
",physics
"  Dual Fabry-Perot cavity based optical refractometry (DFPC-OR) has a high
potential for assessments of gas density. However, drifts of the FP cavity
often limit its performance. We show that by the use of two narrow-linewidth
fiber lasers locked to two high finesse cavities and Allan-Werle plots that
drift-free DFPC-OR can be obtained for short measurement times (for which the
drifts of the cavity can be disregarded). Based on this, a novel strategy,
termed fast switching DFPC-OR (FS-DFPC-OR), is presented. A set of novel
methodologies for assessment of both gas density and flow rates (in particular
from small leaks) that are not restricted by the conventional limitations
imposed by the drifts of the cavity are presented. The methodologies deal with
assessments in both open and closed (finite-sized) compartments. They
circumvent the problem with volumetric expansion, i.e. that the gas density in
a measurement cavity is not the same as that in the closed external compartment
that should be assessed, by performing a pair of measurements in rapid
succession; the first one serves the purpose of assessing the density of the
gas that has been transferred into the measurement cavity by the gas
equilibration process, while the 2nd is used to automatically calibrate the
system with respect to the relative volumes of the measurement cavity and the
external compartment. The methodologies for assessments of leak rates comprise
triple cavity evacuation assessments, comprising two measurements performed in
rapid succession, supplemented by a 3rd measurement a certain time thereafter.
A clear explanation of why the technique has such a small temperature
dependence is given. It is concluded that FS-DFPC-OR constitutes a novel
strategy that can be used for precise and accurate assessment of gas number
density and gas flows under a variety of conditions, in particular
non-temperature stabilized ones.
",physics
"  In this paper we study a special case of the completion of cusp
Kähler-Einstein metric on the regular part of varieties by taking the
continuity method proposed by La Nave and Tian. The differential geometric and
algebro-geometric properties of the noncollapsing limit in the continuity
method with cusp singularities will be investigated.
",mathematics
"  Nested weighted automata (NWA) present a robust and convenient
automata-theoretic formalism for quantitative specifications. Previous works
have considered NWA that processed input words only in the forward direction.
It is natural to allow the automata to process input words backwards as well,
for example, to measure the maximal or average time between a response and the
preceding request. We therefore introduce and study bidirectional NWA that can
process input words in both directions. First, we show that bidirectional NWA
can express interesting quantitative properties that are not expressible by
forward-only NWA. Second, for the fundamental decision problems of emptiness
and universality, we establish decidability and complexity results for the new
framework which match the best-known results for the special case of
forward-only NWA. Thus, for NWA, the increased expressiveness of
bidirectionality is achieved at no additional computational complexity. This is
in stark contrast to the unweighted case, where bidirectional finite automata
are no more expressive but exponentially more succinct than their forward-only
counterparts.
",computer-science
"  We consider the Cauchy problem for the repulsive Vlasov-Poisson system in the
three dimensional space, where the initial datum is the sum of a diffuse
density, assumed to be bounded and integrable, and a point charge. Under some
decay assumptions for the diffuse density close to the point charge, under
bounds on the total energy, and assuming that the initial total diffuse charge
is strictly less than one, we prove existence of global Lagrangian solutions.
Our result extends the Eulerian theory of [16], proving that solutions are
transported by the flow trajectories. The proof is based on the ODE theory
developed in [8] in the setting of vector fields with anisotropic regularity,
where some components of the gradient of the vector field is a singular
integral of a measure.
",mathematics
"  We performed electronic structure calculations based on the first-principles
many-body theory approach in order to study quasiparticle band gaps, and
optical absorption spectra of hydrogen-passivated zigzag SiC nanoribbons.
Self-energy corrections are included using the GW approximation, and excitonic
effects are included using the Bethe-Salpeter equation. We have systematically
studied nanoribbons that have widths between 0.6 $\text{nm}$ and 2.2
$\text{nm}$. Quasiparticle corrections widened the Kohn-Sham band gaps because
of enhanced interaction effects, caused by reduced dimensionality. Zigzag SiC
nanoribbons with widths larger than 1 nm, exhibit half-metallicity at the
mean-field level. The self-energy corrections increased band gaps
substantially, thereby transforming the half-metallic zigzag SiC nanoribbons,
to narrow gap spin-polarized semiconductors. Optical absorption spectra of
these nanoribbons get dramatically modified upon inclusion of electron-hole
interactions, and the narrowest ribbon exhibits strongly bound excitons, with
binding energy of 2.1 eV. Thus, the narrowest zigzag SiC nanoribbon has the
potential to be used in optoelectronic devices operating in the IR region of
the spectrum, while the broader ones, exhibiting spin polarization, can be
utilized in spintronic applications.
",physics
"  One of the ultimate goals in biology is to understand the design principles
of biological systems. Such principles, if they exist, can help us better
understand complex, natural biological systems and guide the engineering of de
novo ones. Towards deciphering design principles, in silico evolution of
biological systems with proper abstraction is a promising approach. Here, we
demonstrate the application of in silico evolution combined with rule-based
modelling for exploring design principles of cellular signaling networks. This
application is based on a computational platform, called BioJazz, which allows
in silico evolution of signaling networks with unbounded complexity. We provide
a detailed introduction to BioJazz architecture and implementation and describe
how it can be used to evolve and/or design signaling networks with defined
dynamics. For the latter, we evolve signaling networks with switch-like
response dynamics and demonstrate how BioJazz can result in new biological
insights on network structures that can endow bistable response dynamics. This
example also demonstrated both the power of BioJazz in evolving and designing
signaling networks and its limitations at the current stage of development.
",quantitative-biology
"  Web request query strings (queries), which pass parameters to the referenced
resource, are always manipulated by attackers to retrieve sensitive data and
even take full control of victim web servers and web applications. However,
existing malicious query detection approaches in the current literature cannot
cope with changing web attacks with constant detection models. In this paper,
we propose AMODS, an adaptive system that periodically updates the detection
model to detect the latest unknown attacks. We also propose an adaptive
learning strategy, called SVM HYBRID, leveraged by our system to minimize
manual work. In the evaluation, an up-to-date detection model is trained on a
ten-day query dataset collected from an academic institute's web server logs.
Our system outperforms existing web attack detection methods, with an F-value
of 94.79% and FP rate of 0.09%. The total number of malicious queries obtained
by SVM HYBRID is 2.78 times that by the popular Support Vector Machine Adaptive
Learning (SVM AL) method. The malicious queries obtained can be used to update
the Web Application Firewall (WAF) signature library.
",computer-science
"  Electrical brain stimulation is currently being investigated as a therapy for
neurological disease. However, opportunities to optimize such therapies are
challenged by the fact that the beneficial impact of focal stimulation on both
neighboring and distant regions is not well understood. Here, we use network
control theory to build a model of brain network function that makes
predictions about how stimulation spreads through the brain's white matter
network and influences large-scale dynamics. We test these predictions using
combined electrocorticography (ECoG) and diffusion weighted imaging (DWI) data
who volunteered to participate in an extensive stimulation regimen. We posit a
specific model-based manner in which white matter tracts constrain stimulation,
defining its capacity to drive the brain to new states, including states
associated with successful memory encoding. In a first validation of our model,
we find that the true pattern of white matter tracts can be used to more
accurately predict the state transitions induced by direct electrical
stimulation than the artificial patterns of null models. We then use a targeted
optimal control framework to solve for the optimal energy required to drive the
brain to a given state. We show that, intuitively, our model predicts larger
energy requirements when starting from states that are farther away from a
target memory state. We then suggest testable hypotheses about which structural
properties will lead to efficient stimulation for improving memory based on
energy requirements. Our work demonstrates that individual white matter
architecture plays a vital role in guiding the dynamics of direct electrical
stimulation, more generally offering empirical support for the utility of
network control theoretic models of brain response to stimulation.
",quantitative-biology
"  We search for $\gamma$-ray and optical periodic modulations in a distant flat
spectrum radio quasar (FSRQ) PKS 0426-380 (the redshift $z=1.1$). Using two
techniques (i.e., the maximum likelihood optimization and the exposure-weighted
aperture photometry), we obtain $\gamma$-ray light curves from \emph{Fermi}-LAT
Pass 8 data covering from 2008 August to 2016 December. We then analyze the
light curves with the Lomb-Scargle Periodogram (LSP) and the Weighted Wavelet
Z-transform (WWZ). A $\gamma$-ray quasi-periodicity with a period of 3.35 $\pm$
0.68 years is found at the significance-level of $\simeq3.6\ \sigma$. The
optical-UV flux covering from 2005 August to 2013 April provided by ASI SCIENCE
DATA CENTER is also analyzed, but no significant quasi-periodicity is found. It
should be pointed out that the result of the optical-UV data could be tentative
because of the incomplete of the data. Further long-term multiwavelength
monitoring of this FSRQ is needed to confirm its quasi-periodicity.
",physics
"  The present work shows the application of transfer learning for a pre-trained
deep neural network (DNN), using a small image dataset ($\approx$ 12,000) on a
single workstation with enabled NVIDIA GPU card that takes up to 1 hour to
complete the training task and archive an overall average accuracy of $94.7\%$.
The DNN presents a $20\%$ score of misclassification for an external test
dataset. The accuracy of the proposed methodology is equivalent to ones using
HSI methodology $(81\%-91\%)$ used for the same task, but with the advantage of
being independent on special equipment to classify wheat kernel for FHB
symptoms.
",statistics
"  The purpose of this paper is to formulate and study a common refinement of a
version of Stark's conjecture and its $p$-adic analogue, in terms of Fontaine's
$p$-adic period ring and $p$-adic Hodge theory. We construct period-ring-valued
functions under a generalization of Yoshida's conjecture on the transcendental
parts of CM-periods. Then we conjecture a reciprocity law on their special
values concerning the absolute Frobenius action. We show that our conjecture
implies a part of Stark's conjecture when the base field is an arbitrary real
field and the splitting place is its real place. It also implies a refinement
of the Gross-Stark conjecture under a certain assumption. When the base field
is the rational number field, our conjecture follows from Coleman's formula on
Fermat curves. We also prove some partial results in other cases.
",mathematics
"  In general, neural networks are not currently capable of learning tasks in a
sequential fashion. When a novel, unrelated task is learnt by a neural network,
it substantially forgets how to solve previously learnt tasks. One of the
original solutions to this problem is pseudo-rehearsal, which involves learning
the new task while rehearsing generated items representative of the previous
task/s. This is very effective for simple tasks. However, pseudo-rehearsal has
not yet been successfully applied to very complex tasks because in these tasks
it is difficult to generate representative items. We accomplish
pseudo-rehearsal by using a Generative Adversarial Network to generate items so
that our deep network can learn to sequentially classify the CIFAR-10, SVHN and
MNIST datasets. After training on all tasks, our network loses only 1.67%
absolute accuracy on CIFAR-10 and gains 0.24% absolute accuracy on SVHN. Our
model's performance is a substantial improvement compared to the current state
of the art solution.
",statistics
"  We investigate the birth and diffusion of lexical innovations in a large
dataset of online social communities. We build on sociolinguistic theories and
focus on the relation between the spread of a novel term and the social role of
the individuals who use it, uncovering characteristics of innovators and
adopters. Finally, we perform a prediction task that allows us to anticipate
whether an innovation will successfully spread within a community.
",computer-science
"  Supersymmetry plays an important role in superstring theory and particle
physics, but has never been observed in experiments. At certain quantum
critical points of condensed matter systems, the fermionic excitations are
gapless due to the special electronic structure whereas the bosonic order
parameter is automatically gapless, offering a promising platform to realize
emergent supersymmetry by tuning a single parameter. Here, we study under what
circumstances can supersymmetry emerge in a quantum critical system. We
demonstrate that the Yukawa-type coupling between the gapless fermion and boson
may induce a number of highly nonlocal self-interacting terms in the effective
field theory of the boson. Only when such terms do not exist or are irrelevant,
could supersymmetry have the chance to be dynamically generated at low
energies. This strong constraint provides an important guidance for the
exploration of emergent supersymmetry in various condensed matter systems, and
also should be carefully considered in the study of quantum critical behaviors.
",physics
"  Anytime almost-surely asymptotically optimal planners, such as RRT*,
incrementally find paths to every state in the search domain. This is
inefficient once an initial solution is found as then only states that can
provide a better solution need to be considered. Exact knowledge of these
states requires solving the problem but can be approximated with heuristics.
This paper formally defines these sets of states and demonstrates how they
can be used to analyze arbitrary planning problems. It uses the well-known
$L^2$ norm (i.e., Euclidean distance) to analyze minimum-path-length problems
and shows that existing approaches decrease in effectiveness factorially (i.e.,
faster than exponentially) with state dimension. It presents a method to
address this curse of dimensionality by directly sampling the prolate
hyperspheroids (i.e., symmetric $n$-dimensional ellipses) that define the $L^2$
informed set.
The importance of this direct informed sampling technique is demonstrated
with Informed RRT*. This extension of RRT* has less theoretical dependence on
state dimension and problem size than existing techniques and allows for linear
convergence on some problems. It is shown experimentally to find better
solutions faster than existing techniques on both abstract planning problems
and HERB, a two-arm manipulation robot.
",computer-science
"  We report the development of indium oxide (In2O3) transistors via a single
step laser-induced photochemical conversion process of a sol-gel metal oxide
precursor. Through careful optimization of the laser annealing conditions we
demonstrated successful conversion of the precursor to In2O3 and its subsequent
implementation in n-channel transistors with electron mobility up to 13 cm2/Vs.
Importantly, the process does not require thermal annealing making it
compatible with temperature sensitive materials such as plastic. On the other
hand, the spatial conversion/densification of the sol-gel layer eliminates
additional process steps associated with semiconductor patterning and hence
significantly reduces fabrication complexity and cost. Our work demonstrates
unambiguously that laser-induced photochemical conversion of sol-gel metal
oxide precursors can be rapid and compatible with large-area electronics
manufacturing.
",physics
"  The quadratic unconstrained binary optimization (QUBO) problem arises in
diverse optimization applications ranging from Ising spin problems to classical
problems in graph theory and binary discrete optimization. The use of
preprocessing to transform the graph representing the QUBO problem into a
smaller equivalent graph is important for improving solution quality and time
for both exact and metaheuristic algorithms and is a step towards mapping large
scale QUBO to hardware graphs used in quantum annealing computers. In an
earlier paper (Lewis and Glover, 2016) a set of rules was introduced that
achieved significant QUBO reductions as verified through computational testing.
Here this work is extended with additional rules that provide further
reductions that succeed in exactly solving 10% of the benchmark QUBO problems.
An algorithm and associated data structures to efficiently implement the entire
set of rules is detailed and computational experiments are reported that
demonstrate their efficacy.
",computer-science
"  We study the following control problem. A fish with bounded aquatic
locomotion speed swims in fast waters. Can this fish, under reasonable
assumptions, get to a desired destination? It can, even if the flow is
time-dependent. Moreover, given a prescribed sufficiently large time $t$, it
can be there at exactly the time $t$. The major difference from our previous
work is the time-dependence of the flow. We also give an application to
homogenization of the G-equation.
",mathematics
"  Motivated by problems in data clustering, we establish general conditions
under which families of nonparametric mixture models are identifiable, by
introducing a novel framework involving clustering overfitted \emph{parametric}
(i.e. misspecified) mixture models. These identifiability conditions generalize
existing conditions in the literature, and are flexible enough to include for
example mixtures of Gaussian mixtures. In contrast to the recent literature on
estimating nonparametric mixtures, we allow for general nonparametric mixture
components, and instead impose regularity assumptions on the underlying mixing
measure. As our primary application, we apply these results to partition-based
clustering, generalizing the notion of a Bayes optimal partition from classical
parametric model-based clustering to nonparametric settings. Furthermore, this
framework is constructive so that it yields a practical algorithm for learning
identified mixtures, which is illustrated through several examples on real
data. The key conceptual device in the analysis is the convex, metric geometry
of probability measures on metric spaces and its connection to the Wasserstein
convergence of mixing measures. The result is a flexible framework for
nonparametric clustering with formal consistency guarantees.
",statistics
"  Brain tumour segmentation plays a key role in computer-assisted surgery. Deep
neural networks have increased the accuracy of automatic segmentation
significantly, however these models tend to generalise poorly to different
imaging modalities than those for which they have been designed, thereby
limiting their applications. For example, a network architecture initially
designed for brain parcellation of monomodal T1 MRI can not be easily
translated into an efficient tumour segmentation network that jointly utilises
T1, T1c, Flair and T2 MRI. To tackle this, we propose a novel scalable
multimodal deep learning architecture using new nested structures that
explicitly leverage deep features within or across modalities. This aims at
making the early layers of the architecture structured and sparse so that the
final architecture becomes scalable to the number of modalities. We evaluate
the scalable architecture for brain tumour segmentation and give evidence of
its regularisation effect compared to the conventional concatenation approach.
",computer-science
"  The paper investigates the problem of fitting protein complexes into electron
density maps. They are represented by high-resolution cryoEM density maps
converted into overlapping matrices and partly show a structure of a complex.
The general purpose is to define positions of all proteins inside it. This
problem is known to be NP-hard, since it lays in the field of combinatorial
optimization over a set of discrete states of the complex. We introduce
quadratic programming approaches to the problem. To find an approximate
solution, we convert a density map into an overlapping matrix, which is
generally indefinite. Since the matrix is indefinite, the optimization problem
for the corresponding quadratic form is non-convex. To treat non-convexity of
the optimization problem, we use different convex relaxations to find which set
of proteins minimizes the quadratic form best.
",mathematics
"  We present an enumeration of orientably-regular maps with automorphism group
isomorphic to the twisted linear fractional group $M(q^2)$ for any odd prime
power $q$.
",mathematics
"  Modeling agent behavior is central to understanding the emergence of complex
phenomena in multiagent systems. Prior work in agent modeling has largely been
task-specific and driven by hand-engineering domain-specific prior knowledge.
We propose a general learning framework for modeling agent behavior in any
multiagent system using only a handful of interaction data. Our framework casts
agent modeling as a representation learning problem. Consequently, we construct
a novel objective inspired by imitation learning and agent identification and
design an algorithm for unsupervised learning of representations of agent
policies. We demonstrate empirically the utility of the proposed framework in
(i) a challenging high-dimensional competitive environment for continuous
control and (ii) a cooperative environment for communication, on supervised
predictive tasks, unsupervised clustering, and policy optimization using deep
reinforcement learning.
",statistics
"  In this paper we introduce a new property of two-dimensional integrable
systems -- existence of infinitely many local three-dimensional conservation
laws for pairs of integrable two-dimensional commuting flows. Infinitely many
three-dimensional local conservation laws for the Korteweg de Vries pair of
commuting flows and for the Benney commuting hydrodynamic chains are
constructed. As a by-product we established a new method for computation of
local conservation laws for three-dimensional integrable systems. The Mikhalev
equation and the dispersionless limit of the Kadomtsev--Petviashvili equation
are investigated. All known local and infinitely many new quasi-local
three-dimensional conservation laws are presented. Also four-dimensional
conservation laws are considered for couples of three-dimensional integrable
quasilinear systems and for triples of corresponding hydrodynamic chains.
",physics
"  We have developed a semi-analytic framework to model the large-scale
evolution of the first Population III (Pop III) stars and the transition to
metal-enriched star formation. Our model follows dark matter halos from
cosmological N-body simulations, utilizing their individual merger histories
and three-dimensional positions, and applies physically motivated prescriptions
for star formation and feedback from Lyman-Werner (LW) radiation, hydrogen
ionizing radiation, and external metal enrichment due to supernovae winds. This
method is intended to complement analytic studies, which do not include
clustering or individual merger histories, and hydrodynamical cosmological
simulations, which include detailed physics, but are computationally expensive
and have limited dynamic range. Utilizing this technique, we compute the
cumulative Pop III and metal-enriched star formation rate density (SFRD) as a
function of redshift at $z \geq 20$. We find that varying the model parameters
leads to significant qualitative changes in the global star formation history.
The Pop III star formation efficiency and the delay time between Pop III and
subsequent metal-enriched star formation are found to have the largest impact.
The effect of clustering (i.e. including the three-dimensional positions of
individual halos) on various feedback mechanisms is also investigated. The
impact of clustering on LW and ionization feedback is found to be relatively
mild in our fiducial model, but can be larger if external metal enrichment can
promote metal-enriched star formation over large distances.
",physics
"  Based on properties of n-subharmonic functions we show that a complete,
noncompact, properly embedded hypersurface with nonnegative Ricci curvature in
hyperbolic space has an asymptotic boundary at infinity of at most two points.
Moreover, the presence of two points in the asymptotic boundary is a rigidity
condition that forces the hypersurface to be an equidistant hypersurface about
a geodesic line in hyperbolic space. This gives an affirmative answer to the
question raised by Alexander and Currier in 1990.
",mathematics
"  We give several sharp estimates for a class of combinations of second order
Riesz transforms on Lie groups ${G}={G}_{x} \times {G}_{y}$ that are multiply
connected, composed of a discrete abelian component ${G}_{x}$ and a connected
component ${G}_{y}$ endowed with a biinvariant measure. These estimates include
new sharp $L^p$ estimates via Choi type constants, depending upon the
multipliers of the operator. They also include weak-type, logarithmic and
exponential estimates. We give an optimal $L^q \to L^p$ estimate as well.
It was shown recently by Arcozzi, Domelevo and Petermichl that such second
order Riesz transforms applied to a function may be written as conditional
expectation of a simple transformation of a stochastic integral associated with
the function.
The proofs of our theorems combine this stochastic integral representation
with a number of deep estimates for pairs of martingales under strong
differential subordination by Choi, Banuelos and Osekowski.
When two continuous directions are available, sharpness is shown via the
laminates technique. We show that sharpness is preserved in the discrete case
using Lax-Richtmyer theorem.
",mathematics
"  We study simultaneous collisions of two, three, and four kinks and antikinks
of the $\phi^6$ model at the same spatial point. Unlike the $\phi^4$ kinks, the
$\phi^6$ kinks are asymmetric and this enriches the variety of the collision
scenarios. In our numerical simulations we observe both reflection and bound
state formation depending on the number of kinks and on their spatial ordering
in the initial configuration. We also analyze the extreme values of the energy
densities and the field gradient observed during the collisions. Our results
suggest that very high energy densities can be produced in multi-kink
collisions in a controllable manner. Appearance of high energy density spots in
multi-kink collisions can be important in various physical applications of the
Klein-Gordon model.
",physics
"  We present the results of a systematic search for Lyman-alpha emitters (LAEs)
at $6 \lesssim z \lesssim 7.6$ using the HST WFC3 Infrared Spectroscopic
Parallel (WISP) Survey. Our total volume over this redshift range is $\sim 8
\times10^5$ Mpc$^3$, comparable to many of the narrowband surveys despite their
larger area coverage. We find two LAEs at $z=6.38$ and $6.44$ with line
luminosities of L$_{\mathrm{Ly}\alpha} \sim 4.7 \times 10^{43}$ erg s$^{-1}$,
putting them among the brightest LAEs discovered at these redshifts. Taking
advantage of the broad spectral coverage of WISP, we are able to rule out
almost all lower-redshift contaminants. The WISP LAEs have a high number
density of $7.7\times10^{-6}$ Mpc$^{-3}$. We argue that the LAEs reside in
Mpc-scale ionized bubbles that allow the Lyman-alpha photons to redshift out of
resonance before encountering the neutral IGM. We discuss possible ionizing
sources and conclude that the observed LAEs alone are not sufficient to ionize
the bubbles.
",physics
"  We analyze a proprietary dataset of trades by a single asset manager,
comparing their price impact with that of the trades of the rest of the market.
In the context of a linear propagator model we find no significant difference
between the two, suggesting that both the magnitude and time dependence of
impact are universal in anonymous, electronic markets. This result is important
as optimal execution policies often rely on propagators calibrated on anonymous
data. We also find evidence that in the wake of a trade the order flow of other
market participants first adds further copy-cat trades enhancing price impact
on very short time scales. The induced order flow then quickly inverts, thereby
contributing to impact decay.
",physics
"  The discriminative power of modern deep learning models for 3D human action
recognition is growing ever so potent. In conjunction with the recent
resurgence of 3D human action representation with 3D skeletons, the quality and
the pace of recent progress have been significant. However, the inner workings
of state-of-the-art learning based methods in 3D human action recognition still
remain mostly black-box. In this work, we propose to use a new class of models
known as Temporal Convolutional Neural Networks (TCN) for 3D human action
recognition. Compared to popular LSTM-based Recurrent Neural Network models,
given interpretable input such as 3D skeletons, TCN provides us a way to
explicitly learn readily interpretable spatio-temporal representations for 3D
human action recognition. We provide our strategy in re-designing the TCN with
interpretability in mind and how such characteristics of the model is leveraged
to construct a powerful 3D activity recognition method. Through this work, we
wish to take a step towards a spatio-temporal model that is easier to
understand, explain and interpret. The resulting model, Res-TCN, achieves
state-of-the-art results on the largest 3D human action recognition dataset,
NTU-RGBD.
",computer-science
"  In recent years, monaural speech separation has been formulated as a
supervised learning problem, which has been systematically researched and shown
the dramatical improvement of speech intelligibility and quality for human
listeners. However, it has not been well investigated whether the methods can
be employed as the front-end processing and directly improve the performance of
a machine listener, i.e., an automatic speech recognizer, without retraining or
joint-training the acoustic model. In this paper, we explore the effectiveness
of the independent front-end processing for the multi-conditional trained ASR
on the CHiME-3 challenge. We find that directly feeding the enhanced features
to ASR can make 36.40% and 11.78% relative WER reduction for the GMM-based and
DNN-based ASR respectively. We also investigate the affect of noisy phase and
generalization ability under unmatched noise condition.
",computer-science
"  Sparse deep neural networks(DNNs) are efficient in both memory and compute
when compared to dense DNNs. But due to irregularity in computation of sparse
DNNs, their efficiencies are much lower than that of dense DNNs on regular
parallel hardware such as TPU. This inefficiency leads to poor/no performance
benefits for sparse DNNs. Performance issue for sparse DNNs can be alleviated
by bringing structure to the sparsity and leveraging it for improving runtime
efficiency. But such structural constraints often lead to suboptimal
accuracies. In this work, we jointly address both accuracy and performance of
sparse DNNs using our proposed class of sparse neural networks called HBsNN
(Hierarchical Block sparse Neural Networks). For a given sparsity, HBsNN models
achieve better runtime performance than unstructured sparse models and better
accuracy than highly structured sparse models.
",statistics
"  The exponential-time hypothesis (ETH) states that 3-SAT is not solvable in
subexponential time, i.e. not solvable in O(c^n) time for arbitrary c > 1,
where n denotes the number of variables. Problems like k-SAT can be viewed as
special cases of the constraint satisfaction problem (CSP), which is the
problem of determining whether a set of constraints is satisfiable. In this
paper we study thef worst-case time complexity of NP-complete CSPs. Our main
interest is in the CSP problem parameterized by a constraint language Gamma
(CSP(Gamma)), and how the choice of Gamma affects the time complexity. It is
believed that CSP(Gamma) is either tractable or NP-complete, and the algebraic
CSP dichotomy conjecture gives a sharp delineation of these two classes based
on algebraic properties of constraint languages. Under this conjecture and the
ETH, we first rule out the existence of subexponential algorithms for
finite-domain NP-complete CSP(Gamma) problems. This result also extends to
certain infinite-domain CSPs and structurally restricted CSP(Gamma) problems.
We then begin a study of the complexity of NP-complete CSPs where one is
allowed to arbitrarily restrict the values of individual variables, which is a
very well-studied subclass of CSPs. For such CSPs with finite domain D, we
identify a relation SD such that (1) CSP({SD}) is NP-complete and (2) if
CSP(Gamma) over D is NP-complete and solvable in O(c^n) time, then CSP({SD}) is
solvable in O(c^n) time, too. Hence, the time complexity of CSP({SD}) is a
lower bound for all CSPs of this particular kind. We also prove that the
complexity of CSP({SD}) is decreasing when |D| increases, unless the ETH is
false. This implies, for instance, that for every c>1 there exists a
finite-domain Gamma such that CSP(Gamma) is NP-complete and solvable in O(c^n)
time.
",computer-science
"  Cell shape is an important biomarker. Previously extensive studies have
established the relation between cell shape and cell function. However, the
morphodynamics, namely the temporal fluctuation of cell shape is much less
understood. We study the morphodynamics of MDA-MB-231 cells in type I collagen
extracellular matrix (ECM). We find ECM mechanics, as tuned by collagen
concentration, controls the morphodynamics but not the static cell morphology.
By employing machine learning techniques, we classify cell shape into five
different morphological phenotypes corresponding to different migration modes.
As a result, cell morphodynamics is mapped into temporal evolution of
morphological phenotypes. We systematically characterize the phenotype dynamics
including occurrence probability, dwell time, transition flux, and also obtain
the invasion characteristics of each phenotype. Using a tumor organoid model,
we show that the distinct invasion potentials of each phenotype modulate the
phenotype homeostasis. Overall invasion of a tumor organoid is facilitated by
individual cells searching for and committing to phenotypes of higher invasive
potential. In conclusion, we show that 3D migrating cancer cells exhibit rich
morphodynamics that is regulated by ECM mechanics and is closely related with
cell motility. Our results pave the way to systematic characterization and
functional understanding of cell morphodynamics.
",quantitative-biology
"  Applying deep learning methods to mammography assessment has remained a
challenging topic. Dense noise with sparse expressions, mega-pixel raw data
resolution, lack of diverse examples have all been factors affecting
performance. The lack of pixel-level ground truths have especially limited
segmentation methods in pushing beyond approximately bounding regions. We
propose a classification approach grounded in high performance tissue
assessment as an alternative to all-in-one localization and assessment models
that is also capable of pinpointing the causal pixels. First, the objective of
the mammography assessment task is formalized in the context of local tissue
classifiers. Then, the accuracy of a convolutional neural net is evaluated on
classifying patches of tissue with suspicious findings at varying scales, where
highest obtained AUC is above $0.9$. The local evaluations of one such expert
tissue classifier is used to augment the results of a heatmap regression model
and additionally recover the exact causal regions at high resolution as a
saliency image suitable for clinical settings.
",statistics
"  As is well known, multivariate Rogers-Szegö polynomials are closely
connected with the partition functions of the $A_{N-1}$ type of Polychronakos
spin chains having long-range interactions. Applying the `freezing trick', here
we derive the partition functions for a class of $BC_N$ type of Polychronakos
spin chains containing supersymmetric analogues of polarized spin reversal
operators and subsequently use those partition functions to obtain novel
multivariate super Rogers-Szegö (SRS) polynomials depending on four types of
variables. We construct the generating functions for such SRS polynomials and
show that these polynomials can be written as some bilinear combinations of the
$A_{N-1}$ type of SRS polynomials. We also use the above mentioned generating
functions to derive a set of recursion relations for the partition functions of
the $BC_N$ type of Polychronakos spin chains involving different numbers of
lattice sites and internal degrees of freedom.
",physics
"  A dual control problem is presented for the optimal stochastic control of a
system governed by partial differential equations. Relationships between the
optimal values of the original and the dual problems are investigated and two
duality theorems are proved. The dual problem serves to provide upper bounds
for the optimal and maximum value of the original one or even to give the
optimal value.
",mathematics
"  For a smooth manifold $M$, possibly with boundary and corners, and a Lie
group $G$, we consider a suitable description of gauge fields in terms of
parallel transport, as groupoid homomorphisms from a certain path groupoid in
$M$ to $G$. Using a cotriangulation $\mathscr{C}$ of $M$, and collections of
finite-dimensional families of paths relative to $\mathscr{C}$, we define a
homotopical equivalence relation of parallel transport maps, leading to the
concept of an extended lattice gauge (ELG) field. A lattice gauge field, as
used in Lattice Gauge Theory, is part of the data contained in an ELG field,
but the latter contains further local topological information sufficient to
reconstruct a principal $G$-bundle on $M$ up to equivalence. The space of ELG
fields of a given pair $(M,\mathscr{C})$ is a covering for the space of fields
in Lattice Gauge Theory, whose connected components parametrize equivalence
classes of principal $G$-bundles on $M$. We give a criterion to determine when
ELG fields over different cotriangulations define equivalent bundles.
",mathematics
"  Autonomic nervous system (ANS) activity is altered in autism spectrum
disorder (ASD). Heart rate variability (HRV) derived from electrocardiogram
(ECG) has been a powerful tool to identify alterations in ANS due to a plethora
of pathophysiological conditions, including psychological ones such as
depression. ECG-derived HRV thus carries a yet to be explored potential to be
used as a diagnostic and follow-up biomarker of ASD. However, few studies have
explored this potential. In a cohort of boys (ages 8 - 11 years) with (n=18)
and without ASD (n=18), we tested a set of linear and nonlinear HRV measures,
including phase rectified signal averaging (PRSA), applied to a segment of ECG
collected under resting conditions for their predictive properties of ASD. We
identified HRV measures derived from time, frequency and geometric
signal-analytical domains which are changed in ASD children relative to peers
without ASD and correlate to psychometric scores (p<0.05 for each). Receiver
operating curves area ranged between 0.71 - 0.74 for each HRV measure. Despite
being a small cohort lacking external validation, these promising preliminary
results warrant larger prospective validation studies.
",quantitative-biology
"  Self-admitted technical debt refers to situations where a software developer
knows that their current implementation is not optimal and indicates this using
a source code comment. In this work, we hypothesize that it is possible to
develop automated techniques to understand a subset of these comments in more
detail, and to propose tool support that can help developers manage
self-admitted technical debt more effectively. Based on a qualitative study of
335 comments indicating self-admitted technical debt, we first identify one
particular class of debt amenable to automated management: ""on-hold""
self-admitted technical debt, i.e., debt which contains a condition to indicate
that a developer is waiting for a certain event or an updated functionality
having been implemented elsewhere. We then design and evaluate an automated
classifier which can automatically identify these ""on-hold"" instances with a
precision of 0.81 as well as detect the specific conditions that developers are
waiting for. Our work presents a first step towards automated tool support that
is able to indicate when certain instances of self-admitted technical debt are
ready to be addressed.
",computer-science
"  We deal with the problem of maintaining a shortest-path tree rooted at some
process r in a network that may be disconnected after topological changes. The
goal is then to maintain a shortest-path tree rooted at r in its connected
component, V\_r, and make all processes of other components detecting that r is
not part of their connected component. We propose, in the composite atomicity
model, a silent self-stabilizing algorithm for this problem working in
semi-anonymous networks, where edges have strictly positive weights. This
algorithm does not require any a priori knowledge about global parameters of
the network. We prove its correctness assuming the distributed unfair daemon,
the most general daemon. Its stabilization time in rounds is at most 3nmax+D,
where nmax is the maximum number of non-root processes in a connected component
and D is the hop-diameter of V\_r. Furthermore, if we additionally assume that
edge weights are positive integers, then it stabilizes in a polynomial number
of steps: namely, we exhibit a bound in O(maxi nmax^3 n), where maxi is the
maximum weight of an edge and n is the number of processes.
",computer-science
"  In visual exploration and analysis of data, determining how to select and
transform the data for visualization is a challenge for data-unfamiliar or
inexperienced users. Our main hypothesis is that for many data sets and common
analysis tasks, there are relatively few ""data slices"" that result in effective
visualizations. By focusing human users on appropriate and suitably transformed
parts of the underlying data sets, these data slices can help the users carry
their task to correct completion.
To verify this hypothesis, we develop a framework that permits us to capture
exemplary data slices for a user task, and to explore and parse
visual-exploration sequences into a format that makes them distinct and easy to
compare. We develop a recommendation system, DataSlicer, that matches a
""currently viewed"" data slice with the most promising ""next effective"" data
slices for the given exploration task. We report the results of controlled
experiments with an implementation of the DataSlicer system, using four common
analytical task types. The experiments demonstrate statistically significant
improvements in accuracy and exploration speed versus users without access to
our system.
",computer-science
"  Using a probabilistic argument we show that the second bounded cohomology of
an acylindrically hyperbolic group $G$ (e.g., a non-elementary hyperbolic or
relatively hyperbolic group, non-exceptional mapping class group, ${\rm
Out}(F_n)$, \dots) embeds via the natural restriction maps into the inverse
limit of the second bounded cohomologies of its virtually free subgroups, and
in fact even into the inverse limit of the second bounded cohomologies of its
hyperbolically embedded virtually free subgroups. This result is new and
non-trivial even in the case where $G$ is a (non-free) hyperbolic group. The
corresponding statement fails in general for the third bounded cohomology, even
for surface groups.
",mathematics
"  Interpretability of deep neural networks is a recently emerging area of
machine learning research targeting a better understanding of how models
perform feature selection and derive their classification decisions. In this
paper, two neural network architectures are trained on spectrogram and raw
waveform data for audio classification tasks on a newly created audio dataset
and layer-wise relevance propagation (LRP), a previously proposed
interpretability method, is applied to investigate the models' feature
selection and decision making. It is demonstrated that the networks are highly
reliant on feature marked as relevant by LRP through systematic manipulation of
the input data. Our results show that by making deep audio classifiers
interpretable, one can analyze and compare the properties and strategies of
different models beyond classification accuracy, which potentially opens up new
ways for model improvements.
",computer-science
"  Theoretically, we recently showed that the scaling relation between the
transition temperature T_c and the superfluid density at zero temperature n_s
(0) might exhibit a parabolic pattern [Scientific Reports 6 (2016) 23863]. It
is significantly different from the linear scaling described by Homes' law,
which is well known as a mean-field result. More recently, Bozovic et al. have
observed such a parabolic scaling in the overdoped copper oxides with a
sufficiently low transition temperature T_c [Nature 536 (2016) 309-311]. They
further point out that this experimental finding is incompatible with the
standard Bardeen-Cooper-Schrieffer (BCS) description. Here we report that if
T_c is sufficiently low, applying the renormalization group approach into the
BCS action at zero temperature will naturally lead to the parabolic scaling.
Our result indicates that when T_c sufficiently approaches zero, quantum
fluctuations will be overwhelmingly amplified so that the mean-field
approximation may break down at zero temperature.
",physics
"  A number of microorganisms leave persistent trails while moving along
surfaces. For single-cell organisms, the trail-mediated self-interaction will
influence its dynamics. It has been discussed recently [Kranz \textit{et al.}
Phys. Rev. Lett. \textbf{117}, 8101 (2016)] that the self-interaction may
localize the organism above a critical coupling $\chi_c$ to the trail. Here we
will derive a generalized active particle model capturing the key features of
the self-interaction and analyze its behavior for smaller couplings $\chi <
\chi_c$. We find that fluctuations in propulsion speed shift the localization
transition to stronger couplings.
",quantitative-biology
"  We present Oncilla robot, a novel mobile, quadruped legged locomotion
machine. This large-cat sized, 5.1 robot is one of a kind of a recent,
bioinspired legged robot class designed with the capability of model-free
locomotion control. Animal legged locomotion in rough terrain is clearly shaped
by sensor feedback systems. Results with Oncilla robot show that agile and
versatile locomotion is possible without sensory signals to some extend, and
tracking becomes robust when feedback control is added (Ajaoolleian 2015). By
incorporating mechanical and control blueprints inspired from animals, and by
observing the resulting robot locomotion characteristics, we aim to understand
the contribution of individual components. Legged robots have a wide mechanical
and control design parameter space, and a unique potential as research tools to
investigate principles of biomechanics and legged locomotion control. But the
hardware and controller design can be a steep initial hurdle for academic
research. To facilitate the easy start and development of legged robots,
Oncilla-robot's blueprints are available through open-source. [...]
",computer-science
"  This article proposes a new way to construct computationally efficient
`wrappers' around fine scale, microscopic, detailed descriptions of dynamical
systems, such as molecular dynamics, to make predictions at the macroscale
`continuum' level. It is often significantly easier to code a microscale
simulator with periodicity: so the challenge addressed here is to develop a
scheme that uses only a given periodic microscale simulator; specifically, one
for atomistic dynamics. Numerical simulations show that applying a suitable
proportional controller within `action regions' of a patch of atomistic
simulation effectively predicts the macroscale transport of heat. Theoretical
analysis establishes that such an approach will generally be effective and
efficient, and also determines good values for the strength of the proportional
controller. This work has the potential to empower systematic analysis and
understanding at a macroscopic system level when only a given microscale
simulator is available.
",mathematics
"  We study the quantum phase transition between a paramagnetic and
ferromagnetic metal in the presence of Rashba spin-orbit coupling in one
dimension. Using bosonization, we analyze the transition by means of
renormalization group, controlled by an $\varepsilon$-expansion around the
upper critical dimension of two. We show that the presence of Rashba spin-orbit
coupling allows for a new nonlinear term in the bosonized action, which
generically leads to a fluctuation driven first-order transition. We further
demonstrate that the Euclidean action of this system maps onto a classical
smectic-A -- C phase transition in a magnetic field in two dimensions. We show
that the smectic transition is second-order and is controlled by a new critical
point.
",physics
"  The global crisis of 2008 provoked a heightened interest among scientists to
study the phenomenon, its propagation and negative consequences. The process of
modelling the spread of a virus is commonly used in epidemiology. Conceptually,
the spread of a disease among a population is similar to the contagion process
in economy. This similarity allows considering the contagion in the world
financial system using the same mathematical model of infection spread that is
often used in epidemiology. Our research focuses on the dynamic behaviour of
contagion spreading in the global financial network. The effect of infection by
a systemic spread of risks in the network of national banking systems of
countries is tested. An optimal control problem is then formulated to simulate
a control that may avoid significant financial losses. The results show that
the proposed approach describes well the reality of the world economy, and
emphasizes the importance of international relations between countries on the
financial stability.
",quantitative-finance
"  We study the spatially homogeneous time dependent solutions and their
bifurcations of the Gray-Scott model. We find the global map of bifurcations by
a combination of rigorous verification of the existence of Takens Bogdanov and
a Bautin bifurcations, in the space of two parameters k and F. With the aid of
numerical continuation of local bifurcation curves we give a global description
of all the possible bifurcations
",mathematics
"  In this article we address the general approach for calculating dynamical
dipole polarizabilities of small quantum systems, based on a sum-over-states
formula involving in principle the entire energy spectrum of the system. We
complement this method by a few-parameter model involving a limited number of
effective transitions, allowing for a compact and accurate representation of
both the isotropic and anisotropic components of the polarizability. We apply
the method to the series of ten heteronuclear molecules composed of two of
($^7$Li,$^{23}$Na,$^{39}$K,$^{87}$Rb,$^{133}$Cs) alkali-metal atoms. We rely on
both up-to-date spectroscopically-determined potential energy curves for the
lowest electronic states, and on our systematic studies of these systems
performed during the last decade for higher excited states and for permanent
and transition dipole moments. Such a compilation is timely for the
continuously growing researches on ultracold polar molecules. Indeed the
knowledge of the dynamic dipole polarizabilities is crucial to model the
optical response of molecules when trapped in optical lattices, and to
determine optimal lattice frequencies ensuring optimal transfer to the absolute
ground state of initially weakly-bound molecules. When they exist, we determine
the so-called ""magic frequencies"" where the ac-Stark shift and thus the viewed
trap depth, is the same for both weakly-bound and ground-state molecules.
",physics
"  Multipartite viruses replicate through a puzzling evolutionary strategy.
Their genome is segmented into two or more parts, and encapsidated in separate
particles that appear to propagate independently. Completing the replication
cycle, however, requires the full genome, so that a persistent infection of a
host requires the concurrent presence of several particles. This represents an
apparent evolutionary drawback of multipartitism, while its advantages remain
unclear. A transition from monopartite to multipartite viral forms has been
described in vitro under conditions of high multiplicity of infection,
suggesting that cooperation between defective mutants is a plausible
evolutionary pathway towards multipartitism. However, it is unknown how the
putative advantages that multipartitism might enjoy affect its epidemiology, or
if an explicit advantage is needed to explain its ecological persistence. To
disentangle which mechanisms might contribute to the rise and fixation of
multipartitism, we here investigate the interaction between viral spreading
dynamics and host population structure. We set up a compartmental model of the
spread of a virus in its different forms and explore its epidemiology using
both analytical and numerical techniques. We uncover that the impact of host
contact structure on spreading dynamics entails a rich phenomenology of
ecological relationships that includes cooperation, competition, and
commensality. Furthermore, we find out that multipartitism might rise to
fixation even in the absence of explicit microscopic advantages. Multipartitism
allows the virus to colonize environments that could not be invaded by the
monopartite form, while homogeneous contacts between hosts facilitate its
spread. We conjecture that there might have been an increase in the diversity
and prevalence of multipartite viral forms concomitantly with the expansion of
agricultural practices.
",quantitative-biology
"  This paper considers channel estimation and uplink achievable rate of the
coarsely quantized massive multiple-input multiple-output (MIMO) system with
radio frequency (RF) impairments. We utilize additive quantization noise model
(AQNM) and extended error vector magnitude (EEVM) model to analyze the impacts
of low-resolution analog-to-digital converters (ADCs) and RF impairments
respectively. We show that hardware impairments cause a nonzero floor on the
channel estimation error, which contraries to the conventional case with ideal
hardware. The maximal-ratio combining (MRC) technique is then used at the
receiver, and an approximate tractable expression for the uplink achievable
rate is derived. The simulation results illustrate the appreciable
compensations between ADCs' resolution and RF impairments. The proposed studies
support the feasibility of equipping economical coarse ADCs and economical
imperfect RF components in practical massive MIMO systems.
",computer-science
"  Quantile estimation is a problem presented in fields such as quality control,
hydrology, and economics. There are different techniques to estimate such
quantiles. Nevertheless, these techniques use an overall fit of the sample when
the quantiles of interest are usually located in the tails of the distribution.
Regression Approach for Quantile Estimation (RAQE) is a method based on
regression techniques and the properties of the empirical distribution to
address this problem. The method was first presented for the problem of
capability analysis. In this paper, a generalization of the method is
presented, extended to the multiple sample scenario, and data from real
examples is used to illustrate the proposed approaches. In addition,
theoretical framework is presented to support the extension for multiple
homogeneous samples and the use of the uncertainty of the estimated
probabilities as a weighting factor in the analysis.
",statistics
"  A general conjecture is stated on the cone of automorphic vector bundles
admitting nonzero global sections on schemes endowed with a smooth, surjective
morphism to a stack of $G$-zips of connected-Hodge-type; such schemes should
include all Hodge-type Shimura varieties with hyperspecial level. We prove our
conjecture for groups of type $A_1^n$, $C_2$ and $\mathbf F_p$-split groups of
type $A_2$ (this includes all Hilbert-Blumenthal varieties and should also
apply to Siegel modular threefolds and Picard modular surfaces). An example is
given to show that our conjecture can fail for zip data not of
connected-Hodge-type.
",mathematics
"  Constrained Markov Decision Process (CMDP) is a natural framework for
reinforcement learning tasks with safety constraints, where agents learn a
policy that maximizes the long-term reward while satisfying the constraints on
the long-term cost. A canonical approach for solving CMDPs is the primal-dual
method which updates parameters in primal and dual spaces in turn. Existing
methods for CMDPs only use on-policy data for dual updates, which results in
sample inefficiency and slow convergence. In this paper, we propose a policy
search method for CMDPs called Accelerated Primal-Dual Optimization (APDO),
which incorporates an off-policy trained dual variable in the dual update
procedure while updating the policy in primal space with on-policy likelihood
ratio gradient. Experimental results on a simulated robot locomotion task show
that APDO achieves better sample efficiency and faster convergence than
state-of-the-art approaches for CMDPs.
",statistics
"  This work proposes the variable exponent Lebesgue modular as a replacement
for the 1-norm in total variation (TV) regularization. It allows the exponent
to vary with spatial location and thus enables users to locally select whether
to preserve edges or smooth intensity variations. In contrast to earlier work
using TV-like methods with variable exponents, the exponent function is here
computed offline as a fixed parameter of the final optimization problem,
resulting in a convex goal functional. The obtained formulas for the convex
conjugate and the proximal operators are simple in structure and can be
evaluated very efficiently, an important property for practical usability.
Numerical results with variable $L^p$ TV prior in denoising and tomography
problems on synthetic data compare favorably to total generalized variation
(TGV) and TV.
",mathematics
"  Two meromorphic functions $f(z)$ and $g(z)$ sharing a small function
$\alpha(z)$ usually is defined in terms of vanishing of the functions
$f-\alpha$ and $g-\alpha$. We argue that it would be better to modify this
definition at the points where $\alpha$ has poles. Related to this issue we
also point out some possible gaps in proofs in the published literature.
",mathematics
"  The ABALONE Photosensor Technology (U.S. Patent 9064678 2015) has the
capability of supplanting the expensive 80 year old Photomultiplier Tube (PMT)
manufacture by providing a modern and cost effective alternative product. An
ABALONE Photosensor comprises only three monolithic glass components, sealed
together by our new thin film adhesive. In 2013, we left one of the early
ABALONE Photosensor prototypes intact for continuous stress testing, and here
we report its long term vacuum integrity. The exceptionally low ion
afterpulsing rate (approximately two orders of magnitude lower than in PMTs)
has been constantly improving. We explain the physical and technological
reasons for this achievement. Due to the cost-effectiveness and the specific
combination of features, including low level of radioactivity, integration into
large-area panels, and robustness, this technology can open new horizons in the
fields of fundamental physics, functional medical imaging, and nuclear
security.
",physics
"  We study the motion of isentropic gas in nozzles. This is a major subject in
fluid dynamics. In fact, the nozzle is utilized to increase the thrust of
rocket engines. Moreover, the nozzle flow is closely related to astrophysics.
These phenomena are governed by the compressible Euler equation, which is one
of crucial equations in inhomogeneous conservation laws.
In this paper, we consider its unsteady flow and devote to proving the global
existence and stability of solutions to the Cauchy problem for the general
nozzle. The theorem has been proved in (Tsuge in Arch. Ration. Mech. Anal.
209:365-400 (2013)). However, this result is limited to small data. Our aim in
the present paper is to remove this restriction, that is, we consider large
data. Although the subject is important in Mathematics, Physics and
engineering, it remained open for a long time. The problem seems to lie in a
bounded estimate of approximate solutions, because we have only method to
investigate the behavior with respect to the time variable. To solve this, we
first introduce a generalized invariant region. Compared with the existing
ones, its upper and lower bounds are extended constants to functions of the
space variable. However, we cannot apply the new invariant region to the
traditional difference method. Therefore, we invent the modified Godunov
scheme. The approximate solutions consist of some functions corresponding to
the upper and lower bounds of the invariant regions. These methods enable us to
investigate the behavior of approximate solutions with respect to the space
variable. The ideas are also applicable to other nonlinear problems involving
similar difficulties.
",mathematics
"  The influence of the surface curvature on the surface tension of small
droplets in equilibrium with a surrounding vapour, or small bubbles in
equilibrium with a surrounding liquid, can be expanded as $\gamma(R) = \gamma_0
+ c_1\gamma_0/R + O(1/R^2)$, where $R = R_\gamma$ is the radius of the surface
of tension and $\gamma_0$ is the surface tension of the planar interface,
corresponding to zero curvature. According to Tolman's law, the first-order
coefficient in this expansion is assumed to be related to the planar limit
$\delta_0$ of the Tolman length, i.e., the difference $\delta = R_\rho -
R_\gamma$ between the equimolar radius and the radius of the surface of
tension, by $c_1 = -2\delta_0$.
We show here that the deduction of Tolman's law from interfacial
thermodynamics relies on an inaccurate application of the Gibbs adsorption
equation to dispersed phases (droplets or bubbles). A revision of the
underlying theory reveals that the adsorption equation needs to be employed in
an alternative manner to that suggested by Tolman. Accordingly, we develop a
generalized Gibbs adsorption equation which consistently takes the size
dependence of interfacial properties into account, and show that from this
equation, a relation between the Tolman length and the influence of the size of
the dispersed phase on the surface tension cannot be deduced, invalidating the
argument which was put forward by Tolman [J. Chem. Phys. 17 (1949) 333].
",physics
"  We give a criterion for the existence of non-commutative crepant resolutions
(NCCR's) for certain toric singularities. In particular we recover Broomhead's
result that a 3-dimensional toric Gorenstein singularity has a NCCR. Our result
also yields the existence of a NCCR for a 4-dimensional toric Gorenstein
singularity which is known to have no toric NCCR.
",mathematics
"  Synthesis of DNA molecules offers unprecedented advances in storage
technology. Yet, the microscopic world in which these molecules reside induces
error patterns that are fundamentally different from their digital
counterparts. Hence, to maintain reliability in reading and writing, new coding
schemes must be developed. In a reading technique called shotgun sequencing, a
long DNA string is read in a sliding window fashion, and a profile vector is
produced. It was recently suggested by Kiah et al. that such a vector can
represent the permutation which is induced by its entries, and hence a
rank-modulation scheme arises. Although this interpretation suggests high error
tolerance, it is unclear which permutations are feasible, and how to produce a
DNA string whose profile vector induces a given permutation. In this paper, by
observing some necessary conditions, an upper bound for the number of feasible
permutations is given. Further, a technique for deciding the feasibility of a
permutation is devised. By using insights from this technique, an algorithm for
producing a considerable number of feasible permutations is given, which
applies to any alphabet size and any window length.
",computer-science
"  We present inferences on the geometry and kinematics of the broad-Hbeta
line-emitting region in four active galactic nuclei monitored as a part of the
fall 2010 reverberation mapping campaign at MDM Observatory led by the Ohio
State University. From modeling the continuum variability and response in
emission-line profile changes as a function of time, we infer the geometry of
the Hbeta- emitting broad line regions to be thick disks that are close to
face-on to the observer with kinematics that are well-described by either
elliptical orbits or inflowing gas. We measure the black hole mass to be log
(MBH) = 7.25 (+/-0.10) for Mrk 335, 7.86 (+0.20, -0.17) for Mrk 1501, 7.84
(+0.14, -0.19) for 3C 120, and 6.92 (+0.24, -0.23) for PG 2130+099. These black
hole mass measurements are not based on a particular assumed value of the
virial scale factor f, allowing us to compute individual f factors for each
target. Our results nearly double the number of targets that have been modeled
in this manner, and investigate the properties of a more diverse sample by
including previously modeled objects. We measure an average scale factor f in
the entire sample to be log10(f) = 0.54 +/- 0.17 when the line dispersion is
used to characterize the line width, which is consistent with values derived
using the normalization of the MBH-sigma relation. We find that the scale
factor f for individual targets is likely correlated with the black hole mass,
inclination angle, and opening angle of the broad line region but we do not
find any correlation with the luminosity.
",physics
"  Shape memory alloys often show a complex hierarchical morphology in the
martensitic state. To understand the formation of this twin-within-twins
microstructure, we examine epitaxial Ni-Mn-Ga films as a model system. In-situ
scanning electron microscopy experiments show beautiful complex twinning
patterns with a number of different mesoscopic twin boundaries and macroscopic
twin boundaries between already twinned regions. We explain the appearance and
geometry of these patterns by constructing an internally twinned martensitic
nucleus, which can take the shape of a diamond or a parallelogram, within the
basic phenomenological theory of martensite. These nucleus contains already the
seeds of different possible mesoscopic twin boundaries. Nucleation and growth
of these nuclei determines the creation of the hierarchical space-filling
martensitic microstructure. This is in contrast to previous approaches to
explain a hierarchical martensitic microstructure. This new picture of creation
and anisotropic, well-oriented growth of twinned martensitic nuclei explains
the morphology and exact geometrical features of our experimentally observed
twins-within-twins microstructure on the meso- and macroscopic scale.
",physics
"  We present a systematic evaluation of JPEG2000 (ISO/IEC 15444) as a transport
data format to enable rapid remote searches for fast transient events as part
of the Deeper Wider Faster program (DWF). DWF uses ~20 telescopes from radio to
gamma-rays to perform simultaneous and rapid-response follow-up searches for
fast transient events on millisecond-to-hours timescales. DWF search demands
have a set of constraints that is becoming common amongst large collaborations.
Here, we focus on the rapid optical data component of DWF led by the Dark
Energy Camera (DECam) at CTIO. Each DECam image has 70 total CCDs saved as a
~1.2 gigabyte FITS file. Near real-time data processing and fast transient
candidate identifications -- in minutes for rapid follow-up triggers on other
telescopes -- requires computational power exceeding what is currently
available on-site at CTIO. In this context, data files need to be transmitted
rapidly to a foreign location for supercomputing post-processing, source
finding, visualization and analysis. This step in the search process poses a
major bottleneck, and reducing the data size helps accommodate faster data
transmission. To maximise our gain in transfer time and still achieve our
science goals, we opt for lossy data compression -- keeping in mind that raw
data is archived and can be evaluated at a later time. We evaluate how lossy
JPEG2000 compression affects the process of finding transients, and find only a
negligible effect for compression ratios up to ~25:1. We also find a linear
relation between compression ratio and the mean estimated data transmission
speed-up factor. Adding highly customized compression and decompression steps
to the science pipeline considerably reduces the transmission time --
validating its introduction to the DWF science pipeline and enabling science
that was otherwise too difficult with current technology.
",physics
"  In this paper, we give a complete characterization of Leavitt path algebras
which are graded $\Sigma $-$V$ rings, that is, rings over which a direct sum of
arbitrary copies of any graded simple module is graded injective. Specifically,
we show that a Leavitt path algebra $L$ over an arbitrary graph $E$ is a graded
$\Sigma $-$V$ ring if and only if it is a subdirect product of matrix rings of
arbitrary size but with finitely many non-zero entries over $K$ or
$K[x,x^{-1}]$ with appropriate matrix gradings. We also obtain a graphical
characterization of such a graded $\Sigma $-$V$ ring $L$% . When the graph $E$
is finite, we show that $L$ is a graded $\Sigma $-$V$ ring $\Longleftrightarrow
L$ is graded directly-finite $\Longleftrightarrow L $ has bounded index of
nilpotence $\Longleftrightarrow $ $L$ is graded semi-simple. Examples show that
the equivalence of these properties in the preceding statement no longer holds
when the graph $E$ is infinite. Following this, we also characterize Leavitt
path algebras $L$ which are non-graded $\Sigma $-$V$ rings. Graded rings which
are graded directly-finite are explored and it is shown that if a Leavitt path
algebra $L$ is a graded $\Sigma$-$V$ ring, then $L$ is always graded
directly-finite. Examples show the subtle differences between graded and
non-graded directly-finite rings. Leavitt path algebras which are graded
directly-finite are shown to be directed unions of graded semisimple rings.
Using this, we give an alternative proof of a theorem of Vaš \cite{V} on
directly-finite Leavitt path algebras.
",mathematics
"  EXor objects are young variables that show episodic variations of brightness
commonly associated to enhanced accretion outbursts. With the aim of
investigating the long-term photometric behaviour of a few EXor sources, we
present here data from the archival plates of the Asiago Observatory, showing
the Orion field where the three EXors V1118, V1143, and NY are located. A total
of 484 plates were investigated, providing a total of more than 1000 magnitudes
for the three stars, which cover a period of about 35 yrs between 1959 to 1993.
We then compared our data with literature data. Apart from a newly discovered
flare-up of V1118, we identify the same outbursts already known, but we provide
two added values: (i) a long-term sampling of the quiescence phase; and (ii)
repeated multi-colour observations (BVRI bands). The former allows us to give a
reliable characterisation of the quiescence, which represents a unique
reference for studies that will analyze future outbursts and the physical
changes induced by these events. The latter is useful for confirming whether
the intermittent increases of brightness are accretion-driven (as in the case
of V1118), or extinction-driven (as in the case of V1143). Accordingly, doubts
arise about the V1143 classification as a pure EXor object. Finally, although
our plates do not separate NY Ori and the star very close to it, they indicate
that this EXor did not undergo any major outbursts during our 40 yrs of
monitoring.
",physics
"  Developing applications for interactive space is different from developing
cross-platform applications for personal computing. Input, output, and
architectural variations in each interactive space introduce big overhead in
terms of cost and time for developing, deploying and maintaining applications
for interactive spaces. Often, these applications become on-off experience tied
to the deployed spaces. To alleviate this problem and enable rapid responsive
space design applications similar to responsive web design, we present CELIO
application development framework for interactive spaces. The framework is
micro services based and neatly decouples application and design specifications
from hardware and architecture specifications of an interactive space. In this
paper, we describe this framework and its implementation details. Also, we
briefly discuss the use cases developed using this framework.
",computer-science
"  This paper examines the noise handling properties of three of the most widely
used algorithms for numerically inverting the Laplace Transform. After
examining the genesis of the algorithms, the regularization properties are
evaluated through a series of standard test functions in which noise is added
to the inverse transform. Comparisons are then made with the exact data. Our
main finding is that the Talbot inversion algorithm is very good at handling
noisy data and performs much better than the Fourier Series and Stehfest
numerical inversion schemes as outlined in this paper. This offers a
considerable advantage for it's use in inverting the Laplace Transform when
seeking numerical solutions to time dependent differential equations.
",mathematics
"  We demonstrate explicitly the correspondence between all protected operators
in a 2+1 dimensional non-supersymmetric bosonization duality in the
non-relativistic limit. Roughly speaking we consider $SU(N)$ Chern-Simons field
theory at level $k$ with $N_f$ flavours of fundamental boson, and match its
chiral sector to that of a $SU(k)$ theory at level $N$ with $N_f$ fundamental
fermions. We present the matching at the level of indices and individual
operators, seeing the mechanism of failure for $N_f > N$, and point out that
the non-relativistic setting is a particularly friendly setting for studying
interesting questions about such dualities.
",physics
"  We consider capillary condensation transitions occurring in open slits of
width $L$ and finite height $H$ immersed in a reservoir of vapour. In this case
the pressure at which condensation occurs is closer to saturation compared to
that occurring in an infinite slit ($H=\infty$) due to the presence of two
menisci which are pinned near the open ends. Using macroscopic arguments we
derive a modified Kelvin equation for the pressure, $p_{cc}(L;H)$, at which
condensation occurs and show that the two menisci are characterised by an edge
contact angle $\theta_e$ which is always larger than the equilibrium contact
angle $\theta$, only equal to it in the limit of macroscopic $H$. For walls
which are completely wet ($\theta=0$) the edge contact angle depends only on
the aspect ratio of the capillary and is well described by $\theta_e\approx
\sqrt{\pi L/2H}$ for large $H$. Similar results apply for condensation in
cylindrical pores of finite length. We have tested these predictions against
numerical results obtained using a microscopic density functional model where
the presence of an edge contact angle characterising the shape of the menisci
is clearly visible from the density profiles. Below the wetting temperature
$T_w$ we find very good agreement for slit pores of widths of just a few tens
of molecular diameters while above $T_w$ the modified Kelvin equation only
becomes accurate for much larger systems.
",physics
"  This paper presents a new way to design a Fuzzy Terminal Iterative Learning
Control (TILC) to control the heater temperature setpoints of a thermoforming
machine. This fuzzy TILC is based on the inverse of a fuzzy model of this
machine, and is built from experimental (or simulation) data with kriging
interpolation. The Fuzzy Inference System usually used for a fuzzy model is the
zero order Takagi Sugeno Kwan system (constant consequents). In this paper, the
1st order Takagi Sugeno Kwan system is used, with the fuzzy model rules
expressed using matrices. This makes the inversion of the fuzzy model much
easier than the inversion of the fuzzy model based on the TSK of order 0. Based
on simulation results, the proposed fuzzy TILC seems able to give a very good
initial guess as to the heater temperature setpoints, making it possible to
have almost no wastage of plastic sheets. Simulation results show the
effectiveness of the fuzzy TILC compared to a crisp TILC, even though the fuzzy
controller is based on a fuzzy model built from noisy data.
",computer-science
"  Incommensurately modulated twin structure of nyerereite Na1.64K0.36Ca(CO3)2
has been first determined in the (3+1)D symmetry group Cmcm({\alpha}00)00s with
modulation vector q = 0.383a*. Unit-cell values are a = 5.062(1), b = 8.790(1),
c = 12.744(1) {\AA}. Three orthorhombic components are related by threefold
rotation about [001]. Discontinuous crenel functions are used to describe
occupation modulation of Ca and some CO3 groups. Strong displacive modulation
of the oxygen atoms in vertexes of such CO3 groups is described using
x-harmonics in crenel intervals. The Na, K atoms occupy mixed sites whose
occupation modulation is described by two ways using either complementary
harmonic functions or crenels. The nyerereite structure has been compared both
with commensurately modulated structure of K-free Na2Ca(CO3)2 and with widely
known incommensurately modulated structure of {\gamma}-Na2CO3.
",physics
"  The finite-difference time-domain (FDTD) method is a well established method
for solving the time evolution of Maxwell's equations. Unfortunately the scheme
introduces numerical dispersion and therefore phase and group velocities which
deviate from the correct values. The solution to Maxwell's equations in more
than one dimension results in non-physical predictions such as numerical
dispersion or numerical Cherenkov radiation emitted by a relativistic electron
beam propagating in vacuum.
Improved solvers, which keep the staggered Yee-type grid for electric and
magnetic fields, generally modify the spatial derivative operator in the
Maxwell-Faraday equation by increasing the computational stencil. These
modified solvers can be characterized by different sets of coefficients,
leading to different dispersion properties. In this work we introduce a norm
function to rewrite the choice of coefficients into a minimization problem. We
solve this problem numerically and show that the minimization procedure leads
to phase and group velocities that are considerably closer to $c$ as compared
to schemes with manually set coefficients available in the literature.
Depending on a specific problem at hand (e.g. electron beam propagation in
plasma, high-order harmonic generation from plasma surfaces, etc), the norm
function can be chosen accordingly, for example, to minimize the numerical
dispersion in a certain given propagation direction. Particle-in-cell
simulations of an electron beam propagating in vacuum using our solver are
provided.
",physics
"  In display advertising, users' online ad experiences are important for the
advertising effectiveness. However, users have not been well accommodated in
real-time bidding (RTB). This further influences their site visits and
perception of the displayed banner ads. In this paper, we propose a novel
computational framework which brings multimedia metrics, like the contextual
relevance, the visual saliency and the ad memorability into RTB to improve the
users' ad experiences as well as maintain the benefits of the publisher and the
advertiser. We aim at developing a vigorous ecosystem by optimizing the
trade-offs among all stakeholders. The framework considers the scenario of a
webpage with multiple ad slots. Our experimental results show that the benefits
of the advertiser and the user can be significantly improved if the publisher
would slightly sacrifice his short-term revenue. The improved benefits will
increase the advertising requests (demand) and the site visits (supply), which
can further boost the publisher's revenue in the long run.
",computer-science
"  In this paper, we examine the statistical soundness of comparative
assessments within the field of recommender systems in terms of reliability and
human uncertainty. From a controlled experiment, we get the insight that users
provide different ratings on same items when repeatedly asked. This volatility
of user ratings justifies the assumption of using probability densities instead
of single rating scores. As a consequence, the well-known accuracy metrics
(e.g. MAE, MSE, RMSE) yield a density themselves that emerges from convolution
of all rating densities. When two different systems produce different RMSE
distributions with significant intersection, then there exists a probability of
error for each possible ranking. As an application, we examine possible ranking
errors of the Netflix Prize. We are able to show that all top rankings are more
or less subject to high probabilities of error and that some rankings may be
deemed to be caused by mere chance rather than system quality.
",computer-science
"  In this communication we present a detailed study of the effect of chemical
disorder on the optical response of Ni$_{1-x}$Pt$_x$ (0.1$\leq$ x $\leq$0.75)
and Ni$_{3(1-x)/3}$Pt$_x$ (0.1$\leq$ x $\leq$0.3). We shall propose a formalism
which will combine a Kubo-Greenwood approach with a DFT based tight-binding
linear muffin-tin orbitals (TB-LMTO) basis and augmented space recursion (ASR)
technique to explicitly incorporate the effect of disorder. We show that
chemical disorder has a large impact on optical response of Ni-Pt systems. In
ordered Ni-Pt alloys, the optical conductivity peaks are sharp. But as we
switch on chemical disorder, the UV peak becomes broadened and its position as
a function of composition and disorder carries the signature of a phase
transition from NiPt to Ni$_3$Pt with decreasing Pt concentration.
Quantitatively this agrees well with Massalski's Ni-Pt phase diagram
\cite{massal}. Both ordered NiPt and Ni$_3$Pt have an optical conductivity
transition at 4.12 eV. But disordered NiPt has an optical conductivity
transition at 3.93 eV. If we decrease the Pt content, it results a chemical
phase transition from NiPt to Ni$_3$Pt and shifts the peak position by 1.67 eV
to the ultraviolet range at 5.6 eV. There is a significant broadening of UV
peak with increasing Pt content due to enhancement of 3d(Ni)-5d(Pt) bonding.
Chemical disorder enhances the optical response of NiPt alloys nearly one order
of magnitude. Our study also shows the fragile magnetic effect on optical
response of disordered Ni$_{1-x}$Pt$_x$ (0.4$<$ x $<$0.6) binary alloys. Our
theoretical predictions agree more than reasonably well with both earlier
experimental as well as theoretical investigations.
",physics
"  Let $(X, T^{1,0}X)$ be a compact connected orientable CR manifold of
dimension $2n+1$ with non-degenerate Levi curvature. Assume that $X$ admits a
connected compact Lie group action $G$. Under certain natural assumptions about
the group action $G$, we show that the $G$-invariant Szegö kernel for $(0,q)$
forms is a complex Fourier integral operator, smoothing away $\mu^{-1}(0)$ and
there is a precise description of the singularity near $\mu^{-1}(0)$, where
$\mu$ denotes the CR moment map. We apply our result to the case when $X$
admits a transversal CR $S^1$ action and deduce an asymptotic expansion for the
$m$-th Fourier component of the $G$-invariant Szegö kernel for $(0,q)$ forms
as $m \to+\infty$. As an application, we show that if $m$ large enough,
quantization commutes with reduction.
",mathematics
"  We consider the global consensus problem for multi-agent systems with input
saturation over digraphs. Under a mild connectivity condition that the
underlying digraph has a directed spanning tree, we use Lyapunov methods to
show that the widely used distributed consensus protocol, which solves the
consensus problem for the case without input saturation constraints, also
solves the global consensus problem for the case with input saturation
constraints. In order to reduce the overall need of communication and system
updates, we then propose a distributed event-triggered control law. Global
consensus is still realized and Zeno behavior is excluded. Numerical
simulations are provided to illustrate the effectiveness of the theoretical
results.
",mathematics
"  Water plays a major role in bio-systems, greatly contributing to determine
their structure, stability and even function. It is well know, for instance,
that proteins require a minimum amount of water to be functionally active.
Since the biological functions of proteins involve changes of conformation, and
sometimes chemical reactions, it is natural to expect a connection of these
functions with dynamical properties of the coupled system of proteins and their
hydration water. However, despite many years of intensive research, the
detailed nature of protein - hydration water interactions, and their effect on
the biochemical activity of proteins through peculiar dynamical effects, is
still partly unknown. In particular, models proposed so far, fail to explain
the full set of experimental data. The well-accepted 'protein dynamical
transition' scenario is based on perfect coupling between the dynamics of
proteins and of their hydration water, which has never been confuted
experimentally. We present high-energy resolution elastic neutron scattering
measurements of the atomistic dynamics of the model protein, lysozyme, in water
that were carried out on the IN16B spectrometer at the Institut Laue-Langevin
in Grenoble, France. These show for the first time that the dynamics of
proteins and of their hydration water are actually de-coupled. This important
result militates against the well-accepted scenario, and requires a new model
to link protein dynamics to the dynamics of its hydration water, and, in turn,
to biochemical function.
",physics
"  In biology, there are several questions that translate to combinatorial
search. For example, vesicle traffic systems that move cargo within eukaryotic
cells have been proposed to exhibit several graph properties such as three
connectivity. These properties are consequences of underlying biophysical
constraints. A natural question for biologists is: what are the possible
networks for various combinations of those properties? In this paper, we
present novel SMT based encodings of the properties over vesicle traffic
systems and a tool that searches for the networks that satisfies the properties
using SMT solvers. In our experiments, we show that our tool can search for
networks of sizes that are considered to be relevant by biologists.
",computer-science
"  The Casimir free energy of dielectric films, both free-standing in vacuum and
deposited on metallic or dielectric plates, is investigated. It is shown that
the values of the free energy depend considerably on whether the calculation
approach used neglects or takes into account the dc conductivity of film
material. We demonstrate that there are the material-dependent and universal
classical limits in the former and latter cases, respectively. The analytic
behavior of the Casimir free energy and entropy for a free-standing dielectric
film at low temperature in found. According to our results, the Casimir entropy
goes to zero when the temperature vanishes if the calculation approach with
neglected dc conductivity of a film is employed. If the dc conductivity is
taken into account, the Casimir entropy takes the positive value at zero
temperature, depending on the parameters of a film, i.e., the Nernst heat
theorem is violated. By considering the Casimir free energy of silica and
sapphire films deposited on a Au plate in the framework of two calculation
approaches, we argue that physically correct values are obtained by
disregarding the role of dc conductivity. A comparison with the well known
results for the configuration of two parallel plates is made. Finally, we
compute the Casimir free energy of silica, sapphire and Ge films deposited on
high-resistivity Si plates of different thicknesses and demonstrate that it can
be positive, negative and equal to zero. Possible applications of the obtained
results to thin films used in microelectronics are discussed.
",physics
"  We consider the 3-point blow-up of the manifold $ (S^2 \times S^2, \sigma
\oplus \sigma)$ where $\sigma$ is the standard symplectic form which gives area
1 to the sphere $S^2$, and study its group of symplectomorphisms $\rm{Symp} (
S^2 \times S^2 \#\, 3\overline{\mathbb C\mathbb P}\,\!^2, \omega)$. So far, the
monotone case was studied by J. Evans and he proved that this group is
contractible. Moreover, J. Li, T. J. Li and W. Wu showed that the group
Symp$_{h}(S^2 \times S^2 \#\, 3\overline{ \mathbb C\mathbb P}\,\!^2,\omega) $
of symplectomorphisms that act trivially on homology is always connected and
recently they also computed its fundamental group. We describe, in full detail,
the rational homotopy Lie algebra of this group.
We show that some particular circle actions contained in the
symplectomorphism group generate its full topology. More precisely, they give
the generators of the homotopy graded Lie algebra of $\rm{Symp} (S^2 \times S^2
\#\, 3\overline{ \mathbb C\mathbb P}\,\!^2, \omega)$. Our study depends on
Karshon's classification of Hamiltonian circle actions and the inflation
technique introduced by Lalonde-McDuff. As an application, we deduce the rank
of the homotopy groups of $\rm{Symp}({\mathbb C\mathbb P}^2 \#\,
5\overline{\mathbb C\mathbb P}\,\!^2, \tilde \omega)$, in the case of small
blow-ups.
",mathematics
"  Topologically protected superfluid phases of $^3$He allow one to simulate
many important aspects of relativistic quantum field theories and quantum
gravity in condensed matter. Here we discuss a topological Lifshitz transition
of the effective quantum vacuum in which the determinant of the tetrad field
changes sign through a crossing to a vacuum state with a degenerate fermionic
metric. Such a transition is realized in polar distorted superfluid $^3$He-A in
terms of the effective tetrad fields emerging in the vicinity of the superfluid
gap nodes: the tetrads of the Weyl points in the chiral A-phase of $^3$He and
the degenerate tetrad in the vicinity of a Dirac nodal line in the polar phase
of $^3$He. The continuous phase transition from the $A$-phase to the polar
phase, i.e. in the transition from the Weyl nodes to the Dirac nodal line and
back, allows one to follow the behavior of the fermionic and bosonic effective
actions when the sign of the tetrad determinant changes, and the effective
chiral space-time transforms to anti-chiral ""anti-spacetime"". This condensed
matter realization demonstrates that while the original fermionic action is
analytic across the transition, the effective action for the orbital degrees of
freedom (pseudo-EM) fields and gravity have non-analytic behavior. In
particular, the action for the pseudo-EM field in the vacuum with Weyl fermions
(A-phase) contains the modulus of the tetrad determinant. In the vacuum with
the degenerate metric (polar phase) the nodal line is effectively a family of
$2+1$d Dirac fermion patches, which leads to a non-analytic $(B^2-E^2)^{3/4}$
QED action in the vicinity of the Dirac line.
",physics
"  It has been suggested that adversarial examples cause deep learning models to
make incorrect predictions with high confidence. In this work, we take the
opposite stance: an overly confident model is more likely to be vulnerable to
adversarial examples. This work is one of the most proactive approaches taken
to date, as we link robustness with non-calibrated model confidence on noisy
images, providing a data-augmentation-free path forward. The adversarial
examples phenomenon is most easily explained by the trend of increasing
non-regularized model capacity, while the diversity and number of samples in
common datasets has remained flat. Test accuracy has incorrectly been
associated with true generalization performance, ignoring that training and
test splits are often extremely similar in terms of the overall representation
space. The transferability property of adversarial examples was previously used
as evidence against overfitting arguments, a perceived random effect, but
overfitting is not always random.
",statistics
"  We derive the finite temperature Keldysh response theory for interacting
fermions in the presence of quenched disorder, as applicable to any of the 10
Altland-Zirnbauer classes in an Anderson delocalized phase with at least a U(1)
continuous symmetry. In this formulation of the interacting Finkel'stein
nonlinear sigma model, the statistics of one-body wave functions are encoded by
the constrained matrix field, while physical correlations follow from the
hydrodynamic density or spin response field, which decouples the interactions.
Integrating out the matrix field first, we obtain weak (anti)localization and
Altshuler-Aronov quantum conductance corrections from the hydrodynamic response
function. This procedure automatically incorporates the correct infrared
physics, and in particular gives the Altshuler-Aronov-Khmelnitsky (AAK)
equations for dephasing of weak (anti)localization due to electron-electron
collisions. We explicate the method by deriving known quantum corrections in
two dimensions for the symplectic metal class AII, as well as the spin-SU(2)
invariant superconductor classes C and CI. We show that conductance corrections
due to the special modes at zero energy in nonstandard classes are
automatically cut off by temperature, as previously expected, while the
Wigner-Dyson class Cooperon modes that persist to all energies are cut by
dephasing. We also show that for short-ranged interactions, the standard
self-consistent solution for the dephasing rate is equivalent to a diagrammatic
summation via the self-consistent Born approximation. This should be compared
to the AAK solution for long-ranged Coulomb interactions, which exploits the
Markovian noise correlations induced by thermal fluctuations of the
electromagnetic field. We discuss prospects for exploring the many-body
localization transition from the ergodic side as a dephasing catastrophe in
short-range interacting models.
",physics
"  We present a slow control system to gather all relevant environment
information necessary to effectively and reliably run an HPC (High Performance
Computing) system at a high value over price ratio. The scalable and reliable
overall concept is presented as well as a newly developed hardware device for
sensor read out. This device incorporates a Raspberry Pi, an Arduino and PoE
(Power over Ethernet) functionality in a compact form factor. The system is in
use at the 2 PFLOPS cluster of the Johannes Gutenberg-University and
Helmholtz-Institute in Mainz.
",computer-science
"  We consider a system of $R$ cubic forms in $n$ variables, with integer
coefficients, which define a smooth complete intersection in projective space.
Provided $n\geq 25R$, we prove an asymptotic formula for the number of integer
points in an expanding box at which these forms simultaneously vanish. In
particular we can handle systems of forms in $O(R)$ variables, previous work
having required that $n \gg R^2$. One conjectures that $n \geq 6R+1$ should be
sufficient. We reduce the problem to an upper bound for the number of solutions
to a certain auxiliary inequality. To prove this bound we adapt a method of
Davenport.
",mathematics
"  We study the loss of coherence of electrochemical oscillations on meso- and
nanosized electrodes with numeric simulations of the electrochemical master
equation for a prototypical electrochemical oscillator, the hydrogen peroxide
reduction on Pt electrodes in the presence of halides. On nanoelectrodes, the
electrode potential changes whenever a stochastic electron-transfer event takes
place. Electrochemical reaction rate coefficients depend exponentially on the
electrode potential and become thus fluctuating quantities as well. Therefore,
also the transition rates between system states become time-dependent which
constitutes a fundamental difference to purely chemical nanoscale oscillators.
Three implications are demonstrated: (a) oscillations and steady states shift
in phase space with decreasing system size, thereby also decreasing
considerably the oscillating parameter regions; (b) the minimal number of
molecules necessary to support correlated oscillations is more than 10 times as
large as for nanoscale chemical oscillators; (c) the relation between
correlation time and variance of the period of the oscillations predicted for
chemical oscillators in the weak noise limit is only fulfilled in a very
restricted parameter range for the electrochemical nano-oscillator.
",physics
"  Genome-wide chromosome conformation capture techniques such as Hi-C enable
the generation of 3D genome contact maps and offer new pathways toward
understanding the spatial organization of genome. One specific feature of the
3D organization is known as topologically associating domains (TADs), which are
densely interacting, contiguous chromatin regions playing important roles in
regulating gene expression. A few algorithms have been proposed to detect TADs.
In particular, the structure of Hi-C data naturally inspires application of
community detection methods. However, one of the drawbacks of community
detection is that most methods take exchangeability of the nodes in the network
for granted; whereas the nodes in this case, i.e. the positions on the
chromosomes, are not exchangeable. We propose a network model for detecting
TADs using Hi-C data that takes into account this non-exchangeability. In
addition, our model explicitly makes use of cell-type specific CTCF binding
sites as biological covariates and can be used to identify conserved TADs
across multiple cell types. The model leads to a likelihood objective that can
be efficiently optimized via relaxation. We also prove that when suitably
initialized, this model finds the underlying TAD structure with high
probability. Using simulated data, we show the advantages of our method and the
caveats of popular community detection methods, such as spectral clustering, in
this application. Applying our method to real Hi-C data, we demonstrate the
domains identified have desirable epigenetic features and compare them across
different cell types. The code is available upon request.
",statistics
"  All possible removals of $n=5$ nodes from networks of size $N=100$ are
performed in order to find the optimal set of nodes which fragments the
original network into the smallest largest connected component. The resulting
attacks are ordered according to the size of the largest connected component
and compared with the state of the art methods of network attacks. We chose
attacks of size $5$ on relative small networks of size $100$ because the number
of all possible attacks, ${100}\choose{5}$ $\approx 10^8$, is at the verge of
the possible to compute with the available standard computers. Besides, we
applied the procedure in a series of networks with controlled and varied
modularity, comparing the resulting statistics with the effect of removing the
same amount of vertices, according to the known most efficient disruption
strategies, i.e., High Betweenness Adaptive attack (HBA), Collective Index
attack (CI), and Modular Based Attack (MBA). Results show that modularity has
an inverse relation with robustness, with $Q_c \approx 0.7$ being the critical
value. For modularities lower than $Q_c$, all heuristic method gives mostly the
same results than with random attacks, while for bigger $Q$, networks are less
robust and highly vulnerable to malicious attacks.
",computer-science
"  We consider estimating the ""de facto"" or effectiveness estimand in a
randomised placebo-controlled or standard-of-care-controlled drug trial with
quantitative outcome, where participants who discontinue an investigational
treatment are not followed up thereafter. Carpenter et al (2013) proposed
reference-based imputation methods which use a reference arm to inform the
distribution of post-discontinuation outcomes and hence to inform an imputation
model. However, the reference-based imputation methods were not formally
justified. We present a causal model which makes an explicit assumption in a
potential outcomes framework about the maintained causal effect of treatment
after discontinuation. We show that the ""jump to reference"", ""copy reference""
and ""copy increments in reference"" reference-based imputation methods, with the
control arm as the reference arm, are special cases of the causal model with
specific assumptions about the causal treatment effect. Results from simulation
studies are presented. We also show that the causal model provides a flexible
and transparent framework for a tipping point sensitivity analysis in which we
vary the assumptions made about the causal effect of discontinued treatment. We
illustrate the approach with data from two longitudinal clinical trials.
",statistics
"  We consider a problem, which we call secure grouping, of dividing a number of
parties into some subsets (groups) in the following manner: Each party has to
know the other members of his/her group, while he/she may not know anything
about how the remaining parties are divided (except for certain public
predetermined constraints, such as the number of parties in each group). In
this paper, we construct an information-theoretically secure protocol using a
deck of physical cards to solve the problem, which is jointly executable by the
parties themselves without a trusted third party. Despite the non-triviality
and the potential usefulness of the secure grouping, our proposed protocol is
fairly simple to describe and execute. Our protocol is based on algebraic
properties of conjugate permutations. A key ingredient of our protocol is our
new techniques to apply multiplication and inverse operations to hidden
permutations (i.e., those encoded by using face-down cards), which would be of
independent interest and would have various potential applications.
",computer-science
"  Can we make a famous rap singer like Eminem sing whatever our favorite song?
Singing style transfer attempts to make this possible, by replacing the vocal
of a song from the source singer to the target singer. This paper presents a
method that learns from unpaired data for singing style transfer using
generative adversarial networks.
",computer-science
"  We present in detail the convolutional neural network used in our previous
work to detect cosmic strings in cosmic microwave background (CMB) temperature
anisotropy maps. By training this neural network on numerically generated CMB
temperature maps, with and without cosmic strings, the network can produce
prediction maps that locate the position of the cosmic strings and provide a
probabilistic estimate of the value of the string tension $G\mu$. Supplying
noiseless simulations of CMB maps with arcmin resolution to the network
resulted in the accurate determination both of string locations and string
tension for sky maps having strings with string tension as low as
$G\mu=5\times10^{-9}$. The code is publicly available online. Though we trained
the network with a long straight string toy model, we show the network performs
well with realistic Nambu-Goto simulations.
",physics
"  In view of a resurgence of concern about the measurement problem, it is
pointed out that the Relativistic Transactional Interpretation (RTI) remedies
issues previously considered as drawbacks or refutations of the original TI.
Specifically, once one takes into account relativistic processes that are not
representable at the non-relativistic level (such as particle creation and
annihilation, and virtual propagation), absorption is quantitatively defined in
unambiguous physical terms. In addition, specifics of the relativistic
transactional model demonstrate that the Maudlin `contingent absorber'
challenge to the original TI cannot even be mounted: basic features of
established relativistic field theories (in particular, the asymmetry between
field sources and the bosonic fields, and the fact that slow-moving bound
states, such as atoms, are not offer waves) dictate that the `slow-moving offer
wave' required for the challenge scenario cannot exist. It is concluded that
issues previously considered obstacles for TI are no longer legitimately viewed
as such, and that reconsideration of the transactional picture is warranted in
connection with solving the measurement problem.
",physics
"  The practice of evidence-based medicine (EBM) urges medical practitioners to
utilise the latest research evidence when making clinical decisions. Because of
the massive and growing volume of published research on various medical topics,
practitioners often find themselves overloaded with information. As such,
natural language processing research has recently commenced exploring
techniques for performing medical domain-specific automated text summarisation
(ATS) techniques-- targeted towards the task of condensing large medical texts.
However, the development of effective summarisation techniques for this task
requires cross-domain knowledge. We present a survey of EBM, the
domain-specific needs for EBM, automated summarisation techniques, and how they
have been applied hitherto. We envision that this survey will serve as a first
resource for the development of future operational text summarisation
techniques for EBM.
",computer-science
"  This note investigates the stability of both linear and nonlinear switched
systems with average dwell time. Two new analysis methods are proposed.
Different from existing approaches, the proposed methods take into account the
sequence in which the subsystems are switched. Depending on the predecessor or
successor subsystems to be considered, sequence-based average preceding dwell
time (SBAPDT) and sequence-based average subsequence dwell time (SBASDT)
approaches are proposed and discussed for both continuous and discrete time
systems. These proposed methods, when considering the switch sequence, have the
potential to further reduce the conservativeness of the existing approaches. A
comparative numerical example is also given to demonstrate the advantages of
the proposed approaches.
",computer-science
"  The spin-1/2 triangular lattice antiferromagnet YbMgGaO$_{4}$ has attracted
recent attention as a quantum spin-liquid candidate with the possible presence
of off-diagonal anisotropic exchange interactions induced by spin-orbit
coupling. Whether a quantum spin-liquid is stabilized or not depends on the
interplay of various exchange interactions with chemical disorder that is
inherent to the layered structure of the compound. We combine time-domain
terahertz spectroscopy and inelastic neutron scattering measurements in the
field polarized state of YbMgGaO$_{4}$ to obtain better microscopic insights on
its exchange interactions. Terahertz spectroscopy in this fashion functions as
high-field electron spin resonance and probes the spin-wave excitations at the
Brillouin zone center, ideally complementing neutron scattering. A global
spin-wave fit to all our spectroscopic data at fields over 4T, informed by the
analysis of the terahertz spectroscopy linewidths, yields stringent constraints
on $g$-factors and exchange interactions. Our results paint YbMgGaO$_{4}$ as an
easy-plane XXZ antiferromagnet with the combined and necessary presence of
sub-leading next-nearest neighbor and weak anisotropic off-diagonal
nearest-neighbor interactions. Moreover, the obtained $g$-factors are
substantially different from previous reports. This works establishes the
hierarchy of exchange interactions in YbMgGaO$_{4}$ from high-field data alone
and thus strongly constrains possible mechanisms responsible for the observed
spin-liquid phenomenology.
",physics
"  We introduce right generating sets, Cayley graphs, growth functions, types
and rates, and isoperimetric constants for left homogeneous spaces equipped
with coordinate systems; characterise right amenable finitely right generated
left homogeneous spaces with finite stabilisers as those whose isoperimetric
constant is $0$; and prove that finitely right generated left homogeneous
spaces with finite stabilisers of sub-exponential growth are right amenable, in
particular, quotient sets of groups of sub-exponential growth by finite
subgroups are right amenable.
",mathematics
"  In recent years, Deep Reinforcement Learning has made impressive advances in
solving several important benchmark problems for sequential decision making.
Many control applications use a generic multilayer perceptron (MLP) for
non-vision parts of the policy network. In this work, we propose a new neural
network architecture for the policy network representation that is simple yet
effective. The proposed Structured Control Net (SCN) splits the generic MLP
into two separate sub-modules: a nonlinear control module and a linear control
module. Intuitively, the nonlinear control is for forward-looking and global
control, while the linear control stabilizes the local dynamics around the
residual of global control. We hypothesize that this will bring together the
benefits of both linear and nonlinear policies: improve training sample
efficiency, final episodic reward, and generalization of learned policy, while
requiring a smaller network and being generally applicable to different
training methods. We validated our hypothesis with competitive results on
simulations from OpenAI MuJoCo, Roboschool, Atari, and a custom 2D urban
driving environment, with various ablation and generalization tests, trained
with multiple black-box and policy gradient training methods. The proposed
architecture has the potential to improve upon broader control tasks by
incorporating problem specific priors into the architecture. As a case study,
we demonstrate much improved performance for locomotion tasks by emulating the
biological central pattern generators (CPGs) as the nonlinear part of the
architecture.
",computer-science
"  We present a first-principles-based many-body typical medium dynamical
cluster approximation method for characterizing electron localization in
disordered structures. This method applied to monolayer hexagonal boron nitride
shows that the presence of a boron vacancies could turn this wide-gap insulator
into a correlated metal. Depending on the strength of the electron
interactions, these calculations suggest that conduction could be obtained at a
boron vacancy concentration as low as $1.0\%$. We also explore the distribution
of the local density of states, a fingerprint of spatial variations, which
allows localized and delocalized states to be distinguished. The presented
method enables the study of disorder-driven insulator-metal transitions not
only in $h$-BN but also in other physical materials.
",physics
"  Muon-spin rotation data collected at ambient pressure ($p$) and at $p=2.42$
GPa in MnP were analyzed to check their consistency with various low- and
high-pressure magnetic structures reported in the literature. Our analysis
confirms that in MnP the low-temperature and low-pressure helimagnetic phase is
characterised by an increased value of the average magnetic moment compared to
the high-temperature ferromagnetic phase. An elliptical double-helical
structure with a propagation vector ${\bf Q}=(0,0,0.117)$, an $a-$axis moment
elongated by approximately 18% and an additional tilt of the rotation plane
towards $c-$direction by $\simeq 4-8^{\rm o}$ leads to a good agreement between
the theory and the experiment. The analysis of the high-pressure $\mu$SR data
reveals that the new magnetic order appearing for pressures exceeding $1.5$ GPa
can not be described by keeping the propagation vector ${\bf Q} \parallel c$.
Even the extreme case -- decoupling the double-helical structure into four
individual helices -- remains inconsistent with the experiment. It is shown
that the high-pressure magnetic phase which is a precursor of superconductivity
is an incommensurate helical state with ${\bf Q} \parallel b$.
",physics
"  We develop a new class of path transformations for one-dimensional diffusions
that are tailored to alter their long-run behaviour from transient to recurrent
or vice versa. This immediately leads to a formula for the distribution of the
first exit times of diffusions, which is recently characterised by Karatzas and
Ruf \cite{KR} as the minimal solution of an appropriate Cauchy problem under
more stringent conditions. A particular limit of these transformations also
turn out to be instrumental in characterising the stochastic solutions of
Cauchy problems defined by the generators of strict local martingales, which
are well-known for not having unique solutions even when one restricts
solutions to have linear growth. Using an appropriate diffusion transformation
we show that the aforementioned stochastic solution can be written in terms of
the unique classical solution of an {\em alternative} Cauchy problem with
suitable boundary conditions. This in particular resolves the long-standing
issue of non-uniqueness with the Black-Scholes equations in derivative pricing
in the presence of {\em bubbles}. Finally, we use these path transformations to
propose a unified framework for solving explicitly the optimal stopping problem
for one-dimensional diffusions with discounting, which in particular is
relevant for the pricing and the computation of optimal exercise boundaries of
perpetual American options.
",mathematics
"  I review the three principal methods to assign meaning to recursion in
process algebra: the denotational, the operational and the algebraic approach,
and I extend the latter to unguarded recursion.
",computer-science
"  We consider Friedlander's wave equation in two space dimensions in the
half-space x > 0 with the boundary condition u(x,y,t)=0 when x=0. For a
Gaussian beam w(x,y,t;k) concentrated on a ray path that is tangent to x=0 at
(x,y,t)=(0,0,0) we calculate the ""reflected"" wave z(x,y,t;k) in t > 0 such that
w(x,y,t;k)+z(x,y,t;k) satisfies Friedlander's wave equation and vanishes on
x=0. These computations are done to leading order in k on the ray path. The
interaction of beams with boundaries has been studied for non-tangential beams
and for beams gliding along the boundary. We find that the amplitude of the
solution on the central ray for large k after leaving the boundary is very
nearly one half of that of the incoming beam.
",mathematics
"  We introduce a compressed suffix array representation that, on a text $T$ of
length $n$ over an alphabet of size $\sigma$, can be built in $O(n)$
deterministic time, within $O(n\log\sigma)$ bits of working space, and counts
the number of occurrences of any pattern $P$ in $T$ in time $O(|P| + \log\log_w
\sigma)$ on a RAM machine of $w=\Omega(\log n)$-bit words. This new index
outperforms all the other compressed indexes that can be built in linear
deterministic time, and some others. The only faster indexes can be built in
linear time only in expectation, or require $\Theta(n\log n)$ bits. We also
show that, by using $O(n\log\sigma)$ bits, we can build in linear time an index
that counts in time $O(|P|/\log_\sigma n + \log n(\log\log n)^2)$, which is
RAM-optimal for $w=\Theta(\log n)$ and sufficiently long patterns.
",computer-science
"  Evolutionary algorithms have recently been used to create a wide range of
artistic work. In this paper, we propose a new approach for the composition of
new images from existing ones, that retain some salient features of the
original images. We introduce evolutionary algorithms that create new images
based on a fitness function that incorporates feature covariance matrices
associated with different parts of the images. This approach is very flexible
in that it can work with a wide range of features and enables targeting
specific regions in the images. For the creation of the new images, we propose
a population-based evolutionary algorithm with mutation and crossover operators
based on random walks. Our experimental results reveal a spectrum of
aesthetically pleasing images that can be obtained with the aid of our
evolutionary process.
",computer-science
"  Doubly occupied configuration interaction (DOCI), the exact diagonalization
of the Hamiltonian in the paired (seniority zero) sector of the Hilbert space,
is a combinatorial cost wave function that can be very efficiently approximated
by pair coupled cluster doubles (pCCD) at mean-field computational cost. As
such, it is a very interesting candidate as a starting point for building the
full configuration interaction (FCI) ground state eigenfunction belonging to
all (not just paired) seniority sectors. The true seniority zero sector of FCI
(referred to here as FCI${}_0$) includes the effect of coupling between all
seniority sectors rather than just seniority zero, and is, in principle,
different from DOCI. We here study the accuracy with which DOCI approximates
FCI${}_0$. Using a set of small Hubbard lattices, where FCI is possible, we
show that DOCI $\sim$ FCI${}_0$ under weak correlation. However, in the strong
correlation regime, the nature of the FCI${}_0$ wavefunction can change
significantly, rendering DOCI and pCCD a less than ideal starting point for
approximating FCI.
",physics
"  We present a formal model for a fragmentation and a reassembly protocol
running on top of the standardised CAN bus, which is widely used in automotive
and aerospace applications. Although the CAN bus comes with an in-built
mechanism for prioritisation, we argue that this is not sufficient and provide
another protocol to overcome this shortcoming.
",computer-science
"  We identify and study a number of new, rapidly growing instabilities of dust
grains in protoplanetary disks, which may be important for planetesimal
formation. The study is based on the recognition that dust-gas mixtures are
generically unstable to a Resonant Drag Instability (RDI), whenever the gas,
absent dust, supports undamped linear modes. We show that the ""streaming
instability"" is an RDI associated with epicyclic oscillations; this provides
simple interpretations for its mechanisms and accurate analytic expressions for
its growth rates and fastest-growing wavelengths. We extend this analysis to
more general dust streaming motions and other waves, including buoyancy and
magnetohydrodynamic oscillations, finding various new instabilities. Most
importantly, we identify the disk ""settling instability,"" which occurs as dust
settles vertically into the midplane of a rotating disk. For small grains, this
instability grows many orders of magnitude faster than the standard streaming
instability, with a growth rate that is independent of grain size. Growth
timescales for realistic dust-to-gas ratios are comparable to the disk orbital
period, and the characteristic wavelengths are more than an order of magnitude
larger than the streaming instability (allowing the instability to concentrate
larger masses). This suggests that in the process of settling, dust will band
into rings then filaments or clumps, potentially seeding dust traps,
high-metallicity regions that in turn seed the streaming instability, or even
overdensities that coagulate or directly collapse to planetesimals.
",physics
"  The present work addressed in this thesis introduces, for the first time, the
use of tilted fiber Bragg grating (TFBG) sensors for accurate, real-time, and
in-situ characterization of CVD and ALD processes for noble metals, but with a
particular focus on gold due to its desirable optical and plasmonic properties.
Through the use of orthogonally-polarized transverse electric (TE) and
transverse magnetic (TM) resonance modes imposed by a boundary condition at the
cladding-metal interface of the optical fiber, polarization-dependent
resonances excited by the TFBG are easily decoupled. It was found that for
ultrathin thicknesses of gold films from CVD (~6-65 nm), the anisotropic
property of these films made it non-trivial to characterize their effective
optical properties such as the real component of the permittivity.
Nevertheless, the TFBG introduces a new sensing platform to the ALD and CVD
community for extremely sensitive in-situ process monitoring. We later also
demonstrate thin film growth at low (<10 cycle) numbers for the well-known
Al2O3 thermal ALD process, as well as the plasma-enhanced gold ALD process.
Finally, the use of ALD-grown gold coatings has been employed for the
development of a plasmonic TFBG-based sensor with ultimate refractometric
sensitivity (~550 nm/RIU).
",physics
"  Cohomological and K-theoretic stable bases originated from the study of
quantum cohomology and quantum K-theory. Restriction formula for cohomological
stable bases played an important role in computing the quantum connection of
cotangent bundle of partial flag varieties. In this paper we study the
K-theoretic stable bases of cotangent bundles of flag varieties. We describe
these bases in terms of the action of the affine Hecke algebra and the twisted
group algebra of Kostant-Kumar. Using this algebraic description and the method
of root polynomials, we give a restriction formula of the stable bases. We
apply it to obtain the restriction formula for partial flag varieties. We also
build a relation between the stable basis and the Casselman basis in the
principal series representations of the Langlands dual group. As an
application, we give a closed formula for the transition matrix between
Casselman basis and the characteristic functions.
",mathematics
"  The paper proposes an expanded version of the Local Variance Gamma model of
Carr and Nadtochiy by adding drift to the governing underlying process. Still
in this new model it is possible to derive an ordinary differential equation
for the option price which plays a role of Dupire's equation for the standard
local volatility model. It is shown how calibration of multiple smiles (the
whole local volatility surface) can be done in such a case. Further, assuming
the local variance to be a piecewise linear function of strike and piecewise
constant function of time this ODE is solved in closed form in terms of
Confluent hypergeometric functions. Calibration of the model to market smiles
does not require solving any optimization problem and, in contrast, can be done
term-by-term by solving a system of non-linear algebraic equations for each
maturity, which is fast.
",quantitative-finance
"  Considerable literature has been developed for various fundamental
distributed problems in the SINR (Signal-to-Interference-plus-Noise-Ratio)
model for radio transmission. A setting typically studied is when all nodes
transmit a signal of the same strength, and each device only has access to
knowledge about the total number of nodes in the network $n$, the range from
which each node's label is taken $[1,\dots,N]$, and the label of the device
itself. In addition, an assumption is made that each node also knows its
coordinates in the Euclidean plane. In this paper, we create a technique which
allows algorithm designers to remove that last assumption. The assumption about
the unavailability of the knowledge of the physical coordinates of the nodes
truly captures the `ad-hoc' nature of wireless networks.
Previous work in this area uses a flavor of a technique called dilution, in
which nodes transmit in a (predetermined) round-robin fashion, and are able to
reach all their neighbors. However, without knowing the physical coordinates,
it's not possible to know the coordinates of their containing (pivotal) grid
box and seemingly not possible to use dilution (to coordinate their
transmissions). We propose a new technique to achieve dilution without using
the knowledge of physical coordinates. This technique exploits the
understanding that the transmitting nodes lie in 2-D space, segmented by an
appropriate pivotal grid, without explicitly referring to the actual physical
coordinates of these nodes. Using this technique, it is possible for every weak
device to successfully transmit its message to all of its neighbors in
$\Theta(\lg N)$ rounds, as long as the density of transmitting nodes in any
physical grid box is bounded by a known constant. This technique, we feel, is
an important generic tool for devising practical protocols when physical
coordinates of the nodes are not known.
",computer-science
"  In the past decade, the information security and threat landscape has grown
significantly making it difficult for a single defender to defend against all
attacks at the same time. This called for introduc- ing information sharing, a
paradigm in which threat indicators are shared in a community of trust to
facilitate defenses. Standards for representation, exchange, and consumption of
indicators are pro- posed in the literature, although various issues are
undermined. In this paper, we rethink information sharing for actionable
intelli- gence, by highlighting various issues that deserve further explo-
ration. We argue that information sharing can benefit from well- defined use
models, threat models, well-understood risk by mea- surement and robust
scoring, well-understood and preserved pri- vacy and quality of indicators and
robust mechanism to avoid free riding behavior of selfish agent. We call for
using the differential nature of data and community structures for optimizing
sharing.
",computer-science
"  We establish interior Lipschitz estimates at the macroscopic scale for
solutions to systems of linear elasticity with rapidly oscillating periodic
coefficients and mixed boundary conditions in domains periodically perforated
at a microscopic scale $\varepsilon$ by establishing $H^1$-convergence rates
for such solutions. The interior estimates are derived directly without the use
of compactness via an argument presented in [3] that was adapted for elliptic
equations in [2] and [11]. As a consequence, we derive a Liouville type
estimate for solutions to the systems of linear elasticity in unbounded
periodically perforated domains.
",mathematics
"  Reflexive polytopes form one of the distinguished classes of lattice
polytopes. Especially reflexive polytopes which possess the integer
decomposition property are of interest. In the present paper, by virtue of the
algebraic technique on Grönbner bases, a new class of reflexive polytopes
which possess the integer decomposition property and which arise from perfect
graphs will be presented. Furthermore, the Ehrhart $\delta$-polynomials of
these polytopes will be studied.
",mathematics
"  When the brain receives input from multiple sensory systems, it is faced with
the question of whether it is appropriate to process the inputs in combination,
as if they originated from the same event, or separately, as if they originated
from distinct events. Furthermore, it must also have a mechanism through which
it can keep sensory inputs calibrated to maintain the accuracy of its internal
representations. We have developed a neural network architecture capable of i)
approximating optimal multisensory spatial integration, based on Bayesian
causal inference, and ii) recalibrating the spatial encoding of sensory
systems. The architecture is based on features of the dorsal processing
hierarchy, including the spatial tuning properties of unisensory neurons and
the convergence of different sensory inputs onto multisensory neurons.
Furthermore, we propose that these unisensory and multisensory neurons play
dual roles in i) encoding spatial location as separate or integrated estimates
and ii) accumulating evidence for the independence or relatedness of
multisensory stimuli. We further propose that top-down feedback connections
spanning the dorsal pathway play key a role in recalibrating spatial encoding
at the level of early unisensory cortices. Our proposed architecture provides
possible explanations for a number of human electrophysiological and
neuroimaging results and generates testable predictions linking neurophysiology
with behaviour.
",quantitative-biology
"  A framework of variational principles for stochastic fluid dynamics was
presented by Holm (2015), and these stochastic equations were also derived by
Cotter et al. (2017). We present a conforming finite element discretisation for
the stochastic quasi-geostrophic equation that was derived from this framework.
The discretisation preserves the first two moments of potential vorticity, i.e.
the mean potential vorticity and the enstrophy. Following the work of Dubinkina
and Frank (2007), who investigated the statistical mechanics of discretisations
of the deterministic quasi-geostrophic equation, we investigate the statistical
mechanics of our discretisation of the stochastic quasi-geostrophic equation.
We compare the statistical properties of our discretisation with the Gibbs
distribution under assumption of these conserved quantities, finding that there
is agreement between the statistics under a wide range of set-ups.
",physics
"  We focus on the cohomology of the $k$-th nilpotent quotient of the free
group, $F/F_k$. This paper describes all the group 2-, 3-cocycles in terms of
Massey products, and gives expressions for some of the 3-cocycles. We also give
simple proofs of some of the results on Milnor invariants and the
Johnson-Morita homomorphisms.
",mathematics
"  We study families of varieties endowed with polarized canonical eigensystems
of several maps, inducing canonical heights on the dominating variety as well
as on the ""good"" fibers of the family. We show explicitely the dependence on
the parameter for global and local canonical heights defined by Kawaguchi when
the fibers change, extending previous works of J. Silverman and others.
Finally, fixing an absolute value $v \in K$ and a variety $V/K$, we descript
the Kawaguchi`s canonical local height $\hat{\lambda}_{V,E,\mathcal{Q},}(.,v)$
as an intersection number, provided that the polarized system $(V,\mathcal{Q})$
has a certain weak Néron model over Spec$(\mathcal{O}_v)$ to be defined and
under some conditions depending on the special fiber. With this we extend
Néron's work strengthening Silverman's results, which were for systems
having only one map.
",mathematics
"  Machine learning (ML) techniques such as (deep) artificial neural networks
(DNN) are solving very successfully a plethora of tasks and provide new
predictive models for complex physical, chemical, biological and social
systems. However, in most cases this comes with the disadvantage of acting as a
black box, rarely providing information about what made them arrive at a
particular prediction. This black box aspect of ML techniques can be
problematic especially in medical diagnoses, so far hampering a clinical
acceptance. The present paper studies the uniqueness of individual gait
patterns in clinical biomechanics using DNNs. By attributing portions of the
model predictions back to the input variables (ground reaction forces and
full-body joint angles), the Layer-Wise Relevance Propagation (LRP) technique
reliably demonstrates which variables at what time windows of the gait cycle
are most relevant for the characterisation of gait patterns from a certain
individual. By measuring the timeresolved contribution of each input variable
to the prediction of ML techniques such as DNNs, our method describes the first
general framework that enables to understand and interpret non-linear ML
methods in (biomechanical) gait analysis and thereby supplies a powerful tool
for analysis, diagnosis and treatment of human gait.
",statistics
"  The zero-temperature limit of a continuous phase transition is marked by a
quantum critical point, which can generate exotic physics that extends to
elevated temperatures. Magnetic quantum criticality is now well known, and has
been explored in systems ranging from heavy fermion metals to quantum Ising
materials. Ferroelectric quantum critical behaviour has also been recently
established, motivating a flurry of research investigating its consequences.
Here, we introduce the concept of multiferroic quantum criticality, in which
both magnetic and ferroelectric quantum criticality occur in the same system.
We develop the phenomenology of multiferroic quantum critical behaviour,
describe the associated experimental signatures, and propose material systems
and schemes to realize it.
",physics
"  We provide a fast method for computing constraints on impactor pre-impact
orbits, applying this to the late giant impacts in the Solar System. These
constraints can be used to make quick, broad comparisons of different collision
scenarios, identifying some immediately as low-probability events, and
narrowing the parameter space in which to target follow-up studies with
expensive N-body simulations. We benchmark our parameter space predictions,
finding good agreement with existing N-body studies for the Moon. We suggest
that high-velocity impact scenarios in the inner Solar System, including all
currently proposed single impact scenarios for the formation of Mercury, should
be disfavoured. This leaves a multiple hit-and-run scenario as the most
probable currently proposed for the formation of Mercury.
",physics
"  The aim of this paper is to investigate the non-relativistic limit of
integrable quantum field theories with fermionic fields, such as the O(N)
Gross-Neveu model, the supersymmetric Sinh-Gordon and non-linear sigma models.
The non-relativistic limit of these theories is implemented by a double scaling
limit which consists of sending the speed of light c to infinity and rescaling
at the same time the relevant coupling constant of the model in such a way to
have finite energy excitations. For the general purpose of mapping the space of
continuous non-relativistic integrable models, this paper completes and
integrates the analysis done in Ref.[1] on the non-relativistic limit of purely
bosonic theories.
",physics
"  A main goal of NASA's Kepler Mission is to establish the frequency of
potentially habitable Earth-size planets (eta Earth). Relatively few such
candidates identified by the mission can be confirmed to be rocky via dynamical
measurement of their mass. Here we report an effort to validate 18 of them
statistically using the BLENDER technique, by showing that the likelihood they
are true planets is far greater than that of a false positive. Our analysis
incorporates follow-up observations including high-resolution optical and
near-infrared spectroscopy, high-resolution imaging, and information from the
analysis of the flux centroids of the Kepler observations themselves. While
many of these candidates have been previously validated by others, the
confidence levels reported typically ignore the possibility that the planet may
transit a different star than the target along the same line of sight. If that
were the case, a planet that appears small enough to be rocky may actually be
considerably larger and therefore less interesting from the point of view of
habitability. We take this into consideration here, and are able to validate 15
of our candidates at a 99.73% (3 sigma) significance level or higher, and the
other three at slightly lower confidence. We characterize the GKM host stars
using available ground-based observations and provide updated parameters for
the planets, with sizes between 0.8 and 2.9 Earth radii. Seven of them
(KOI-0438.02, 0463.01, 2418.01, 2626.01, 3282.01, 4036.01, and 5856.01) have a
better than 50% chance of being smaller than 2 Earth radii and being in the
habitable zone of their host stars.
",physics
"  The High Luminosity LHC (HL-LHC) will integrate 10 times more luminosity than
the LHC, posing significant challenges for radiation tolerance and event pileup
on detectors, especially for forward calorimetry, and hallmarks the issue for
future colliders. As part of its HL-LHC upgrade program, the CMS collaboration
is designing a High Granularity Calorimeter to replace the existing endcap
calorimeters. It features unprecedented transverse and longitudinal
segmentation for both electromagnetic (ECAL) and hadronic (HCAL) compartments.
This will facilitate particle-flow calorimetry, where the fine structure of
showers can be measured and used to enhance pileup rejection and particle
identification, whilst still achieving good energy resolution. The ECAL and a
large fraction of HCAL will be based on hexagonal silicon sensors of
0.5-1cm$^{2}$ cell size, with the remainder of the HCAL based on
highly-segmented scintillators with SiPM readout. The intrinsic high-precision
timing capabilities of the silicon sensors will add an extra dimension to event
reconstruction, especially in terms of pileup rejection. An overview of the
HGCAL project is presented, covering motivation, engineering design, readout
and trigger concepts, and performance (simulated and from beam tests).
",physics
"  Measures of neuroelectric activity from each of 18 automatically identified
white matter tracts were extracted from resting MEG recordings from a
normative, n=588, and a chronic TBI, traumatic brain injury, n=63, cohort, 60
of whose TBIs were mild. Activity in the TBI cohort was significantly reduced
compared with the norms for ten of the tracts, p < 10-6 for each. Significantly
reduced activity (p < 10-3) was seen in more than one tract in seven mTBI
individuals and one member of the normative cohort.
",quantitative-biology
"  We present and analyze a new space-time finite element method for the
solution of neural field equations with transmission delays. The numerical
treatment of these systems is rare in the literature and currently has several
restrictions on the spatial domain and the functions involved, such as
connectivity and delay functions. The use of a space-time discretization, with
basis functions that are discontinuous in time and continuous in space
(dGcG-FEM), is a natural way to deal with space-dependent delays, which is
important for many neural field applications. In this article we provide a
detailed description of a space-time dGcG-FEM algorithm for neural delay
equations, including an a-priori error analysis. We demonstrate the application
of the dGcG-FEM algorithm on several neural field models, including problems
with an inhomogeneous kernel.
",mathematics
"  We study a connection between synchronizing automata and its set $M$ of
minimal reset words, i.e., such that no proper factor is a reset word. We first
show that any synchronizing automaton having the set of minimal reset words
whose set of factors does not contain a word of length at most
$\frac{1}{4}\min\{|u|: u\in I\}+\frac{1}{16}$ has a reset word of length at
most $(n-\frac{1}{2})^{2}$ In the last part of the paper we focus on the
existence of synchronizing automata with a given ideal $I$ that serves as the
set of reset words. To this end, we introduce the notion of the tail structure
of the (not necessarily regular) ideal $I=\Sigma^{*}M\Sigma^{*}$. With this
tool, we first show the existence of an infinite strongly connected
synchronizing automaton $\mathcal{A}$ having $I$ as the set of reset words and
such that every other strongly connected synchronizing automaton having $I$ as
the set of reset words is an homomorphic image of $\mathcal{A}$. Finally, we
show that for any non-unary regular ideal $I$ there is a strongly connected
synchronizing automaton having $I$ as the set of reset words with at most
$(km^{k})2^{km^{k}n}$ states, where $k=|\Sigma|$, $m$ is the length of a
shortest word in $M$, and $n$ is the dimension of the smallest automaton
recognizing $M$ (state complexity of $M$). This automaton is computable and we
show an algorithm to compute it in time $\mathcal{O}((k^{2}m^{k})2^{km^{k}n})$.
",computer-science
"  In recent work, Pomerance and Shparlinski have obtained results on the number
of cycles in the functional graph of the map $x \mapsto x^a$ in
$\mathbb{F}_p^*$. We prove similar results for other families of finite groups.
In particular, we obtain estimates for the number of cycles for cyclic groups,
symmetric groups, dihedral groups and $SL_2(\mathbb{F}_q)$. We also show that
the cyclic group of order $n$ minimizes the number of cycles among all
nilpotent groups of order $n$ for a fixed exponent. Finally, we pose several
problems.
",mathematics
"  Data-driven brain parcellations aim to provide a more accurate representation
of an individual's functional connectivity, since they are able to capture
individual variability that arises due to development or disease. This renders
comparisons between the emerging brain connectivity networks more challenging,
since correspondences between their elements are not preserved. Unveiling these
correspondences is of major importance to keep track of local functional
connectivity changes. We propose a novel method based on graph edit distance
for the comparison of brain graphs directly in their domain, that can
accurately reflect similarities between individual networks while providing the
network element correspondences. This method is validated on a dataset of 116
twin subjects provided by the Human Connectome Project.
",computer-science
"  We study flat FLRW $\alpha$-attractor $\mathrm{E}$- and $\mathrm{T}$-models
by introducing a dynamical systems framework that yields regularized
unconstrained field equations on two-dimensional compact state spaces. This
results in both illustrative figures and a complete description of the entire
solution spaces of these models, including asymptotics. In particular, it is
shown that observational viability, which requires a sufficient number of
$e$-folds, is associated with a solution given by a one-dimensional center
manifold of a past asymptotic de Sitter state, where the center manifold
structure also explains why nearby solutions are attracted to this
`inflationary attractor solution.' A center manifold expansion yields a
description of the inflationary regime with arbitrary analytic accuracy, where
the slow-roll approximation asymptotically describes the tangency condition of
the center manifold at the asymptotic de Sitter state.
",physics
"  Brain-Machine Interaction (BMI) system motivates interesting and promising
results in forward/feedback control consistent with human intention. It holds
great promise for advancements in patient care and applications to
neurorehabilitation. Here, we propose a novel neurofeedback-based BCI robotic
platform using a personalized social robot in order to assist patients having
cognitive deficits through bilateral rehabilitation and mental training. For
initial testing of the platform, electroencephalography (EEG) brainwaves of a
human user were collected in real time during tasks of imaginary movements.
First, the brainwaves associated with imagined body kinematics parameters were
decoded to control a cursor on a computer screen in training protocol. Then,
the experienced subject was able to interact with a social robot via our
real-time BMI robotic platform. Corresponding to subject's imagery performance,
he/she received specific gesture movements and eye color changes as
neural-based feedback from the robot. This hands-free neurofeedback interaction
not only can be used for mind control of a social robot's movements, but also
sets the stage for application to enhancing and recovering mental abilities
such as attention via training in humans by providing real-time neurofeedback
from a social robot.
",computer-science
"  Traditional face editing methods often require a number of sophisticated and
task specific algorithms to be applied one after the other --- a process that
is tedious, fragile, and computationally intensive. In this paper, we propose
an end-to-end generative adversarial network that infers a face-specific
disentangled representation of intrinsic face properties, including shape (i.e.
normals), albedo, and lighting, and an alpha matte. We show that this network
can be trained on ""in-the-wild"" images by incorporating an in-network
physically-based image formation module and appropriate loss functions. Our
disentangling latent representation allows for semantically relevant edits,
where one aspect of facial appearance can be manipulated while keeping
orthogonal properties fixed, and we demonstrate its use for a number of facial
editing applications.
",computer-science
"  Planar object tracking is an actively studied problem in vision-based robotic
applications. While several benchmarks have been constructed for evaluating
state-of-the-art algorithms, there is a lack of video sequences captured in the
wild rather than in constrained laboratory environment. In this paper, we
present a carefully designed planar object tracking benchmark containing 210
videos of 30 planar objects sampled in the natural environment. In particular,
for each object, we shoot seven videos involving various challenging factors,
namely scale change, rotation, perspective distortion, motion blur, occlusion,
out-of-view, and unconstrained. The ground truth is carefully annotated
semi-manually to ensure the quality. Moreover, eleven state-of-the-art
algorithms are evaluated on the benchmark using two evaluation metrics, with
detailed analysis provided for the evaluation results. We expect the proposed
benchmark to benefit future studies on planar object tracking.
",computer-science
"  This work is the first step towards a description of the Gromov boundary of
the free factor graph of a free product, with applications to subgroup
classification for outer automorphisms. We extend the theory of algebraic
laminations dual to trees, as developed by Coulbois, Hilion, Lustig and
Reynolds, to the context of free products; this also gives us an opportunity to
give a unified account of this theory. We first show that any $\mathbb{R}$-tree
with dense orbits in the boundary of the corresponding outer space can be
reconstructed as a quotient of the boundary of the group by its dual
lamination. We then describe the dual lamination in terms of a band complex on
compact $\mathbb{R}$-trees (generalizing Coulbois-Hilion-Lustig's compact
heart), and we analyze this band complex using versions of the Rips machine and
of the Rauzy-Veech induction. An important output of the theory is that the
above map from the boundary of the group to the $\mathbb{R}$-tree is 2-to-1
almost everywhere.
A key point for our intended application is a unique duality result for
arational trees. It says that if two trees have a leaf in common in their dual
laminations, and if one of the trees is arational and relatively free, then
they are equivariantly homeomorphic.
This statement is an analogue of a result in the free group saying that if
two trees are dual to a common current and one of the trees is free arational,
then the two trees are equivariantly homeomorphic. However, we notice that in
the setting of free products, the continuity of the pairing between trees and
currents fails. For this reason, in all this paper, we work with laminations
rather than with currents.
",mathematics
"  We survey different classification results for surfaces with parallel mean
curvature immersed into some Riemannian homogeneous four-manifolds, including
real and complex space forms, and product spaces. We provide a common framework
for this problem, with special attention to the existence of holomorphic
quadratic differentials on such surfaces. The case of spheres with parallel
mean curvature is also explained in detail, as well as the state-of-the-art
advances in the general problem.
",mathematics
"  An important novelty of 5G is its role in transforming the industrial
production into Industry 4.0. Specifically, Ultra-Reliable Low Latency
Communications (URLLC) will, in many cases, enable replacement of cables with
wireless connections and bring freedom in designing and operating
interconnected machines, robots, and devices. However, not all industrial links
will be of URLLC type; e.g. some applications will require high data rates.
Furthermore, these industrial networks will be highly heterogeneous, featuring
various communication technologies. We consider network slicing as a mechanism
to handle the diverse set of requirements to the network. We present methods
for slicing deterministic and packet-switched industrial communication
protocols at an abstraction level that is decoupled from the specific
implementation of the underlying technologies. Finally, we show how network
calculus can be used to assess the end-to-end properties of the network slices.
",computer-science
"  Deep learning has become the state of the art approach in many machine
learning problems such as classification. It has recently been shown that deep
learning is highly vulnerable to adversarial perturbations. Taking the camera
systems of self-driving cars as an example, small adversarial perturbations can
cause the system to make errors in important tasks, such as classifying traffic
signs or detecting pedestrians. Hence, in order to use deep learning without
safety concerns a proper defense strategy is required. We propose to use
ensemble methods as a defense strategy against adversarial perturbations. We
find that an attack leading one model to misclassify does not imply the same
for other networks performing the same task. This makes ensemble methods an
attractive defense strategy against adversarial attacks. We empirically show
for the MNIST and the CIFAR-10 data sets that ensemble methods not only improve
the accuracy of neural networks on test data but also increase their robustness
against adversarial perturbations.
",statistics
"  The variational autoencoder (VAE) is a popular model for density estimation
and representation learning. Canonically, the variational principle suggests to
prefer an expressive inference model so that the variational approximation is
accurate. However, it is often overlooked that an overly-expressive inference
model can be detrimental to the test set performance of both the amortized
posterior approximator and, more importantly, the generative density estimator.
In this paper, we leverage the fact that VAEs rely on amortized inference and
propose techniques for amortized inference regularization (AIR) that control
the smoothness of the inference model. We demonstrate that, by applying AIR, it
is possible to improve VAE generalization on both inference and generative
performance. Our paper challenges the belief that amortized inference is simply
a mechanism for approximating maximum likelihood training and illustrates that
regularization of the amortization family provides a new direction for
understanding and improving generalization in VAEs.
",statistics
"  This paper analyzes Airbnb listings in the city of San Francisco to better
understand how different attributes such as bedrooms, location, house type
amongst others can be used to accurately predict the price of a new listing
that optimal in terms of the host's profitability yet affordable to their
guests. This model is intended to be helpful to the internal pricing tools that
Airbnb provides to its hosts. Furthermore, additional analysis is performed to
ascertain the likelihood of a listings availability for potential guests to
consider while making a booking. The analysis begins with exploring and
examining the data to make necessary transformations that can be conducive for
a better understanding of the problem at large while helping us make
hypothesis. Moving further, machine learning models are built that are
intuitive to use to validate the hypothesis on pricing and availability and run
experiments in that context to arrive at a viable solution. The paper then
concludes with a discussion on the business implications, associated risks and
future scope.
",quantitative-finance
"  This paper addresses the problem of coordination of a fleet of mobile robots
- the problem of finding an optimal set of collision-free trajectories for
individual robots in the fleet. Many approaches have been introduced during the
last decades, but a minority of them is practically applicable, i.e. fast,
producing near-optimal solutions, and complete. We propose a novel
probabilistic approach based on the Rapidly Exploring Random Tree algorithm
(RRT) by significantly improving its multi-robot variant for discrete
environments. The presented experimental results show that the proposed
approach is fast enough to solve problems with tens of robots in seconds.
Although the solutions generated by the approach are slightly worse than one of
the best state-of-the-art algorithms presented in (ter Mors et al., 2010), it
solves problems where ter Mors's algorithm fails.
",computer-science
"  We study the problem of generating adversarial examples in a black-box
setting in which only loss-oracle access to a model is available. We introduce
a framework that conceptually unifies much of the existing work on black-box
attacks, and we demonstrate that the current state-of-the-art methods are
optimal in a natural sense. Despite this optimality, we show how to improve
black-box attacks by bringing a new element into the problem: gradient priors.
We give a bandit optimization-based algorithm that allows us to seamlessly
integrate any such priors, and we explicitly identify and incorporate two
examples. The resulting methods use two to four times fewer queries and fail
two to five times less often than the current state-of-the-art.
",statistics
"  It is well known that the affine matrix rank minimization problem is NP-hard
and all known algorithms for exactly solving it are doubly exponential in
theory and in practice due to the combinational nature of the rank function. In
this paper, a generalized singular value thresholding operator is generated to
solve the affine matrix rank minimization problem. Numerical experiments show
that our algorithm performs effectively in finding a low-rank matrix compared
with some state-of-art methods.
",mathematics
"  We introduce the notion of a ""crystallographic sphere packing,"" defined to be
one whose limit set is that of a geometrically finite hyperbolic reflection
group in one higher dimension. We exhibit for the first time an infinite family
of conformally-inequivalent such with all radii being reciprocals of integers.
We then prove a result in the opposite direction: the ""superintegral"" ones
exist only in finitely many ""commensurability classes,"" all in dimensions below
30.
",mathematics
"  General relativistic effects have long been predicted to subtly influence the
observed large-scale structure of the universe. The current generation of
galaxy redshift surveys have reached a size where detection of such effects is
becoming feasible. In this paper, we report the first detection of the redshift
asymmetry from the cross-correlation function of two galaxy populations which
is consistent with relativistic effects. The dataset is taken from the Sloan
Digital Sky Survey DR12 CMASS galaxy sample, and we detect the asymmetry at the
$2.7\sigma$ level by applying a shell-averaged estimator to the
cross-correlation function. Our measurement dominates at scales around $10$
h$^{-1}$Mpc, larger than those over which the gravitational redshift profile
has been recently measured in galaxy clusters, but smaller than scales for
which linear perturbation theory is likely to be accurate. The detection
significance varies by 0.5$\sigma$ with the details of our measurement and
tests for systematic effects. We have also devised two null tests to check for
various survey systematics and show that both results are consistent with the
null hypothesis. We measure the dipole moment of the cross-correlation
function, and from this the asymmetry is also detected, at the $2.8 \sigma$
level. The amplitude and scale-dependence of the clustering asymmetries are
approximately consistent with the expectations of General Relativity and a
biased galaxy population, within large uncertainties. We explore theoretical
predictions using numerical simulations in a companion paper.
",physics
"  Feedback control theory has been extensively implemented to theoretically
model human sensorimotor control. However, experimental platforms capable of
manipulating important components of multiple feedback loops lack development.
This paper describes the WheelCon, which is an open source platform aimed at
resolving such insufficiencies. WheelCon enables safely simulation of the
canonical sensorimotor task such as riding a mountain bike down a steep,
twisting, bumpy trail etc., with provided only a computer, standard display,
and an inexpensive gaming steering wheel with a force feedback motor. The
platform provides flexibility, as will be demonstrated in the demos provided,
so that researchers may manipulate the disturbances, delay, and quantization
(data rate) in the layered feedback loops, including a high-level advanced plan
layer and a low-level delayed reflex layer. In this paper, we illustrate
WheelCon's graphical user interface (GUI), the input and output of existing
demos, and how to design new games. In addition, we present the basic feedback
model, and we show the testing results from our demo games which align well
with prediction from the model. In short, the platform is featured as cheap,
simple to use, and flexible to program for effective sensorimotor neuroscience
research and control engineering education.
",quantitative-biology
"  The blooming availability of traces for social, biological, and communication
networks opens up unprecedented opportunities in analyzing diffusion processes
in networks. However, the sheer sizes of the nowadays networks raise serious
challenges in computational efficiency and scalability.
In this paper, we propose a new hyper-graph sketching framework for inflence
dynamics in networks. The central of our sketching framework, called SKIS, is
an efficient importance sampling algorithm that returns only non-singular
reverse cascades in the network. Comparing to previously developed sketches
like RIS and SKIM, our sketch significantly enhances estimation quality while
substantially reducing processing time and memory-footprint. Further, we
present general strategies of using SKIS to enhance existing algorithms for
influence estimation and influence maximization which are motivated by
practical applications like viral marketing. Using SKIS, we design high-quality
influence oracle for seed sets with average estimation error up to 10x times
smaller than those using RIS and 6x times smaller than SKIM. In addition, our
influence maximization using SKIS substantially improves the quality of
solutions for greedy algorithms. It achieves up to 10x times speed-up and 4x
memory reduction for the fastest RIS-based DSSA algorithm, while maintaining
the same theoretical guarantees.
",computer-science
"  Many modern applications deal with multi-label data, such as functional
categorizations of genes, image labeling and text categorization.
Classification of such data with a large number of labels and latent
dependencies among them is a challenging task, and it becomes even more
challenging when the data is received online and in chunks. Many of the current
multi-label classification methods require a lot of time and memory, which make
them infeasible for practical real-world applications. In this paper, we
propose a fast linear label space dimension reduction method that transforms
the labels into a reduced encoded space and trains models on the obtained
pseudo labels. Additionally, it provides an analytical method to update the
decoding matrix which maps the labels into the original space and is used
during the test phase. Experimental results show the effectiveness of this
approach in terms of running times and the prediction performance over
different measures.
",statistics
"  Graphs are naturally sparse objects that are used to study many problems
involving networks, for example, distributed learning and graph signal
processing. In some cases, the graph is not given, but must be learned from the
problem and available data. Often it is desirable to learn sparse graphs.
However, making a graph highly sparse can split the graph into several
disconnected components, leading to several separate networks. The main
difficulty is that connectedness is often treated as a combinatorial property,
making it hard to enforce in e.g. convex optimization problems. In this
article, we show how connectedness of undirected graphs can be formulated as an
analytical property and can be enforced as a convex constraint. We especially
show how the constraint relates to the distributed consensus problem and graph
Laplacian learning. Using simulated and real data, we perform experiments to
learn sparse and connected graphs from data.
",statistics
"  We explore the impact of dimensionality on the scattering of a small bosonic
ensemble in an elongated harmonic trap off a centered repulsive barrier,
thereby taking particle correlations into account. The loss of coherence as
well as the oscillation of the center of mass are studied and we analyze the
influence of both particle and spatial correlations. Two different mechanisms
of coherence losses in dependence of the aspect ratio are found. For small
aspect ratios, loss of coherence between the region close to the barrier and
outer regions occurs, due to spatial correlations, and for large aspect ratios,
incoherence between the two density fragments of the left and right side of the
barrier arises, due to particle correlations. Apart form the decay of the
center of mass motion induced by the reflection and transmission, further
effects due to the particle and spatial correlations are explored. For tight
transversal traps, the amplitude of the center of mass oscillation experiences
a weaker damping, which can be traced back to the population of a second
natural orbital, and for a weaker transversal confinement, we detect a strong
decay, due to the possibility of transferring energy to transversal excited
modes. These effects are enhanced if the aspect ratio is integer valued.
",physics
"  Blog is becoming an increasingly popular media for information publishing.
Besides the main content, most of blog pages nowadays also contain noisy
information such as advertisements etc. Removing these unrelated elements can
improves user experience, but also can better adapt the content to various
devices such as mobile phones. Though template-based extractors are highly
accurate, they may incur expensive cost in that a large number of template need
to be developed and they will fail once the template is updated. To address
these issues, we present a novel template-independent content extractor for
blog pages. First, we convert a blog page into a DOM-Tree, where all elements
including the title and body blocks in a page correspond to subtrees. Then we
construct subtree candidate set for the title and the body blocks respectively,
and extract both spatial and content features for elements contained in the
subtree. SVM classifiers for the title and the body blocks are trained using
these features. Finally, the classifiers are used to extract the main content
from blog pages. We test our extractor on 2,250 blog pages crawled from nine
blog sites with obviously different styles and templates. Experimental results
verify the effectiveness of our extractor.
",computer-science
"  The temperature-dependent optical response of excitons in semiconductors is
controlled by the exciton-phonon interaction. When the exciton-lattice coupling
is weak, the excitonic line has a Lorentzian profile resulting from motional
narrowing, with a width increasing linearly with the lattice temperature $T$.
In contrast, when the exciton-lattice coupling is strong, the lineshape is
Gaussian with a width increasing sublinearly with the lattice temperature,
proportional to $\sqrt{T}$. While the former case is commonly reported in the
literature, here the latter is reported for the first time, for hexagonal boron
nitride. Thus the theoretical predictions of Toyozawa [Progr. Theor. Phys. 20,
53 (1958)] are supported by demonstrating that the exciton-phonon interaction
is in the strong coupling regime in this Van der Waals crystal.
",physics
"  Low-rank matrix completion (MC) has achieved great success in many real-world
data applications. A latent feature model formulation is usually employed and,
to improve prediction performance, the similarities between latent variables
can be exploited by pairwise learning, e.g., the graph regularized matrix
factorization (GRMF) method. However, existing GRMF approaches often use a
squared L2 norm to measure the pairwise difference, which may be overly
influenced by dissimilar pairs and lead to inferior prediction. To fully
empower pairwise learning for matrix completion, we propose a general
optimization framework that allows a rich class of (non-)convex pairwise
penalty functions. A new and efficient algorithm is further developed to
uniformly solve the optimization problem, with a theoretical convergence
guarantee. In an important situation where the latent variables form a small
number of subgroups, its statistical guarantee is also fully characterized. In
particular, we theoretically characterize the complexity-regularized maximum
likelihood estimator, as a special case of our framework. It has a better error
bound when compared to the standard trace-norm regularized matrix completion.
We conduct extensive experiments on both synthetic and real datasets to
demonstrate the superior performance of this general framework.
",statistics
"  This paper derives an upper limit on the density $\rho_{\scriptstyle\Lambda}$
of dark energy based on the requirement that cosmological structure forms
before being frozen out by the eventual acceleration of the universe. By
allowing for variations in both the cosmological parameters and the strength of
gravity, the resulting constraint is a generalization of previous limits. The
specific parameters under consideration include the amplitude $Q$ of the
primordial density fluctuations, the Planck mass $M_{\rm pl}$, the
baryon-to-photon ratio $\eta$, and the density ratio $\Omega_M/\Omega_b$. In
addition to structure formation, we use considerations from stellar structure
and Big Bang Nucleosynthesis (BBN) to constrain these quantities. The resulting
upper limit on the dimensionless density of dark energy becomes
$\rho_{\scriptstyle\Lambda}/M_{\rm pl}^4<10^{-90}$, which is $\sim30$ orders of
magnitude larger than the value in our universe
$\rho_{\scriptstyle\Lambda}/M_{\rm pl}^4\sim10^{-120}$. This new limit is much
less restrictive than previous constraints because additional parameters are
allowed to vary. With these generalizations, a much wider range of universes
can develop cosmic structure and support observers. To constrain the
constituent parameters, new BBN calculations are carried out in the regime
where $\eta$ and $G=M_{\rm pl}^{-2}$ are much larger than in our universe. If
the BBN epoch were to process all of the protons into heavier elements, no
hydrogen would be left behind to make water, and the universe would not be
viable. However, our results show that some hydrogen is always left over, even
under conditions of extremely large $\eta$ and $G$, so that a wide range of
alternate universes are potentially habitable.
",physics
"  Spectral based heuristics belong to well-known commonly used methods which
determines provably minimal graph bisection or outputs ""fail"" when the
optimality cannot be certified. In this paper we focus on Boppana's algorithm
which belongs to one of the most prominent methods of this type. It is well
known that the algorithm works well in the random \emph{planted bisection
model} -- the standard class of graphs for analysis minimum bisection and
relevant problems. In 2001 Feige and Kilian posed the question if Boppana's
algorithm works well in the semirandom model by Blum and Spencer. In our paper
we answer this question affirmatively. We show also that the algorithm achieves
similar performance on graph classes which extend the semirandom model.
Since the behavior of Boppana's algorithm on the semirandom graphs remained
unknown, Feige and Kilian proposed a new semidefinite programming (SDP) based
approach and proved that it works on this model. The relationship between the
performance of the SDP based algorithm and Boppana's approach was left as an
open problem. In this paper we solve the problem in a complete way by proving
that the bisection algorithm of Feige and Kilian provides exactly the same
results as Boppana's algorithm. As a consequence we get that Boppana's
algorithm achieves the optimal threshold for exact cluster recovery in the
\emph{stochastic block model}. On the other hand we prove some limitations of
Boppana's approach: we show that if the density difference on the parameters of
the planted bisection model is too small then the algorithm fails with high
probability in the model.
",computer-science
"  A Bose-Einstein condensate confined in ring shaped lattices interrupted by a
weak link and pierced by an effective magnetic flux defines the atomic
counterpart of the superconducting quantum interference device: the atomtronic
quantum interference device (AQUID). In this paper, we report on the detection
of current states in the system through a self-heterodyne protocol. Following
the original proposal of the NIST and Paris groups, the ring-condensate
many-body wave function interferes with a reference condensate expanding from
the center of the ring. We focus on the rf-AQUID which realizes effective qubit
dynamics. Both the Bose-Hubbard and Gross-Pitaevskii dynamics are studied. For
the Bose-Hubbard dynamics, we demonstrate that the self-heterodyne protocol can
be applied, but higher-order correlations in the evolution of the interfering
condensates are measured to readout of the current states of the system. We
study how states with macroscopic quantum coherence can be told apart analyzing
the noise in the time of flight of the ring condensate.
",physics
"  We study the convergence of an inexact version of the classical
Krasnosel'skii-Mann iteration for computing fixed points of nonexpansive maps.
Our main result establishes a new metric bound for the fixed-point residuals,
from which we derive their rate of convergence as well as the convergence of
the iterates towards a fixed point. The results are applied to three variants
of the basic iteration: infeasible iterations with approximate projections, the
Ishikawa iteration, and diagonal Krasnosels'kii-Mann schemes. The results are
also extended to continuous time in order to study the asymptotics of
nonautonomous evolution equations governed by nonexpansive operators.
",mathematics
"  This thesis presents the design, analysis, and validation of a hierarchical
transactive control system that engages demand response resources to enhance
the integration of renewable electricity generation resources. This control
system joins energy, capacity and regulation markets together in a unified
homeostatic and economically efficient electricity operation that increases
total surplus while improving reliability and decreasing carbon emissions from
fossil-based generation resources.
The work encompasses: (1) the derivation of a short-term demand response
model suitable for transactive control systems and its validation with field
demonstration data; (2) an aggregate load model that enables effective control
of large populations of thermal loads using a new type of thermostat (discrete
time with zero deadband); (3) a methodology for optimally controlling response
to frequency deviations while tracking schedule area exports in areas that have
high penetration of both intermittent renewable resources and fast-acting
demand response; and (4) the development of a system-wide (continental
interconnection) scale strategy for optimal power trajectory and resource
dispatch based on a shift from primarily energy cost-based approach to a
primarily ramping cost-based one.
The results show that multi-layer transactive control systems can be
constructed, will enhance renewable resource utilization, and will operate in a
coordinated manner with bulk power systems that include both regions with and
without organized power markets. Estimates of Western Electric Coordionating
Council (WECC) system cost savings under target renewable energy generation
levels resulting from the proposed system exceed US$150B annually by the year
2024, when compared to the existing control system.
",physics
"  Phase retrieval refers to the problem of recovering real- or complex-valued
vectors from magnitude measurements. The best-known algorithms for this problem
are iterative in nature and rely on so-called spectral initializers that
provide accurate initialization vectors. We propose a novel class of estimators
suitable for general nonlinear measurement systems, called linear spectral
estimators (LSPEs), which can be used to compute accurate initialization
vectors for phase retrieval problems. The proposed LSPEs not only provide
accurate initialization vectors for noisy phase retrieval systems with
structured or random measurement matrices, but also enable the derivation of
sharp and nonasymptotic mean-squared error bounds. We demonstrate the efficacy
of LSPEs on synthetic and real-world phase retrieval problems, and show that
our estimators significantly outperform existing methods for structured
measurement systems that arise in practice.
",statistics
"  Mixtures of Mallows models are a popular generative model for ranking data
coming from a heterogeneous population. They have a variety of applications
including social choice, recommendation systems and natural language
processing. Here we give the first polynomial time algorithm for provably
learning the parameters of a mixture of Mallows models with any constant number
of components. Prior to our work, only the two component case had been settled.
Our analysis revolves around a determinantal identity of Zagier which was
proven in the context of mathematical physics, which we use to show polynomial
identifiability and ultimately to construct test functions to peel off one
component at a time.
To complement our upper bounds, we show information-theoretic lower bounds on
the sample complexity as well as lower bounds against restricted families of
algorithms that make only local queries. Together, these results demonstrate
various impediments to improving the dependence on the number of components.
They also motivate the study of learning mixtures of Mallows models from the
perspective of beyond worst-case analysis. In this direction, we show that when
the scaling parameters of the Mallows models have separation, there are much
faster learning algorithms.
",statistics
"  We propose a generalization of the best arm identification problem in
stochastic multi-armed bandits (MAB) to the setting where every pull of an arm
is associated with delayed feedback. The delay in feedback increases the
effective sample complexity of standard algorithms, but can be offset if we
have access to partial feedback received before a pull is completed. We propose
a general framework to model the relationship between partial and delayed
feedback, and as a special case we introduce efficient algorithms for settings
where the partial feedback are biased or unbiased estimators of the delayed
feedback. Additionally, we propose a novel extension of the algorithms to the
parallel MAB setting where an agent can control a batch of arms. Our
experiments in real-world settings, involving policy search and hyperparameter
optimization in computational sustainability domains for fast charging of
batteries and wildlife corridor construction, demonstrate that exploiting the
structure of partial feedback can lead to significant improvements over
baselines in both sequential and parallel MAB.
",statistics
"  The defect in diamond formed by a vacancy surrounded by three
nearest-neighbor nitrogen atoms and one carbon atom,
$\mathrm{N}_{3}\mathrm{V}$, is found in $\approx98\%$ of natural diamonds.
Despite $\mathrm{N}_{3}\mathrm{V}^{0}$ being the earliest electron paramagnetic
resonance spectrum observed in diamond, to date no satisfactory simulation of
the spectrum for an arbitrary magnetic field direction has been produced due to
its complexity. In this work, $\mathrm{N}_{3}\mathrm{V}^{0}$ is identified in
$^{15}\mathrm{N}$-doped synthetic diamond following irradiation and annealing.
The $\mathrm{^{15}N}_{3}\mathrm{V}^{0}$ spin Hamiltonian parameters are revised
and used to refine the parameters for $\mathrm{^{14}N}_{3}\mathrm{V}^{0}$,
enabling the latter to be accurately simulated and fitted for an arbitrary
magnetic field direction. Study of $\mathrm{^{15}N}_{3}\mathrm{V}^{0}$ under
excitation with green light indicates charge transfer between
$\mathrm{N}_{3}\mathrm{V}$ and $\mathrm{N_s}$. It is argued that this charge
transfer is facilitated by direct ionization of $\mathrm{N}_{3}\mathrm{V}^{-}$,
an as-yet unobserved charge state of $\mathrm{N}_{3}\mathrm{V}$.
",physics
"  With the recent focus in the accessibility field, researchers from academia
and industry have been very active in developing innovative techniques and
tools for assistive technology. Especially with handheld devices getting ever
powerful and being able to recognize the user's voice, screen magnification for
individuals with low-vision, and eye tracking devices used in studies with
individuals with physical and intellectual disabilities, the science field is
quickly adapting and creating conclusions as well as products to help. In this
paper, we will focus on new technology and tools to help make reading
easier--including reformatting document presentation (for people with physical
vision impairments) and text simplification to make information itself easier
to interpret (for people with intellectual disabilities). A real-world case
study is reported based on our experience to make documents more accessible.
",computer-science
"  Dynamical materials that capable of responding to optical stimuli have always
been pursued for designing novel photonic devices and functionalities, of which
the response speed and amplitude as well as integration adaptability and energy
effectiveness are especially critical. Here we show ultrafast pulse generation
by exploiting the ultrafast and sensitive nonlinear dynamical processes in
tunably solution-processed colloidal epsilon-near-zero (ENZ) transparent
conducting oxide (TCO) nanocrystals (NCs), of which the potential respond
response speed is >2 THz and modulation depth is ~23% pumped at ~0.7 mJ/cm2,
benefiting from the highly confined geometry in addition to the ENZ enhancement
effect. These ENZ NCs may offer a scalable and printable material solution for
dynamic photonic and optoelectronic devices.
",physics
"  In this work we examine how the updates addressing Meltdown and Spectre
vulnerabilities impact the performance of HPC applications. To study this we
use the application kernel module of XDMoD to test the performance before and
after the application of the vulnerability patches. We tested the performance
difference for multiple application and benchmarks including: NWChem, NAMD,
HPCC, IOR, MDTest and IMB. The results show that although some specific
functions can have performance decreased by as much as 74%, the majority of
individual metrics indicates little to no decrease in performance. The
real-world applications show a 2-3% decrease in performance for single node
jobs and a 5-11% decrease for parallel multi node jobs.
",computer-science
"  Approximate full configuration interaction (FCI) calculations have recently
become tractable for systems of unforeseen size thanks to stochastic and
adaptive approximations to the exponentially scaling FCI problem. The result of
an FCI calculation is a weighted set of electronic configurations, which can
also be expressed in terms of excitations from a reference configuration. The
excitation amplitudes contain information on the complexity of the electronic
wave function, but this information is contaminated by contributions from
disconnected excitations, i.e. those excitations that are just products of
independent lower-level excitations. The unwanted contributions can be removed
via a cluster decomposition procedure, making it possible to examine the
importance of connected excitations in complicated multireference molecules
which are outside the reach of conventional algorithms. We present an
implementation of the cluster decomposition analysis and apply it to both true
FCI wave functions, as well as wave functions generated from the adaptive
sampling CI (ASCI) algorithm. The cluster decomposition is useful for
interpreting calculations in chemical studies, as a diagnostic for the
convergence of various excitation manifolds, as well as as a guidepost for
polynomially scaling electronic structure models. Applications are presented
for (i) the double dissociation of water, (ii) the carbon dimer, (iii) the
{\pi} space of polyacenes, as well as (iv) the chromium dimer. While the
cluster amplitudes exhibit rapid decay with increasing rank for the first three
systems, even connected octuple excitations still appear important in Cr$_2$,
suggesting that spin-restricted single-reference coupled-cluster approaches may
not be tractable for some problems in transition metal chemistry.
",physics
"  We present a new class of service for location based social networks, called
the Flexible Group Spatial Keyword Query, which enables a group of users to
collectively find a point of interest (POI) that optimizes an aggregate cost
function combining both spatial distances and keyword similarities. In
addition, our query service allows users to consider the tradeoffs between
obtaining a sub-optimal solution for the entire group and obtaining an
optimimized solution but only for a subgroup.
We propose algorithms to process three variants of the query: (i) the group
nearest neighbor with keywords query, which finds a POI that optimizes the
aggregate cost function for the whole group of size n, (ii) the subgroup
nearest neighbor with keywords query, which finds the optimal subgroup and a
POI that optimizes the aggregate cost function for a given subgroup size m (m
<= n), and (iii) the multiple subgroup nearest neighbor with keywords query,
which finds optimal subgroups and corresponding POIs for each of the subgroup
sizes in the range [m, n]. We design query processing algorithms based on
branch-and-bound and best-first paradigms. Finally, we provide theoretical
bounds and conduct extensive experiments with two real datasets which verify
the effectiveness and efficiency of the proposed algorithms.
",computer-science
"  With the advent of large-scale heterogeneous search engines comes the problem
of unified search control resulting in mismatches that could have otherwise
avoided. A mechanism is needed to determine exact patterns in web mining and
ubiquitous device searching. In this paper we demonstrate the use of an
optimized string searching algorithm to recognize exact patterns from a large
database. The underlying principle in designing the algorithm is that each
letter that maps to a fixed real values and some arithmetic operations which
are applied to compute corresponding pattern and substring values. We have
implemented this algorithm in C. We have tested the algorithm using a large
dataset. We created our own dataset using DNA sequences. The experimental
result shows the number of mismatch occurred in string search from a large
database. Furthermore, some of the inherent weaknesses in the use of this
algorithm are highlighted.
",computer-science
"  During the High Luminosity LHC, the CMS detector will need charged particle
tracking at the hardware trigger level to maintain a manageable trigger rate
and achieve its physics goals. The tracklet approach is a track-finding
algorithm based on a road-search algorithm that has been implemented on
commercially available FPGA technology. The tracklet algorithm has achieved
high performance in track-finding and completes tracking within 3.4 $\mu$s on a
Xilinx Virtex-7 FPGA. An overview of the algorithm and its implementation on an
FPGA is given, results are shown from a demonstrator test stand and system
performance studies are presented.
",physics
"  We derive the uniqueness of weak solutions to the Shigesada-Kawasaki-Teramoto
(SKT) systems using the adjoint problem argument. Combining with [PT17] we then
derive the well-posedness for the SKT systems in space dimension $d\le 4$
",mathematics
"  The fundamental purpose of the present research article is to introduce the
basic principles of Dimensional Analysis in the context of the neoclassical
economic theory, in order to apply such principles to the fundamental relations
that underlay most models of economic growth. In particular, basic instruments
from Dimensional Analysis are used to evaluate the analytical consistency of
the Neoclassical economic growth model. The analysis shows that an adjustment
to the model is required in such a way that the principle of dimensional
homogeneity is satisfied.
",quantitative-finance
"  Gravitational wave observations of eccentric binary black hole mergers will
provide unequivocal evidence for the formation of these systems through
dynamical assembly in dense stellar environments. The study of these
astrophysically motivated sources is timely in view of electromagnetic
observations, consistent with the existence of stellar mass black holes in the
globular cluster M22 and in the Galactic center, and the proven detection
capabilities of ground-based gravitational wave detectors. In order to get
insights into the physics of these objects in the dynamical, strong-field
gravity regime, we present a catalog of 89 numerical relativity waveforms that
describe binary systems of non-spinning black holes with mass-ratios $1\leq q
\leq 10$, and initial eccentricities as high as $e_0=0.18$ fifteen cycles
before merger. We use this catalog to provide landmark results regarding the
loss of energy through gravitational radiation, both for quadrupole and
higher-order waveform multipoles, and the astrophysical properties, final mass
and spin, of the post-merger black hole as a function of eccentricity and
mass-ratio. We discuss the implications of these results for gravitational wave
source modeling, and the design of algorithms to search for and identify the
complex signatures of these events in realistic detection scenarios.
",computer-science
"  In this paper, we find a condition under which a Finsler space with Kropina
change of mth-root metric is projectively related to a mth-root metric and also
we find a condition under which this Kropina transformed mth-root metric is
locally dually flat. Moreover we find the condition for its Projective
flatness.
",mathematics
"  Gravitationally collapsed objects are known to be biased tracers of an
underlying density contrast. Using symmetry arguments, generalised biasing
schemes have recently been developed to relate the halo density contrast
$\delta_h$ with the underlying density contrast $\delta$, divergence of
velocity $\theta$ and their higher-order derivatives. This is done by
constructing invariants such as $s, t, \psi,\eta$. We show how the generating
function formalism in Eulerian standard perturbation theory (SPT) can be used
to show that many of the additional terms based on extended Galilean and
Lifshitz symmetry actually do not make any contribution to the higher-order
statistics of biased tracers. Other terms can also be drastically simplified
allowing us to write the vertices associated with $\delta_h$ in terms of the
vertices of $\delta$ and $\theta$, the higher-order derivatives and the bias
coefficients. We also compute the cumulant correlators (CCs) for two different
tracer populations. These perturbative results are valid for tree-level
contributions but at an arbitrary order. We also take into account the
stochastic nature bias in our analysis. Extending previous results of a local
polynomial model of bias, we express the one-point cumulants ${\cal S}_N$ and
their two-point counterparts, the CCs i.e. ${\cal C}_{pq}$, of biased tracers
in terms of that of their underlying density contrast counterparts. As a
by-product of our calculation we also discuss the results using approximations
based on Lagrangian perturbation theory (LPT).
",physics
"  As an injury heals, an embryo develops, or a carcinoma spreads, epithelial
cells systematically change their shape. In each of these processes cell shape
is studied extensively, whereas variation of shape from cell-to-cell is
dismissed most often as biological noise. But where do cell shape and variation
of cell shape come from? Here we report that cell shape and shape variation are
mutually constrained through a relationship that is purely geometrical. That
relationship is shown to govern maturation of the pseudostratified bronchial
epithelial layer cultured from both non-asthmatic and asthmatic donors as well
as formation of the ventral furrow in the epithelial monolayer of the
Drosophila embryo in vivo. Across these and other vastly different epithelial
systems, cell shape variation collapses to a family of distributions that is
common to all and potentially universal. That distribution, in turn, is
accounted for quantitatively by a mechanistic theory of cell-cell interaction
showing that cell shape becomes progressively less elongated and less variable
as the layer becomes progressively more jammed. These findings thus uncover a
connection between jamming and geometry that is generic -spanning jammed living
and inert systems alike- and demonstrate that proximity of the cell layer to
the jammed state is the principal determinant of the most primitive features of
epithelial cell shape and shape variation.
",physics
"  We report a sample of 463 high-mass starless clump (HMSC) candidates within
$-60°<l<60°$ and $-1°<b<1°$. This sample has been singled out from
10861 ATLASGAL clumps. All of these sources are not associated with any known
star-forming activities collected in SIMBAD and young stellar objects
identified using color-based criteria. We also make sure that the HMSC
candidates have neither point sources at 24 and 70 \micron~nor strong extended
emission at 24 $\mu$m. Most of the identified HMSCs are infrared ($\le24$
$\mu$m) dark and some are even dark at 70 $\mu$m. Their distribution shows
crowding in Galactic spiral arms and toward the Galactic center and some
well-known star-forming complexes. Many HMSCs are associated with large-scale
filaments. Some basic parameters were attained from column density and dust
temperature maps constructed via fitting far-infrared and submillimeter
continuum data to modified blackbodies. The HMSC candidates have sizes, masses,
and densities similar to clumps associated with Class II methanol masers and
HII regions, suggesting they will evolve into star-forming clumps. More than
90% of the HMSC candidates have densities above some proposed thresholds for
forming high-mass stars. With dust temperatures and luminosity-to-mass ratios
significantly lower than that for star-forming sources, the HMSC candidates are
externally heated and genuinely at very early stages of high-mass star
formation. Twenty sources with equivalent radius $r_\mathrm{eq}<0.15$ pc and
mass surface density $\Sigma>0.08$ g cm$^{-2}$ could be possible high-mass
starless cores. Further investigations toward these HMSCs would undoubtedly
shed light on comprehensively understanding the birth of high-mass stars.
",physics
"  We develop an on-line monitoring procedure to detect a change in a large
approximate factor model. Our statistics are based on a well-known property of
the $% \left( r+1\right) $-th eigenvalue of the sample covariance matrix of the
data (having defined $r$ as the number of common factors): whilst under the
null the $\left( r+1\right) $-th eigenvalue is bounded, under the alternative
of a change (either in the loadings, or in the number of factors itself) it
becomes spiked. Given that the sample eigenvalue cannot be estimated
consistently under the null, we regularise the problem by randomising the test
statistic in conjunction with sample conditioning, obtaining a sequence of
\textit{i.i.d.}, asymptotically chi-square statistics which are then employed
to build the monitoring scheme. Numerical evidence shows that our procedure
works very well in finite samples, with a very small probability of false
detections and tight detection times in presence of a genuine change-point.
",statistics
"  In this paper, we applied the multifractal detrended fluctuation analysis to
the daily means of wind speed measured by 119 weather stations distributed over
the territory of Switzerland. The analysis was focused on the inner time
fluctuations of wind speed, which could be more linked with the local
conditions of the highly varying topography of Switzerland. Our findings point
out to a persistent behaviour of all the measured wind speed series (indicated
by a Hurst exponent significantly larger than 0.5), and to a high
multifractality degree indicating a relative dominance of the large
fluctuations in the dynamics of wind speed, especially in the Swiss plateau,
which is comprised between the Jura and Alp mountain ranges. The study
represents a contribution to the understanding of the dynamical mechanisms of
wind speed variability in mountainous regions.
",statistics
"  This paper presents the notion of AND-OR reduction, which reduces a WF net to
a smaller net by iteratively contracting certain well-formed subnets into
single nodes until no more such contractions are possible. This reduction can
reveal the hierarchical structure of a WF net, and since it preserves certain
semantical properties such as soundness, it can help with analysing and
understanding why a WF net is sound or not. The reduction can also be used to
verify if a WF net is an AND-OR net. This class of WF nets was introduced in
earlier work, and arguably describes nets that follow good hierarchical design
principles. It is shown that the AND-OR reduction is confluent up to
isomorphism, which means that despite the inherent non-determinism that comes
from the choice of subnets that are contracted, the final result of the
reduction is always the same up to the choice of the identity of the nodes.
Based on this result, a polynomial-time algorithm is presented that computes
this unique result of the AND-OR reduction. Finally, it is shown how this
algorithm can be used to verify if a WF net is an AND-OR net.
",computer-science
"  We prove that the Teichmüller space of surfaces with given boundary lengths
equipped with the arc metric (resp. the Teichmüller metric) is almost
isometric to the Teichmüller space of punctured surfaces equipped with the
Thurston metric (resp. the Teichmüller metric).
",mathematics
"  Nefarious actors on social media and other platforms often spread rumors and
falsehoods through images whose metadata (e.g., captions) have been modified to
provide visual substantiation of the rumor/falsehood. This type of modification
is referred to as image repurposing, in which often an unmanipulated image is
published along with incorrect or manipulated metadata to serve the actor's
ulterior motives. We present the Multimodal Entity Image Repurposing (MEIR)
dataset, a substantially challenging dataset over that which has been
previously available to support research into image repurposing detection. The
new dataset includes location, person, and organization manipulations on
real-world data sourced from Flickr. We also present a novel, end-to-end, deep
multimodal learning model for assessing the integrity of an image by combining
information extracted from the image with related information from a knowledge
base. The proposed method is compared against state-of-the-art techniques on
existing datasets as well as MEIR, where it outperforms existing methods across
the board, with AUC improvement up to 0.23.
",computer-science
"  In this paper we give a close-to-sharp answer to the basic questions: When is
there a continuous way to add a point to a configuration of $n$ ordered points
on a surface $S$ of finite type so that all the points are still distinct? When
this is possible, what are all the ways to do it? More precisely, let
PConf$_n(S)$ be the space of ordered $n$-tuples of distinct points in $S$. Let
$f_n(S): \text{PConf}_{n+1}(S) \to \text{PConf}_n(S)$ be the map given by
$f_n(x_0,x_1,\ldots,x_n):=(x_1,\ldots,x_n)$. We will classify all continuous
sections of $f_n$ by proving:
1. If $S=\mathbb{R}^2$ and $n>3$, any section of $f_{n}(S)$ is either ""adding
a point at infinity"" or ""adding a point near $x_k$"". (We define these two terms
in Section 2.1, whether we can define ""adding a point near $x_k$"" or ""adding a
point at infinity"" depends in a delicate way on properties of $S$.)
2. If $S=S^2$ a $2$-sphere and $n>4$, any section of $f_{n}(S)$ is ""adding a
point near $x_k$"", if $S=S^2$ and $n=2$, the bundle $f_n(S)$ does not have a
section. (We define this term in Section 3.2)
3. If $S=S_g$ a surface of genus $g>1$ and for $n>1$, the bundle $f_{n}(S)$
does not have a section.
",mathematics
"  The physical properties of polycrystalline materials depend on their
microstructure, which is the nano-to-centimeter-scale arrangement of phases and
defects in their interior. Such microstructure depends on the shape,
crystallographic phase and orientation, and interfacing of the grains
constituting the material. This article presents a new non-destructive 3D
technique to study bulk samples with sizes in the cm range with a resolution of
hundred micrometers: time-of-flight three-dimensional neutron diffraction (ToF
3DND). Compared to existing analogous X-ray diffraction techniques, ToF 3DND
enables studies of samples that can be both larger in size and made of heavier
elements. Moreover, ToF 3DND facilitates the use of complicated sample
environments. The basic ToF 3DND setup, utilizing an imaging detector with high
spatial and temporal resolution, can easily be implemented at a time-of-flight
neutron beamline. The technique was developed and tested with data collected at
the Materials and Life Science Experimental Facility of the Japan Proton
Accelerator Complex (J-PARC) for an iron sample. We successfully reconstructed
the shape of 108 grains and developed an indexing procedure. The reconstruction
algorithms have been validated by reconstructing two stacked Co-Ni-Ga single
crystals and by comparison with a grain map obtained by post-mortem electron
backscatter diffraction (EBSD).
",physics
"  We use the Hubble Space Telescope to obtain WFC3/F390W imaging of the
supergroup SG1120-1202 at z=0.37, mapping the UV emission of 138
spectroscopically confirmed members. We measure total (F390W-F814W) colors and
visually classify the UV morphology of individual galaxies as ""clumpy"" or
""smooth."" Approximately 30% of the members have pockets of UV emission (clumpy)
and we identify for the first time in the group environment galaxies with UV
morphologies similar to the jellyfish galaxies observed in massive clusters. We
stack the clumpy UV members and measure a shallow internal color gradient,
which indicates unobscured star formation is occurring throughout these
galaxies. We also stack the four galaxy groups and measure a strong trend of
decreasing UV emission with decreasing projected group distance ($R_{proj}$).
We find that the strong correlation between decreasing UV emission and
increasing stellar mass can fully account for the observed trend in
(F390W-F814W) - $R_{proj}$, i.e., mass-quenching is the dominant mechanism for
extinguishing UV emission in group galaxies. Our extensive multi-wavelength
analysis of SG1120-1202 indicates that stellar mass is the primary predictor of
UV emission, but that the increasing fraction of massive (red/smooth) galaxies
at $R_{proj}$ < 2$R_{200}$ and existence of jellyfish candidates is due to the
group environment.
",physics
"  This book chapter introduces to the problem to which extent search strategies
of foraging biological organisms can be identified by statistical data analysis
and mathematical modeling. A famous paradigm in this field is the Levy Flight
Hypothesis: It states that under certain mathematical conditions Levy flights,
which are a key concept in the theory of anomalous stochastic processes,
provide an optimal search strategy. This hypothesis may be understood
biologically as the claim that Levy flights represent an evolutionary adaptive
optimal search strategy for foraging organisms. Another interpretation,
however, is that Levy flights emerge from the interaction between a forager and
a given (scale-free) distribution of food sources. These hypotheses are
discussed controversially in the current literature. We give examples and
counterexamples of experimental data and their analyses supporting and
challenging them.
",quantitative-biology
"  In 2012, Ananthnarayan, Avramov and Moore gave a new construction of
Gorenstein rings from two Gorenstein local rings, called their connected sum.
In this article, we investigate conditions on the associated graded ring of a
Gorenstein Artin local ring Q, which force it to be a connected sum over its
residue field. In particular, we recover some results regarding short, and
stretched, Gorenstein Artin rings. Finally, using these decompositions, we
obtain results about the rationality of the Poincare series of Q.
",mathematics
"  We study a unique network dataset including periodic surveys and electronic
logs of dyadic contacts via smartphones. The participants were a sample of
freshmen entering university in the Fall 2011. Their opinions on a variety of
political and social issues and lists of activities on campus were regularly
recorded at the beginning and end of each semester for the first three years of
study. We identify a behavioral network defined by call and text data, and a
cognitive network based on friendship nominations in ego-network surveys. Both
networks are limited to study participants. Since a wide range of attributes on
each node were collected in self-reports, we refer to these networks as
attribute-rich networks. We study whether student preferences for certain
attributes of friends can predict formation and dissolution of edges in both
networks. We introduce a method for computing student preferences for different
attributes which we use to predict link formation and dissolution. We then rank
these attributes according to their importance for making predictions. We find
that personal preferences, in particular political views, and preferences for
common activities help predict link formation and dissolution in both the
behavioral and cognitive networks.
",computer-science
"  The coherent optical response from 140~nm and 65~nm thick ZnO epitaxial
layers is studied using transient four-wave-mixing spectroscopy with picosecond
temporal resolution. Resonant excitation of neutral donor-bound excitons
results in two-pulse and three-pulse photon echoes. For the donor-bound A
exciton (D$^0$X$_\text{A}$) at temperature of 1.8~K we evaluate optical
coherence times $T_2=33-50$~ps corresponding to homogeneous linewidths of
$13-19~\mu$eV, about two orders of magnitude smaller as compared with the
inhomogeneous broadening of the optical transitions. The coherent dynamics is
determined mainly by the population decay with time $T_1=30-40$~ps, while pure
dephasing is negligible in the studied high quality samples even for strong
optical excitation. Temperature increase leads to a significant shortening of
$T_2$ due to interaction with acoustic phonons. In contrast, the loss of
coherence of the donor-bound B exciton (D$^0$X$_\text{B}$) is significantly
faster ($T_2=3.6$~ps) and governed by pure dephasing processes.
",physics
"  This paper extends a conventional, general framework for online adaptive
estimation problems for systems governed by unknown nonlinear ordinary
differential equations. The central feature of the theory introduced in this
paper represents the unknown function as a member of a reproducing kernel
Hilbert space (RKHS) and defines a distributed parameter system (DPS) that
governs state estimates and estimates of the unknown function. This paper 1)
derives sufficient conditions for the existence and stability of the infinite
dimensional online estimation problem, 2) derives existence and stability of
finite dimensional approximations of the infinite dimensional approximations,
and 3) determines sufficient conditions for the convergence of finite
dimensional approximations to the infinite dimensional online estimates. A new
condition for persistency of excitation in a RKHS in terms of its evaluation
functionals is introduced in the paper that enables proof of convergence of the
finite dimensional approximations of the unknown function in the RKHS. This
paper studies two particular choices of the RKHS, those that are generated by
exponential functions and those that are generated by multiscale kernels
defined from a multiresolution analysis.
",computer-science
"  This work addresses the problem of robust attitude control of quadcopters.
First, the mathematical model of the quadcopter is derived considering factors
such as nonlinearity, external disturbances, uncertain dynamics and strong
coupling. An adaptive twisting sliding mode control algorithm is then developed
with the objective of controlling the quadcopter to track desired attitudes
under various conditions. For this, the twisting sliding mode control law is
modified with a proposed gain adaptation scheme to improve the control
transient and tracking performance. Extensive simulation studies and
comparisons with experimental data have been carried out for a Solo quadcopter.
The results show that the proposed control scheme can achieve strong robustness
against disturbances while is adaptable to parametric variations.
",computer-science
"  This paper proposes a new method which builds a simplex based approximation
of a $d-1$-dimensional manifold $M$ separating a $d$-dimensional compact set
into two parts, and an efficient algorithm classifying points according to this
approximation. In a first variant, the approximation is made of simplices that
are defined in the cubes of a regular grid covering the compact set, from
boundary points that approximate the intersection between $M$ and the edges of
the cubes. All the simplices defined in a cube share the barycentre of the
boundary points located in the cube and include simplices similarly defined in
cube facets, and so on recursively. In a second variant, the Kuhn triangulation
is used to break the cubes into simplices and the approximation is defined in
these simplices from the boundary points computed on their edges, with the same
principle. Both the approximation in cubes and in simplices define a separating
surface on the whole grid and classifying a point on one side or the other of
this surface requires only a small number (at most $d$) of simple tests. Under
some conditions on the definition of the boundary points and on the reach of
$M$, for both variants the Hausdorff distance between $M$ and its approximation
decreases like $\mathcal{O}(d n_G^{-2})$, where $n_G$ is the number of points
on each axis of the grid. The approximation in cubes requires computing less
boundary points than the approximation in simplices but the latter is always a
manifold and is more accurate for a given value of $n_G$. The paper reports
tests of the method when varying $n_G$ and the dimensionality of the space (up
to 9).
",computer-science
"  We study polynomial generalizations of the Kontsevich automorphisms acting on
the skew-field of formal rational expressions in two non-commuting variables.
Our main result is the Laurentness and pseudo-positivity of iterations of these
automorphisms. The resulting expressions are described combinatorially using a
generalization of the combinatorics of compatible pairs in a maximal Dyck path
developed by Lee, Li, and Zelevinsky. By specializing to quasi-commuting
variables we obtain pseudo-positive expressions for rank 2 quantum generalized
cluster variables. In the case that all internal exchange coefficients are
zero, this quantum specialization provides a combinatorial construction of
counting polynomials for Grassmannians of submodules in exceptional
representations of valued quivers with two vertices.
",mathematics
"  This is the second paper of a series aimed to study the stellar kinematics
and population properties of bulges in highly-inclined barred galaxies. In this
work, we carry out a detailed analysis of the stellar age, metallicity and
[Mg/Fe] of 28 highly-inclined ($i > 65^{o}$) disc galaxies, from S0 to S(B)c,
observed with the SAURON integral-field spectrograph. The sample is divided
into two clean samples of barred and unbarred galaxies, on the basis of the
correlation between the stellar velocity and h$_3$ profiles, as well as the
level of cylindrical rotation within the bulge region. We find that while the
mean stellar age, metallicity and [Mg/Fe] in the bulges of barred and unbarred
galaxies are not statistically distinct, the [Mg/Fe] gradients along the minor
axis (away from the disc) of barred galaxies are significantly different than
those without bars. For barred galaxies, stars that are vertically further away
from the midplane are in general more [Mg/Fe]--enhanced and thus the vertical
gradients in [Mg/Fe] for barred galaxies are mostly positive, while for
unbarred bulges the [Mg/Fe] profiles are typically negative or flat. This
result, together with the old populations observed in the barred sample,
indicates that bars are long-lasting structures, and therefore are not easily
destroyed. The marked [Mg/Fe] differences with the bulges of unbarred galaxies
indicate that different formation/evolution scenarios are required to explain
their build-up, and emphasizes the role of bars in redistributing stellar
material in the bulge dominated regions.
",physics
"  By using the unfolding operators for periodic homogenization, we give a
general compactness result for a class of functions defined on bounded domains
presenting perforations of two different size. Then we apply this result to the
homogenization of the flow of a Bingham fluid in a porous medium with solid
obstacles of different size. Next we give the interpretation of the limit
problem in term of a non linear Darcy law.
",mathematics
"  The GW method is a many-body approach capable of providing quasiparticle
bands for realistic systems spanning physics, chemistry, and materials science.
Despite its power, GW is not routinely applied to large complex materials due
to its computational expense. We perform an exact recasting of the GW
polarizability and the self-energy as Laplace integrals over imaginary time
propagators. We then ""shred"" the propagators (via energy windowing) and
approximate them in a controlled manner by using Gauss-Laguerre quadrature and
discrete variable methods to treat the imaginary time propagators in real
space. The resulting cubic scaling GW method has a sufficiently small prefactor
to outperform standard quartic scaling methods on small systems (>=10 atoms)
and also represents a substantial improvement over several other cubic methods
tested. This approach is useful for evaluating quantum mechanical response
function involving large sums containing energy (difference) denominators.
",physics
"  Degeneracy loci of morphisms between vector bundles have been used in a wide
variety of situations. We introduce a vast generalization of this notion, based
on orbit closures of algebraic groups in their linear representations. A
preferred class of our orbital degeneracy loci is characterized by a certain
crepancy condition on the orbit closure, that allows to get some control on the
canonical sheaf. This condition is fulfilled for Richardson nilpotent orbits,
and also for partially decomposable skew-symmetric three-forms in six
variables. In order to illustrate the efficiency and flexibility of our
methods, we construct in both situations many Calabi--Yau manifolds of
dimension three and four, as well as a few Fano varieties, including some new
Fano fourfolds.
",mathematics
"  This is the second companion paper of arXiv:1601.03586. We consider the
morphism from the variety of triples introduced in arXiv:1601.03586 to the
affine Grassmannian. The direct image of the dualizing complex is a ring object
in the equivariant derived category on the affine Grassmannian (equivariant
derived Satake category). We show that various constructions in
arXiv:1601.03586 work for an arbitrary commutative ring object.
The second purpose of this paper is to study Coulomb branches associated with
star shaped quivers, which are expected to be conjectural Higgs branches of
$3d$ Sicilian theories in type $A$ by arXiv:1007.0992.
",mathematics
"  Nearly two centuries ago Talbot first observed the fascinating effect whereby
light propagating through a periodic structure generates a `carpet' of image
revivals in the near field. Here we report the first observation of the spatial
Talbot effect for light interacting with periodic Bose-Einstein condensate
interference fringes. The Talbot effect can lead to dramatic loss of fringe
visibility in images, degrading precision interferometry, however we
demonstrate how the effect can also be used as a tool to enhance visibility, as
well as extend the useful focal range of matter wave detection systems by
orders of magnitude. We show that negative optical densities arise from
matter-wave induced lensing of detuned imaging light -- yielding
Talbot-enhanced single-shot interference visibility of >135% compared to the
ideal visibility for resonant light.
",physics
"  Fluid-structure interactions are ubiquitous in nature and technology.
However, the systems are often so complex that numerical simulations or ad hoc
assumptions must be used to gain insight into the details of the complex
interactions between the fluid and solid mechanics. In this paper, we present
experiments and theory on viscous flow in a simple bioinspired soft valve which
illustrate essential features of interactions between hydrodynamic and elastic
forces at low Reynolds numbers. The setup comprises a sphere connected to a
spring located inside a tapering cylindrical channel. The spring is aligned
with the central axis of the channel and a pressure drop is applied across the
sphere, thus forcing the liquid through the narrow gap between the sphere and
the channel walls. The sphere's equilibrium position is determined by a balance
between spring and hydrodynamic forces. Since the gap thickness changes with
the sphere's position, the system has a pressure-dependent hydraulic
resistance. This leads to a non-linear relation between applied pressure and
flow rate: flow initially increases with pressure, but decreases when the
pressure exceeds a certain critical value as the gap closes. To rationalize
these observations, we propose a mathematical model that reduced the complexity
of the flow to a two-dimensional lubrication approximation. A closed-form
expression for the pressure-drop/flow rate is obtained which reveals that the
flow rate $Q$ depends on the pressure drop $\Delta p$, sphere radius $a$, gap
thickness $h_0$, and viscosity $\eta$ as $Q\sim \eta^{-1}
a^{1/2}h_0^{5/2}\left(\Delta p_c-\Delta p\right)^{5/2}\Delta p$, where the
critical pressure $\Delta p_c$ scales with the spring constant $k$ and sphere
radius $a$ as $\Delta p_c\sim k a^{-2}$. These predictions compared favorably
to the results of our experiments with no free parameters.
",physics
"  Generalization performance of classifiers in deep learning has recently
become a subject of intense study. Deep models, typically over-parametrized,
tend to fit the training data exactly. Despite this ""overfitting"", they perform
well on test data, a phenomenon not yet fully understood.
The first point of our paper is that strong performance of overfitted
classifiers is not a unique feature of deep learning. Using six real-world and
two synthetic datasets, we establish experimentally that kernel machines
trained to have zero classification or near zero regression error perform very
well on test data, even when the labels are corrupted with a high level of
noise. We proceed to give a lower bound on the norm of zero loss solutions for
smooth kernels, showing that they increase nearly exponentially with data size.
We point out that this is difficult to reconcile with the existing
generalization bounds. Moreover, none of the bounds produce non-trivial results
for interpolating solutions.
Second, we show experimentally that (non-smooth) Laplacian kernels easily fit
random labels, a finding that parallels results for ReLU neural networks. In
contrast, fitting noisy data requires many more epochs for smooth Gaussian
kernels. Similar performance of overfitted Laplacian and Gaussian classifiers
on test, suggests that generalization is tied to the properties of the kernel
function rather than the optimization process.
Certain key phenomena of deep learning are manifested similarly in kernel
methods in the modern ""overfitted"" regime. The combination of the experimental
and theoretical results presented in this paper indicates a need for new
theoretical ideas for understanding properties of classical kernel methods. We
argue that progress on understanding deep learning will be difficult until more
tractable ""shallow"" kernel methods are better understood.
",statistics
"  This paper explores the discrete Dynamic Causal Modeling (DDCM) and its
relationship with Directed Information (DI). We prove the conditional
equivalence between DDCM and DI in characterizing the causal relationship
between two brain regions. The theoretical results are demonstrated using fMRI
data obtained under both resting state and stimulus based state. Our numerical
analysis is consistent with that reported in previous study.
",statistics
"  Electrical forces are the background of all the interactions occurring in
biochemical systems. From here and by using a combination of ab-initio and
ad-hoc models, we introduce the first description of electric field profiles
with intrabond resolution to support a characterization of single bond forces
attending to its electrical origin. This fundamental issue has eluded a
physical description so far. Our method is applied to describe hydrogen bonds
(HB) in DNA base pairs. Numerical results reveal that base pairs in DNA could
be equivalent considering HB strength contributions, which challenges previous
interpretations of thermodynamic properties of DNA based on the assumption that
Adenine/Thymine pairs are weaker than Guanine/Cytosine pairs due to the sole
difference in the number of HB. Thus, our methodology provides solid
foundations to support the development of extended models intended to go deeper
into the molecular mechanisms of DNA functioning.
",quantitative-biology
"  Transport and security protocols are essential to ensure reliable and secure
communication between two parties. For IoT applications, these protocols must
be lightweight, since IoT devices are usually resource constrained.
Unfortunately, the existing transport and security protocols -- namely TCP/TLS
and UDP/DTLS -- fall short in terms of connection overhead, latency, and
connection migration when used in IoT applications. In this paper, after
studying the root causes of these shortcomings, we show how utilizing QUIC in
IoT scenarios results in a higher performance. Based on these observations, and
given the popularity of MQTT as an IoT application layer protocol, we integrate
MQTT with QUIC. By presenting the main APIs and functions developed, we explain
how connection establishment and message exchange functionalities work. We
evaluate the performance of MQTTw/QUIC versus MQTTw/TCP using wired, wireless,
and long-distance testbeds. Our results show that MQTTw/QUIC reduces connection
overhead in terms of the number of packets exchanged with the broker by up to
56%. In addition, by eliminating half-open connections, MQTTw/QUIC reduces
processor and memory usage by up to 83% and 50%, respectively. Furthermore, by
removing the head-of-line blocking problem, delivery latency is reduced by up
to 55%. We also show that the throughput drops experienced by MQTTw/QUIC when a
connection migration happens is considerably lower than that of MQTTw/TCP.
",computer-science
"  Let ${\bf M}=(M_1,\ldots, M_k)$ be a tuple of real $d\times d$ matrices.
Under certain irreducibility assumptions, we give checkable criteria for
deciding whether ${\bf M}$ possesses the following property: there exist two
constants $\lambda\in {\Bbb R}$ and $C>0$ such that for any $n\in {\Bbb N}$ and
any $i_1, \ldots, i_n \in \{1,\ldots, k\}$, either $M_{i_1} \cdots M_{i_n}={\bf
0}$ or $C^{-1} e^{\lambda n} \leq \| M_{i_1} \cdots M_{i_n} \| \leq C
e^{\lambda n}$, where $\|\cdot\|$ is a matrix norm. The proof is based on
symbolic dynamics and the thermodynamic formalism for matrix products. As
applications, we are able to check the absolute continuity of a class of
overlapping self-similar measures on ${\Bbb R}$, the absolute continuity of
certain self-affine measures in ${\Bbb R}^d$ and the dimensional regularity of
a class of sofic affine-invariant sets in the plane.
",mathematics
"  We image vortex creep at very low temperatures using Scanning Tunneling
Microscopy (STM) in the superconductor Rh$_9$In$_4$S$_4$ ($T_c$=2.25 K). We
measure the superconducting gap of Rh$_9$In$_4$S$_4$, finding $\Delta\approx
0.33$meV and image a hexagonal vortex lattice up to close to H$_{c2}$,
observing slow vortex creep at temperatures as low as 150 mK. We estimate
thermal and quantum barriers for vortex motion and show that thermal
fluctuations likely cause vortex creep, in spite of being at temperatures
$T/T_c<0.1$. We study creeping vortex lattices by making images during long
times and show that the vortex lattice remains hexagonal during creep with
vortices moving along one of the high symmetry axis of the vortex lattice.
Furthermore, the creep velocity changes with the scanning window suggesting
that creep depends on the local arrangements of pinning centers. Vortices
fluctuate on small scale erratic paths, indicating that the vortex lattice
makes jumps trying different arrangements during its travel along the main
direction for creep. The images provide a visual account of how vortex lattice
motion maintains hexagonal order, while showing dynamic properties
characteristic of a glass.
",physics
"  We study the problem of cooperative multi-agent reinforcement learning with a
single joint reward signal. This class of learning problems is difficult
because of the often large combined action and observation spaces. In the fully
centralized and decentralized approaches, we find the problem of spurious
rewards and a phenomenon we call the ""lazy agent"" problem, which arises due to
partial observability. We address these problems by training individual agents
with a novel value decomposition network architecture, which learns to
decompose the team value function into agent-wise value functions. We perform
an experimental evaluation across a range of partially-observable multi-agent
domains and show that learning such value-decompositions leads to superior
results, in particular when combined with weight sharing, role information and
information channels.
",computer-science
"  In this study, we explore peer-interaction effects in online networks on
speaking skill development. In particular, we present an evidence for gradual
buildup of skills in a small-group setting that has not been reported in the
literature. We introduce a novel dataset of six online communities consisting
of 158 participants focusing on improving their speaking skills. They
video-record speeches for 5 prompts in 10 days and exchange comments and
performance-ratings with their peers. We ask (i) whether the participants'
ratings are affected by their interaction patterns with peers, and (ii) whether
there is any gradual buildup of speaking skills in the communities towards
homogeneity. To analyze the data, we employ tools from the emerging field of
Graph Signal Processing (GSP). GSP enjoys a distinction from Social Network
Analysis in that the latter is concerned primarily with the connection
structures of graphs, while the former studies signals on top of graphs. We
study the performance ratings of the participants as graph signals atop
underlying interaction topologies. Total variation analysis of the graph
signals show that the participants' rating differences decrease with time
(slope=-0.04, p<0.01), while average ratings increase (slope=0.07,
p<0.05)--thereby gradually building up the ratings towards community-wide
homogeneity. We provide evidence for peer-influence through a prediction
formulation. Our consensus-based prediction model outperforms baseline
network-agnostic regression models by about 23% in predicting performance
ratings. This, in turn, shows that participants' ratings are affected by their
peers' ratings and the associated interaction patterns, corroborating previous
findings. Then, we formulate a consensus-based diffusion model that captures
these observations of peer-influence from our analyses.
",computer-science
"  We present a data-driven framework called generative adversarial privacy
(GAP). Inspired by recent advancements in generative adversarial networks
(GANs), GAP allows the data holder to learn the privatization mechanism
directly from the data. Under GAP, finding the optimal privacy mechanism is
formulated as a constrained minimax game between a privatizer and an adversary.
We show that for appropriately chosen adversarial loss functions, GAP provides
privacy guarantees against strong information-theoretic adversaries. We also
evaluate the performance of GAP on multi-dimensional Gaussian mixture models
and the GENKI face database.
",statistics
"  Superhydrophobic surfaces (SHSs) have the potential to achieve large drag
reduction for internal and external flow applications. However, experiments
have shown inconsistent results, with many studies reporting significantly
reduced performance. Recently, it has been proposed that surfactants,
ubiquitous in flow applications, could be responsible, by creating adverse
Marangoni stresses. Yet, testing this hypothesis is challenging. Careful
experiments with purified water show large interfacial stresses and,
paradoxically, adding surfactants yields barely measurable drag increases. This
suggests that other physical processes, such as thermal Marangoni stresses or
interface deflection, could explain the lower performance. To test the
surfactant hypothesis, we perform the first numerical simulations of flows over
a SHS inclusive of surfactant kinetics. These simulations reveal that
surfactant-induced stresses are significant at extremely low concentrations,
potentially yielding a no-slip boundary condition on the air--water interface
(the ""plastron"") for surfactant amounts below typical environmental values.
These stresses decrease as the streamwise distance between plastron stagnation
points increases. We perform microchannel experiments with thermally-controlled
SHSs consisting of streamwise parallel gratings, which confirm this numerical
prediction. We introduce a new, unsteady test of surfactant effects. When we
rapidly remove the driving pressure following a loading phase, a backflow
develops at the plastron, which can only be explained by surfactant gradients
formed in the loading phase. This demonstrates the significance of surfactants
in deteriorating drag reduction, and thus the importance of including
surfactant stresses in SHS models. Our time-dependent protocol can assess the
impact of surfactants in SHS testing and guide future mitigating designs.
",physics
"  The Abella interactive theorem prover has proven to be an effective vehicle
for reasoning about relational specifications. However, the system has a
limitation that arises from the fact that it is based on a simply typed logic:
formalizations that are identical except in the respect that they apply to
different types have to be repeated at each type. We develop an approach that
overcomes this limitation while preserving the logical underpinnings of the
system. In this approach object constructors, formulas and other relevant
logical notions are allowed to be parameterized by types, with the
interpretation that they stand for the (infinite) collection of corresponding
constructs that are obtained by instantiating the type parameters. The proof
structures that we consider for formulas that are schematized in this fashion
are limited to ones whose type instances are valid proofs in the simply typed
logic. We develop schematic proof rules that ensure this property, a task that
is complicated by the fact that type information influences the notion of
unification that plays a key role in the logic. Our ideas, which have been
implemented in an updated version of the system, accommodate schematic
polymorphism both in the core logic of Abella and in the executable
specification logic that it embeds.
",computer-science
"  We present an efficient coresets-based neural network compression algorithm
that provably sparsifies the parameters of a trained fully-connected neural
network in a manner that approximately preserves the network's output. Our
approach is based on an importance sampling scheme that judiciously defines a
sampling distribution over the neural network parameters, and as a result,
retains parameters of high importance while discarding redundant ones. We
leverage a novel, empirical notion of sensitivity and extend traditional
coreset constructions to the application of compressing parameters. Our
theoretical analysis establishes guarantees on the size and accuracy of the
resulting compressed neural network and gives rise to new generalization bounds
that may provide novel insights on the generalization properties of neural
networks. We demonstrate the practical effectiveness of our algorithm on a
variety of neural network configurations and real-world data sets.
",statistics
"  We describe the LoopInvGen tool for generating loop invariants that can
provably guarantee correctness of a program with respect to a given
specification. LoopInvGen is an efficient implementation of the inference
technique originally proposed in our earlier work on PIE
(this https URL).
In contrast to existing techniques, LoopInvGen is not restricted to a fixed
set of features -- atomic predicates that are composed together to build
complex loop invariants. Instead, we start with no initial features, and use
program synthesis techniques to grow the set on demand. This not only enables a
less onerous and more expressive approach, but also appears to be significantly
faster than the existing tools over the SyGuS-COMP 2017 benchmarks from the INV
track.
",computer-science
"  Transition metal oxides are well known for their complex magnetic and
electrical properties. When brought together in heterostructure geometries,
they show particular promise for spintronics and colossal magnetoresistance
applications. In this letter, we propose a new mechanism for the coupling
between layers of itinerant ferromagnetic materials in heterostructures. The
coupling is mediated by charge carriers that strive to maximally delocalize
through the heterostructure to gain kinetic energy. In doing so, they force a
ferromagnetic or antiferromagnetic coupling between the constituent layers. To
illustrate this, we focus on heterostructures composed of SrRuO$_3$ and
La$_{1-x}$A$_{x}$MnO$_3$ (A=Ca/Sr). Our mechanism is consistent with
antiferromagnetic alignment that is known to occur in multilayers of
SrRuO$_3$-La$_{1-x}$A$_{x}$MnO$_3$. To support our assertion, we present a
minimal Kondo-lattice model which reproduces the known magnetization properties
of such multilayers. In addition, we discuss a quantum well model for
heterostructures and argue that the spin-dependent density of states determines
the nature of the coupling. As a smoking gun signature, we propose that
bilayers with the same constituents will oscillate between ferromagnetic and
antiferromagnetic coupling upon tuning the relative thicknesses of the layers.
",physics
"  Microservice Architectures (MA) have the potential to increase the agility of
software development. In an era where businesses require software applications
to evolve to support software emerging requirements, particularly for Internet
of Things (IoT) applications, we examine the issue of microservice granularity
and explore its effect upon application latency. Two approaches to microservice
deployment are simulated; the first with microservices in a single container,
and the second with microservices partitioned across separate containers. We
observed a neglibible increase in service latency for the multiple container
deployment over a single container.
",computer-science
"  Thanks to modern sky surveys, over twenty stellar streams and overdensity
structures have been discovered in the halo of the Milky Way. In this paper, we
present an analysis of spectroscopic observations of individual stars from one
such structure, ""A13"", first identified as an overdensity using the M giant
catalog from the Two Micron All-Sky Survey. Our spectroscopic observations show
that stars identified with A13 have a velocity dispersion of $\lesssim$ 40
$\mathrm{km~s^{-1}}$, implying that it is a genuine coherent structure rather
than a chance super-position of random halo stars. From its position on the
sky, distance ($\sim$15~kpc heliocentric), and kinematical properties, A13 is
likely to be an extension of another low Galactic latitude substructure -- the
Galactic Anticenter Stellar Structure (also known as the Monoceros Ring) --
towards smaller Galactic longitude and farther distance. Furthermore, the
kinematics of A13 also connect it with another structure in the southern
Galactic hemisphere -- the Triangulum-Andromeda overdensity. We discuss these
three connected structures within the context of a previously proposed scenario
that one or all of these features originate from the disk of the Milky Way.
",physics
"  This paper reports on a data-driven, interaction-aware motion prediction
approach for pedestrians in environments cluttered with static obstacles. When
navigating in such workspaces shared with humans, robots need accurate motion
predictions of the surrounding pedestrians. Human navigation behavior is mostly
influenced by their surrounding pedestrians and by the static obstacles in
their vicinity. In this paper we introduce a new model based on Long-Short Term
Memory (LSTM) neural networks, which is able to learn human motion behavior
from demonstrated data. To the best of our knowledge, this is the first
approach using LSTMs, that incorporates both static obstacles and surrounding
pedestrians for trajectory forecasting. As part of the model, we introduce a
new way of encoding surrounding pedestrians based on a 1d-grid in polar angle
space. We evaluate the benefit of interaction-aware motion prediction and the
added value of incorporating static obstacles on both simulation and real-world
datasets by comparing with state-of-the-art approaches. The results show, that
our new approach outperforms the other approaches while being very
computationally efficient and that taking into account static obstacles for
motion predictions significantly improves the prediction accuracy, especially
in cluttered environments.
",computer-science
"  Event detection is a critical feature in data-driven systems as it assists
with the identification of nominal and anomalous behavior. Event detection is
increasingly relevant in robotics as robots operate with greater autonomy in
increasingly unstructured environments. In this work, we present an accurate,
robust, fast, and versatile measure for skill and anomaly identification. A
theoretical proof establishes the link between the derivative of the
log-likelihood of the HMM filtered belief state and the latest emission
probabilities. The key insight is the inverse relationship in which gradient
analysis is used for skill and anomaly identification. Our measure showed
better performance across all metrics than related state-of-the art works. The
result is broadly applicable to domains that use HMMs for event detection.
",computer-science
"  Metamaterial analogues of electromagnetically induced transparency (EIT) have
been intensively studied and widely employed for slow light and enhanced
nonlinear effects. In particular, the active modulation of the EIT analogue and
well-controlled group delay in metamaterials have shown great prospects in
optical communication networks. Previous studies have focused on the optical
control of the EIT analogue by integrating the photoactive materials into the
unit cell, however, the response time is limited by the recovery time of the
excited carriers in these bulk materials. Graphene has recently emerged as an
exceptional optoelectronic material. It shows an ultrafast relaxation time on
the order of picosecond and its conductivity can be tuned via manipulating the
Fermi energy. Here we integrate a monolayer graphene into metal-based terahertz
(THz) metamaterials, and realize a complete modulation in the resonance
strength of the EIT analogue at the accessible Fermi energy. The physical
mechanism lies in the active tuning the damping rate of the dark mode resonator
through the recombination effect of the conductive graphene. Note that the
monolayer morphology in our work is easier to fabricate and manipulate than
isolated fashion. This work presents a novel modulation strategy of the EIT
analogue in the hybrid metamaterials, and pave the way towards designing very
compact slow light devices to meet future demand of ultrafast optical signal
processing.
",physics
"  We introduce an algorithm to generate (not solve) spin-glass instances with
planted solutions of arbitrary size and structure. First, a set of small
problem patches with open boundaries is solved either exactly or with a
heuristic, and then the individual patches are stitched together to create a
large problem with a known planted solution. Because in these problems
frustration is typically smaller than in random problems, we first assess the
typical computational complexity of the individual patches using population
annealing Monte Carlo, and introduce an approach that allows one to fine-tune
the typical computational complexity of the patch-planted system. The scaling
of the typical computational complexity of these planted instances with various
numbers of patches and patch sizes is investigated and compared to random
instances.
",physics
"  The paper presents a new state estimation algorithm for a bilinear equation
representing the Fourier- Galerkin (FG) approximation of the Navier-Stokes (NS)
equations on a torus in R2. This state equation is subject to uncertain but
bounded noise in the input (Kolmogorov forcing) and initial conditions, and its
output is incomplete and contains bounded noise. The algorithm designs a
time-dependent gain such that the estimation error converges to zero
exponentially. The sufficient condition for the existence of the gain are
formulated in the form of algebraic Riccati equations. To demonstrate the
results we apply the proposed algorithm to the reconstruction a chaotic fluid
flow from incomplete and noisy data.
",physics
"  In order to achieve a good level of autonomy in unmanned helicopters, an
accurate replication of vehicle dynamics is required, which is achievable
through precise mathematical modeling. This paper aims to identify a parametric
state-space system for an unmanned helicopter to a good level of accuracy using
Invasive Weed Optimization (IWO) algorithm. The flight data of Align TREX 550
flybarless helicopter is used in the identification process. The rigid-body
dynamics of the helicopter is modeled in a state-space form that has 40
parameters, which serve as control variables for the IWO algorithm. The results
after 1000 iterations were compared with the traditionally used Prediction
Error Minimization (PEM) method and also with Genetic Algorithm (GA), which
serve as references. Results show a better level of correlation between the
actual and estimated responses of the system identified using IWO to that of
PEM and GA.
",computer-science
"  Operation of an atomtronic battery is demonstrated where a finite-temperature
Bose-Einstein condensate stored in one half of a double-well potential is
coupled to an initially empty load well that is impedance matched by a resonant
terminator beam. The atom number and temperature of the condensate are
monitored during the discharge cycle, and are used to calculate fundamental
properties of the battery. The discharge behavior is analyzed according to a
Thévenin equivalent circuit that contains a finite internal resistance to
account for dissipation in the battery. Battery performance at multiple
discharge rates is characterized by the peak power output, and the current and
energy capacities of the system.
",physics
"  Cross-correlations in the activity in neural networks are commonly used to
characterize their dynamical states and their anatomical and functional
organizations. Yet, how these latter network features affect the spatiotemporal
structure of the correlations in recurrent networks is not fully understood.
Here, we develop a general theory for the emergence of correlated neuronal
activity from the dynamics in strongly recurrent networks consisting of several
populations of binary neurons. We apply this theory to the case in which the
connectivity depends on the anatomical or functional distance between the
neurons. We establish the architectural conditions under which the system
settles into a dynamical state where correlations are strong, highly robust and
spatially modulated. We show that such strong correlations arise if the network
exhibits an effective feedforward structure. We establish how this feedforward
structure determines the way correlations scale with the network size and the
degree of the connectivity. In networks lacking an effective feedforward
structure correlations are extremely small and only weakly depend on the number
of connections per neuron. Our work shows how strong correlations can be
consistent with highly irregular activity in recurrent networks, two key
features of neuronal dynamics in the central nervous system.
",quantitative-biology
"  Data warehouse performance is usually achieved through physical data
structures such as indexes or materialized views. In this context, cost models
can help select a relevant set ofsuch performance optimization structures.
Nevertheless, selection becomes more complex in the cloud. The criterion to
optimize is indeed at least two-dimensional, with monetary cost balancing
overall query response time. This paper introduces new cost models that fit
into the pay-as-you-go paradigm of cloud computing. Based on these cost models,
an optimization problem is defined to discover, among candidate views, those to
be materialized to minimize both the overall cost of using and maintaining the
database in a public cloud and the total response time ofa given query
workload. We experimentally show that maintaining materialized views is always
advantageous, both in terms of performance and cost.
",computer-science
"  We tabulate spontaneous emission rates for all possible 811
electric-dipole-allowed transitions between the 75 lowest-energy states of Ca
I. These involve the $4sns$ ($n=4-8$), $4snp$ ($n=4-7$), $4snd$ ($n=3-6$),
$4snf$ ($n=4-6$), $4p^2$, and $3d4p$ electronic configurations. We compile the
transition rates by carrying out ab initio relativistic calculations using the
combined method of configuration interaction and many-body perturbation theory.
The results are compared to the available literature values. The tabulated
rates can be useful in various applications, such as optimizing laser cooling
in magneto-optical traps, estimating various systematic effects in optical
clocks and evaluating static or dynamic polarizabilities and long-range
atom-atom interaction coefficients and related atomic properties.
",physics
"  The emissivity of common materials remains constant with temperature
variations, and cannot drastically change. However, it is possible to design
its entire behaviour as a function of temperature, and to significantly modify
the thermal emissivity of a surface through the combination of different
materials and patterns. Here, we show that smart patterned surfaces consisting
of smaller structures (motifs) may be designed to respond uniquely through
combinatorial design strategies by transforming themselves from 2D to 3D
complex structures with a two-way shape memory effect. The smart surfaces can
passively manipulate thermal radiation without-the use of controllers and power
supplies-because their modus operandi has already been programmed and
integrated into their intrinsic characteristics; the environment provides the
energy required for their activation. Each motif emits thermal radiation in a
certain manner, as it changes its geometry; however, the spatial distribution
of these motifs causes them to interact with each other. Therefore, their
combination and interaction determine the global behaviour of the surfaces,
thus enabling their a priori design. The emissivity behaviour is not random; it
is determined by two fundamental parameters, namely the combination of
orientations in which the motifs open (n-fold rotational symmetry (rn)) and the
combination of materials (colours) on the motifs; these generate functions
which fully determine the dependency of the emissivity on the temperature.
",physics
"  Finding actions that satisfy the constraints imposed by both external inputs
and internal representations is central to decision making. We demonstrate that
some important classes of constraint satisfaction problems (CSPs) can be solved
by networks composed of homogeneous cooperative-competitive modules that have
connectivity similar to motifs observed in the superficial layers of neocortex.
The winner-take-all modules are sparsely coupled by programming neurons that
embed the constraints onto the otherwise homogeneous modular computational
substrate. We show rules that embed any instance of the CSPs planar four-color
graph coloring, maximum independent set, and Sudoku on this substrate, and
provide mathematical proofs that guarantee these graph coloring problems will
convergence to a solution. The network is composed of non-saturating linear
threshold neurons. Their lack of right saturation allows the overall network to
explore the problem space driven through the unstable dynamics generated by
recurrent excitation. The direction of exploration is steered by the constraint
neurons. While many problems can be solved using only linear inhibitory
constraints, network performance on hard problems benefits significantly when
these negative constraints are implemented by non-linear multiplicative
inhibition. Overall, our results demonstrate the importance of instability
rather than stability in network computation, and also offer insight into the
computational role of dual inhibitory mechanisms in neural circuits.
",quantitative-biology
"  Convolutional neural networks (CNNs) have become the dominant neural network
architecture for solving many state-of-the-art (SOA) visual processing tasks.
Even though Graphical Processing Units (GPUs) are most often used in training
and deploying CNNs, their power efficiency is less than 10 GOp/s/W for
single-frame runtime inference. We propose a flexible and efficient CNN
accelerator architecture called NullHop that implements SOA CNNs useful for
low-power and low-latency application scenarios. NullHop exploits the sparsity
of neuron activations in CNNs to accelerate the computation and reduce memory
requirements. The flexible architecture allows high utilization of available
computing resources across kernel sizes ranging from 1x1 to 7x7. NullHop can
process up to 128 input and 128 output feature maps per layer in a single pass.
We implemented the proposed architecture on a Xilinx Zynq FPGA platform and
present results showing how our implementation reduces external memory
transfers and compute time in five different CNNs ranging from small ones up to
the widely known large VGG16 and VGG19 CNNs. Post-synthesis simulations using
Mentor Modelsim in a 28nm process with a clock frequency of 500 MHz show that
the VGG19 network achieves over 450 GOp/s. By exploiting sparsity, NullHop
achieves an efficiency of 368%, maintains over 98% utilization of the MAC
units, and achieves a power efficiency of over 3TOp/s/W in a core area of
6.3mm$^2$. As further proof of NullHop's usability, we interfaced its FPGA
implementation with a neuromorphic event camera for real time interactive
demonstrations.
",computer-science
"  Biological plastic neural networks are systems of extraordinary computational
capabilities shaped by evolution, development, and lifetime learning. The
interplay of these elements leads to the emergence of adaptive behavior and
intelligence. Inspired by such intricate natural phenomena, Evolved Plastic
Artificial Neural Networks (EPANNs) use simulated evolution in-silico to breed
plastic neural networks with a large variety of dynamics, architectures, and
plasticity rules: these artificial systems are composed of inputs, outputs, and
plastic components that change in response to experiences in an environment.
These systems may autonomously discover novel adaptive algorithms, and lead to
hypotheses on the emergence of biological adaptation. EPANNs have seen
considerable progress over the last two decades. Current scientific and
technological advances in artificial neural networks are now setting the
conditions for radically new approaches and results. In particular, the
limitations of hand-designed networks could be overcome by more flexible and
innovative solutions. This paper brings together a variety of inspiring ideas
that define the field of EPANNs. The main methods and results are reviewed.
Finally, new opportunities and developments are presented.
",computer-science
"  Let $S=\{x_1,x_2,\dots,x_n\}$ be a set of distinct positive integers, and let
$f$ be an arithmetical function. The GCD matrix $(S)_f$ on $S$ associated with
$f$ is defined as the $n\times n$ matrix having $f$ evaluated at the greatest
common divisor of $x_i$ and $x_j$ as its $ij$ entry. The LCM matrix $[S]_f$ is
defined similarly. We consider inertia, positive definiteness and $\ell_p$ norm
of GCD and LCM matrices and their unitary analogs. Proofs are based on matrix
factorizations and convolutions of arithmetical functions.
",mathematics
"  This paper proposes a general framework for structure-preserving model
reduction of a secondorder network system based on graph clustering. In this
approach, vertex dynamics are captured by the transfer functions from inputs to
individual states, and the dissimilarities of vertices are quantified by the
H2-norms of the transfer function discrepancies. A greedy hierarchical
clustering algorithm is proposed to place those vertices with similar dynamics
into same clusters. Then, the reduced-order model is generated by the
Petrov-Galerkin method, where the projection is formed by the characteristic
matrix of the resulting network clustering. It is shown that the simplified
system preserves an interconnection structure, i.e., it can be again
interpreted as a second-order system evolving over a reduced graph.
Furthermore, this paper generalizes the definition of network controllability
Gramian to second-order network systems. Based on it, we develop an efficient
method to compute H2-norms and derive the approximation error between the
full-order and reduced-order models. Finally, the approach is illustrated by
the example of a small-world network.
",computer-science
"  A fundamental question in language learning concerns the role of a speaker's
first language in second language acquisition. We present a novel methodology
for studying this question: analysis of eye-movement patterns in second
language reading of free-form text. Using this methodology, we demonstrate for
the first time that the native language of English learners can be predicted
from their gaze fixations when reading English. We provide analysis of
classifier uncertainty and learned features, which indicates that differences
in English reading are likely to be rooted in linguistic divergences across
native languages. The presented framework complements production studies and
offers new ground for advancing research on multilingualism.
",computer-science
"  Deep learning has yielded state-of-the-art performance on many natural
language processing tasks including named entity recognition (NER). However,
this typically requires large amounts of labeled data. In this work, we
demonstrate that the amount of labeled training data can be drastically reduced
when deep learning is combined with active learning. While active learning is
sample-efficient, it can be computationally expensive since it requires
iterative retraining. To speed this up, we introduce a lightweight architecture
for NER, viz., the CNN-CNN-LSTM model consisting of convolutional character and
word encoders and a long short term memory (LSTM) tag decoder. The model
achieves nearly state-of-the-art performance on standard datasets for the task
while being computationally much more efficient than best performing models. We
carry out incremental active learning, during the training process, and are
able to nearly match state-of-the-art performance with just 25\% of the
original training data.
",computer-science
"  We present the discovery of four low-mass ($M<0.6$ $M_\odot$) eclipsing
binary (EB) systems in the sub-Gyr old Praesepe open cluster using Kepler/K2
time-series photometry and Keck/HIRES spectroscopy. We present a new Gaussian
process eclipsing binary model, GP-EBOP, as well as a method of simultaneously
determining effective temperatures and distances for EBs. Three of the reported
systems (AD 3814, AD 2615 and AD 1508) are detached and double-lined, and
precise solutions are presented for the first two. We determine masses and
radii to 1-3% precision for AD 3814 and to 5-6% for AD 2615. Together with
effective temperatures determined to $\sim$50 K precision, we test the PARSEC
v1.2 and BHAC15 stellar evolution models. Our EB parameters are more consistent
with the PARSEC models, primarily because the BHAC15 temperature scale is
hotter than our data over the mid M-dwarf mass range probed. Both ADs 3814 and
2615, which have orbital periods of 6.0 and 11.6 days, are circularized but not
synchronized. This suggests that either synchronization proceeds more slowly in
fully convective stars than the theory of equilibrium tides predicts or
magnetic braking is currently playing a more important role than tidal forces
in the spin evolution of these binaries. The fourth system (AD 3116) comprises
a brown dwarf transiting a mid M-dwarf, which is the first such system
discovered in a sub-Gyr open cluster. Finally, these new discoveries increase
the number of characterized EBs in sub-Gyr open clusters by 20% (40%) below
$M<1.5$ $M_{\odot}$ ($M<0.6$ $M_{\odot}$).
",physics
"  In statistics cumulants are defined to be functions that measure the linear
independence of random variables. In the non-communicative case the Boolean
cumulants can be described as functions that measure deviation of a map between
algebras from being an algebra morphism. In Algebraic topology maps that are
homotopic to being algebra morphisms are studied using the theory of $A_\infty$
algebras. In this paper we will explore the link between these two points of
views on maps between algebras that are not algebra maps.
",mathematics
"  Persistent spread measurement is to count the number of distinct elements
that persist in each network flow for predefined time periods. It has many
practical applications, including detecting long-term stealthy network
activities in the background of normal-user activities, such as stealthy DDoS
attack, stealthy network scan, or faked network trend, which cannot be detected
by traditional flow cardinality measurement. With big network data, one
challenge is to measure the persistent spreads of a massive number of flows
without incurring too much memory overhead as such measurement may be performed
at the line speed by network processors with fast but small on-chip memory. We
propose a highly compact Virtual Intersection HyperLogLog (VI-HLL) architecture
for this purpose. It achieves far better memory efficiency than the best prior
work of V-Bitmap, and in the meantime drastically extends the measurement
range. Theoretical analysis and extensive experiments demonstrate that VI-HLL
provides good measurement accuracy even in very tight memory space of less than
1 bit per flow.
",computer-science
"  We use new X-ray data obtained with the Nuclear Spectroscopic Telescope Array
(NuSTAR), near-infrared (NIR) fluxes, and mid-infrared (MIR) spectra of a
sample of 24 unobscured type 1 active galactic nuclei (AGN) to study the
correlation between various hard X-ray bands between 3 and 80 keV and the
infrared (IR) emission. The IR to X-ray correlation spectrum (IRXCS) shows a
maximum at ~15-20 micron, coincident with the peak of the AGN contribution to
the MIR spectra of the majority of the sample. There is also a NIR correlation
peak at ~2 micron, which we associate with the NIR bump observed in some type 1
AGN at ~1-5 micron and is likely produced by nuclear hot dust emission. The
IRXCS shows practically the same behaviour in all the X-ray bands considered,
indicating a common origin for all of them. We finally evaluated correlations
between the X-ray luminosities and various MIR emission lines. All the lines
show a good correlation with the hard X-rays (rho>0.7), but we do not find the
expected correlation between their ionization potentials and the strength of
the IRXCS.
",physics
"  We introduce a novel method for defining geographic districts in road
networks using stable matching. In this approach, each geographic district is
defined in terms of a center, which identifies a location of interest, such as
a post office or polling place, and all other network vertices must be labeled
with the center to which they are associated. We focus on defining geographic
districts that are equitable, in that every district has the same number of
vertices and the assignment is stable in terms of geographic distance. That is,
there is no unassigned vertex-center pair such that both would prefer each
other over their current assignments. We solve this problem using a version of
the classic stable matching problem, called symmetric stable matching, in which
the preferences of the elements in both sets obey a certain symmetry. In our
case, we study a graph-based version of stable matching in which nodes are
stably matched to a subset of nodes denoted as centers, prioritized by their
shortest-path distances, so that each center is apportioned a certain number of
nodes. We show that, for a planar graph or road network with $n$ nodes and $k$
centers, the problem can be solved in $O(n\sqrt{n}\log n)$ time, which improves
upon the $O(nk)$ runtime of using the classic Gale-Shapley stable matching
algorithm when $k$ is large. Finally, we provide experimental results on road
networks for these algorithms and a heuristic algorithm that performs better
than the Gale-Shapley algorithm for any range of values of $k$.
",computer-science
"  Many systems of structured argumentation explicitly require that the facts
and rules that make up the argument for a conclusion be the minimal set
required to derive the conclusion. ASPIC+ does not place such a requirement on
arguments, instead requiring that every rule and fact that are part of an
argument be used in its construction. Thus ASPIC+ arguments are minimal in the
sense that removing any element of the argument would lead to a structure that
is not an argument. In this brief note we discuss these two types of minimality
and show how the first kind of minimality can, if desired, be recovered in
ASPIC+.
",computer-science
"  Many classical results in relativity theory concerning spherically symmetric
space-times have easy generalizations to warped product space-times, with a
two-dimensional Lorentzian base and arbitrary dimensional Riemannian fibers. We
first give a systematic presentation of the main geometric constructions, with
emphasis on the Kodama vector field and the Hawking energy; the construction is
signature independent. This leads to proofs of general Birkhoff-type theorems
for warped product manifolds; our theorems in particular apply to situations
where the warped product manifold is not necessarily Einstein, and thus can be
applied to solutions with matter content in general relativity. Next we
specialize to the Lorentzian case and study the propagation of null expansions
under the assumption of the dominant energy condition. We prove several
non-existence results relating to the Yamabe class of the fibers, in the spirit
of the black-hole topology theorem of Hawking-Galloway-Schoen. Finally we
discuss the effect of the warped product ansatz on matter models. In particular
we construct several cosmological solutions to the Einstein-Euler equations
whose spatial geometry is generally not isotropic.
",mathematics
"  Different questions related with analysis of extreme values and outliers
arise frequently in practice. To exclude extremal observations and outliers is
not a good decision because they contain important information about the
observed distribution. The difficulties with their usage are usually related to
the estimation of the tail index in case it exists. There are many measures for
the center of the distribution, e.g. mean, mode, median. There are many
measures of the variance, asymmetry, and kurtosis, but there is no easy
characteristic for heavy-tailedness of the observed distribution. Here we
propose such a measure, give some examples and explore some of its properties.
This allows us to introduce a classification of the distributions, with respect
to their heavy-tailedness. The idea is to help and navigate practitioners for
accurate and easier work in the field of probability distributions.
Using the properties of the defined characteristics some distribution
sensitive extremal index estimators are proposed and their properties are
partially investigated.
",statistics
"  This document consists of two papers, both submitted, and supplementary
material. The submitted papers are here given as Parts I and II.
Part I establishes results, used in Part II, 'on functions and inverses, both
positive, decreasing and convex'.
Part II uses results from Part I to extablish 'inequalities for the
fundamental Robin eigenvalue for the Laplacian on N-dimensional boxes'
",mathematics
"  Motivated by the ${\rm \Psi}$-Riemann-Liouville $({\rm \Psi-RL})$ fractional
derivative and by the ${\rm \Psi}$-Hilfer $({\rm \Psi-H})$ fractional
derivative, we introduced a new fractional operator the so-called
$\rm\Psi-$fractional integral. We present some important results by means of
theorems and in particular, that the $\rm\Psi-$fractional integration operator
is limited. In this sense, we discuss some examples, in particular, involving
the Mittag-Leffler $({\rm M-L})$ function, of paramount importance in the
solution of population growth problem, as approached. On the other hand, we
realize a brief discussion on the uniqueness of nonlinear $\Psi$-fractional
Volterra integral equation (${\rm VIE}$) using $\beta-$distance functions.
",mathematics
"  Nowadays, modern earth observation programs produce huge volumes of satellite
images time series (SITS) that can be useful to monitor geographical areas
through time. How to efficiently analyze such kind of information is still an
open question in the remote sensing field. Recently, deep learning methods
proved suitable to deal with remote sensing data mainly for scene
classification (i.e. Convolutional Neural Networks - CNNs - on single images)
while only very few studies exist involving temporal deep learning approaches
(i.e Recurrent Neural Networks - RNNs) to deal with remote sensing time series.
In this letter we evaluate the ability of Recurrent Neural Networks, in
particular the Long-Short Term Memory (LSTM) model, to perform land cover
classification considering multi-temporal spatial data derived from a time
series of satellite images. We carried out experiments on two different
datasets considering both pixel-based and object-based classification. The
obtained results show that Recurrent Neural Networks are competitive compared
to state-of-the-art classifiers, and may outperform classical approaches in
presence of low represented and/or highly mixed classes. We also show that
using the alternative feature representation generated by LSTM can improve the
performances of standard classifiers.
",computer-science
"  Nine transiting Earth-sized planets have recently been discovered around
nearby late M dwarfs, including the TRAPPIST-1 planets and two planets
discovered by the MEarth survey, GJ 1132b and LHS 1140b. These planets are the
smallest known planets that may have atmospheres amenable to detection with
JWST. We present model thermal emission and transmission spectra for each
planet, varying composition and surface pressure of the atmosphere. We base
elemental compositions on those of Earth, Titan, and Venus and calculate the
molecular compositions assuming chemical equilibrium, which can strongly depend
on temperature. Both thermal emission and transmission spectra are sensitive to
the atmospheric composition; thermal emission spectra are sensitive to surface
pressure and temperature. We predict the observability of each planet's
atmosphere with JWST. GJ 1132b and TRAPPIST-1b are excellent targets for
emission spectroscopy with JWST/MIRI, requiring fewer than 10 eclipse
observations. Emission photometry for TRAPPIST-1c requires 5-15 eclipses; LHS
1140b and TRAPPIST-1d, TRAPPIST-1e, and TRAPPIST-1f, which could possibly have
surface liquid water, may be accessible with photometry. Seven of the nine
planets are strong candidates for transmission spectroscopy measurements with
JWST, though the number of transits required depends strongly on the planets'
actual masses. Using the measured masses, fewer than 20 transits are required
for a 5 sigma detection of spectral features for GJ 1132b and six of the
TRAPPIST-1 planets. Dedicated campaigns to measure the atmospheres of these
nine planets will allow us, for the first time, to probe formation and
evolution processes of terrestrial planetary atmospheres beyond our solar
system.
",physics
"  The temperature dependence of the electrical resistivity of the
heterostructures consisting of single crystalline LaMnO$_3$ samples with
different crystallographic orientations covered by the epitaxial ferroelectric
Ba$_{0.8}$Sr$_{0.2}$TiO$_3$ film has been studied. Results obtained for the
heterostructure have been compared with the electrical resistivity of the
single crystalline LaMnO$_3$ without the film. It was found that for the
samples with the films where the polarization axis is perpendicular to the
crystal surface the electrical resistivity strongly decreases, and at the
temperature below ~160 K undergoes the insulator-metal transition. Ab-initio
calculations were also performed for the structural and electronic properties
of the BaTiO$_3$/LaMnO$_3$ heterostructure. Transition to the 2D electron gas
at the interface is shown.
",physics
"  In this paper, we compute the number of z-classes (conjugacy classes of
centralizers of elements) in the symmetric group S_n, when n is greater or
equal to 3 and alternating group A_n, when n is greater or equal to 4. It turns
out that the difference between the number of conjugacy classes and the number
of z-classes for S_n is determined by those restricted partitions of n-2 in
which 1 and 2 do not appear as its part. And, in the case of alternating
groups, it is determined by those restricted partitions of n-3 which has all
its parts distinct, odd and in which 1 (and 2) does not appear as its part,
along with an error term. The error term is given by those partitions of n
which have each of its part distinct, odd and perfect square. Further, we prove
that the number of rational-valued irreducible complex characters for A_n is
same as the number of conjugacy classes which are rational.
",mathematics
"  Effective gauge fields have allowed the emulation of matter under strong
magnetic fields leading to the realization of Harper-Hofstadter, Haldane
models, and led to demonstrations of one-way waveguides and topologically
protected edge states. Central to these discoveries is the chirality induced by
time-symmetry breaking. Due to the discovery of quantum search algorithms based
on walks on graphs, recent work has discovered new implications the effect of
time-reversal symmetry breaking has on the transport of quantum states and has
brought with it a host of new experimental implementations. We provide a full
classification of the unitary operators defining quantum processes which break
time-reversal symmetry in their induced transition properties between basis
elements in a preferred site-basis. Our results are furthermore proven in terms
of the geometry of the corresponding Hamiltonian support graph and hence
provide a topological classification. A quantum process of this type is
necessarily time-symmetric for any choice of time-independent Hamiltonian if
and only if the underlying support graph is bipartite. Moreover, for
non-bipartite support, there exists a time-independent Hamiltonian with
necessarily complex edge weights that induces time-asymmetric transition
probabilities between edge(s). We further prove that certain bipartite graphs
give rise to transition probability suppression, but not broken time-reversal
symmetry. These results fill an important missing gap in understanding the role
this omnipresent effect has in quantum physics. Furthermore, through our
development of a general framework, along the way to our results we completely
characterize gauge potentials on combinatorial graphs.
",physics
"  A rectangular grid formed by liquid filaments on a partially wetting
substrate evolves in a series of breakups leading to arrays of drops with
different shapes distributed in a rather regular bidimensional pattern. Our
study is focused on the configuration produced when two long parallel filaments
of silicone oil, which are placed upon a glass substrate previously coated with
a fluorinated solution, are crossed perpendicularly by another pair of long
parallel filaments. A remarkable feature of this kind of grids is that there
are two qualitatively different types of drops. While one set is formed at the
crossing points, the rest are consequence of the breakup of shorter filaments
formed between the crossings. Here, we analyze the main geometric features of
all types of drops, such as shape of the footprint and contact angle
distribution along the drop periphery. The formation of a series of short
filaments with similar geometric and physical properties allows us to have
simultaneously quasi identical experiments to study the subsequent breakups. We
develop a simple hydrodynamic model to predict the number of drops that results
from a filament of given initial length and width. This model is able to yield
the length intervals corresponding to a small number of drops and its
predictions are successfully compared with the experimental data as well as
with numerical simulations of the full Navier--Stokes equation that provide a
detailed time evolution of the dewetting motion of the filament till the
breakup into drops. Finally, the prediction for finite filaments is contrasted
with the existing theories for infinite ones.
",physics
"  We report on experimentally measured light shifts of superconducting flux
qubits deep-strongly coupled to LC oscillators, where the coupling constants
are comparable to the qubit and oscillator resonance frequencies. By using
two-tone spectroscopy, the energies of the six lowest levels of each circuit
are determined. We find huge Lamb shifts that exceed 90% of the bare qubit
frequencies and inversions of the qubits' ground and excited states when there
are a finite number of photons in the oscillator. Our experimental results
agree with theoretical predictions based on the quantum Rabi model.
",physics
"  Global and partial synchronization are the two distinctive forms of
synchronization in coupled oscillators and have been well studied in the past
decades. Recent attention on synchronization is focused on the chimera state
(CS) and explosive synchronization (ES), but little attention has been paid to
their relationship. We here study this topic by presenting a model to bridge
these two phenomena, which consists of two groups of coupled oscillators and
its coupling strength is adaptively controlled by a local order parameter. We
find that this model displays either CS or ES in two limits. In between the two
limits, this model exhibits both CS and ES, where CS can be observed for a
fixed coupling strength and ES appears when the coupling is increased
adiabatically. Moreover, we show both theoretically and numerically that there
are a variety of CS basin patterns for the case of identical oscillators,
depending on the distributions of both the initial order parameters and the
initial average phases. This model suggests a way to easily observe CS, in
contrast to others models having some (weak or strong) dependence on initial
conditions.
",physics
"  The Nash equilibrium paradigm, and Rational Choice Theory in general, rely on
agents acting independently from each other. This note shows how this
assumption is crucial in the definition of Rational Choice Theory. It explains
how a consistent Alternate Rational Choice Theory, as suggested by Jean-Pierre
Dupuy, can be built on the exact opposite assumption, and how it provides a
viable account for alternate, actually observed behavior of rational agents
that is based on correlations between their decisions.
The end goal of this note is three-fold: (i) to motivate that the Perfect
Prediction Equilibrium, implementing Dupuy's notion of projected time and
previously called ""projected equilibrium"", is a reasonable approach in certain
real situations and a meaningful complement to the Nash paradigm, (ii) to
summarize common misconceptions about this equilibrium, and (iii) to give a
concise motivation for future research on non-Nashian game theory.
",computer-science
"  Knowledge distillation (KD) consists of transferring knowledge from one
machine learning model (the teacher}) to another (the student). Commonly, the
teacher is a high-capacity model with formidable performance, while the student
is more compact. By transferring knowledge, one hopes to benefit from the
student's compactness. %we desire a compact model with performance close to the
teacher's. We study KD from a new perspective: rather than compressing models,
we train students parameterized identically to their teachers. Surprisingly,
these {Born-Again Networks (BANs), outperform their teachers significantly,
both on computer vision and language modeling tasks. Our experiments with BANs
based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10
(3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional
experiments explore two distillation objectives: (i) Confidence-Weighted by
Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP).
Both methods elucidate the essential components of KD, demonstrating a role of
the teacher outputs on both predicted and non-predicted classes. We present
experiments with students of various capacities, focusing on the under-explored
case where students overpower teachers. Our experiments show significant
advantages from transferring knowledge between DenseNets and ResNets in either
direction.
",statistics
"  Context. Transit events of extrasolar planets offer the opportunity to study
the composition of their atmospheres. Previous work on transmission
spectroscopy of the close-in gas giant TrES-3 b revealed an increase in
absorption towards blue wavelengths of very large amplitude in terms of
atmospheric pressure scale heights, too large to be explained by
Rayleigh-scattering in the planetary atmosphere. Aims. We present a follow-up
study of the optical transmission spectrum of the hot Jupiter TrES-3 b to
investigate the strong increase in opacity towards short wavelengths found by a
previous study. Furthermore, we aim to estimate the effect of stellar spots on
the transmission spectrum. Methods. This work uses previously published long
slit spectroscopy transit data of the Gran Telescopio Canarias (GTC) and
published broad band observations as well as new observations in different
bands from the near-UV to the near-IR, for a homogeneous transit light curve
analysis. Additionally, a long-term photometric monitoring of the TrES-3 host
star was performed. Results. Our newly analysed GTC spectroscopic transit
observations show a slope of much lower amplitude than previous studies. We
conclude from our results the previously reported increasing signal towards
short wavelengths is not intrinsic to the TrES-3 system. Furthermore, the broad
band spectrum favours a flat spectrum. Long-term photometric monitoring rules
out a significant modification of the transmission spectrum by unocculted star
spots.
",physics
"  In this note we present an $\infty$-categorical framework for descent along
adjunctions and a general formula for counting conjugates up to equivalence
which unifies several known formulae from different fields.
",mathematics
"  PT-symmetry in optics is a condition whereby the real and imaginary parts of
the refractive index across a photonic structure are deliberately balanced.
This balance can lead to a host of novel optical phenomena, such as
unidirectional invisibility, loss-induced lasing, single-mode lasing from
multimode resonators, and non-reciprocal effects in conjunction with
nonlinearities. Because PT-symmetry has been thought of as fragile,
experimental realizations to date have been usually restricted to on-chip
micro-devices. Here, we demonstrate that certain features of PT-symmetry are
sufficiently robust to survive the statistical fluctuations associated with a
macroscopic optical cavity. We construct optical-fiber-based coupled-cavities
in excess of a kilometer in length (the free spectral range is less than 0.8
fm) with balanced gain and loss in two sub-cavities and examine the lasing
dynamics. In such a macroscopic system, fluctuations can lead to a
cavity-detuning exceeding the free spectral range. Nevertheless, by varying the
gain-loss contrast, we observe that both the lasing threshold and the growth of
the laser power follow the predicted behavior of a stable PT-symmetric
structure. Furthermore, a statistical symmetry-breaking point is observed upon
varying the cavity loss. These findings indicate that PT-symmetry is a more
robust optical phenomenon than previously expected, and points to potential
applications in optical fiber networks and fiber lasers.
",physics
"  We investigate the apparent power-law scaling of the pseudo phase space
density (PPSD) in CDM halos. We study fluid collapse, using the close analogy
between the gas entropy and the PPSD in the fluid approximation. Our
hydrodynamic calculations allow for a precise evaluation of logarithmic
derivatives. For scale-free initial conditions, entropy is a power law in
Lagrangian (mass) coordinates, but not in Eulerian (radial) coordinates. The
deviation from a radial power law arises from incomplete hydrostatic
equilibrium (HSE), linked to bulk inflow and mass accretion, and the
convergence to the asymptotic central power-law slope is very slow. For more
realistic collapse, entropy is not a power law with either radius or mass due
to deviations from HSE and scale-dependent initial conditions. Instead, it is a
slowly rolling power law that appears approximately linear on a log-log plot.
Our fluid calculations recover PPSD power-law slopes and residual amplitudes
similar to N-body simulations, indicating that deviations from a power law are
not numerical artefacts. In addition, we find that realistic collapse is not
self-similar: scale lengths such as the shock radius and the turnaround radius
are not power-law functions of time. We therefore argue that the apparent
power-law PPSD cannot be used to make detailed dynamical inferences or
extrapolate halo profiles inward, and that it does not indicate any hidden
integrals of motion. We also suggest that the apparent agreement between the
PPSD and the asymptotic Bertschinger slope is purely coincidental.
",physics
"  Differential testing to solve the oracle problem has been applied in many
scenarios where multiple supposedly equivalent implementations exist, such as
multiple implementations of a C compiler. If the multiple systems disagree on
the output for a given test input, we have likely discovered a bug without
every having to specify what the expected output is. Research on variational
analyses (or variability-aware or family-based analyses) can benefit from
similar ideas. The goal of most variational analyses is to perform an analysis,
such as type checking or model checking, over a large number of configurations
much faster than an existing traditional analysis could by analyzing each
configuration separately. Variational analyses are very suitable for
differential testing, since the existence nonvariational analysis can provide
the oracle for test cases that would otherwise be tedious or difficult to
write. In this experience paper, I report how differential testing has helped
in developing KConfigReader, a tool for translating the Linux kernel's kconfig
model into a propositional formula. Differential testing allows us to quickly
build a large test base and incorporate external tests that avoided many
regressions during development and made KConfigReader likely the most precise
kconfig extraction tool available.
",computer-science
"  Let $X$ be a compact metrizable group and $\Gamma$ a countable group acting
on $X$ by continuous group automorphisms. We give sufficient conditions under
which the dynamical system $(X,\Gamma)$ is surjunctive, i.e., every injective
continuous map $\tau \colon X \to X$ commuting with the action of $\Gamma$ is
surjective.
",mathematics
"  Motivated by posted price auctions where buyers are grouped in an unknown
number of latent types characterized by their private values for the good on
sale, we investigate revenue maximization in stochastic dynamic pricing when
the distribution of buyers' private values is supported on an unknown set of
points in [0,1] of unknown cardinality K. This setting can be viewed as an
instance of a stochastic K-armed bandit problem where the location of the arms
(the K unknown valuations) must be learned as well. In the distribution-free
case, we show that our setting is just as hard as K-armed stochastic bandits:
we prove that no algorithm can achieve a regret significantly better than
$\sqrt{KT}$, (where T is the time horizon) and present an efficient algorithm
matching this lower bound up to logarithmic factors. In the
distribution-dependent case, we show that for all K>2 our setting is strictly
harder than K-armed stochastic bandits by proving that it is impossible to
obtain regret bounds that grow logarithmically in time or slower. On the other
hand, when a lower bound $\gamma>0$ on the smallest drop in the demand curve is
known, we prove an upper bound on the regret of order $(1/\Delta+(\log \log
T)/\gamma^2)(K\log T)$. This is a significant improvement on previously known
regret bounds for discontinuous demand curves, that are at best of order
$(K^{12}/\gamma^8)\sqrt{T}$. When K=2 in the distribution-dependent case, the
hardness of our setting reduces to that of a stochastic 2-armed bandit: we
prove that an upper bound of order $(\log T)/\Delta$ (up to $\log\log$ factors)
on the regret can be achieved with no information on the demand curve. Finally,
we show a $O(\sqrt{T})$ upper bound on the regret for the setting in which the
buyers' decisions are nonstochastic, and the regret is measured with respect to
the best between two fixed valuations one of which is known to the seller.
",statistics
"  Lindel{ö}f's hypothesis, one of the most important open problems in the
history of mathematics, states that for large $t$, Riemann's zeta function
$\zeta(\frac{1}{2}+it)$ is of order $O(t^{\varepsilon})$ for any
$\varepsilon>0$. It is well known that for large $t$, the leading order
asymptotics of the Riemann zeta function can be expressed in terms of a
transcendental exponential sum. The usual approach to the Lindelöf hypothesis
involves the use of ingenious techniques for the estimation of this sum.
However, since such estimates can not yield an asymptotic formula for the above
sum, it appears that this approach cannot lead to the proof of the Lindelöf
hypothesis. Here, a completely different approach is introduced: the Riemann
zeta function is embedded in a classical problem in the theory of complex
analysis known as a Riemann-Hilbert problem, and then, the large
$t$-asymptotics of the associated integral equation is formally computed. This
yields two different results. First, the formal proof that a certain Riemann
zeta-type double exponential sum satisfies the asymptotic estimate of the
Lindelöf hypothesis. Second, it is formally shown that the sum of
$|\zeta(1/2+it)|^2$ and of a certain sum which depends on $\epsilon$, satisfies
for large $t$ the estimate of the Lindelöf hypothesis. Hence, since the above
identity is valid for all $\epsilon$, this asymptotic identity suggests the
validity of Lindelöf's hypothesis. The completion of the rigorous derivation
of the above results will be presented in a companion paper.
",mathematics
"  The observations of solar photosphere from the ground encounter significant
problems due to the presence of Earth's turbulent atmosphere. Prior to applying
image reconstruction techniques, the frames obtained in most favorable
atmospheric conditions (so-called lucky frames) have to be carefully selected.
However, the estimation of the quality of images containing complex
photospheric structures is not a trivial task and the standard routines applied
in night-time Lucky Imaging observations are not applicable. In this paper we
evaluate 36 methods dedicated for the assessment of image quality which were
presented in the rich literature over last 40 years. We compare their
effectiveness on simulated solar observations of both active regions and
granulation patches, using reference data obtained by the Solar Optical
Telescope on the Hindoe satellite. To create the images affected by a known
degree of atmospheric degradation, we employ the Random Wave Vector method
which faithfully models all the seeing characteristics. The results provide
useful information about the methods performance depending on the average
seeing conditions expressed by the ratio of the telescope's aperture to the
Fried parameter, $D/r_0$. The comparison identifies three methods for
consideration by observers: Helmli and Scherer's Mean, Median Filter Gradient
Similarity, and Discrete Cosine Transform Energy Ratio. While the first one
requires less computational effort and can be used effectively virtually in any
atmospherics conditions, the second one shows its superiority at good seeing
($D/r_0<4$). The last one should be considered mainly for the post-processing
of strongly blurred images.
",physics
"  Magnetohydrodynamic (MHD) ships represent a clear demonstration of the
Lorentz force in fluids, which explains the number of students practicals or
exercises described on the web. However, the related literature is rather
specific and no complete comparison between theory and typical small scale
experiments is currently available. This work provides, in a self-consistent
framework, a detailed presentation of the relevant theoretical equations for
small MHD ships and experimental measurements for future benchmarks.
Theoretical results of the literature are adapted to these simple
battery/magnets powered ships moving on salt water. Comparison between theory
and experiments are performed to validate each theoretical step such as the
Tafel and the Kohlrausch laws, or the predicted ship speed. A successful
agreement is obtained without any adjustable parameter. Finally, based on these
results, an optimal design is then deduced from the theory. Therefore this work
provides a solid theoretical and experimental ground for small scale MHD ships,
by presenting in detail several approximations and how they affect the boat
efficiency. Moreover, the theory is general enough to be adapted to other
contexts, such as large scale ships or industrial flow measurement techniques.
",physics
"  Unique among alkali-doped $\textit {A}$$_3$C$_{60}$ fullerene compounds, the
A15 and fcc forms of Cs$_3$C$_{60}$ exhibit superconducting states varying
under hydrostatic pressure with highest transition temperatures at $T_\textrm
{C}$$^\textrm {meas}$ = 38.3 and 35.2 K, respectively. Herein it is argued that
these two compounds under pressure represent the optimal materials of the
$\textit {A}$$_3$C$_{60}$ family, and that the C$_{60}$-associated
superconductivity is mediated through Coulombic interactions with charges on
the alkalis. A derivation of the interlayer Coulombic pairing model of
high-$T_\textrm {C}$ superconductivity employing non-planar geometry is
introduced, generalizing the picture of two interacting layers to an
interaction between charge reservoirs located on the C$_{60}$ and alkali ions.
The optimal transition temperature follows the algebraic expression, $T_\textrm
{C0}$ = (12.474 nm$^2$ K)/$\ell$${\zeta}$, where $\ell$ relates to the mean
spacing between interacting surface charges on the C$_{60}$ and ${\zeta}$ is
the average radial distance between the C$_{60}$ surface and the neighboring Cs
ions. Values of $T_\textrm {C0}$ for the measured cation stoichiometries of
Cs$_{3-\textrm{x}}$C$_{60}$ with x $\approx$ 0 are found to be 38.19 and 36.88
K for the A15 and fcc forms, respectively, with the dichotomy in transition
temperature reflecting the larger ${\zeta}$ and structural disorder in the fcc
form. In the A15 form, modeled interacting charges and Coulomb potential
e$^2$/${\zeta}$ are shown to agree quantitatively with findings from
nuclear-spin relaxation and mid-infrared optical conductivity. In the fcc form,
suppression of $T_\textrm {C}$$^\textrm {meas}$ below $T_\textrm {C0}$ is
ascribed to native structural disorder. Phononic effects in conjunction with
Coulombic pairing are discussed.
",physics
"  In this paper we explore the role of duality principles within the problem of
rotation averaging, a fundamental task in a wide range of computer vision
applications. In its conventional form, rotation averaging is stated as a
minimization over multiple rotation constraints. As these constraints are
non-convex, this problem is generally considered challenging to solve globally.
We show how to circumvent this difficulty through the use of Lagrangian
duality. While such an approach is well-known it is normally not guaranteed to
provide a tight relaxation. Based on spectral graph theory, we analytically
prove that in many cases there is no duality gap unless the noise levels are
severe. This allows us to obtain certifiably global solutions to a class of
important non-convex problems in polynomial time.
We also propose an efficient, scalable algorithm that out-performs general
purpose numerical solvers and is able to handle the large problem instances
commonly occurring in structure from motion settings. The potential of this
proposed method is demonstrated on a number of different problems, consisting
of both synthetic and real-world data.
",computer-science
"  We propose a new active learning strategy designed for deep neural networks.
The goal is to minimize the number of data annotation queried from an oracle
during training. Previous active learning strategies scalable for deep networks
were mostly based on uncertain sample selection. In this work, we focus on
examples lying close to the decision boundary. Based on theoretical works on
margin theory for active learning, we know that such examples may help to
considerably decrease the number of annotations. While measuring the exact
distance to the decision boundaries is intractable, we propose to rely on
adversarial examples. We do not consider anymore them as a threat instead we
exploit the information they provide on the distribution of the input space in
order to approximate the distance to decision boundaries. We demonstrate
empirically that adversarial active queries yield faster convergence of CNNs
trained on MNIST, the Shoe-Bag and the Quick-Draw datasets.
",statistics
"  Standard models of reaction kinetics in condensed materials rely on the
Boltzmann-Gibbs distribution for the population of reactants at the top of the
free energy barrier separating them from the products. While energy dissipation
and quantum effects at the barrier top can potentially affect the transmission
coefficient entering the rate preexponential factor, much stronger dynamical
effects on the reaction barrier are caused by the breakdown of ergodicity for
populating the reaction barrier (violation of the Boltzmann-Gibbs statistics).
When the spectrum of medium modes coupled to the reaction coordinate includes
fluctuations slower than the reaction rate, such nuclear motions dynamically
freeze on the reaction time-scale and do not contribute to the activation
barrier. Here we consider the consequences of this scenario for electrode
reactions in slowly relaxing media. Changing electrode overpotential speeds
electrode electron transfer up, potentially cutting through the spectrum of
nuclear modes coupled to the reaction coordinate. The reorganization energy of
electrochemical electron transfer becomes a function of the electrode
overpotential, switching between the thermodynamic value at low rates to the
nonergodic limit at higher rates. The sharpness of this transition depends of
the relaxation spectrum of the medium. The reorganization energy experiences a
sudden drop with increasing overpotential for a medium with a Debye relaxation,
but becomes a much shallower function of the overpotential for media with
stretched exponential dynamics. The latter scenario characterizes electron
transfer in ionic liquids. The analysis of electrode reactions in
room-temperature ionic liquids shows that the magnitude of the free energy of
nuclear solvation is significantly below its thermodynamic limit.
",physics
"  Clustering mixtures of Gaussian distributions is a fundamental and
challenging problem that is ubiquitous in various high-dimensional data
processing tasks. While state-of-the-art work on learning Gaussian mixture
models has focused primarily on improving separation bounds and their
generalization to arbitrary classes of mixture models, less emphasis has been
paid to practical computational efficiency of the proposed solutions. In this
paper, we propose a novel and highly efficient clustering algorithm for $n$
points drawn from a mixture of two arbitrary Gaussian distributions in
$\mathbb{R}^p$. The algorithm involves performing random 1-dimensional
projections until a direction is found that yields a user-specified clustering
error $e$. For a 1-dimensional separation parameter $\gamma$ satisfying
$\gamma=Q^{-1}(e)$, the expected number of such projections is shown to be
bounded by $o(\ln p)$, when $\gamma$ satisfies $\gamma\leq
c\sqrt{\ln{\ln{p}}}$, with $c$ as the separability parameter of the two
Gaussians in $\mathbb{R}^p$. Consequently, the expected overall running time of
the algorithm is linear in $n$ and quasi-linear in $p$ at $o(\ln{p})O(np)$, and
the sample complexity is independent of $p$. This result stands in contrast to
prior works which provide polynomial, with at-best quadratic, running time in
$p$ and $n$. We show that our bound on the expected number of 1-dimensional
projections extends to the case of three or more Gaussian components, and we
present a generalization of our results to mixture distributions beyond the
Gaussian model.
",computer-science
"  We analyze a rich dataset including Subaru/SuprimeCam, HST/ACS and WFC3,
Keck/DEIMOS, Chandra/ACIS-I, and JVLA/C and D array for the merging galaxy
cluster ZwCl 0008.8+5215. With a joint Subaru/HST weak gravitational lensing
analysis, we identify two dominant subclusters and estimate the masses to be
M$_{200}=\text{5.7}^{+\text{2.8}}_{-\text{1.8}}\times\text{10}^{\text{14}}\,\text{M}_{\odot}$
and 1.2$^{+\text{1.4}}_{-\text{0.6}}\times10^{14}$ M$_{\odot}$. We estimate the
projected separation between the two subclusters to be
924$^{+\text{243}}_{-\text{206}}$ kpc. We perform a clustering analysis on
confirmed cluster member galaxies and estimate the line of sight velocity
difference between the two subclusters to be 92$\pm$164 km s$^{-\text{1}}$. We
further motivate, discuss, and analyze the merger scenario through an analysis
of the 42 ks of Chandra/ACIS-I and JVLA/C and D polarization data. The X-ray
surface brightness profile reveals a remnant core reminiscent of the Bullet
Cluster. The X-ray luminosity in the 0.5-7.0 keV band is
1.7$\pm$0.1$\times$10$^{\text{44}}$ erg s$^{-\text{1}}$ and the X-ray
temperature is 4.90$\pm$0.13 keV. The radio relics are polarized up to 40$\%$.
We implement a Monte Carlo dynamical analysis and estimate the merger velocity
at pericenter to be 1800$^{+\text{400}}_{-\text{300}}$ km s$^{-\text{1}}$. ZwCl
0008.8+5215 is a low-mass version of the Bullet Cluster and therefore may prove
useful in testing alternative models of dark matter. We do not find significant
offsets between dark matter and galaxies, as the uncertainties are large with
the current lensing data. Furthermore, in the east, the BCG is offset from
other luminous cluster galaxies, which poses a puzzle for defining dark matter
-- galaxy offsets.
",physics
"  In 1993, Bismut and Zhang establish a mod Z embedding formula of
Atiyah-Patodi-Singer reduced eta invariants. In this paper, we explain the
hidden mod Z term as a spectral flow and extend this embedding formula to the
equivariant family case. In this case, the spectral flow is generalized to the
equivariant chern character of some equivariant Dai-Zhang higher spectral flow.
",mathematics
"  We are concerned with unbounded sets of $\mathbb{R}^N$ whose boundary has
constant nonlocal (or fractional) mean curvature, which we call CNMC sets. This
is the equation associated to critical points of the fractional perimeter
functional under a volume constraint. We construct CNMC sets which are the
countable union of a certain bounded domain and all its translations through a
periodic integer lattice of dimension $M\leq N$. Our CNMC sets form a $C^2$
branch emanating from the unit ball alone and where the parameter in the branch
is essentially the distance to the closest lattice point. Thus, the new
translated near-balls (or near-spheres) appear from infinity. We find their
exact asymptotic shape as the parameter tends to infinity.
",mathematics
"  The paper solves the problem of optimal portfolio choice when the parameters
of the asset returns distribution, like the mean vector and the covariance
matrix are unknown and have to be estimated by using historical data of the
asset returns. The new approach employs the Bayesian posterior predictive
distribution which is the distribution of the future realization of the asset
returns given the observable sample. The parameters of the posterior predictive
distributions are functions of the observed data values and, consequently, the
solution of the optimization problem is expressed in terms of data only and
does not depend on unknown quantities. In contrast, the optimization problem of
the traditional approach is based on unknown quantities which are estimated in
the second step leading to a suboptimal solution. We also derive a very useful
stochastic representation of the posterior predictive distribution whose
application leads not only to the solution of the considered optimization
problem, but provides the posterior predictive distribution of the optimal
portfolio return used to construct a prediction interval. A Bayesian efficient
frontier, a set of optimal portfolios obtained by employing the posterior
predictive distribution, is constructed as well. Theoretically and using real
data we show that the Bayesian efficient frontier outperforms the sample
efficient frontier, a common estimator of the set of optimal portfolios known
to be overoptimistic.
",quantitative-finance
"  We present a general analytical formalism to determine the energy spectrum of
a quantum particle in a cubic lattice subject to translationally invariant
commensurate magnetic fluxes and in the presence of a general space-independent
non-Abelian gauge potential. We first review and analyze the case of purely
Abelian potentials, showing also that the so-called Hasegawa gauge yields a
decomposition of the Hamiltonian into sub-matrices having minimal dimension.
Explicit expressions for such matrices are derived, also for general
anisotropic fluxes. Later on, we show that the introduction of a translational
invariant non-Abelian coupling for multi-component spinors does not affect the
dimension of the minimal Hamiltonian blocks, nor the dimension of the magnetic
Brillouin zone. General formulas are presented for the U(2) case and explicit
examples are investigated involving $\pi$ and $2\pi/3$ magnetic fluxes.
Finally, we numerically study the effect of random flux perturbations.
",physics
"  Emission of electromagnetic radiation by accelerated particles with electric,
toroidal and anapole dipole moments is analyzed. It is shown that ellipticity
of the emitted light can be used to differentiate between electric and toroidal
dipole sources, and that anapoles, elementary neutral non-radiating
configurations, which consist of electric and toroidal dipoles, can emit light
under uniform acceleration. The existence of non-radiating configurations in
electrodynamics implies that it is impossible to fully determine the internal
makeup of the emitter given only the distribution of the emitted light. Here we
demonstrate that there is a loop-hole in this `inverse source problem'. Our
results imply that there may be a whole range of new phenomena to be discovered
by studying the electromagnetic response of matter under acceleration.
",physics
"  We propose a dynamic boosted ensemble learning method based on random forest
(DBRF), a novel ensemble algorithm that incorporates the notion of hard example
mining into Random Forest (RF) and thus combines the high accuracy of Boosting
algorithm with the strong generalization of Bagging algorithm. Specifically, we
propose to measure the quality of each leaf node of every decision tree in the
random forest to determine hard examples. By iteratively training and then
removing easy examples from training data, we evolve the random forest to focus
on hard examples dynamically so as to learn decision boundaries better. Data
can be cascaded through these random forests learned in each iteration in
sequence to generate predictions, thus making RF deep. We also propose to use
evolution mechanism and smart iteration mechanism to improve the performance of
the model. DBRF outperforms RF on three UCI datasets and achieved
state-of-the-art results compared to other deep models. Moreover, we show that
DBRF is also a new way of sampling and can be very useful when learning from
imbalanced data.
",statistics
"  Detectability of discrete event systems (DESs) is a question whether the
current and subsequent states can be determined based on observations. Shu and
Lin designed a polynomial-time algorithm to check strong (periodic)
detectability and an exponential-time (polynomial-space) algorithm to check
weak (periodic) detectability. Zhang showed that checking weak (periodic)
detectability is PSpace-complete. This intractable complexity opens a question
whether there are structurally simpler DESs for which the problem is tractable.
In this paper, we show that it is not the case by considering DESs represented
as deterministic finite automata without non-trivial cycles, which are
structurally the simplest deadlock-free DESs. We show that even for such very
simple DESs, checking weak (periodic) detectability remains intractable. On the
contrary, we show that strong (periodic) detectability of DESs can be
efficiently verified on a parallel computer.
",computer-science
"  The APerture SYNthesis SIMulator is a simple interactive tool to help the
students visualize and understand the basics of the Aperture Synthesis
technique, applied to astronomical interferometers. The users can load many
different interferometers and source models (and also create their own), change
the observing parameters (e.g., source coordinates, observing wavelength,
antenna location, integration time, etc.), and even deconvolve the
interferometric images and corrupt the data with gain errors (amplitude and
phase). The program is fully interactive and all the figures are updated in
real time. APSYNSIM has already been used in several interferometry schools and
has got very positive feedback from the students.
",physics
"  Most traditional video summarization methods are designed to generate
effective summaries for single-view videos, and thus they cannot fully exploit
the complicated intra and inter-view correlations in summarizing multi-view
videos in a camera network. In this paper, with the aim of summarizing
multi-view videos, we introduce a novel unsupervised framework via joint
embedding and sparse representative selection. The objective function is
two-fold. The first is to capture the multi-view correlations via an embedding,
which helps in extracting a diverse set of representatives. The second is to
use a `2;1- norm to model the sparsity while selecting representative shots for
the summary. We propose to jointly optimize both of the objectives, such that
embedding can not only characterize the correlations, but also indicate the
requirements of sparse representative selection. We present an efficient
alternating algorithm based on half-quadratic minimization to solve the
proposed non-smooth and non-convex objective with convergence analysis. A key
advantage of the proposed approach with respect to the state-of-the-art is that
it can summarize multi-view videos without assuming any prior
correspondences/alignment between them, e.g., uncalibrated camera networks.
Rigorous experiments on several multi-view datasets demonstrate that our
approach clearly outperforms the state-of-the-art methods.
",computer-science
"  The problem of quantizing the activations of a deep neural network is
considered. An examination of the popular binary quantization approach shows
that this consists of approximating a classical non-linearity, the hyperbolic
tangent, by two functions: a piecewise constant sign function, which is used in
feedforward network computations, and a piecewise linear hard tanh function,
used in the backpropagation step during network learning. The problem of
approximating the ReLU non-linearity, widely used in the recent deep learning
literature, is then considered. An half-wave Gaussian quantizer (HWGQ) is
proposed for forward approximation and shown to have efficient implementation,
by exploiting the statistics of of network activations and batch normalization
operations commonly used in the literature. To overcome the problem of gradient
mismatch, due to the use of different forward and backward approximations,
several piece-wise backward approximators are then investigated. The
implementation of the resulting quantized network, denoted as HWGQ-Net, is
shown to achieve much closer performance to full precision networks, such as
AlexNet, ResNet, GoogLeNet and VGG-Net, than previously available low-precision
networks, with 1-bit binary weights and 2-bit quantized activations.
",computer-science
"  We show that $R^{cl}(\omega\cdot 2,3)^2$ is equal to $\omega^3\cdot 2$.
",mathematics
"  Given a smooth non-trapping compact manifold with strictly con- vex boundary,
we consider an inverse problem of reconstructing the manifold from the
scattering data initiated from internal sources. This data consist of the exit
directions of geodesics that are emaneted from interior points of the manifold.
We show that under certain generic assumption of the metric, one can
reconstruct an isometric copy of the manifold from such scattering data
measured on the boundary.
",mathematics
"  Recognition of Handwritten Mathematical Expressions (HMEs) is a challenging
problem because of the ambiguity and complexity of two-dimensional handwriting.
Moreover, the lack of large training data is a serious issue, especially for
academic recognition systems. In this paper, we propose pattern generation
strategies that generate shape and structural variations to improve the
performance of recognition systems based on a small training set. For data
generation, we employ the public databases: CROHME 2014 and 2016 of online
HMEs. The first strategy employs local and global distortions to generate shape
variations. The second strategy decomposes an online HME into sub-online HMEs
to get more structural variations. The hybrid strategy combines both these
strategies to maximize shape and structural variations. The generated online
HMEs are converted to images for offline HME recognition. We tested our
strategies in an end-to-end recognition system constructed from a recent deep
learning model: Convolutional Neural Network and attention-based
encoder-decoder. The results of experiments on the CROHME 2014 and 2016
databases demonstrate the superiority and effectiveness of our strategies: our
hybrid strategy achieved classification rates of 48.78% and 45.60%,
respectively, on these databases. These results are competitive compared to
others reported in recent literature. Our generated datasets are openly
available for research community and constitute a useful resource for the HME
recognition research in future.
",computer-science
"  We give sufficient conditions of the nonnegative inverse eigenvalue problem
(NIEP) for normal centrosymmetric matrices. These sufficient conditions are
analogous to the sufficient conditions of the NIEP for normal matrices given by
Xu [16] and Julio, Manzaneda and Soto [2].
",mathematics
"  Small drops impinging angularly on thin flowing soap films frequently
demonstrate the rare emergence of bulk elastic effects working in-tandem with
the more common-place hydrodynamic interactions. Three collision regimes are
observable: (a) drop piercing through the film, (b) it coalescing with the
flow, and (c) it bouncing off the film surface. During impact, the drop deforms
along with a bulk elastic deformation of the film. For impacts that are
close-to-tangential, the bounce-off regime predominates. We outline a reduced
order analytical framework assuming a deformable drop and a deformable
three-dimensional film, and the idealization invokes a phase-based parametric
study. Angular inclination of the film and the ratio of post and pre impact
drop sizes entail the phase parameters. We also perform experiments with
vertically descending droplets impacting against an inclined soap film, flowing
under constant pressure head. Model predicted phase domain for bounce-off
compares well to our experimental findings. Additionally, the experiments
exhibit momentum transfer to the film in the form of shed vortex dipole, along
with propagation of free surface waves. On consulting prior published work, we
note that for locomotion of water-walking insects using an impulsive action,
the momentum distribution to the shed vortices and waves are both significant,
taking up respectively 2/3-rd and 1/3-rd of the imparted streamwise momentum.
In view of the potentially similar impulse actions, this theory is applied to
the bounce-off examples in our experiments, and the resultant shed vortex
dipole momenta are compared to the momenta computed from particle imaging
velocimetry data. The magnitudes reveal identical order ($10^{-7}$ N$\cdot$s),
suggesting that the bounce-off regime can be tapped as a simple analogue for
interfacial bio-locomotion relying on impulse reactions.
",physics
"  In this paper we study entire radial solutions for the quasilinear
$p$-Laplace equation $\Delta_p u + k(x) f(u) = 0$ where $k$ is a radial
positive weight and the nonlinearity behaves e.g. as
$f(u)=u|u|^{q-2}-u|u|^{Q-2}$ with $q<Q$. In particular we focus our attention
on solutions (positive and sign changing) which are infinitesimal at infinity,
thus providing an extension of a previous result by Tang (2001).
",mathematics
"  A Large Size air Cherenkov Telescope (LST) prototype, proposed for the
Cherenkov Telescope Array (CTA), is under construction at the Canary Island of
La Palma (Spain) this year. The LST camera, which comprises an array of about
500 photomultipliers (PMTs), requires a precise and regular calibration over a
large dynamic range, up to $10^3$ photo-electrons (pe's), for each PMT. We
present a system built to provide the optical calibration of the camera
consisting of a pulsed laser (355 nm wavelength, 400 ps pulse width), a set of
filters to guarantee a large dynamic range of photons on the sensors, and a
diffusing sphere to uniformly spread the laser light, with flat fielding within
3%, over the camera focal plane 28 m away. The prototype of the system
developed at INFN is hermetically closed and filled with dry air to make the
system completely isolated from the external environment. In the paper we
present the results of the tests for the evaluation of the photon density at
the camera plane, the system isolation from the environment, and the shape of
the signal as detected by the PMTs. The description of the communication of the
system with the rest of detector is also given.
",physics
"  Transistors incorporating single-wall carbon nanotubes (CNTs) as the channel
material are used in a variety of electronics applications. However, a
competitive CNT-based technology requires the precise placement of CNTs at
predefined locations of a substrate. One promising placement approach is to use
chemical recognition to bind CNTs from solution at the desired locations on a
surface. Producing the chemical pattern on the substrate is challenging. Here
we describe a one-step patterning approach based on a highly photosensitive
surface monolayer. The monolayer contains chromophopric group as light
sensitive body with heteroatoms as high quantum yield photolysis center. As
deposited, the layer will bind CNTs from solution. However, when exposed to
ultraviolet (UV) light with a low dose (60 mJ/cm2) similar to that used for
conventional photoresists, the monolayer cleaves and no longer binds CNTs.
These features allow standard, wafer-scale UV lithography processes to be used
to form a patterned chemical monolayer without the need for complex substrate
patterning or monolayer stamping.
",physics
"  We describe high resolution observations of a GOES B-class flare
characterized by a circular ribbon at chromospheric level, corresponding to the
network at photospheric level. We interpret the flare as a consequence of a
magnetic reconnection event occurred at a three-dimensional (3D) coronal null
point located above the supergranular cell. The potential field extrapolation
of the photospheric magnetic field indicates that the circular chromospheric
ribbon is cospatial with the fan footpoints, while the ribbons of the inner and
outer spines look like compact kernels. We found new interesting observational
aspects that need to be explained by models: 1) a loop corresponding to the
outer spine became brighter a few minutes before the onset of the flare; 2) the
circular ribbon was formed by several adjacent compact kernels characterized by
a size of 1""-2""; 3) the kernels with stronger intensity emission were located
at the outer footpoint of the darker filaments departing radially from the
center of the supergranular cell; 4) these kernels start to brighten
sequentially in clockwise direction; 5) the site of the 3D null point and the
shape of the outer spine were detected by RHESSI in the low energy channel
between 6.0 and 12.0 keV. Taking into account all these features and the length
scales of the magnetic systems involved by the event we argued that the low
intensity of the flare may be ascribed to the low amount of magnetic flux and
to its symmetric configuration.
",physics
"  The Cherenkov Telescope Array (CTA) is the next generation ground-based
$\gamma$-ray observatory. It will provide an order of magnitude better
sensitivity and an extended energy coverage, 20 GeV - 300 TeV, relative to
current Imaging Atmospheric Cherenkov Telescopes (IACTs). IACTs, despite
featuring an excellent sensitivity, are characterized by a limited field of
view that makes the blind search of new sources very time inefficient.
Fortunately, the $\textit{Fermi}$-LAT collaboration recently released a new
catalog of 1,556 sources detected in the 10 GeV - 2 TeV range by the Large Area
Telescope (LAT) in the first 7 years of its operation (the 3FHL catalog). This
catalog is currently the most appropriate description of the sky that will be
energetically accessible to CTA. Here, we discuss a detailed analysis of the
extragalactic source population (mostly blazars) that will be studied in the
near future by CTA. This analysis is based on simulations built from the
expected array configurations and information reported in the 3FHL catalog.
These results show the improvements that CTA will provide on the extragalactic
TeV source population studies, which will be carried out by Key Science
Projects as well as dedicated proposals.
",physics
"  Bias is a common problem in today's media, appearing frequently in text and
in visual imagery. Users on social media websites such as Twitter need better
methods for identifying bias. Additionally, activists --those who are motivated
to effect change related to some topic, need better methods to identify and
counteract bias that is contrary to their mission. With both of these use cases
in mind, in this paper we propose a novel tool called UnbiasedCrowd that
supports identification of, and action on bias in visual news media. In
particular, it addresses the following key challenges (1) identification of
bias; (2) aggregation and presentation of evidence to users; (3) enabling
activists to inform the public of bias and take action by engaging people in
conversation with bots. We describe a preliminary study on the Twitter platform
that explores the impressions that activists had of our tool, and how people
reacted and engaged with online bots that exposed visual bias. We conclude by
discussing design and implication of our findings for creating future systems
to identify and counteract the effects of news bias.
",computer-science
"  Motivated by the need to detect an underground cavity within the procedure of
an On-Site-Inspection (OSI), of the Comprehensive Nuclear Test Ban Treaty
Organization, the aim of this paper is to present results on the comparison of
our numerical simulations with an analytic solution. The accurate numerical
modeling can facilitate the development of proper analysis techniques to detect
the remnants of an underground nuclear test. The larger goal is to help set a
rigorous scientific base of OSI and to contribute to bringing the Treaty into
force. For our 3D numerical simulations, we use the discontinuous Galerkin
Spectral Element Code SPEED jointly developed at MOX (The Laboratory for
Modeling and Scientific Computing, Department of Mathematics) and at DICA
(Department of Civil and Environmental Engineering) of the Politecnico di
Milano.
",physics
"  In this paper, we propose a novel method to estimate and characterize spatial
variations on dies or wafers. This new technique exploits recent developments
in matrix completion, enabling estimation of spatial variation across wafers or
dies with a small number of randomly picked sampling points while still
achieving fairly high accuracy. This new approach can be easily generalized,
including for estimation of mixed spatial and structure or device type
information.
",computer-science
"  Let $G$ be a finite group and let $p_1,\dots,p_n$ be distinct primes. If $G$
contains an element of order $p_1\cdots p_n,$ then there is an element in $G$
which is not contained in the Frattini subgroup of $G$ and whose order is
divisible by $p_1\cdots p_n.$
",mathematics
"  We prove a reducibility result for a quantum harmonic oscillator in arbitrary
dimensions with arbitrary frequencies perturbed by a linear operator which is a
polynomial of degree two in $x_j$, $-i \partial_j$ with coefficients which
depend quasiperiodically on time.
",mathematics
"  Although the definition of what empathetic preferences exactly are is still
evolving, there is a general consensus in the psychology, science and
engineering communities that the evolution toward players' behaviors in
interactive decision-making problems will be accompanied by the exploitation of
their empathy, sympathy, compassion, antipathy, spitefulness, selfishness,
altruism, and self-abnegating states in the payoffs. In this article, we study
one-shot bimatrix games from a psychological game theory viewpoint. A new
empathetic payoff model is calculated to fit empirical observations and both
pure and mixed equilibria are investigated. For a realized empathy structure,
the bimatrix game is categorized among four generic class of games. Number of
interesting results are derived. A notable level of involvement can be observed
in the empathetic one-shot game compared the non-empathetic one and this holds
even for games with dominated strategies. Partial altruism can help in breaking
symmetry, in reducing payoff-inequality and in selecting social welfare and
more efficient outcomes. By contrast, partial spite and self-abnegating may
worsen payoff equity. Empathetic evolutionary game dynamics are introduced to
capture the resulting empathetic evolutionarily stable strategies under wide
range of revision protocols including Brown-von Neumann-Nash, Smith, imitation,
replicator, and hybrid dynamics. Finally, mutual support and Berge solution are
investigated and their connection with empathetic preferences are established.
We show that pure altruism is logically inconsistent, only by balancing it with
some partial selfishness does it create a consistent psychology.
",computer-science
"  Here, we suggest a method to represent general directed uniform and
non-uniform hypergraphs by different connectivity tensors. We show many results
on spectral properties of undirected hypergraphs also hold for general directed
uniform hypergraphs. Our representation of a connectivity tensor will be very
useful for the further development in spectral theory of directed hypergraphs.
At the end, we have also introduced the concept of weak* irreducible
hypermatrix to better explain connectivity of a directed hypergraph.
",mathematics
"  We investigate the formation of optical localized nonlinear structures,
evolving upon a non-zero background plane wave, in a dispersive quadratic
medium. We show the existence of quadratic Akhmediev breathers and Peregrine
solitary waves, in the regime of cascading second-harmonic generation. This
finding opens a novel path for the excitation of extreme rogue waves and for
the description of modulation instability in quadratic nonlinear optics.
",physics
"  In recent years, constrained optimization has become increasingly relevant to
the machine learning community, with applications including Neyman-Pearson
classification, robust optimization, and fair machine learning. A natural
approach to constrained optimization is to optimize the Lagrangian, but this is
not guaranteed to work in the non-convex setting, and, if using a first-order
method, cannot cope with non-differentiable constraints (e.g. constraints on
rates or proportions).
The Lagrangian can be interpreted as a two-player game played between a
player who seeks to optimize over the model parameters, and a player who wishes
to maximize over the Lagrange multipliers. We propose a non-zero-sum variant of
the Lagrangian formulation that can cope with non-differentiable--even
discontinuous--constraints, which we call the ""proxy-Lagrangian"". The first
player minimizes external regret in terms of easy-to-optimize ""proxy
constraints"", while the second player enforces the original constraints by
minimizing swap regret.
For this new formulation, as for the Lagrangian in the non-convex setting,
the result is a stochastic classifier. For both the proxy-Lagrangian and
Lagrangian formulations, however, we prove that this classifier, instead of
having unbounded size, can be taken to be a distribution over no more than m+1
models (where m is the number of constraints). This is a significant
improvement in practical terms.
",statistics
"  We present several upper bounds for the height of global residues of rational
forms on an affine variety. As a consequence, we deduce upper bounds for the
height of the coefficients in the Bergman-Weil trace formula.
We also present upper bounds for the degree and the height of the polynomials
in the elimination theorem on an affine variety. This is an arithmetic analogue
of Jelonek's effective elimination theorem, that plays a crucial role in the
proof of our bounds for the height of global residues.
",mathematics
"  Szilard engine(SZE) is one of the best example of how information can be used
to extract work from a system. Initially, the working substance of SZE was
considered to be a single particle. Later on, researchers has extended the
studies of SZE to multi-particle systems and even to quantum regime. Here we
present a detailed study of classical SZE consisting of $N$ particles with
inter-particle interactions, i.e., the working substance is a low density
non-ideal gas and compare the work extraction with respect to SZE with
non-interacting multi particle system as working substance. We have considered
two cases of interactions namely: (i) hard core interactions and (ii) square
well interaction. Our study reveals that work extraction is less when more
particles are interacting through hard core interactions. More work is
extracted when the particles are interacting via square well interaction.
Another important result for the second case is that as we increase the
particle number the work extraction becomes independent of the initial position
of the partition, as opposed to the first case. Work extraction depends
crucially on the initial position of the partition. More work can be extracted
with larger number of particles when partition is inserted at positions near
the boundary walls.
",physics
"  To aid a variety of research studies, we propose TWIROLE, a hybrid model for
role-related user classification on Twitter, which detects male-related,
female-related, and brand-related (i.e., organization or institution) users.
TWIROLE leverages features from tweet contents, user profiles, and profile
images, and then applies our hybrid model to identify a user's role. To
evaluate it, we used two existing large datasets about Twitter users, and
conducted both intra- and inter-comparison experiments. TWIROLE outperforms
existing methods and obtains more balanced results over the several roles. We
also confirm that user names and profile images are good indicators for this
task. Our research extends prior work that does not consider brand-related
users, and is an aid to future evaluation efforts relative to investigations
that rely upon self-labeled datasets.
",computer-science
"  We examine by a perturbation method how the self-trapping of g-mode
oscillations in geometrically thin relativistic disks is affected by uniform
vertical magnetic fields. Disks which we consider are isothermal in the
vertical direction, but are truncated at a certain height by presence of hot
coronae. We find that the characteristics of self-trapping of axisymmetric
g-mode oscillations in non-magnetized disks is kept unchanged in magnetized
disks at least till a strength of the fields, depending on vertical thickness
of disks. These magnetic fields become stronger as the disk becomes thinner.
This result suggests that trapped g-mode oscillations still remain as one of
possible candidates of quasi-periodic oscillations observed in black-hole and
neutron-star X-ray binaries in the cases where vertical magnetic fields in
disks are weak.
",physics
"  The ability of the mammalian ear in processing high frequency sounds, up to
$\sim$100 kHz, is based on the capability of outer hair cells (OHCs) responding
to stimulation at high frequencies. These cells show a unique motility in their
cell body coupled with charge movement. With this motile element, voltage
changes generated by stimuli at their hair bundles drives the cell body and
that, in turn, amplifies the stimuli. In vitro experiments show that the
movement of these charges significantly increases the membrane capacitance,
limiting the motile activity by additionally attenuating voltage changes. It
was found, however, that such an effect is due to the absence of mechanical
load. In the presence of mechanical resonance, such as in vivo conditions, the
movement of motile charges is expected to create negative capacitance near the
resonance frequency. Therefore this motile mechanism is effective at high
frequencies.
",physics
"  We introduce a new class of sequential Monte Carlo methods called Nested
Sampling via Sequential Monte Carlo (NS-SMC), which reframes the Nested
Sampling method of Skilling (2006) in terms of sequential Monte Carlo
techniques. This new framework allows convergence results to be obtained in the
setting when Markov chain Monte Carlo (MCMC) is used to produce new samples. An
additional benefit is that marginal likelihood estimates are unbiased. In
contrast to NS, the analysis of NS-SMC does not require the (unrealistic)
assumption that the simulated samples be independent. As the original NS
algorithm is a special case of NS-SMC, this provides insights as to why NS
seems to produce accurate estimates despite a typical violation of its
assumptions. For applications of NS-SMC, we give advice on tuning MCMC kernels
in an automated manner via a preliminary pilot run, and present a new method
for appropriately choosing the number of MCMC repeats at each iteration.
Finally, a numerical study is conducted where the performance of NS-SMC and
temperature-annealed SMC is compared on several challenging and realistic
problems. MATLAB code for our experiments is made available at
this https URL .
",statistics
"  Fermi Large Area Telescope data reveal an excess of GeV gamma rays from the
direction of the Galactic Center and bulge. Several explanations have been
proposed for this excess including an unresolved population of millisecond
pulsars (MSPs) and self-annihilating dark matter. It has been claimed that a
key discriminant for or against the MSP explanation can be extracted from the
properties of the luminosity function describing this source population.
Specifically, is the luminosity function of the putative MSPs in the Galactic
Center consistent with that characterizing the resolved MSPs in the Galactic
disk? To investigate this we have used a Bayesian Markov Chain Monte Carlo to
evaluate the posterior distribution of the parameters of the MSP luminosity
function describing both resolved MSPs and the Galactic Center excess. At
variance with some other claims, our analysis reveals that, within current
uncertainties, both data sets can be well fit with the same luminosity
function.
",physics
"  We propose a novel couple mappings method for low resolution face recognition
using deep convolutional neural networks (DCNNs). The proposed architecture
consists of two branches of DCNNs to map the high and low resolution face
images into a common space with nonlinear transformations. The branch
corresponding to transformation of high resolution images consists of 14 layers
and the other branch which maps the low resolution face images to the common
space includes a 5-layer super-resolution network connected to a 14-layer
network. The distance between the features of corresponding high and low
resolution images are backpropagated to train the networks. Our proposed method
is evaluated on FERET data set and compared with state-of-the-art competing
methods. Our extensive experimental results show that the proposed method
significantly improves the recognition performance especially for very low
resolution probe face images (11.4% improvement in recognition accuracy).
Furthermore, it can reconstruct a high resolution image from its corresponding
low resolution probe image which is comparable with state-of-the-art
super-resolution methods in terms of visual quality.
",computer-science
"  We study algebro-geometric consequences of the quantised extremal Kähler
metrics, introduced in the previous work of the author. We prove that the
existence of quantised extremal metrics implies weak relative Chow
polystability. As a consequence, we obtain asymptotic weak relative Chow
polystability and $K$-semistability of extremal manifolds by using quantised
extremal metrics; this gives an alternative proof of the results of Mabuchi and
Stoppa--Székelyhidi. In proving them, we further provide an explicit local
density formula for the equivariant Riemann--Roch theorem.
",mathematics
"  Machine learning models are notoriously difficult to interpret and debug.
This is particularly true of neural networks. In this work, we introduce
automated software testing techniques for neural networks that are well-suited
to discovering errors which occur only for rare inputs. Specifically, we
develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF,
random mutations of inputs to a neural network are guided by a coverage metric
toward the goal of satisfying user-specified constraints. We describe how fast
approximate nearest neighbor algorithms can provide this coverage metric. We
then discuss the application of CGF to the following goals: finding numerical
errors in trained neural networks, generating disagreements between neural
networks and quantized versions of those networks, and surfacing undesirable
behavior in character level language models. Finally, we release an open source
library called TensorFuzz that implements the described techniques.
",statistics
"  We prove that if a smooth projective algebraic variety of dimension less or
equal to three has a unit type integral $K$-motive, then its integral Chow
motive is of Lefschetz type. As a consequence, the integral Chow motive is of
Lefschetz type for a smooth projective variety of dimension less or equal to
three that admits a full exceptional collection.
",mathematics
"  The environmental impacts of medium to large scale buildings receive
substantial attention in research, industry, and media. This paper studies the
energy savings potential of a commercial soccer stadium during day-to-day
operation. Buildings of this kind are characterized by special purpose system
installations like grass heating systems and by event-driven usage patterns.
This work presents a methodology to holistically analyze the stadiums
characteristics and integrate its existing instrumentation into a
Cyber-Physical System, enabling to deploy different control strategies
flexibly. In total, seven different strategies for controlling the studied
stadiums grass heating system are developed and tested in operation.
Experiments in winter season 2014/2015 validated the strategies impacts within
the real operational setup of the Commerzbank Arena, Frankfurt, Germany. With
95% confidence, these experiments saved up to 66% of median daily
weather-normalized energy consumption. Extrapolated to an average heating
season, this corresponds to savings of 775 MWh and 148 t of CO2 emissions. In
winter 2015/2016 an additional predictive nighttime heating experiment targeted
lower temperatures, which increased the savings to up to 85%, equivalent to 1
GWh (197 t CO2) in an average winter. Beyond achieving significant energy
savings, the different control strategies also met the target temperature
levels to the satisfaction of the stadiums operational staff. While the case
study constitutes a significant part, the discussions dedicated to the
transferability of this work to other stadiums and other building types show
that the concepts and the approach are of general nature. Furthermore, this
work demonstrates the first successful application of Deep Belief Networks to
regress and predict the thermal evolution of building systems.
",computer-science
"  We study configuration spaces of linkages whose underlying graph are polygons
with diagonal constrains, or more general, partial two-trees. We show that
(with an appropriate definition) the oriented area is a Bott-Morse function on
the configuration space. Its critical points are described and Bott-Morse
indices are computed. This paper is a generalization of analogous results for
polygonal linkages (obtained earlier by G. Khimshiashvili, G. Panina, and A.
Zhukova).
",mathematics
"  Betweenness centrality---measuring how many shortest paths pass through a
vertex---is one of the most important network analysis concepts for assessing
the relative importance of a vertex. The well-known algorithm of Brandes [2001]
computes, on an $n$-vertex and $m$-edge graph, the betweenness centrality of
all vertices in $O(nm)$ worst-case time. In follow-up work, significant
empirical speedups were achieved by preprocessing degree-one vertices and by
graph partitioning based on cut vertices. We further contribute an algorithmic
treatment of degree-two vertices, which turns out to be much richer in
mathematical structure than the case of degree-one vertices. Based on these
three algorithmic ingredients, we provide a strengthened worst-case running
time analysis for betweenness centrality algorithms. More specifically, we
prove an adaptive running time bound $O(kn)$, where $k < m$ is the size of a
minimum feedback edge set of the input graph.
",computer-science
"  Principal component analysis (PCA) and singular value decomposition (SVD) are
widely used in statistics, machine learning, and applied mathematics. It has
been well studied in the case of homoskedastic noise, where the noise levels of
the contamination are homogeneous.
In this paper, we consider PCA and SVD in the presence of heteroskedastic
noise, which arises naturally in a range of applications. We introduce a
general framework for heteroskedastic PCA and propose an algorithm called
HeteroPCA, which involves iteratively imputing the diagonal entries to remove
the bias due to heteroskedasticity. This procedure is computationally efficient
and provably optimal under the generalized spiked covariance model. A key
technical step is a deterministic robust perturbation analysis on the singular
subspace, which can be of independent interest. The effectiveness of the
proposed algorithm is demonstrated in a suite of applications, including
heteroskedastic low-rank matrix denoising, Poisson PCA, and SVD based on
heteroskedastic and incomplete data.
",statistics
"  The single-particle spectral function measures the density of electronic
states (DOS) in a material as a function of both momentum and energy, providing
central insights into phenomena such as superconductivity and Mott insulators.
While scanning tunneling microscopy (STM) and other tunneling methods have
provided partial spectral information, until now only angle-resolved
photoemission spectroscopy (ARPES) has permitted a comprehensive determination
of the spectral function of materials in both momentum and energy. However,
ARPES operates only on electronic systems at the material surface and cannot
work in the presence of applied magnetic fields. Here, we demonstrate a new
method for determining the full momentum and energy resolved electronic
spectral function of a two-dimensional (2D) electronic system embedded in a
semiconductor. In contrast with ARPES, the technique remains operational in the
presence of large externally applied magnetic fields and functions for
electronic systems with zero electrical conductivity or with zero electron
density. It provides a direct high-resolution and high-fidelity probe of the
dispersion and dynamics of the interacting 2D electron system. By ensuring the
system of interest remains under equilibrium conditions, we uncover delicate
signatures of many-body effects involving electron-phonon interactions,
plasmons, polarons, and a novel phonon analog of the vacuum Rabi splitting in
atomic systems.
",physics
"  In this article, we present a cut finite element method for two-phase
Navier-Stokes flows. The main feature of the method is the formulation of a
unified continuous interior penalty stabilisation approach for, on the one
hand, stabilising advection and the pressure-velocity coupling and, on the
other hand, stabilising the cut region. The accuracy of the algorithm is
enhanced by the development of extended fictitious domains to guarantee a well
defined velocity from previous time steps in the current geometry. Finally, the
robustness of the moving-interface algorithm is further improved by the
introduction of a curvature smoothing technique that reduces spurious
velocities. The algorithm is shown to perform remarkably well for low capillary
number flows, and is a first step towards flexible and robust CutFEM algorithms
for the simulation of microfluidic devices.
",computer-science
"  Auxetic materials are of great engineering interest not only because of their
fascinating negative Poisson's ratio, but also due to their increased toughness
and indentation resistance. These materials are typically synthesized polyester
foams with a very heterogeneous structure, but the role of disorder in auxetic
behavior is not fully understood. Here, we provide a systematic theoretical and
experimental investigation in to the effect of disorder on the mechanical
properties of a paradigmatic auxetic lattice with a re-entrant hexagonal
geometry. We show that disorder has a marginal effect on the Poisson's ratio
unless the lattice topology is altered, and in all cases examined the disorder
preserves the auxetic characteristics. Depending on the direction of loading
applied to these disordered auxetic lattices, either brittle or ductile failure
is observed. It is found that brittle failure is associated with a
disorder-dependent tensile strength, whereas in ductile failure disorder does
not affect strength. Our work thus provides general guidelines to optimize
elasticity and strength of disordered auxetic metamaterials.
",physics
"  In this paper we analyse the convergence properties of V-cycle multigrid
algorithms for the numerical solution of the linear system of equations arising
from discontinuous Galerkin discretization of second-order elliptic partial
differential equations on polytopal meshes. Here, the sequence of spaces that
stands at the basis of the multigrid scheme is possibly non nested and is
obtained based on employing agglomeration with possible edge/face coarsening.
We prove that the method converges uniformly with respect to the granularity of
the grid and the polynomial approximation degree p, provided that the number of
smoothing steps, which depends on p, is chosen sufficiently large.
",computer-science
"  Categorization is necessary for many decision making tasks. However, the
categorization process may interfere the decision making result and the law of
total probability can be violated in some situations. To predict the
interference effect of categorization, some model based on quantum probability
has been proposed. In this paper, a new quantum dynamic belief (QDB) model is
proposed. Considering the precise decision may not be made during the process,
the concept of uncertainty is introduced in our model to simulate real human
thinking process. Then the interference effect categorization can be predicted
by handling the uncertain information. The proposed model is applied to a
categorization decision-making experiment to explain the interference effect of
categorization. Compared with other models, our model is relatively more
succinct and the result shows the correctness and effectiveness of our model.
",computer-science
"  A famous result of Jurgen Moser states that a symplectic form on a compact
manifold cannot be deformed within its cohomology class to an inequivalent
symplectic form. It is well known that this does not hold in general for
noncompact symplectic manifolds. The notion of Eliashberg-Gromov convex ends
provides a natural restricted setting for the study of analogs of Moser's
symplectic stability result in the noncompact case, and this has been
significantly developed in work of Cieliebak-Eliashberg. Retaining the end
structure on the underlying smooth manifold, but dropping the convexity and
completeness assumptions on the symplectic forms at infinity we show that
symplectic stability holds under a natural growth condition on the path of
symplectic forms. The result can be straightforwardly applied as we show
through explicit examples.
",mathematics
"  A map merging component is crucial for the proper functionality of a
multi-robot system performing exploration, since it provides the means to
integrate and distribute the most important information carried by the agents:
the explored-covered space and its exact (depending on the SLAM accuracy)
morphology. Map merging is a prerequisite for an intelligent multi-robot team
aiming to deploy a smart exploration technique. In the current work, a metric
map merging approach based on environmental information is proposed, in
conjunction with spatially scattered RFID tags localization. This approach is
divided into the following parts: the maps approximate rotation calculation via
the obstacles poses and localized RFID tags, the translation employing the best
localized common RFID tag and finally the transformation refinement using an
ICP algorithm.
",computer-science
"  In this paper, we study the algebraic symplectic geometry of the singular
moduli spaces of Higgs bundles of degree $0$ and rank $n$ on a compact Riemann
surface $X$ of genus $g$. In particular, we prove that such moduli spaces are
symplectic singularities, in the sense of Beauville [Bea00], and admit a
projective symplectic resolution if and only if $g=1$ or $(g, n)=(2,2)$. These
results are an application of a recent paper by Bellamy and Schedler [BS16] via
the so-called Isosingularity Theorem.
",mathematics
"  The principle of material frame indifference is shown to be incompatible with
the basic balance laws of continuum mechanics. In its role of providing
constraints on possible constitutive prescriptions it must be replaced by the
classical principle of Galilean invariance.
",physics
"  Unsupervised domain adaptation is the problem setting where data generating
distributions in the source and target domains are different, and labels in the
target domain are unavailable. One important question in unsupervised domain
adaptation is how to measure the difference between the source and target
domains. A previously proposed discrepancy that does not use the source domain
labels requires high computational cost to estimate and may lead to a loose
generalization error bound in the target domain. To mitigate these problems, we
propose a novel discrepancy called source-guided discrepancy (S-disc), which
exploits labels in the source domain. As a consequence, S-disc can be computed
efficiently with a finite sample convergence guarantee. In addition, we show
that S-disc can provide a tighter generalization error bound than the one based
on an existing discrepancy. Finally, we report experimental results that
demonstrate the advantages of S-disc over the existing discrepancies.
",statistics
"  Recent experiments revealed a striking asymmetry in the phase diagram of the
high temperature cuprate superconductors. The correlation effect seems strong
in the hole-doped systems and weak in the electron-doped systems. On the other
hand, a recent theoretical study shows that the interaction strengths (the
Hubbard U) are comparable in these systems. Therefore, it is difficult to
explain this asymmetry by their interaction strengths. Given this background,
we analyze the one-particle spectrum of a single band model of a cuprate
superconductor near the Fermi level using the dynamical mean field theory. We
find the difference in the ""visibility"" of the strong correlation effect
between the hole- and electron-doped systems. This can explain the
electron-hole asymmetry of the correlation strength without introducing the
difference in the interaction strength.
",physics
"  In this paper, we consider the nonlinear inhomogeneous compressible elastic
waves in three spatial dimensions when the density is a small disturbance
around a constant state. In homogeneous case, the almost global existence was
established by Klainerman-Sideris [1996_CPAM], and global existence was built
by Agemi [2000_Invent. Math.] and Sideris [1996_Invent. Math., 2000_Ann. Math.]
independently. Here we establish the corresponding almost global and global
existence theory in the inhomogeneous case.
",mathematics
"  Purpose: MRI cell tracking can be used to monitor immune cells involved in
the immunotherapy response, providing insight into the mechanism of action,
temporal progression of tumour growth and individual potency of therapies. To
evaluate whether MRI could be used to track immune cell populations in response
to immunotherapy, CD8+ cytotoxic T cells (CTLs), CD4+CD25+FoxP3+ regulatory T
cells (Tregs) and myeloid derived suppressor cells (MDSCs) were labelled with
superparamagnetic iron oxide (SPIO) particles.
Methods: SPIO-labelled cells were injected into mice (one cell type/mouse)
implanted with an HPV-based cervical cancer model. Half of these mice were also
vaccinated with DepoVaxTM, a lipid-based vaccine platform that was developed to
enhance the potency of peptide-based vaccines.
Results: MRI visualization of CTLs, Tregs and MDSCs was apparent 24 hours
post-injection, with hypointensities due to iron labelled cells clearing
approximately 72 hours post-injection. Vaccination resulted in increased
recruitment of CTLs and decreased recruitment of MDSCs and Tregs to the tumour.
We also found that MDSC and Treg recruitment was positively correlated with
final tumour volume.
Conclusion: This type of analysis can be used to non-invasively study changes
in immune cell recruitment in individual mice over time, potentially allowing
improved application and combination of immunotherapies.
",physics
"  We present a thorough analysis of the interplay of magnetic moment and the
Jahn-Teller effect in the $\Gamma_8$ cubic multiplet. We find that in the
presence of dynamical Jahn-Teller effect, the Zeeman interaction remains
isotropic, whereas the $g$ and $G$ factors can change their signs. The static
Jahn-Teller distortion also can change the sign of these $g$ factors as well as
the nature of the magnetic anisotropy. Combining the theory with
state-of-the-art {\it ab initio} calculations, we analyzed the magnetic
properties of Np$^{4+}$ and Ir$^{4+}$ impurity ions in cubic environment. The
calculated $g$ factors of Np$^{4+}$ impurity agree well with experimental data.
The {\it ab initio} calculation predicts strong Jahn-Teller effect in Ir$^{4+}$
ion in cubic environment and the strong vibronic reduction of $g$ and $G$
factors.
",physics
"  In this paper, we present a novel approach to identify the generators and
states responsible for the small-signal stability of power networks. To this
end, the newly developed notion of information transfer between the states of a
dynamical system is used. In particular, using the concept of information
transfer, which characterizes influence between the various states and a linear
combination of states of a dynamical system, we identify the generators and
states which are responsible for causing instability of the power network.
While characterizing influence from state to state, information transfer can
also describe influence from state to modes thereby generalizing the well-known
notion of participation factor while at the same time overcoming some of the
limitations of the participation factor. The developed framework is applied to
study the three bus system identifying various cause of instabilities in the
system. The simulation study is extended to IEEE 39 bus system.
",computer-science
"  Atomically thin PtSe2 films have attracted extensive research interests for
potential applications in high-speed electronics, spintronics and
photodetectors. Obtaining high quality, single crystalline thin films with
large size is critical. Here we report the first successful layer-by-layer
growth of high quality PtSe2 films by molecular beam epitaxy. Atomically thin
films from 1 ML to 22 ML have been grown and characterized by low-energy
electron diffraction, Raman spectroscopy and X-ray photoemission spectroscopy.
Moreover, a systematic thickness dependent study of the electronic structure is
revealed by angle-resolved photoemission spectroscopy (ARPES), and helical spin
texture is revealed by spin-ARPES. Our work provides new opportunities for
growing large size single crystalline films for investigating the physical
properties and potential applications of PtSe2.
",physics
"  The recent experimental discovery of three-dimensional (3D) materials hosting
a strong Rashba spin-orbit coupling calls for the theoretical investigation of
their transport properties. Here we study the zero temperature dc conductivity
of a 3D Rashba metal in the presence of static diluted impurities. We show
that, at variance with the two-dimensional case, in 3D systems spin-orbit
coupling affects dc charge transport in all density regimes. We find in
particular that the effect of spin-orbit interaction strongly depends on the
direction of the current, and we show that this yields strongly anisotropic
transport characteristics. In the dominant spin-orbit coupling regime where
only the lowest band is occupied, the SO-induced conductivity anisotropy is
governed entirely by the anomalous component of the renormalized current. We
propose that measurements of the conductivity anisotropy in bulk Rashba metals
may give a direct experimental assessment of the spin-orbit strength.
",physics
"  The majority of industrial-strength object-oriented (OO) software is written
using nominally-typed OO programming languages. Extant domain-theoretic models
of OOP developed to analyze OO type systems miss, however, a crucial feature of
these mainstream OO languages: nominality. This paper presents the construction
of NOOP as the first domain-theoretic model of OOP that includes full
class/type names information found in nominally-typed OOP. Inclusion of nominal
information in objects of NOOP and asserting that type inheritance in
statically-typed OO programming languages is an inherently nominal notion allow
readily proving that type inheritance and subtyping are completely identified
in these languages. This conclusion is in full agreement with intuitions of
developers and language designers of these OO languages, and contrary to the
belief that ""inheritance is not subtyping,"" which came from assuming
non-nominal (a.k.a., structural) models of OOP.
To motivate the construction of NOOP, this paper briefly presents the
benefits of nominal-typing to mainstream OO developers and OO language
designers, as compared to structural-typing. After presenting NOOP, the paper
further briefly compares NOOP to the most widely known domain-theoretic models
of OOP. Leveraging the development of NOOP, the comparisons presented in this
paper provide clear, brief and precise technical and mathematical accounts for
the relation between nominal and structural OO type systems. NOOP, thus,
provides a firmer semantic foundation for analyzing and progressing
nominally-typed OO programming languages.
",computer-science
"  We prove a Chernoff-type bound for sums of matrix-valued random variables
sampled via a random walk on an expander, confirming a conjecture due to
Wigderson and Xiao. Our proof is based on a new multi-matrix extension of the
Golden-Thompson inequality which improves in some ways the inequality of
Sutter, Berta, and Tomamichel, and may be of independent interest, as well as
an adaptation of an argument for the scalar case due to Healy. Secondarily, we
also provide a generic reduction showing that any concentration inequality for
vector-valued martingales implies a concentration inequality for the
corresponding expander walk, with a weakening of parameters proportional to the
squared mixing time.
",computer-science
"  We give a bordered extension of involutive HF-hat and use it to give an
algorithm to compute involutive HF-hat for general 3-manifolds. We also explain
how the mapping class group action on HF-hat can be computed using bordered
Floer homology. As applications, we prove that involutive HF-hat satisfies a
surgery exact triangle and compute HFI-hat of the branched double covers of all
10-crossing knots.
",mathematics
"  Swarms of robots will revolutionize many industrial applications, from
targeted material delivery to precision farming. Controlling the motion and
behavior of these swarms presents unique challenges for human operators, who
cannot yet effectively convey their high-level intentions to a group of robots
in application. This work proposes a new human-swarm interface based on novel
wearable gesture-control and haptic-feedback devices. This work seeks to
combine a wearable gesture recognition device that can detect high-level
intentions, a portable device that can detect Cartesian information and finger
movements, and a wearable advanced haptic device that can provide real-time
feedback. This project is the first to envisage a wearable Human-Swarm
Interaction (HSI) interface that separates the input and feedback components of
the classical control loop (input, output, feedback), as well as being the
first of its kind suitable for both indoor and outdoor environments.
",computer-science
"  We present HornDroid, a new tool for the static analysis of information flow
properties in Android applications. The core idea underlying HornDroid is to
use Horn clauses for soundly abstracting the semantics of Android applications
and to express security properties as a set of proof obligations that are
automatically discharged by an off-the-shelf SMT solver. This approach makes it
possible to fine-tune the analysis in order to achieve a high degree of
precision while still using off-the-shelf verification tools, thereby
leveraging the recent advances in this field. As a matter of fact, HornDroid
outperforms state-of-the-art Android static analysis tools on benchmarks
proposed by the community. Moreover, HornDroid is the first static analysis
tool for Android to come with a formal proof of soundness, which covers the
core of the analysis technique: besides yielding correctness assurances, this
proof allowed us to identify some critical corner-cases that affect the
soundness guarantees provided by some of the previous static analysis tools for
Android.
",computer-science
"  Initializing all elements of an array to a specified value is a basic
operation that frequently appears in numerous algorithms and programs.
Initializable arrays are abstract arrays that support initialization as well as
reading and writing of any element of the array in less than linear time
proportional to the length of the array. On the word RAM model with $w$ bits
word size, we propose an in-place algorithm using only 1 extra bit which
implements an initializable array of length $N$ each of whose elements can
store $\ell \in O(w)$ bits value, and supports all operations in constant worst
case time. We also show that our algorithm is not only time optimal but also
space optimal. Our algorithm significantly improves upon the previous best
algorithm [Navarro, CSUR 2014] using $N + \ell + o(N)$ extra bits supporting
all operations in constant worst case time.
Moreover, for a special cast that $\ell \ge 2 \lceil \log N \rceil$ and $\ell
\in O(w)$, we also propose an algorithm so that each element of initializable
array can store $2^\ell$ normal states and a one optional state, which uses
$\ell + \lceil \log N \rceil + 1$ extra bits and supports all operations in
constant worst case time.
",computer-science
"  The limitations in performance of the present RICH system in the LHCb
experiment are given by the natural chromatic dispersion of the gaseous
Cherenkov radiator, the aberrations of the optical system and the pixel size of
the photon detectors. Moreover, the overall PID performance can be affected by
high detector occupancy as the pattern recognition becomes more difficult with
high particle multiplicities. This paper shows a way to improve performance by
systematically addressing each of the previously mentioned limitations. These
ideas are applied in the present and future upgrade phases of the LHCb
experiment. Although applied to specific circumstances, they are used as a
paradigm on what is achievable in the development and realisation of high
precision RICH detectors.
",physics
"  Given a graph on $n$ vertices and an integer $k$, the feedback vertex set
problem asks for the deletion of at most $k$ vertices to make the graph
acyclic. We show that a greedy branching algorithm, which always branches on an
undecided vertex with the largest degree, runs in single-exponential time,
i.e., $O(c^k\cdot n^2)$ for some constant $c$.
",computer-science
"  Misunderstanding of driver correction behaviors (DCB) is the primary reason
for false warnings of lane-departure-prediction systems. We propose a
learning-based approach to predicting unintended lane-departure behaviors (LDB)
and the chance for drivers to bring the vehicle back to the lane. First, in
this approach, a personalized driver model for lane-departure and lane-keeping
behavior is established by combining the Gaussian mixture model and the hidden
Markov model. Second, based on this model, we develop an online model-based
prediction algorithm to predict the forthcoming vehicle trajectory and judge
whether the driver will demonstrate an LDB or a DCB. We also develop a warning
strategy based on the model-based prediction algorithm that allows the
lane-departure warning system to be acceptable for drivers according to the
predicted trajectory. In addition, the naturalistic driving data of 10 drivers
is collected through the University of Michigan Safety Pilot Model Deployment
program to train the personalized driver model and validate this approach. We
compare the proposed method with a basic time-to-lane-crossing (TLC) method and
a TLC-directional sequence of piecewise lateral slopes (TLC-DSPLS) method. The
results show that the proposed approach can reduce the false-warning rate to
3.07\%.
",computer-science
"  Let $G$ be a finite group with the property that if $a,b$ are powers of
$\delta_1^*$-commutators such that $(|a|,|b|)=1$, then $|ab|=|a||b|$. We show
that $\gamma_{\infty}(G)$ is nilpotent.
",mathematics
"  Elasticity is one of the key features of cloud computing that attracts many
SaaS providers to minimize their services' cost. Cost is minimized by
automatically provision and release computational resources depend on actual
computational needs. However, delay of starting up new virtual resources can
cause Service Level Agreement violation. Consequently, predicting cloud
resources provisioning gains a lot of attention to scale computational
resources in advance. However, most of current approaches do not consider
multi-seasonality in cloud workloads. This paper proposes cloud resource
provisioning prediction algorithm based on Holt-Winters exponential smoothing
method. The proposed algorithm extends Holt-Winters exponential smoothing
method to model cloud workload with multi-seasonal cycles. Prediction accuracy
of the proposed algorithm has been improved by employing Artificial Bee Colony
algorithm to optimize its parameters. Performance of the proposed algorithm has
been evaluated and compared with double and triple exponential smoothing
methods. Our results have shown that the proposed algorithm outperforms other
methods.
",computer-science
"  In this paper, we present a gated convolutional recurrent neural network
based approach to solve task 4, large-scale weakly labelled semi-supervised
sound event detection in domestic environments, of the DCASE 2018 challenge.
Gated linear units and a temporal attention layer are used to predict the onset
and offset of sound events in 10s long audio clips. Whereby for training only
weakly-labelled data is used. Virtual adversarial training is used for
regularization, utilizing both labelled and unlabeled data. Furthermore, we
introduce self-adaptive label refinement, a method which allows unsupervised
adaption of our trained system to refine the accuracy of frame-level class
predictions. The proposed system reaches an overall macro averaged event-based
F-score of 34.6%, resulting in a relative improvement of 20.5% over the
baseline system.
",computer-science
"  This paper gives drastically faster gossip algorithms to compute exact and
approximate quantiles.
Gossip algorithms, which allow each node to contact a uniformly random other
node in each round, have been intensely studied and been adopted in many
applications due to their fast convergence and their robustness to failures.
Kempe et al. [FOCS'03] gave gossip algorithms to compute important aggregate
statistics if every node is given a value. In particular, they gave a beautiful
$O(\log n + \log \frac{1}{\epsilon})$ round algorithm to $\epsilon$-approximate
the sum of all values and an $O(\log^2 n)$ round algorithm to compute the exact
$\phi$-quantile, i.e., the the $\lceil \phi n \rceil$ smallest value.
We give an quadratically faster and in fact optimal gossip algorithm for the
exact $\phi$-quantile problem which runs in $O(\log n)$ rounds. We furthermore
show that one can achieve an exponential speedup if one allows for an
$\epsilon$-approximation. We give an $O(\log \log n + \log \frac{1}{\epsilon})$
round gossip algorithm which computes a value of rank between $\phi n$ and
$(\phi+\epsilon)n$ at every node.% for any $0 \leq \phi \leq 1$ and $0 <
\epsilon < 1$. Our algorithms are extremely simple and very robust - they can
be operated with the same running times even if every transmission fails with
a, potentially different, constant probability. We also give a matching
$\Omega(\log \log n + \log \frac{1}{\epsilon})$ lower bound which shows that
our algorithm is optimal for all values of $\epsilon$.
",computer-science
"  In this article, we continue the study of the problem of $L^p$-boundedness of
the maximal operator $M$ associated to averages along isotropic dilates of a
given, smooth hypersurface $S$ of finite type in 3-dimensional Euclidean space.
An essentially complete answer to this problem had been given about seven years
ago by the last named two authors in joint work with M. Kempe for the case
where the height h of the given surface is at least two. In the present
article, we turn to the case $h<2.$ More precisely, in this Part I, we study
the case where $h<2,$ assuming that $S$ is contained in a sufficiently small
neighborhood of a given point $x^0\in S$ at which both principal curvatures of
$S$ vanish. Under these assumptions and a natural transversality assumption, we
show that, as in the case where $h\ge 2,$ the critical Lebesgue exponent for
the boundedness of $M$ remains to be $p_c=h,$ even though the proof of this
result turns out to require new methods, some of which are inspired by the more
recent work by the last named two authors on Fourier restriction to S. Results
on the case where $h<2$ and exactly one principal curvature of $S$ does not
vanish at $x^0$ will appear elsewhere.
",mathematics
"  Solvothermal intercalation of ethylenediamine molecules into FeSe separates
the layers by 1078 pm and creates a different stacking. FeSe(en)0.3 is not
superconducting although each layer exhibits the stripe-type crystal structure
and the Fermi surface topology of superconducting FeSe. FeSe(en)0.3 requires
electron-doping for high-Tc similar to monolayers of FeSe@SrTiO3, whose much
higher Tc may arise from the proximity of the oxide surface.
",physics
"  Models involving branched structures are employed to describe several
supply-demand systems such as the structure of the nerves of a leaf, the system
of roots of a tree and the nervous or cardiovascular systems. Given a flow
(traffic path) that transports a given measure $\mu^-$ onto a target measure
$\mu^+$, along a 1-dimensional network, the transportation cost per unit length
is supposed in these models to be proportional to a concave power $\alpha \in
(0,1)$ of the intensity of the flow.
In this paper we address an open problem in the book ""Optimal transportation
networks"" by Bernot, Caselles and Morel and we improve the stability for
optimal traffic paths in the Euclidean space $\mathbb{R}^d$, with respect to
variations of the given measures $(\mu^-,\mu^+)$, which was known up to now
only for $\alpha>1-\frac1d$. We prove it for exponents $\alpha>1-\frac1{d-1}$
(in particular, for every $\alpha \in (0,1)$ when $d=2$), for a fairly large
class of measures $\mu^+$ and $\mu^-$.
",mathematics
"  While enormous progress has been made to Variational Autoencoder (VAE) in
recent years, similar to other deep networks, VAE with deep networks suffers
from the problem of degeneration, which seriously weakens the correlation
between the input and the corresponding latent codes, deviating from the goal
of the representation learning. To investigate how degeneration affects VAE
from a theoretical perspective, we illustrate the information transmission in
VAE and analyze the intermediate layers of the encoders/decoders. Specifically,
we propose a Fisher Information measure for the layer-wise analysis. With such
measure, we demonstrate that information loss is ineluctable in feed-forward
networks and causes the degeneration in VAE. We show that skip connections in
VAE enable the preservation of information without changing the model
architecture. We call this class of VAE equipped with skip connections as SCVAE
and perform a range of experiments to show its advantages in information
preservation and degeneration mitigation.
",statistics
"  Principal component regression is a linear regression model with principal
components as regressors. This type of modelling is particularly useful for
prediction in settings with high-dimensional covariates. Surprisingly, the
existing literature treating of Bayesian approaches is relatively sparse. In
this paper, we aim at filling some gaps through the following practical
contribution: we introduce a Bayesian approach with detailed guidelines for a
straightforward implementation. The approach features two characteristics that
we believe are important. First, it effectively involves the relevant principal
components in the prediction process. This is achieved in two steps. The first
one is model selection; the second one is to average out the predictions
obtained from the selected models according to model averaging mechanisms,
allowing to account for model uncertainty. The model posterior probabilities
are required for model selection and model averaging. For this purpose, we
include a procedure leading to an efficient reversible jump algorithm. The
second characteristic of our approach is whole robustness, meaning that the
impact of outliers on inference gradually vanishes as they approach plus or
minus infinity. The conclusions obtained are consequently consistent with the
majority of observations (the bulk of the data).
",statistics
"  We consider the stochastic damped Navier-Stokes equations in $\mathbb R^d$
($d=2,3$), assuming as in our previous work [4] that the covariance of the
noise is not too regular, so Itô calculus cannot be applied in the space of
finite energy vector fields. We prove the existence of an invariant measure
when $d=2$ and of a stationary solution when $d=3$.
",mathematics
"  The frequency responses of the K-Rb-$^{21}$Ne co-magnetometer to magnetic
field and exotic spin dependent forces are experimentally studied and simulated
in this paper. Both the relationship between the output amplitude, the phase
shift and frequencies are studied. The responses of magnetic field are
experimentally investigated. Due to a lack of input methods, others are
numerically simulated.
",physics
"  We compared positions of the Gaia first data release (DR1) secondary data set
at its faint limit with CCD positions of stars in 20 fields observed with the
VLT/FORS2 camera. The FORS2 position uncertainties are smaller than one
milli-arcsecond (mas) and allowed us to perform an independent verification of
the DR1 astrometric precision. In the fields that we observed with FORS2, we
projected the Gaia DR1 positions into the CCD plane, performed a polynomial fit
between the two sets of matching stars, and carried out statistical analyses of
the residuals in positions. The residual RMS roughly matches the expectations
given by the Gaia DR1 uncertainties, where we identified three regimes in terms
of Gaia DR1 precision: for G = 17-20 stars we found that the formal DR1
position uncertainties of stars with DR1 precisions in the range of 0.5-5 mas
are underestimated by 63 +/- 5\%, whereas the DR1 uncertainties of stars in the
range 7-10 mas are overestimated by a factor of two. For the best-measured and
generally brighter G = 16-18 stars with DR1 positional uncertainties of <0.5
mas, we detected 0.44 +/- 0.13 mas excess noise in the residual RMS, whose
origin can be in both FORS2 and Gaia DR1. By adopting Gaia DR1 as the absolute
reference frame we refined the pixel scale determination of FORS2, leading to
minor updates to the parallaxes of 20 ultracool dwarfs that we published
previously. We also updated the FORS2 absolute parallax of the Luhman 16 binary
brown dwarf system to 501.42 +/- 0.11 mas
",physics
"  A novel approach to quintessential inflation model building is studied,
within the framework of $\alpha$-attractors, motivated by supergravity
theories. Inflationary observables are in excellent agreement with the latest
CMB observations, while quintessence explains the dark energy observations
without any fine-tuning. The model is kept intentionally minimal, avoiding the
introduction of many degrees of freedom, couplings and mass scales. In stark
contrast to $\Lambda$CDM, for natural values of the parameters, the model
attains transient accelerated expansion, which avoids the future horizon
problem, while it maintains the field displacement mildly sub-Planckian such
that the flatness of the quintessential tail is not lifted by radiative
corrections and violations of the equivalence principle (fifth force) are under
control. In particular, the required value of the cosmological constant is near
the eletroweak scale. Attention is paid to the reheating of the Universe, which
avoids gravitino overproduction and respects nucleosynthesis constraints.
Kination is treated in a model independent way. A spike in gravitational waves,
due to kination, is found not to disturb nucleosynthesis as well.
",physics
"  Many real-world networks are known to exhibit facts that counter our
knowledge prescribed by the theories on network creation and communication
patterns. A common prerequisite in network analysis is that information on
nodes and links will be complete because network topologies are extremely
sensitive to missing information of this kind. Therefore, many real-world
networks that fail to meet this criterion under random sampling may be
discarded.
In this paper we offer a framework for interpreting the missing observations
in network data under the hypothesis that these observations are not missing at
random. We demonstrate the methodology with a case study of a financial trade
network, where the awareness of agents to the data collection procedure by a
self-interested observer may result in strategic revealing or withholding of
information. The non-random missingness has been overlooked despite the
possibility of this being an important feature of the processes by which the
network is generated. The analysis demonstrates that strategic information
withholding may be a valid general phenomenon in complex systems. The evidence
is sufficient to support the existence of an influential observer and to offer
a compelling dynamic mechanism for the creation of the network.
",statistics
"  Recent results of Laca, Raeburn, Ramagge and Whittaker show that any
self-similar action of a groupoid on a graph determines a 1-parameter family of
self-mappings of the trace space of the groupoid C*-algebra. We investigate the
fixed points for these self-mappings, under the same hypotheses that Laca et
al. used to prove that the C*-algebra of the self-similar action admits a
unique KMS state. We prove that for any value of the parameter, the associated
self-mapping admits a unique fixed point, which is in fact a universal
attractor. This fixed point is precisely the trace that extends to a KMS state
on the C*-algebra of the self-similar action.
",mathematics
"  The surface tension of flowing soap films is measured with respect to the
film thickness and the concentration of soap solution. We perform this
measurement by measuring the curvature of the nylon wires that bound the soap
film channel and use the measured curvature to parametrize the relation between
the surface tension and the tension of the wire. We find the surface tension of
our soap films increases when the film is relatively thin or made of soap
solution of low concentration, otherwise it approaches an asymptotic value 30
mN/m. A simple adsorption model with only two parameters describes our
observations reasonably well. With our measurements, we are also able to
measure Gibbs elasticity for our soap film.
",physics
"  We prove an inverse theorem for the Gowers $U^2$-norm for maps $G\to\mathcal
M$ from an countable, discrete, amenable group $G$ into a von Neumann algebra
$\mathcal M$ equipped with an ultraweakly lower semi-continuous, unitarily
invariant (semi-)norm $\Vert\cdot\Vert$. We use this result to prove a
stability result for unitary-valued $\varepsilon$-representations $G\to\mathcal
U(\mathcal M)$ with respect to $\Vert\cdot \Vert$.
",mathematics
"  Casimir forces between material surfaces at close proximity of less than 200
nm can lead to increased chaotic behavior of actuating devices depending on the
strength of the Casimir interaction. We investigate these phenomena for phase
change materials in torsional oscillators, where the amorphous to crystalline
phase transitions lead to transitions between high and low Casimir force and
torque states respectively, without material compositions. For a conservative
system bifurcation curve and Poincare maps analysis show the absence of chaotic
behavior but with the crystalline phase (high force/torque state) favoring more
unstable behavior and stiction. However, for a non-conservative system chaotic
behavior can occur introducing significant risk for stiction, which is again
more pronounced for the crystalline phase. The latter illustrates the more
general scenario that stronger Casimir forces and torques increase the
possibility for chaotic behavior. The latter is making impossible to predict
whether stiction or stable actuation will occur on a long term basis, and it is
setting limitations in the design of micro/nano devices operating at short
range nanoscale separations.
",physics
"  Approximate model counting for bit-vector SMT formulas (generalizing \#SAT)
has many applications such as probabilistic inference and quantitative
information-flow security, but it is computationally difficult. Adding random
parity constraints (XOR streamlining) and then checking satisfiability is an
effective approximation technique, but it requires a prior hypothesis about the
model count to produce useful results. We propose an approach inspired by
statistical estimation to continually refine a probabilistic estimate of the
model count for a formula, so that each XOR-streamlined query yields as much
information as possible. We implement this approach, with an approximate
probability model, as a wrapper around an off-the-shelf SMT solver or SAT
solver. Experimental results show that the implementation is faster than the
most similar previous approaches which used simpler refinement strategies. The
technique also lets us model count formulas over floating-point constraints,
which we demonstrate with an application to a vulnerability in differential
privacy mechanisms.
",computer-science
"  In this note we prove a conjecture by Li, Qu, Li, and Fu on permutation
trinomials over $\mathbb{F}_3^{2k}$. In addition, new examples and
generalizations of some families of permutation polynomials of
$\mathbb{F}_{3^k}$ and $\mathbb{F}_{5^k}$ are given. We also study permutation
quadrinomials of type $Ax^{q(q-1)+1} + Bx^{2(q-1)+1} + Cx^{q} + x$. Our method
is based on the investigation of an algebraic curve associated with a
{fractional polynomial} over a finite field.
",mathematics
"  We determine the structure of the W-group $\mathcal{G}_F$, the small Galois
quotient of the absolute Galois group $G_F$ of the Pythagorean formally real
field $F$ when the space of orderings $X_F$ has finite order. Based on
Marshall's work (1979), we reduce the structure of $\mathcal{G}_F$ to that of
$\mathcal{G}_{\bar{F}}$, the W-group of the residue field $\bar{F}$ when $X_F$
is a connected space. In the disconnected case, the structure of
$\mathcal{G}_F$ is the free product of the W-groups $\mathcal{G}_{F_i}$
corresponding to the connected components $X_i$ of $X_F$. We also give a
completely Galois theoretic proof for Marshall's Basic Lemma.
",mathematics
"  Filters in a Convolutional Neural Network (CNN) contain model parameters
learned from enormous amounts of data. In this paper, we suggest to decompose
convolutional filters in CNN as a truncated expansion with pre-fixed bases,
namely the Decomposed Convolutional Filters network (DCFNet), where the
expansion coefficients remain learned from data. Such a structure not only
reduces the number of trainable parameters and computation, but also imposes
filter regularity by bases truncation. Through extensive experiments, we
consistently observe that DCFNet maintains accuracy for image classification
tasks with a significant reduction of model parameters, particularly with
Fourier-Bessel (FB) bases, and even with random bases. Theoretically, we
analyze the representation stability of DCFNet with respect to input
variations, and prove representation stability under generic assumptions on the
expansion coefficients. The analysis is consistent with the empirical
observations.
",statistics
"  TaSb$_{2}$ has been predicted theoretically and proposed through
magnetotransport experiment to be a topological semimetal. In earlier reports,
the Shubnikov-de Haas oscillation has been analyzed to probe the Fermi surface,
with magnetic field along a particular crystallographic axis only. By employing
a sample rotator, we reveal highly anisotropic transverse magnetoresistance by
rotating the magnetic field along different crystallographic directions. To
probe the anisotropy in the Fermi surface, we have performed magnetization
measurements and detected strong de Haas-van Alphen (dHvA) oscillations for the
magnetic field applied along \textbf{b} and \textbf{c} axes as well as
perpendicular to \textbf{bc} plane of the crystals. Three Fermi pockets have
been identified by analyzing the dHvA oscillations. Hall measurement reveals
electron as the only charge carrier, i.e., all the three Fermi pockets are
electron type. With the application of magnetic field along different crystal
directions, the cross sectional areas of the Fermi pockets have been found
significantly different. Other physical parameters, such as the effective mass
of the charge carrier and Fermi velocity have also been calculated using the
Lifshitz-Kosevich formula.
",physics
"  Preventable medical errors are estimated to be among the leading causes of
injury and death in the United States. To prevent such errors, healthcare
systems have implemented patient safety and incident reporting systems. These
systems enable clinicians to report unsafe conditions and cases where patients
have been harmed due to errors in medical care. These reports are narratives in
natural language and while they provide detailed information about the
situation, it is non-trivial to perform large scale analysis for identifying
common causes of errors and harm to the patients. In this work, we present a
method based on attentive convolutional and recurrent networks for identifying
harm events in patient care and categorize the harm based on its severity
level. We demonstrate that our methods can significantly improve the
performance over existing methods in identifying harm in clinical care.
",computer-science
"  A stress is applied at the flat face and the apex of a prismatic
piezoelectric crystal. The voltage generated at these points differs in order
of magnitude. The result may be used to nondestructively test the uniformity of
surfaces of piezoelectric crystals.
",physics
"  Many theories have emerged which investigate how in- variance is generated in
hierarchical networks through sim- ple schemes such as max and mean pooling.
The restriction to max/mean pooling in theoretical and empirical studies has
diverted attention away from a more general way of generating invariance to
nuisance transformations. We con- jecture that hierarchically building
selective invariance (i.e. carefully choosing the range of the transformation
to be in- variant to at each layer of a hierarchical network) is im- portant
for pattern recognition. We utilize a novel pooling layer called adaptive
pooling to find linear pooling weights within networks. These networks with the
learnt pooling weights have performances on object categorization tasks that
are comparable to max/mean pooling networks. In- terestingly, adaptive pooling
can converge to mean pooling (when initialized with random pooling weights),
find more general linear pooling schemes or even decide not to pool at all. We
illustrate the general notion of selective invari- ance through object
categorization experiments on large- scale datasets such as SVHN and ILSVRC
2012.
",computer-science
"  We consider statistical estimation of superhedging prices using historical
stock returns in a frictionless market with d traded assets. We introduce a
simple plugin estimator based on empirical measures, show it is consistent but
lacks suitable robustness. This is addressed by our improved estimators which
use a larger set of martingale measures defined through a tradeoff between the
radius of Wasserstein balls around the empirical measure and the allowed norm
of martingale densities. We also study convergence rates, convergence of
superhedging strategies, and our study extends, in part, to the case of a
market with traded options and to a multiperiod setting.
",quantitative-finance
"  We theoretically study scattering process and superconducting triplet
correlations in a graphene junction comprised of ferromagnet-RSO-superconductor
in which RSO stands for a region with Rashba spin orbit interaction. Our
results reveal spin-polarized subgap transport through the system due to an
anomalous equal-spin Andreev reflection in addition to conventional back
scatterings. We calculate equal- and opposite-spin pair correlations near the
F-RSO interface and demonstrate direct link of the anomalous Andreev reflection
and equal-spin pairings arised due to the proximity effect in the presence of
RSO interaction. Moreover, we show that the amplitude of anomalous Andreev
reflection, and thus the triplet pairings, are experimentally controllable when
incorporating the influences of both tunable strain and Fermi level in the
nonsuperconducting region. Our findings can be confirmed by a conductance
spectroscopy experiment and provide better insights into the proximity-induced
RSO coupling in graphene layers reported in recent experiments.
",physics
"  In this paper, a new type of 3D bin packing problem (BPP) is proposed, in
which a number of cuboid-shaped items must be put into a bin one by one
orthogonally. The objective is to find a way to place these items that can
minimize the surface area of the bin. This problem is based on the fact that
there is no fixed-sized bin in many real business scenarios and the cost of a
bin is proportional to its surface area. Our research shows that this problem
is NP-hard. Based on previous research on 3D BPP, the surface area is
determined by the sequence, spatial locations and orientations of items. Among
these factors, the sequence of items plays a key role in minimizing the surface
area. Inspired by recent achievements of deep reinforcement learning (DRL)
techniques, especially Pointer Network, on combinatorial optimization problems
such as TSP, a DRL-based method is applied to optimize the sequence of items to
be packed into the bin. Numerical results show that the method proposed in this
paper achieve about 5% improvement than heuristic method.
",computer-science
"  Wardrop equilibria in nonatomic congestion games are in general inefficient
as they do not induce an optimal flow that minimizes the total travel time.
Network tolls are a prominent and popular way to induce an optimum flow in
equilibrium. The classical approach to find such tolls is marginal cost pricing
which requires the exact knowledge of the demand on the network. In this paper,
we investigate under which conditions demand-independent optimum tolls exist
that induce the system optimum flow for any travel demand in the network. We
give several characterizations for the existence of such tolls both in terms of
the cost structure and the network structure of the game. Specifically we show
that demand-independent optimum tolls exist if and only if the edge cost
functions are shifted monomials as used by the Bureau of Public Roads.
Moreover, non-negative demand-independent optimum tolls exist when the network
is a directed acyclic multi-graph. Finally, we show that any network with a
single origin-destination pair admits demand-independent optimum tolls that,
although not necessarily non-negative, satisfy a budget constraint.
",computer-science
"  Human-in-the-loop manipulation is useful in when autonomous grasping is not
able to deal sufficiently well with corner cases or cannot operate fast enough.
Using the teleoperator's hand as an input device can provide an intuitive
control method but requires mapping between pose spaces which may not be
similar. We propose a low-dimensional and continuous teleoperation subspace
which can be used as an intermediary for mapping between different hand pose
spaces. We present an algorithm to project between pose space and teleoperation
subspace. We use a non-anthropomorphic robot to experimentally prove that it is
possible for teleoperation subspaces to effectively and intuitively enable
teleoperation. In experiments, novice users completed pick and place tasks
significantly faster using teleoperation subspace mapping than they did using
state of the art teleoperation methods.
",computer-science
"  Functions or 'functionnings' enable to give a structure to any economic
activity whether they are used to describe a good or a service that is
exchanged on a market or they constitute the capability of an agent to provide
the labor market with specific work and skills. That structure encompasses the
basic law of supply and demand and the conditions of growth within a
transaction and of the inflation control. Functional requirements can be
followed from the design of a product to the delivery of a solution to a
customer needs with different levels of externalities while value is created
integrating organizational and technical constraints whereas a budget is
allocated to the various entities of the firm involved in the production.
Entering the market through that structure leads to designing basic equations
of its dynamics and to finding canonical solutions out of particular
equilibria. This approach enables to tackle behavioral foundations of Prospect
Theory within a generalization of its probability weighting function turned
into an operator which applies to Western, Educated, Industrialized, Rich, and
Democratic societies as well as to the poorest ones. The nature of reality and
well-being appears then as closely related to the relative satisfaction reached
on the market, as it can be conceived by an agent, according to business
cycles. This reality being the result of the complementary systems that govern
human mind as structured by rational psychologists.
",quantitative-finance
"  The FEAST eigenvalue algorithm is a subspace iteration algorithm that uses
contour integration in the complex plane to obtain the eigenvectors of a matrix
for the eigenvalues that are located in any user-defined search interval. By
computing small numbers of eigenvalues in specific regions of the complex
plane, FEAST is able to naturally parallelize the solution of eigenvalue
problems by solving for multiple eigenpairs simultaneously. The traditional
FEAST algorithm is implemented by directly solving collections of shifted
linear systems of equations; in this paper, we describe a variation of the
FEAST algorithm that uses iterative Krylov subspace algorithms for solving the
shifted linear systems inexactly. We show that this iterative FEAST algorithm
(which we call IFEAST) is mathematically equivalent to a block Krylov subspace
method for solving eigenvalue problems. By using Krylov subspaces indirectly
through solving shifted linear systems, rather than directly for projecting the
eigenvalue problem, IFEAST is able to solve eigenvalue problems using very
large dimension Krylov subspaces, without ever having to store a basis for
those subspaces. IFEAST thus combines the flexibility and power of Krylov
methods, requiring only matrix-vector multiplication for solving eigenvalue
problems, with the natural parallelism of the traditional FEAST algorithm. We
discuss the relationship between IFEAST and more traditional Krylov methods,
and provide numerical examples illustrating its behavior.
",computer-science
"  We describe general multilevel Monte Carlo methods that estimate the price of
an Asian option monitored at $m$ fixed dates. Our approach yields unbiased
estimators with standard deviation $O(\epsilon)$ in $O(m + (1/\epsilon)^{2})$
expected time for a variety of processes including the Black-Scholes model,
Merton's jump-diffusion model, the Square-Root diffusion model, Kou's double
exponential jump-diffusion model, the variance gamma and NIG exponential Levy
processes and, via the Milstein scheme, processes driven by scalar stochastic
differential equations. Using the Euler scheme, our approach estimates the
Asian option price with root mean square error $O(\epsilon)$ in
$O(m+(\ln(\epsilon)/\epsilon)^{2})$ expected time for processes driven by
multidimensional stochastic differential equations. Numerical experiments
confirm that our approach outperforms the conventional Monte Carlo method by a
factor of order $m$.
",quantitative-finance
"  As the first step to model emotional state of a person, we build sentiment
analysis models with existing deep neural network algorithms and compare the
models with psychological measurements to enlighten the relationship. In the
experiments, we first examined psychological state of 64 participants and asked
them to summarize the story of a book, Chronicle of a Death Foretold (Marquez,
1981). Secondly, we trained models using crawled 365,802 movie review data;
then we evaluated participants' summaries using the pretrained model as a
concept of transfer learning. With the background that emotion affects on
memories, we investigated the relationship between the evaluation score of the
summaries from computational models and the examined psychological
measurements. The result shows that although CNN performed the best among other
deep neural network algorithms (LSTM, GRU), its results are not related to the
psychological state. Rather, GRU shows more explainable results depending on
the psychological state. The contribution of this paper can be summarized as
follows: (1) we enlighten the relationship between computational models and
psychological measurements. (2) we suggest this framework as objective methods
to evaluate the emotion; the real sentiment analysis of a person.
",computer-science
"  The stochastic Landau--Lifshitz--Gilbert (LLG) equation coupled with the
Maxwell equations (the so called stochastic MLLG system) describes the creation
of domain walls and vortices (fundamental objects for the novel nanostructured
magnetic memories). We first reformulate the stochastic LLG equation into an
equation with time-differentiable solutions. We then propose a convergent
$\theta$-linear scheme to approximate the solutions of the reformulated system.
As a consequence, we prove convergence of the approximate solutions, with no or
minor conditions on time and space steps (depending on the value of $\theta$).
Hence, we prove the existence of weak martingale solutions of the stochastic
MLLG system. Numerical results are presented to show applicability of the
method.
",mathematics
"  Single magnetic skyrmions are localized whirls in the magnetization with an
integer winding number. They have been observed on nano-meter scales up to room
temperature in multilayer structures. Due to their small size, topological
winding number, and their ability to be manipulated by extremely tiny forces,
they are often called interesting candidates for future memory devices. The
two-lane racetrack has to exhibit two lanes that are separated by an energy
barrier. The information is then encoded in the position of a skyrmion which is
located in one of these close-by lanes. The artificial barrier between the
lanes can be created by an additional nanostrip on top of the track. Here we
study the dependence of the potential barrier on the shape of the additional
nanostrip, calculating the potentials for a rectangular, triangular, and
parabolic cross section, as well as interpolations between the first two. We
find that a narrow barrier is always repulsive and that the height of the
potential strongly depends on the shape of the nanostrip, whereas the shape of
the potential is more universal. We finally show that the shape-dependence is
redundant for possible applications.
",physics
"  Given a polynomial system f associated with a simple multiple zero x of
multiplicity {\mu}, we give a computable lower bound on the minimal distance
between the simple multiple zero x and other zeros of f. If x is only given
with limited accuracy, we propose a numerical criterion that f is certified to
have {\mu} zeros (counting multiplicities) in a small ball around x.
Furthermore, for simple double zeros and simple triple zeros whose Jacobian is
of normalized form, we define modified Newton iterations and prove the
quantified quadratic convergence when the starting point is close to the exact
simple multiple zero. For simple multiple zeros of arbitrary multiplicity whose
Jacobian matrix may not have a normalized form, we perform unitary
transformations and modified Newton iterations, and prove its non-quantified
quadratic convergence and its quantified convergence for simple triple zeros.
",mathematics
"  Using holography, we model experiments in which a 2+1D strange metal is
pumped by a laser pulse into a highly excited state, after which the time
evolution of the optical conductivity is probed. We consider a finite-density
state with mildly broken translation invariance and excite it by oscillating
electric field pulses. At zero density, the optical conductivity would assume
its thermalized value immediately after the pumping has ended. At finite
density, pulses with significant DC components give rise to slow exponential
relaxation, governed by a vector quasinormal mode. In contrast, for
high-frequency pulses the amplitude of the quasinormal mode is strongly
suppressed, so that the optical conductivity assumes its thermalized value
effectively instantaneously. This surprising prediction may provide a stimulus
for taking up the challenge to realize these experiments in the laboratory.
Such experiments would test a crucial open question faced by applied
holography: Are its predictions artefacts of the large $N$ limit or do they
enjoy sufficient UV independence to hold at least qualitatively in real-world
systems?
",physics
"  Graphitic carbon nitride nanosheets are among 2D attractive materials due to
presenting unusual physicochemical properties.Nevertheless, no adequate
information exists about their mechanical and thermal properties. Therefore, we
used classical molecular dynamics simulations to explore the thermal
conductivity and mechanical response of two main structures of single-layer
triazine-basedg-C3N4 films.By performing uniaxial tensile modeling, we found
remarkable elastic modulus of 320 and 210 GPa, and tensile strength of 47 GPa
and 30 GPa for two different structures of g-C3N4sheets. Using equilibrium
molecular dynamics simulations, the thermal conductivity of free-standing
g-C3N4 structures were also predicted to be around 7.6 W/mK and 3.5 W/mK. Our
study suggests the g-C3N4films as exciting candidate for reinforcement of
polymeric materials mechanical properties.
",physics
"  We introduce a semi-supervised discrete choice model to calibrate discrete
choice models when relatively few requests have both choice sets and stated
preferences but the majority only have the choice sets. Two classic
semi-supervised learning algorithms, the expectation maximization algorithm and
the cluster-and-label algorithm, have been adapted to our choice modeling
problem setting. We also develop two new algorithms based on the
cluster-and-label algorithm. The new algorithms use the Bayesian Information
Criterion to evaluate a clustering setting to automatically adjust the number
of clusters. Two computational studies including a hotel booking case and a
large-scale airline itinerary shopping case are presented to evaluate the
prediction accuracy and computational effort of the proposed algorithms.
Algorithmic recommendations are rendered under various scenarios.
",statistics
"  Topological data analysis is an emerging area in exploratory data analysis
and data mining. Its main tool, persistent homology, has become a popular
technique to study the structure of complex, high-dimensional data. In this
paper, we propose a novel method using persistent homology to quantify
structural changes in time-varying graphs. Specifically, we transform each
instance of the time-varying graph into metric spaces, extract topological
features using persistent homology, and compare those features over time. We
provide a visualization that assists in time-varying graph exploration and
helps to identify patterns of behavior within the data. To validate our
approach, we conduct several case studies on real world data sets and show how
our method can find cyclic patterns, deviations from those patterns, and
one-time events in time-varying graphs. We also examine whether
persistence-based similarity measure as a graph metric satisfies a set of
well-established, desirable properties for graph metrics.
",computer-science
"  There are so many vehicles in the world and the number of vehicles is
increasing rapidly. To alleviate the parking problems caused by that, the smart
parking system has been developed. The parking planning is one of the most
important parts of it. An effective parking planning strategy makes the better
use of parking resources possible. In this paper, we present a feasible method
to do parking planning. We transform the parking planning problem into a kind
of linear assignment problem. We take vehicles as jobs and parking spaces as
agents. We take distances between vehicles and parking spaces as costs for
agents doing jobs. Then we design an algorithm for this particular assignment
problem and solve the parking planning problem. The method proposed can give
timely and efficient guide information to vehicles for a real time smart
parking system. Finally, we show the effectiveness of the method with
experiments over some data, which can simulate the situation of doing parking
planning in the real world.
",computer-science
"  We introduce the notion of $K$-ideals associated with Kuratowski partitions
and we prove that each $\kappa$-complete ideal on a measurable cardinal
$\kappa$ can be represented as a $K$-ideal. Moreover, we show some results
concerning precipitous and Fréchet ideals.
",mathematics
"  Since multimedia streaming has become very popular research topic in the
recent years, this paper surveys the state of art techniques introduced for
multimedia multicasting over mobile networks. In this paper, we give an
overview of multimedia multicasting mechanisms in respect to cloud mobile
communications, and we present some proposed solutions in perspective. We focus
on the algorithms designed specifically for the video-on-demand applications.
Our study on video-on-demand applications will eventually cover a wide range of
applications such as cloud gaming without violating the limited scope of this
survey.
",computer-science
"  The test of gravitational force on antimatter in the field of the matter
gravitational field, produced by earth, can be done by a free fall experiment
which involves only General Relativity, and with a Mach-Zehnder interferometer
which involves Quantum Mechanics. This article presents a new method to produce
a tunable low energy (Ps ) beam suitable for trapping the (Hbar + ) ion in a
free fall experiment, and suitable for a gravity Mach-Zehnder interferometer
with (Ps). The low energy (Ps) beam is tunable in the [10 eV, 100 eV] range.
",physics
"  Most state-of-the-art information extraction approaches rely on token-level
labels to find the areas of interest in text. Unfortunately, these labels are
time-consuming and costly to create, and consequently, not available for many
real-life IE tasks. To make matters worse, token-level labels are usually not
the desired output, but just an intermediary step. End-to-end (E2E) models,
which take raw text as input and produce the desired output directly, need not
depend on token-level labels. We propose an E2E model based on pointer
networks, which can be trained directly on pairs of raw input and output text.
We evaluate our model on the ATIS data set, MIT restaurant corpus and the MIT
movie corpus and compare to neural baselines that do use token-level labels. We
achieve competitive results, within a few percentage points of the baselines,
showing the feasibility of E2E information extraction without the need for
token-level labels. This opens up new possibilities, as for many tasks
currently addressed by human extractors, raw input and output data are
available, but not token-level labels.
",computer-science
"  Efficient methods are proposed, for computing integrals appeaing in
electronic structure calculations. The methods consist of two parts: the first
part is to represent the integrals as contour integrals and the second one is
to evaluate the contour integrals by the Clenshaw-Curtis quadrature. The
efficiency of the proposed methods is demonstrated through numerical
experiments.
",physics
"  Consider the Tate twist $\tau \in H^{0,1}(S^{0,0})$ in the mod 2 cohomology
of the motivic sphere. After 2-completion, the motivic Adams spectral sequence
realizes this element as a map $\tau \colon S^{0,-1} \to S^{0,0}$, with cofiber
$C\tau$. We show that this motivic 2-cell complex can be endowed with a unique
$E_{\infty}$ ring structure. Moreover, this promotes the known isomorphism
$\pi_{\ast,\ast} C\tau \cong
\mathrm{Ext}^{\ast,\ast}_{BP_{\ast}BP}(BP_{\ast},BP_{\ast})$ to an isomorphism
of rings which also preserves higher products.
We then consider the closed symmetric monoidal category $({
}_{C\tau}\textbf{Mod}, - \wedge_{C\tau} -)$ which lives in the kernel of Betti
realization. Given a motivic spectrum $X$, the $C\tau$-induced spectrum $X
\wedge C\tau$ is usually better behaved and easier to understand than $X$
itself. We specifically illustrate this concept in the examples of the mod 2
Eilenberg-Maclane spectrum $H\mathbb{F}_2$, the mod 2 Moore spectrum
$S^{0,0}/2$ and the connective hermitian $K$-theory spectrum $kq$.
",mathematics
"  An important problem in many domains is to predict how a system will respond
to interventions. This task is inherently linked to estimating the system's
underlying causal structure. To this end, Invariant Causal Prediction (ICP)
(Peters et al., 2016) has been proposed which learns a causal model exploiting
the invariance of causal relations using data from different environments. When
considering linear models, the implementation of ICP is relatively
straightforward. However, the nonlinear case is more challenging due to the
difficulty of performing nonparametric tests for conditional independence. In
this work, we present and evaluate an array of methods for nonlinear and
nonparametric versions of ICP for learning the causal parents of given target
variables. We find that an approach which first fits a nonlinear model with
data pooled over all environments and then tests for differences between the
residual distributions across environments is quite robust across a large
variety of simulation settings. We call this procedure ""invariant residual
distribution test"". In general, we observe that the performance of all
approaches is critically dependent on the true (unknown) causal structure and
it becomes challenging to achieve high power if the parental set includes more
than two variables. As a real-world example, we consider fertility rate
modelling which is central to world population projections. We explore
predicting the effect of hypothetical interventions using the accepted models
from nonlinear ICP. The results reaffirm the previously observed central causal
role of child mortality rates.
",statistics
"  In this paper we present a strategy for the the synthesis of acoustic sources
with controllable near fields in free space and finite depth homogeneous ocean
environments. We first present the theoretical results at the basis of our
discussion and then, to illustrate our findings we focus on the following three
particular examples:
1. acoustic source approximating a prescribed field pattern in a given
bounded sub- region of its near field. 2. acoustic source approximating
different prescribed field patterns in given disjoint bounded near field
sub-regions. 3. acoustic source approximating a prescribed back-propagating
field in a given bounded near field sub-region while maintaining a very low far
field signature.
For each of these three examples, we discuss the optimization scheme used to
approx- imate their solutions and support our claims through relevant numerical
simulations.
",mathematics
"  The irreducible representations of full support in the rational Cherednik
category $\mathcal{O}_c(W)$ attached to a Coxeter group $W$ are in bijection
with the irreducible representations of an associated Iwahori-Hecke algebra.
Recent work has shown that the irreducible representations in
$\mathcal{O}_c(W)$ of arbitrary given support are similarly governed by certain
generalized Hecke algebras. In this paper we compute the parameters for these
generalized Hecke algebras in the remaining previously unknown cases,
corresponding to the parabolic subgroup $B_n \times S_k$ in $B_{n+k}$ for $k
\geq 2$ and $n \geq 0$.
",mathematics
"  In an earlier work, we constructed the almost strict Morse $n$-category
$\mathcal X$ which extends Cohen $\&$ Jones $\&$ Segal's flow category. In this
article, we define two other almost strict $n$-categories $\mathcal V$ and
$\mathcal W$ where $\mathcal V$ is based on homomorphisms between real vector
spaces and $\mathcal W$ consists of tuples of positive integers. The Morse
index and the dimension of the Morse moduli spaces give rise to almost strict
$n$-category functors $\mathcal F : \mathcal X \to \mathcal V$ and $\mathcal G
: \mathcal X \to \mathcal W$.
",mathematics
"  In the use of deep neural networks, it is crucial to provide appropriate
input representations for the network to learn from. In this paper, we propose
an approach to learn a representation that focus on rhythmic representation
which is named as DLR (Deep Learning Rhythmic representation). The proposed
approach aims to learn DLR from the raw audio signal and use it for other music
informatics tasks. A 1-dimensional convolutional network is utilised in the
learning of DLR. In the experiment, we present the results from the source task
and the target task as well as visualisations of DLRs. The results reveals that
DLR provides compact rhythmic information which can be used on multi-tagging
task.
",computer-science
"  This paper presents a fast and effective computer algebraic method for
analyzing and verifying non-linear integer arithmetic circuits using a novel
algebraic spectral model. It introduces a concept of algebraic spectrum, a
numerical form of polynomial expression; it uses the distribution of
coefficients of the monomials to determine the type of arithmetic function
under verification. In contrast to previous works, the proof of functional
correctness is achieved by computing an algebraic spectrum combined with a
local rewriting of word-level polynomials. The speedup is achieved by
propagating coefficients through the circuit using And-Inverter Graph (AIG)
datastructure. The effectiveness of the method is demonstrated with experiments
including standard and Booth multipliers, and other synthesized non-linear
arithmetic circuits up to 1024 bits containing over 12 million gates.
",computer-science
"  It has recently been shown that the problem of testing global convexity of
polynomials of degree four is {strongly} NP-hard, answering an open question of
N.Z. Shor. This result is minimal in the degree of the polynomial when global
convexity is of concern. In a number of applications however, one is interested
in testing convexity only over a compact region, most commonly a box (i.e.,
hyper-rectangle). In this paper, we show that this problem is also strongly
NP-hard, in fact for polynomials of degree as low as three. This result is
minimal in the degree of the polynomial and in some sense justifies why
convexity detection in nonlinear optimization solvers is limited to quadratic
functions or functions with special structure. As a byproduct, our proof shows
that the problem of testing whether all matrices in an interval family are
positive semidefinite is strongly NP-hard. This problem, which was previously
shown to be (weakly) NP-hard by Nemirovski, is of independent interest in the
theory of robust control.
",statistics
"  This note is devoted to the study of the homology class of a compact Poisson
transversal in a Poisson manifold. For specific classes of Poisson structures,
such as unimodular Poisson structures and Poisson manifolds with closed leaves,
we prove that all their compact Poisson transversals represent non-trivial
homology classes, generalizing the symplectic case. We discuss several examples
in which this property does not hold, as well as a weaker version of this
property, which holds for log-symplectic structures. Finally, we extend our
results to Dirac geometry.
",mathematics
"  We discover a population of short-period, Neptune-size planets sharing key
similarities with hot Jupiters: both populations are preferentially hosted by
metal-rich stars, and both are preferentially found in Kepler systems with
single transiting planets. We use accurate LAMOST DR4 stellar parameters for
main-sequence stars to study the distributions of short-period 1d < P < 10d
Kepler planets as a function of host star metallicity. The radius distribution
of planets around metal-rich stars is more ""puffed up"" as compared to that
around metal-poor hosts. In two period-radius regimes, planets preferentially
reside around metal-rich stars, while there are hardly any planets around
metal-poor stars. One is the well-known hot Jupiters, and the other is a
population of Neptune-size planets (2 R_Earth <~ R_p <~ 6 R_Earth), dubbed as
""Hoptunes"". Also like hot Jupiters, Hoptunes occur more frequently in systems
with single transiting planets though the fraction of Hoptunes occurring in
multiples is larger than that of hot Jupiters. About 1% of solar-type stars
host ""Hoptunes"", and the frequencies of Hoptunes and hot Jupiters increase with
consistent trends as a function of [Fe/H]. In the planet radius distribution,
hot Jupiters and Hoptunes are separated by a ""valley"" at approximately Saturn
size (in the range of 6 R_Earth <~ R_p <~ 10 R_Earth), and this ""hot-Saturn
valley"" represents approximately an order-of-magnitude decrease in planet
frequency compared to hot Jupiters and Hoptunes. The empirical ""kinship""
between Hoptunes and hot Jupiters suggests likely common processes (migration
and/or formation) responsible for their existence.
",physics
"  We propose an Encoder-Classifier framework to model the Mandarin tones using
recurrent neural networks (RNN). In this framework, extracted frames of
features for tone classification are fed in to the RNN and casted into a fixed
dimensional vector (tone embedding) and then classified into tone types using a
softmax layer along with other auxiliary inputs. We investigate various
configurations that help to improve the model, including pooling, feature
splicing and utilization of syllable-level tone embeddings. Besides, tone
embeddings and durations of the contextual syllables are exploited to
facilitate tone classification. Experimental results on Mandarin tone
classification show the proposed network setups improve tone classification
accuracy. The results indicate that the RNN encoder-classifier based tone model
flexibly accommodates heterogeneous inputs (sequential and segmental) and hence
has the advantages from both the sequential classification tone models and
segmental classification tone models.
",computer-science
"  Platinum diselenide (PtSe2) is an exciting new member of the two-dimensional
(2D) transition metal dichalcogenide (TMD) family. it has a semimetal to
semiconductor transition when approaching monolayer thickness and has already
shown significant potential for use in device applications. Notably, PtSe2 can
be grown at low temperature making it potentially suitable for industrial
usage. Here, we address thickness dependent transport properties and
investigate electrical contacts to PtSe2, a crucial and universal element of
TMD-based electronic devices. PtSe2 films have been synthesized at various
thicknesses and structured to allow contact engineering and the accurate
extraction of electrical properties. Contact resistivity and sheet resistance
extracted from transmission line method (TLM) measurements are compared for
different contact metals and different PtSe2 film thicknesses. Furthermore, the
transition from semimetal to semiconductor in PtSe2 has been indirectly
verified by electrical characterization of field-effect devices. Finally, the
influence of edge contacts at the metal - PtSe2 interface has been studied by
nanostructuring the contact area using electron beam lithography. By increasing
the edge contact length, the contact resistivity was improved by up to 70%
compared to devices with conventional top contacts. The results presented here
represent crucial steps towards realizing high-performance nanoelectronic
devices based on group-10 TMDs.
",physics
"  Structured Peer Learning (SPL) is a form of peer-based supplemental
instruction that focuses on mentoring, guidance, and development of technical,
communication, and social skills in both the students receiving assistance and
the students in teaching roles. This paper explores the methodology, efficacy,
and reasoning behind the practical realization of a SPL program designed to
increase student knowledge and success in undergraduate Computer Science
courses. Students expressed an increased level of comfort when asking for help
from student teachers versus traditional educational resources, historically
showed an increased average grade in lower-level courses, and felt that the
program positively impacted their desire to continue in or switch to a Computer
major. Additionally, results indicated that advances in programming, analytical
thinking, and abstract analysis skills were evident in not only the students
but also the student teachers, suggesting a strong bidirectional flow of
knowledge.
",computer-science
"  Despite decades of inquiry, the origin of giant planets residing within a few
tenths of an astronomical unit from their host stars remains unclear.
Traditionally, these objects are thought to have formed further out before
subsequently migrating inwards. However, the necessity of migration has been
recently called into question with the emergence of in-situ formation models of
close-in giant planets. Observational characterization of the transiting
sub-sample of close-in giants has revealed that ""warm"" Jupiters, possessing
orbital periods longer than roughly 10 days more often possess close-in,
co-transiting planetary companions than shorter period ""hot"" Jupiters, that are
usually lonely. This finding has previously been interpreted as evidence that
smooth, early migration or in situ formation gave rise to warm Jupiter-hosting
systems, whereas more violent, post-disk migration pathways sculpted hot
Jupiter-hosting systems. In this work, we demonstrate that both classes of
planet may arise via early migration or in-situ conglomeration, but that the
enhanced loneliness of hot Jupiters arises due to a secular resonant
interaction with the stellar quadrupole moment. Such an interaction tilts the
orbits of exterior, lower mass planets, removing them from transit surveys
where the hot Jupiter is detected. Warm Jupiter-hosting systems, in contrast,
retain their coplanarity due to the weaker influence of the host star's
quadrupolar potential relative to planet-disk interactions. In this way, hot
Jupiters and warm Jupiters are placed within a unified theoretical framework
that may be readily validated or falsified using data from upcoming missions
such as TESS.
",physics
"  This paper analyzes the use of 3D Convolutional Neural Networks for brain
tumor segmentation in MR images. We address the problem using three different
architectures that combine fine and coarse features to obtain the final
segmentation. We compare three different networks that use multi-resolution
features in terms of both design and performance and we show that they improve
their single-resolution counterparts.
",statistics
"  We study connected, locally compact metric spaces with transitive isometry
groups. For all $\epsilon \in \mathbb{R}^+$, each such space is
$(1,\epsilon)$-quasi-isometric to a Lie group equipped with a left-invariant
metric. Further, every metric Lie group is $(1, C)$-quasi-isometric to a
solvable Lie group, and every simply connected metric Lie group is $(1,
C)$-quasi-isometrically homeomorphic to a solvable-by-compact metric Lie group.
While any contractible Lie group may be made isometric to a solvable group,
only those that are solvable and of type (R) may be made isometric to a
nilpotent Lie group, in which case the nilpotent group is the nilshadow of the
group. Finally, we give a complete metric characterisation of metric Lie groups
for which there exists an automorphic dilation. These coincide with the metric
spaces that are locally compact, connected, homogeneous, and admit a metric
dilation.
",mathematics
"  In a previous paper, we assembled a collection of medium-resolution spectra
of 35 carbon stars, covering optical and near-infrared wavelengths from 400 to
2400 nm. The sample includes stars from the Milky Way and the Magellanic
Clouds, with a variety of $(J-K_s)$ colors and pulsation properties. In the
present paper, we compare these observations to a new set of high-resolution
synthetic spectra, based on hydrostatic model atmospheres. We find that the
broad-band colors and the molecular-band strengths measured by
spectrophotometric indices match those of the models when $(J-K_s)$ is bluer
than about 1.6, while the redder stars require either additional reddening or
dust emission or both. Using a grid of models to fit the full observed spectra,
we estimate the most likely atmospheric parameters $T_\mathrm{eff}$, $\log(g)$,
$[\mathrm{Fe/H}]$ and C/O. These parameters derived independently in the
optical and near-infrared are generally consistent when $(J-K_s)<1.6$. The
temperatures found based on either wavelength range are typically within
$\pm$100K of each other, and $\log(g)$ and $[\mathrm{Fe/H}]$ are consistent
with the values expected for this sample. The reddest stars ($(J-K_s)$ $>$ 1.6)
are divided into two families, characterized by the presence or absence of an
absorption feature at 1.53\,$\mu$m, generally associated with HCN and
C$_2$H$_2$. Stars from the first family begin to be more affected by
circumstellar extinction. The parameters found using optical or near-infrared
wavelengths are still compatible with each other, but the error bars become
larger. In stars showing the 1.53\,$\mu$m feature, which are all
large-amplitude variables, the effects of pulsation are strong and the spectra
are poorly matched with hydrostatic models. For these, atmospheric parameters
could not be derived reliably, and dynamical models are needed for proper
interpretation.
",physics
"  State-of-the-art knowledge compilers generate deterministic subsets of DNNF,
which have been recently shown to be exponentially less succinct than DNNF. In
this paper, we propose a new method to compile DNNFs without enforcing
determinism necessarily. Our approach is based on compiling deterministic DNNFs
with the addition of auxiliary variables to the input formula. These variables
are then existentially quantified from the deterministic structure in linear
time, which would lead to a DNNF that is equivalent to the input formula and
not necessarily deterministic. On the theoretical side, we show that the new
method could generate exponentially smaller DNNFs than deterministic ones, even
by adding a single auxiliary variable. Further, we show that various existing
techniques that introduce auxiliary variables to the input formulas can be
employed in our framework. On the practical side, we empirically demonstrate
that our new method can significantly advance DNNF compilation on certain
benchmarks.
",computer-science
"  We summarize our recent findings, where we proposed a framework for learning
a Kolmogorov model, for a collection of binary random variables. More
specifically, we derive conditions that link outcomes of specific random
variables, and extract valuable relations from the data. We also propose an
algorithm for computing the model and show its first-order optimality, despite
the combinatorial nature of the learning problem. We apply the proposed
algorithm to recommendation systems, although it is applicable to other
scenarios. We believe that the work is a significant step toward interpretable
machine learning.
",statistics
"  Pose Graph Optimization involves the estimation of a set of poses from
pairwise measurements and provides a formalization for many problems arising in
mobile robotics and geometric computer vision. In this paper, we consider the
case in which a subset of the measurements fed to pose graph optimization is
spurious. Our first contribution is to develop robust estimators that can cope
with heavy-tailed measurement noise, hence increasing robustness to the
presence of outliers. Since the resulting estimators require solving nonconvex
optimization problems, we further develop convex relaxations that approximately
solve those problems via semidefinite programming. We then provide conditions
under which the proposed relaxations are exact. Contrarily to existing
approaches, our convex relaxations do not rely on the availability of an
initial guess for the unknown poses, hence they are more suitable for setups in
which such guess is not available (e.g., multi-robot localization, recovery
after localization failure). We tested the proposed techniques in extensive
simulations, and we show that some of the proposed relaxations are indeed tight
(i.e., they solve the original nonconvex problem 10 exactly) and ensure
accurate estimation in the face of a large number of outliers.
",computer-science
"  Optimal scheduling of hydrogen production in dynamic pricing power market can
maximize the profit of hydrogen producer; however, it highly depends on the
accurate forecast of hydrogen consumption. In this paper, we propose a deep
leaning based forecasting approach for predicting hydrogen consumption of fuel
cell vehicles in future taxi industry. The cost of hydrogen production is
minimized by utilizing the proposed forecasting tool to reduce the hydrogen
produced during high cost on-peak hours and guide hydrogen producer to store
sufficient hydrogen during low cost off-peak hours.
",statistics
"  Invoking Maxwell's classical equations in conjunction with expressions for
the electromagnetic (EM) energy, momentum, force, and torque, we use a few
simple examples to demonstrate the nature of the EM angular momentum. The
energy and the angular momentum of an EM field will be shown to have an
intimate relationship; a source radiating EM angular momentum will, of
necessity, pick up an equal but opposite amount of mechanical angular momentum;
and the spin and orbital angular momenta of the EM field, when absorbed by a
small particle, will be seen to elicit different responses from the particle.
",physics
"  The flexibility of short DNA chains is investigated via computation of the
average correlation function between dimers which defines the persistence
length. Path integration techniques have been applied to confine the phase
space available to base pair fluctuations and derive the partition function.
The apparent persistence lengths of a set of short chains have been computed as
a function of the twist conformation both in the over-twisted and the untwisted
regimes, whereby the equilibrium twist is selected by free energy minimization.
The obtained values are significantly lower than those generally attributed to
kilo-base long DNA. This points to an intrinsic helix flexibility at short
length scales, arising from large fluctuational effects and local bending, in
line with recent experimental indications. The interplay between helical
untwisting and persistence length has been discussed for a heterogeneous
fragment by weighing the effects of the sequence specificities through the
non-linear stacking potential.
",quantitative-biology
"  Nowadays, the construction of a complex robotic system requires a high level
of specialization in a large number of diverse scientific areas. It is
reasonable that a single researcher cannot create from scratch the entirety of
this system, as it is impossible for him to have the necessary skills in the
necessary fields. This obstacle is being surpassed with the existent robotic
frameworks. This paper tries to give an extensive review of the most famous
robotic frameworks and middleware, as well as to provide the means to
effortlessly compare them. Additionally, we try to investigate the differences
between the definitions of a robotic framework, a robotic middleware and a
robotic architecture.
",computer-science
"  The beams at the ILC produce electron positron pairs due to beam-beam
interactions. This note presents for the first time a study of these processes
in a detailed simulation, which shows that these pair background particles
appear at angles that extend to the inner layers of the detector. The full data
set of pairs produced in one bunch crossing was used to calculate the helix
tracks, which the particles form in the solenoid field of the SiD detector. The
results suggest to further study the reduction of the beam pipe radius and
therefore to either add another SiD vertex detector layer, or reduce the radius
of the existing vertex detector layers, without increasing the detector
occupancy significantly. This has to go along with additional studies whether
the improvement in physics reconstruction methods, like c-tagging, is worth the
increased background level at smaller radii.
",physics
"  A fundamental component of the game theoretic approach to distributed control
is the design of local utility functions. In Part I of this work we showed how
to systematically design local utilities so as to maximize the induced worst
case performance. The purpose of the present manuscript is to specialize the
general results obtained in Part I to a class of monotone submodular,
supermodular and set covering problems. In the case of set covering problems,
we show how any distributed algorithm capable of computing a Nash equilibrium
inherits a performance certificate matching the well known 1-1/e approximation
of Nemhauser. Relative to the class of submodular maximization problems
considered here, we show how the performance offered by the game theoretic
approach improves on existing approximation algorithms. We briefly discuss the
algorithmic complexity of computing (pure) Nash equilibria and show how our
approach generalizes and subsumes previously fragmented results in the area of
optimal utility design. Two applications and corresponding numerics are
presented: the vehicle target assignment problem and a coverage problem arising
in distributed caching for wireless networks.
",computer-science
"  This paper considers the problem of designing maximum distance separable
(MDS) codes over small fields with constraints on the support of their
generator matrices. For any given $m\times n$ binary matrix $M$, the GM-MDS
conjecture, due to Dau et al., states that if $M$ satisfies the so-called MDS
condition, then for any field $\mathbb{F}$ of size $q\geq n+m-1$, there exists
an $[n,m]_q$ MDS code whose generator matrix $G$, with entries in $\mathbb{F}$,
fits $M$ (i.e., $M$ is the support matrix of $G$). Despite all the attempts by
the coding theory community, this conjecture remains still open in general. It
was shown, independently by Yan et al. and Dau et al., that the GM-MDS
conjecture holds if the following conjecture, referred to as the TM-MDS
conjecture, holds: if $M$ satisfies the MDS condition, then the determinant of
a transformation matrix $T$, such that $TV$ fits $M$, is not identically zero,
where $V$ is a Vandermonde matrix with distinct parameters. In this work, we
generalize the TM-MDS conjecture, and present an algebraic-combinatorial
approach based on polynomial-degree reduction for proving this conjecture. Our
proof technique's strength is based primarily on reducing inherent
combinatorics in the proof. We demonstrate the strength of our technique by
proving the TM-MDS conjecture for the cases where the number of rows ($m$) of
$M$ is upper bounded by $5$. For this class of special cases of $M$ where the
only additional constraint is on $m$, only cases with $m\leq 4$ were previously
proven theoretically, and the previously used proof techniques are not
applicable to cases with $m > 4$.
",computer-science
"  We propose a Monte Carlo algorithm to sample from high-dimensional
probability distributions that combines Markov chain Monte Carlo (MCMC) and
importance sampling. We provide a careful theoretical analysis, including
guarantees on robustness to high-dimensionality, explicit comparison with
standard MCMC and illustrations of the potential improvements in efficiency.
Simple and concrete intuition is provided for when the novel scheme is expected
to outperform standard schemes. When applied to Bayesian Variable Selection
problems, the novel algorithm is orders of magnitude more efficient than
available alternative sampling schemes and allows to perform fast and reliable
fully Bayesian inferences with tens of thousands regressors.
",statistics
"  Chiral magnets with topologically nontrivial spin order such as Skyrmions
have generated enormous interest in both fundamental and applied sciences. We
report broadband microwave spectroscopy performed on the insulating chiral
ferrimagnet Cu$_{2}$OSeO$_{3}$. For the damping of magnetization dynamics we
find a remarkably small Gilbert damping parameter of about $1\times10^{-4}$ at
5 K. This value is only a factor of 4 larger than the one reported for the best
insulating ferrimagnet yttrium iron garnet. We detect a series of sharp
resonances and attribute them to confined spin waves in the mm-sized samples.
Considering the small damping, insulating chiral magnets turn out to be
promising candidates when exploring non-collinear spin structures for high
frequency applications.
",physics
"  Let $p(z)=a_0+a_1z+a_2z^2+a_3z^3+\cdots+a_nz^n$ be a polynomial of degree
$n$. Rivlin \cite{Rivlin} proved that if $p(z)\neq 0$ in the unit disk, then
for $0<r\leq 1$, $\displaystyle{\max_{|z| = r}|p(z)|} \geq
\Big(\dfrac{r+1}{2}\Big)^n \displaystyle{\max_{|z|=1} |p(z)|}.$ ~In this paper,
we prove a sharpening and generalization of this result, and show by means of
examples that for some polynomials our result can significantly improve the
bound obtained by the Rivlin's Theorem.
",mathematics
"  We propose a novel deep learning architecture for regressing disparity from a
rectified pair of stereo images. We leverage knowledge of the problem's
geometry to form a cost volume using deep feature representations. We learn to
incorporate contextual information using 3-D convolutions over this volume.
Disparity values are regressed from the cost volume using a proposed
differentiable soft argmin operation, which allows us to train our method
end-to-end to sub-pixel accuracy without any additional post-processing or
regularization. We evaluate our method on the Scene Flow and KITTI datasets and
on KITTI we set a new state-of-the-art benchmark, while being significantly
faster than competing approaches.
",computer-science
"  Nodal-line semimetals, one of the topological semimetals, have degeneracy
along nodal lines where the band gap is closed. In many cases, the nodal lines
appear accidentally, and in such cases it is impossible to determine whether
the nodal lines appear or not, only from the crystal symmetry and the electron
filling. In this paper, for spinless systems, we show that in specific space
groups at $4N+2$ fillings ($8N+4$ fillings including the spin degree of
freedom), presence of the nodal lines is required regardless of the details of
the systems. Here, the spinless systems refer to crystals where the spin-orbit
coupling is negligible and the spin degree of freedom can be omitted because of
the SU(2) spin degeneracy. In this case the shape of the band structure around
these nodal lines is like an hourglass, and we call this a spinless hourglass
nodal-line semimetal. We construct a model Hamiltonian as an example and we
show that it is always in the spinless hourglass nodal-line semimetal phase
even when the model parameters are changed without changing the symmetries of
the system. We also establish a list of all the centrosymmetric space groups,
under which spinless systems always have hourglass nodal lines, and illustrate
where the nodal lines are located. We propose that Al$_3$FeSi$_2$, whose
space-group symmetry is Pbcn (No. 60), is one of the nodal-line semimetals
arising from this mechanism.
",physics
"  This paper presents a novel technique that allows for both computationally
fast and sufficiently plausible simulation of vehicles with non-deformable
tracks. The method is based on an effect we have called Contact Surface Motion.
A comparison with several other methods for simulation of tracked vehicle
dynamics is presented with the aim to evaluate methods that are available
off-the-shelf or with minimum effort in general-purpose robotics simulators.
The proposed method is implemented as a plugin for the open-source
physics-based simulator Gazebo using the Open Dynamics Engine.
",computer-science
"  Thomassen conjectured that triangle-free planar graphs have an exponential
number of $3$-colorings. We show this conjecture to be equivalent to the
following statement: there exists a positive real $\alpha$ such that whenever
$G$ is a planar graph and $A$ is a subset of its edges whose deletion makes $G$
triangle-free, there exists a subset $A'$ of $A$ of size at least $\alpha|A|$
such that $G-(A\setminus A')$ is $3$-colorable. This equivalence allows us to
study restricted situations, where we can prove the statement to be true.
",mathematics
"  Stochastic Gradient Descent (SGD) is the central workhorse for training
modern CNNs. Although giving impressive empirical performance it can be slow to
converge. In this paper we explore a novel strategy for training a CNN using an
alternation strategy that offers substantial speedups during training. We make
the following contributions: (i) replace the ReLU non-linearity within a CNN
with positive hard-thresholding, (ii) reinterpret this non-linearity as a
binary state vector making the entire CNN linear if the multi-layer support is
known, and (iii) demonstrate that under certain conditions a global optima to
the CNN can be found through local descent. We then employ a novel alternation
strategy (between weights and support) for CNN training that leads to
substantially faster convergence rates, nice theoretical properties, and
achieving state of the art results across large scale datasets (e.g. ImageNet)
as well as other standard benchmarks.
",computer-science
"  The problem of analyzing the number of number field extensions $L/K$ with
bounded (relative) discriminant has been the subject of renewed interest in
recent years, with significant advances made by Schmidt, Ellenberg-Venkatesh,
Bhargava, Bhargava-Shankar-Wang, and others. In this paper, we use the geometry
of numbers and invariant theory of finite groups, in a manner similar to
Ellenberg and Venkatesh, to give an upper bound on the number of extensions
$L/K$ with fixed degree, bounded relative discriminant, and specified Galois
closure.
",mathematics
"  A minimal constructed language (conlang) is useful for experiments and
comfortable for making tools. The Toki Pona (TP) conlang is minimal both in the
vocabulary (with only 14 letters and 124 lemmas) and in the (about) 10 syntax
rules. The language is useful for being a used and somewhat established minimal
conlang with at least hundreds of fluent speakers. This article exposes current
concepts and resources for TP, and makes available Python (and Vim) scripted
routines for the analysis of the language, synthesis of texts, syntax
highlighting schemes, and the achievement of a preliminary TP Wordnet. Focus is
on the analysis of the basic vocabulary, as corpus analyses were found. The
synthesis is based on sentence templates, relates to context by keeping track
of used words, and renders larger texts by using a fixed number of phonemes
(e.g. for poems) and number of sentences, words and letters (e.g. for
paragraphs). Syntax highlighting reflects morphosyntactic classes given in the
official dictionary and different solutions are described and implemented in
the well-established Vim text editor. The tentative TP Wordnet is made
available in three patterns of relations between synsets and word lemmas. In
summary, this text holds potentially novel conceptualizations about, and tools
and results in analyzing, synthesizing and syntax highlighting the TP language.
",computer-science
"  The aim of this paper is to present a comprehensive review of method of the
wave-front expansion, also known in the literature as the Buchen-Mainardi
algorithm. In particular, many applications of this technique to the
fundamental models of both ordinary and fractional linear viscoelasticity are
thoroughly presented and discussed.
",physics
"  We theoretically investigate a spin-orbit coupled $s$-wave superfluid Fermi
gas, to examine the time evolution of the system, after an $s$-wave pairing
interaction is replaced by a $p$-wave one at $t=0$. In our recent paper, we
proposed that this manipulation may realize a $p$-wave superfluid Fermi gas,
because the $p$-wave pair amplitude that is induced in the $s$-wave superfluid
state by a parity-broken antisymmetric spin-orbit interaction gives a
non-vanishing $p$-wave superfluid order parameter, immediately after the
$p$-wave interaction is turned on. In this paper, using a time-dependent
Bogoliubov-de Gennes theory, we assess this idea under various conditions with
respect to the $s$-wave and $p$-wave interaction strengths, as well as the
spin-orbit coupling strength. From these, we clarify that the momentum
distribution of Fermi atoms in the initial $s$-wave state ($t<0$) is a key to
produce a large $p$-wave superfluid order parameter. Since the realization of a
$p$-wave superfluid state is one of the most exciting and difficult challenges
in cold Fermi gas physics, our results may provide a possible way to accomplish
this.
",physics
"  Background: Unstructured and textual data is increasing rapidly and Latent
Dirichlet Allocation (LDA) topic modeling is a popular data analysis methods
for it. Past work suggests that instability of LDA topics may lead to
systematic errors. Aim: We propose a method that relies on replicated LDA runs,
clustering, and providing a stability metric for the topics. Method: We
generate k LDA topics and replicate this process n times resulting in n*k
topics. Then we use K-medioids to cluster the n*k topics to k clusters. The k
clusters now represent the original LDA topics and we present them like normal
LDA topics showing the ten most probable words. For the clusters, we try
multiple stability metrics, out of which we recommend Rank-Biased Overlap,
showing the stability of the topics inside the clusters. Results: We provide an
initial validation where our method is used for 270,000 Mozilla Firefox commit
messages with k=20 and n=20. We show how our topic stability metrics are
related to the contents of the topics. Conclusions: Advances in text mining
enable us to analyze large masses of text in software engineering but
non-deterministic algorithms, such as LDA, may lead to unreplicable
conclusions. Our approach makes LDA stability transparent and is also
complementary rather than alternative to many prior works that focus on LDA
parameter tuning.
",computer-science
"  We present a performance analysis appropriate for comparing algorithms using
different numerical discretizations. By taking into account the total
time-to-solution, numerical accuracy with respect to an error norm, and the
computation rate, a cost-benefit analysis can be performed to determine which
algorithm and discretization are particularly suited for an application. This
work extends the performance spectrum model in Chang et. al. 2017 for
interpretation of hardware and algorithmic tradeoffs in numerical PDE
simulation. As a proof-of-concept, popular finite element software packages are
used to illustrate this analysis for Poisson's equation.
",computer-science
"  In this work, we are concerned with existence of solutions for a nonlinear
second-order distributional differential equation, which contains measure
differential equations and stochastic differential equations as special cases.
The proof is based on the Leray--Schauder nonlinear alternative and
Kurzweil--Henstock--Stieltjes integrals. Meanwhile, examples are worked out to
demonstrate that the main results are sharp.
",mathematics
"  We present the extension of the effective field theory framework to the
mildly non-linear scales. The effective field theory approach has been
successfully applied to the late time cosmic acceleration phenomenon and it has
been shown to be a powerful method to obtain predictions about cosmological
observables on linear scales. However, mildly non-linear scales need to be
consistently considered when testing gravity theories because a large part of
the data comes from those scales. Thus, non-linear corrections to predictions
on observables coming from the linear analysis can help in discriminating among
different gravity theories. We proceed firstly by identifying the necessary
operators which need to be included in the effective field theory Lagrangian in
order to go beyond the linear order in perturbations and then we construct the
corresponding non-linear action. Moreover, we present the complete recipe to
map any single field dark energy and modified gravity models into the
non-linear effective field theory framework by considering a general action in
the Arnowitt-Deser-Misner formalism. In order to illustrate this recipe we
proceed to map the beyond-Horndeski theory and low-energy Horava gravity into
the effective field theory formalism. As a final step we derived the 4th order
action in term of the curvature perturbation. This allowed us to identify the
non-linear contributions coming from the linear order perturbations which at
the next order act like source terms. Moreover, we confirm that the stability
requirements, ensuring the positivity of the kinetic term and the speed of
propagation for scalar mode, are automatically satisfied once the viability of
the theory is demanded at linear level. The approach we present here will allow
to construct, in a model independent way, all the relevant predictions on
observables at mildly non-linear scales.
",physics
"  Mammography screening for early detection of breast lesions currently suffers
from high amounts of false positive findings, which result in unnecessary
invasive biopsies. Diffusion-weighted MR images (DWI) can help to reduce many
of these false-positive findings prior to biopsy. Current approaches estimate
tissue properties by means of quantitative parameters taken from generative,
biophysical models fit to the q-space encoded signal under certain assumptions
regarding noise and spatial homogeneity. This process is prone to fitting
instability and partial information loss due to model simplicity. We reveal
unexplored potentials of the signal by integrating all data processing
components into a convolutional neural network (CNN) architecture that is
designed to propagate clinical target information down to the raw input images.
This approach enables simultaneous and target-specific optimization of image
normalization, signal exploitation, global representation learning and
classification. Using a multicentric data set of 222 patients, we demonstrate
that our approach significantly improves clinical decision making with respect
to the current state of the art.
",computer-science
"  Deep neural networks have gained tremendous popularity in last few years.
They have been applied for the task of classification in almost every domain.
Despite the success, deep networks can be incredibly slow to train for even
moderate sized models on sufficiently large datasets. Additionally, these
networks require large amounts of data to be able to generalize. The importance
of speeding up convergence, and generalization in deep networks can not be
overstated. In this work, we develop an optimization algorithm based on
generalized-optimal updates derived from minibatches that lead to faster
convergence. Towards the end, we demonstrate on two benchmark datasets that the
proposed method achieves two orders of magnitude speed up over traditional
back-propagation, and is more robust to noise/over-fitting.
",statistics
"  Rapport plays an important role during communication because it can help
people understand each other's feelings or ideas and leads to a smooth
communication. Computational rapport model has been proposed based on theory in
previous work. But there lacks solid verification. In this paper, we apply
structural equation model (SEM) to the theoretical model on both dyads of
friend and stranger. The results indicate some unfavorable paths. Based on the
results and more literature, we modify the original model to integrate more
nonverbal behaviors, including gaze and smile. Fit indices and other
examination show the goodness of our new models, which can give us more insight
into rapport management during conversation.
",computer-science
"  Gaussian processes (GPs) are powerful non-parametric function estimators.
However, their applications are largely limited by the expensive computational
cost of the inference procedures. Existing stochastic or distributed
synchronous variational inferences, although have alleviated this issue by
scaling up GPs to millions of samples, are still far from satisfactory for
real-world large applications, where the data sizes are often orders of
magnitudes larger, say, billions. To solve this problem, we propose ADVGP, the
first Asynchronous Distributed Variational Gaussian Process inference for
regression, on the recent large-scale machine learning platform,
PARAMETERSERVER. ADVGP uses a novel, flexible variational framework based on a
weight space augmentation, and implements the highly efficient, asynchronous
proximal gradient optimization. While maintaining comparable or better
predictive performance, ADVGP greatly improves upon the efficiency of the
existing variational methods. With ADVGP, we effortlessly scale up GP
regression to a real-world application with billions of samples and demonstrate
an excellent, superior prediction accuracy to the popular linear models.
",statistics
"  We consider the problem of detecting a deformation from a symmetric Gaussian
random $p$-tensor $(p\geq 3)$ with a rank-one spike sampled from the Rademacher
prior. Recently in Lesieur et al. (2017), it was proved that there exists a
critical threshold $\beta_p$ so that when the signal-to-noise ratio exceeds
$\beta_p$, one can distinguish the spiked and unspiked tensors and weakly
recover the prior via the minimal mean-square-error method. On the other side,
Perry, Wein, and Bandeira (2017) proved that there exists a $\beta_p'<\beta_p$
such that any statistical hypothesis test can not distinguish these two
tensors, in the sense that their total variation distance asymptotically
vanishes, when the signa-to-noise ratio is less than $\beta_p'$. In this work,
we show that $\beta_p$ is indeed the critical threshold that strictly separates
the distinguishability and indistinguishability between the two tensors under
the total variation distance. Our approach is based on a subtle analysis of the
high temperature behavior of the pure $p$-spin model with Ising spin, arising
initially from the field of spin glasses. In particular, we identify the
signal-to-noise criticality $\beta_p$ as the critical temperature,
distinguishing the high and low temperature behavior, of the Ising pure
$p$-spin mean-field spin glass model.
",mathematics
"  In this paper we show how the defense relation among abstract arguments can
be used to encode the reasons for accepting arguments. After introducing a
novel notion of defenses and defense graphs, we propose a defense semantics
together with a new notion of defense equivalence of argument graphs, and
compare defense equivalence with standard equivalence and strong equivalence,
respectively. Then, based on defense semantics, we define two kinds of reasons
for accepting arguments, i.e., direct reasons and root reasons, and a notion of
root equivalence of argument graphs. Finally, we show how the notion of root
equivalence can be used in argumentation summarization.
",computer-science
"  With a rapidly increasing number of devices connected to the internet, big
data has been applied to various domains of human life. Nevertheless, it has
also opened new venues for breaching users' privacy. Hence it is highly
required to develop techniques that enable data owners to privatize their data
while keeping it useful for intended applications. Existing methods, however,
do not offer enough flexibility for controlling the utility-privacy trade-off
and may incur unfavorable results when privacy requirements are high. To tackle
these drawbacks, we propose a compressive-privacy based method, namely RUCA
(Ratio Utility and Cost Analysis), which can not only maximize performance for
a privacy-insensitive classification task but also minimize the ability of any
classifier to infer private information from the data. Experimental results on
Census and Human Activity Recognition data sets demonstrate that RUCA
significantly outperforms existing privacy preserving data projection
techniques for a wide range of privacy pricings.
",statistics
"  This paper is dedicated to new methods of constructing weight structures and
weight-exact localizations; our arguments generalize their bounded versions
considered in previous papers of the authors. We start from a class of objects
$P$ of triangulated category $C$ that satisfies a certain negativity condition
(there are no $C$-extensions of positive degrees between elements of $P$; we
actually need a somewhat stronger condition of this sort) to obtain a weight
structure both ""halves"" of which are closed either with respect to
$C$-coproducts of less than $\alpha$ objects (for $\alpha$ being a fixed
regular cardinal) or with respect to all coproducts (provided that $C$ is
closed with respect to coproducts of this sort). This construction gives all
""reasonable"" weight structures satisfying the latter condition. In particular,
we obtain certain weight structures on spectra (in $SH$) consisting of less
than $\alpha$ cells and on certain localizations of $SH$; these results are
new.
",mathematics
"  We prove that along any marked point the Green function of a meromorphic
family of polynomials parameterized by the punctured unit disk explodes
exponentially fast near the origin with a continuous error term.
",mathematics
"  Germanium telluride features special spin-electric effects originating from
spin-orbit coupling and symmetry breaking by the ferroelectric lattice
polarization, which opens up many prospectives for electrically tunable and
switchable spin electronic devices. By Mn doping of the {\alpha}-GeTe host
lattice, the system becomes a multiferroic semiconductor possessing
magnetoelectric properties in which the electric polarization, magnetization
and spin texture are coupled to each other. Employing spin- and angle-resolved
photoemission spectroscopy in bulk- and surface-sensitive energy ranges and by
varying dipole transition matrix elements, we disentangle the bulk, surface and
surface-resonance states of the electronic structure and determine the spin
textures for selected parameters. From our results, we derive a comprehensive
model of the {\alpha}-GeTe surface electronic structure which fits experimental
data and first principle theoretical predictions and we discuss the
unconventional evolution of the Rashba-type spin splitting upon manipulation by
external B- and E-fields.
",physics
"  This paper presents the kinematic analysis of the 3-PPPS parallel robot with
an equilateral mobile platform and a U-shape base. The proposed design and
appropriate selection of parameters allow to formulate simpler direct and
inverse kinematics for the manipulator under study. The parallel singularities
associated with the manipulator depend only on the orientation of the
end-effector, and thus depend only on the orientation of the end effector. The
quaternion parameters are used to represent the aspects, i.e. the singularity
free regions of the workspace. A cylindrical algebraic decomposition is used to
characterize the workspace and joint space with a low number of cells. The
dis-criminant variety is obtained to describe the boundaries of each cell. With
these simplifications, the 3-PPPS parallel robot with proposed design can be
claimed as the simplest 6 DOF robot, which further makes it useful for the
industrial applications.
",computer-science
"  Large-scale extragalactic magnetic fields may induce conversions between
very-high-energy photons and axionlike particles (ALPs), thereby shielding the
photons from absorption on the extragalactic background light. However, in
simplified ""cell"" models, used so far to represent extragalactic magnetic
fields, this mechanism would be strongly suppressed by current astrophysical
bounds. Here we consider a recent model of extragalactic magnetic fields
obtained from large-scale cosmological simulations. Such simulated magnetic
fields would have large enhancement in the filaments of matter. As a result,
photon-ALP conversions would produce a significant spectral hardening for
cosmic TeV photons. This effect would be probed with the upcoming Cherenkov
Telescope Array detector. This possible detection would give a unique chance to
perform a tomography of the magnetized cosmic web with ALPs.
",physics
"  Many astronomical sources produce transient phenomena at radio frequencies,
but the transient sky at low frequencies (<300 MHz) remains relatively
unexplored. Blind surveys with new widefield radio instruments are setting
increasingly stringent limits on the transient surface density on various
timescales. Although many of these instruments are limited by classical
confusion noise from an ensemble of faint, unresolved sources, one can in
principle detect transients below the classical confusion limit to the extent
that the classical confusion noise is independent of time. We develop a
technique for detecting radio transients that is based on temporal matched
filters applied directly to time series of images rather than relying on
source-finding algorithms applied to individual images. This technique has
well-defined statistical properties and is applicable to variable and transient
searches for both confusion-limited and non-confusion-limited instruments.
Using the Murchison Widefield Array as an example, we demonstrate that the
technique works well on real data despite the presence of classical confusion
noise, sidelobe confusion noise, and other systematic errors. We searched for
transients lasting between 2 minutes and 3 months. We found no transients and
set improved upper limits on the transient surface density at 182 MHz for flux
densities between ~20--200 mJy, providing the best limits to date for hour- and
month-long transients.
",physics
"  This paper presents contributions on nonlinear tracking control systems for a
quadrotor unmanned micro aerial vehicle. New controllers are proposed based on
nonlinear surfaces composed by tracking errors that evolve directly on the
nonlinear configuration manifold thus inherently including in the control
design the nonlinear characteristics of the SE(3) configuration space. In
particular geometric surface-based controllers are developed, and through
rigorous stability proofs they are shown to have desirable closed loop
properties that are almost global. A region of attraction, independent of the
position error, is produced and its effects are analyzed. A strategy allowing
the quadrotor to achieve precise attitude tracking while simultaneously
following a desired position command and complying to actuator constraints in a
computationally inexpensive manner is derived. This important contribution
differentiates this work from existing Geometric Nonlinear Control System
solutions (GNCSs) since the commanded thrusts can be realized by the majority
of quadrotors produced by the industry. The new features of the proposed GNCSs
are illustrated by numerical simulations of aggressive maneuvers and a
comparison with a GNCSs from the bibliography.
",computer-science
"  The purpose of the paper is to study Yamabe solitons on three-dimensional
para-Sasakian, paracosymplectic and para-Kenmotsu manifolds. Mainly, we proved
that *If the semi-Riemannian metric of a three-dimensional para-Sasakian
manifold is a Yamabe soliton, then it is of constant scalar curvature, and the
flow vector field V is Killing. In the next step, we proved that either
manifold has constant curvature -1 and reduces to an Einstein manifold, or V is
an infinitesimal automorphism of the paracontact metric structure on the
manifold. *If the semi-Riemannian metric of a three-dimensional
paracosymplectic manifold is a Yamabe soliton, then it has constant scalar
curvature. Furthermore either manifold is $\eta$-Einstein, or Ricci flat. *If
the semi-Riemannian metric on a three-dimensional para-Kenmotsu manifold is a
Yamabe soliton, then the manifold is of constant sectional curvature -1,
reduces to an Einstein manifold. Furthermore, Yamabe soliton is expanding with
$\lambda$=-6 and the vector field V is Killing. Finally, we construct examples
to illustrate the results obtained in previous sections.
",mathematics
"  We address unsupervised optical flow estimation for ego-centric motion. We
argue that optical flow can be cast as a geometrical warping between two
successive video frames and devise a deep architecture to estimate such
transformation in two stages. First, a dense pixel-level flow is computed with
a geometric prior imposing strong spatial constraints. Such prior is typical of
driving scenes, where the point of view is coherent with the vehicle motion. We
show how such global transformation can be approximated with an homography and
how spatial transformer layers can be employed to compute the flow field
implied by such transformation. The second stage then refines the prediction
feeding a second deeper network. A final reconstruction loss compares the
warping of frame X(t) with the subsequent frame X(t+1) and guides both
estimates. The model, which we named TransFlow, performs favorably compared to
other unsupervised algorithms, and shows better generalization compared to
supervised methods with a 3x reduction in error on unseen data.
",computer-science
"  In multi-server distributed queueing systems, the access of stochastically
arriving jobs to resources is often regulated by a dispatcher, also known as
load balancer. A fundamental problem consists in designing a load balancing
algorithm that minimizes the delays experienced by jobs. During the last two
decades, the power-of-$d$-choice algorithm, based on the idea of dispatching
each job to the least loaded server out of $d$ servers randomly sampled at the
arrival of the job itself, has emerged as a breakthrough in the foundations of
this area due to its versatility and appealing asymptotic properties. In this
paper, we consider the power-of-$d$-choice algorithm with the addition of a
local memory that keeps track of the latest observations collected over time on
the sampled servers. Then, each job is sent to a server with the lowest
observation. We show that this algorithm is asymptotically optimal in the sense
that the load balancer can always assign each job to an idle server in the
large-server limit. This holds true if and only if the system load $\lambda$ is
less than $1-\frac{1}{d}$. If this condition is not satisfied, we show that
queue lengths are bounded by $j^\star+1$, where $j^\star\in\mathbb{N}$ is given
by the solution of a polynomial equation. This is in contrast with the classic
version of the power-of-$d$-choice algorithm, where queue lengths are
unbounded. Our upper bound on the size of the most loaded server, $j^*+1$, is
tight and increases slowly when $\lambda$ approaches its critical value from
below. For instance, when $\lambda= 0.995$ and $d=2$ (respectively, $d=3$), we
find that no server will contain more than just $5$ ($3$) jobs in equilibrium.
Our results quantify and highlight the importance of using memory as a means to
enhance performance in randomized load balancing.
",computer-science
"  This paper presents a procedure to retrieve subsets of relevant documents
from large text collections for Content Analysis, e.g. in social sciences.
Document retrieval for this purpose needs to take account of the fact that
analysts often cannot describe their research objective with a small set of key
terms, especially when dealing with theoretical or rather abstract research
interests. Instead, it is much easier to define a set of paradigmatic documents
which reflect topics of interest as well as targeted manner of speech. Thus, in
contrast to classic information retrieval tasks we employ manually compiled
collections of reference documents to compose large queries of several hundred
key terms, called dictionaries. We extract dictionaries via Topic Models and
also use co-occurrence data from reference collections. Evaluations show that
the procedure improves retrieval results for this purpose compared to
alternative methods of key term extraction as well as neglecting co-occurrence
data.
",computer-science
"  We investigate the relation between quadrics and their Christoffel duals on
the one hand, and certain zero mean curvature surfaces and their Gauss maps on
the other hand. To study the relation between timelike minimal surfaces and the
Christoffel duals of 1-sheeted hyperboloids we introduce para-holomorphic
elliptic functions. The curves of type change for real isothermic surfaces of
mixed causal type turn out to be aligned with the real curvature line net.
",mathematics
"  Cell injection is a technique in the domain of biological cell
micro-manipulation for the delivery of small volumes of samples into the
suspended or adherent cells. It has been widely applied in various areas, such
as gene injection, in-vitro fertilization (IVF), intracytoplasmic sperm
injection (ISCI) and drug development. However, the existing manual and
semi-automated cell injection systems require lengthy training and suffer from
high probability of contamination and low success rate. In the recently
introduced fully automated cell injection systems, the injection force plays a
vital role in the success of the process since even a tiny excessive force can
destroy the membrane or tissue of the biological cell. Traditionally, the force
control algorithms are analyzed using simulation, which is inherently
non-exhaustive and incomplete in terms of detecting system failures. Moreover,
the uncertainties in the system are generally ignored in the analysis. To
overcome these limitations, we present a formal analysis methodology based on
probabilistic model checking to analyze a robotic cell injection system
utilizing the impedance force control algorithm. The proposed methodology,
developed using the PRISM model checker, allowed to find a discrepancy in the
algorithm, which was not found by any of the previous analysis using the
traditional methods.
",computer-science
"  Currently, eXtensible Access Control Markup Language (XACML) has becoming the
standard for implementing access control policies and consequently more
attention is dedicated to testing the correctness of XACML policies. In
particular, coverage measures can be adopted for assessing test strategy
effectiveness in exercising the policy elements. This study introduces a set of
XACML coverage criteria and describes the access control infrastructure, based
on a monitor engine, enabling the coverage criterion selection and the on-line
tracing of the testing activity. Examples of infrastructure usage and of
assessment of different test strategies are provided.
",computer-science
"  NA62 is a fixed-target experiment at the CERN SPS dedicated to measurements
of rare kaon decays. Such measurements, like the branching fraction of the
$K^{+} \rightarrow \pi^{+} \nu \bar\nu$ decay, have the potential to bring
significant insights into new physics processes when comparison is made with
precise theoretical predictions. For this purpose, innovative techniques have
been developed, in particular, in the domain of low-mass tracking devices.
Detector construction spanned several years from 2009 to 2014. The
collaboration started detector commissioning in 2014 and will collect data
until the end of 2018. The beam line and detector components are described
together with their early performance obtained from 2014 and 2015 data.
",physics
"  Deep neural networks (DNNs) have begun to have a pervasive impact on various
applications of machine learning. However, the problem of finding an optimal
DNN architecture for large applications is challenging. Common approaches go
for deeper and larger DNN architectures but may incur substantial redundancy.
To address these problems, we introduce a network growth algorithm that
complements network pruning to learn both weights and compact DNN architectures
during training. We propose a DNN synthesis tool (NeST) that combines both
methods to automate the generation of compact and accurate DNNs. NeST starts
with a randomly initialized sparse network called the seed architecture. It
iteratively tunes the architecture with gradient-based growth and
magnitude-based pruning of neurons and connections. Our experimental results
show that NeST yields accurate, yet very compact DNNs, with a wide range of
seed architecture selection. For the LeNet-300-100 (LeNet-5) architecture, we
reduce network parameters by 70.2x (74.3x) and floating-point operations
(FLOPs) by 79.4x (43.7x). For the AlexNet and VGG-16 architectures, we reduce
network parameters (FLOPs) by 15.7x (4.6x) and 30.2x (8.6x), respectively.
NeST's grow-and-prune paradigm delivers significant additional parameter and
FLOPs reduction relative to pruning-only methods.
",computer-science
"  In this paper, we introduce the notions of an iterated planar Lefschetz
fibration and an iterated planar open book decomposition and prove the
Weinstein conjecture for contact manifolds supporting an open book that has
iterated planar pages. For $n\geq 1$, we show that a $(2n+1)$-dimensional
contact manifold $M$ supporting an iterated planar open book decomposition
satisfies the Weinstein conjecture.
",mathematics
"  We analyze the definitions of generalized quantifiers of imperfect
information that have been proposed by F.Engström. We argue that these
definitions are just embeddings of the first-order generalized quantifiers into
team semantics, and fail to capture an adequate notion of team-theoretical
generalized quantifier, save for the special cases in which the quantifiers are
applied to flat formulas. We also criticize the meaningfulness of the
monotone/nonmonotone distinction in this context. We make some proposals for a
more adequate definition of generalized quantifiers of imperfect information.
",mathematics
"  This paper investigates reverse auctions that involve continuous values of
different types of goods, general nonconvex constraints, and second stage
costs. We seek to design the payment rules and conditions under which
coalitions of participants cannot influence the auction outcome in order to
obtain higher collective utility. Under the incentive-compatible
Vickrey-Clarke-Groves mechanism, we show that coalition-proof outcomes are
achieved if the submitted bids are convex and the constraint sets are of a
polymatroid-type. These conditions, however, do not capture the complexity of
the general class of reverse auctions under consideration. By relaxing the
property of incentive-compatibility, we investigate further payment rules that
are coalition-proof without any extra conditions on the submitted bids and the
constraint sets. Since calculating the payments directly for these mechanisms
is computationally difficult for auctions involving many participants, we
present two computationally efficient methods. Our results are verified with
several case studies based on electricity market data.
",computer-science
"  Automated service classification plays a crucial role in service management
such as service discovery, selection, and composition. In recent years, machine
learning techniques have been used for service classification. However, they
can only predict around 10 to 20 service categories due to the quality of
feature engineering and the imbalance problem of service dataset. In this
paper, we present a deep neural network ServeNet with a novel dataset splitting
algorithm to deal with these issues. ServeNet can automatically abstract
low-level representation to high-level features, and then predict service
classification based on the service datasets produced by the proposed splitting
algorithm. To demonstrate the effectiveness of our approach, we conducted a
comprehensive experimental study on 10,000 real-world services in 50
categories. The result shows that ServeNet can achieve higher accuracy than
other machine learning methods.
",statistics
"  The upcoming Fermilab E989 experiment will measure the muon anomalous
magnetic moment $a_{\mu}$ . This measurement is motivated by the previous
measurement performed in 2001 by the BNL E821 experiment that reported a 3-4
standard deviation discrepancy between the measured value and the Standard
Model prediction. The new measurement at Fermilab aims to improve the precision
by a factor of four reducing the total uncertainty from 540 parts per billion
(BNL E821) to 140 parts per billion (Fermilab E989). This paper gives the
status of the experiment.
",physics
"  Capable of significantly reducing cell size and enhancing spatial reuse,
network densification is shown to be one of the most dominant approaches to
expand network capacity. Due to the scarcity of available spectrum resources,
nevertheless, the over-deployment of network infrastructures, e.g., cellular
base stations (BSs), would strengthen the inter-cell interference as well, thus
in turn deteriorating the system performance. On this account, we investigate
the performance of downlink cellular networks in terms of user coverage
probability (CP) and network spatial throughput (ST), aiming to shed light on
the limitation of network densification. Notably, it is shown that both CP and
ST would be degraded and even diminish to be zero when BS density is
sufficiently large, provided that practical antenna height difference (AHD)
between BSs and users is involved to characterize pathloss. Moreover, the
results also reveal that the increase of network ST is at the expense of the
degradation of CP. Therefore, to balance the tradeoff between user and network
performance, we further study the critical density, under which ST could be
maximized under the CP constraint. Through a special case study, it follows
that the critical density is inversely proportional to the square of AHD. The
results in this work could provide helpful guideline towards the application of
network densification in the next-generation wireless networks.
",computer-science
"  The stabilization of lasers to absolute frequency references is a fundamental
requirement in several areas of atomic, molecular and optical physics. A range
of techniques are available to produce a suitable reference onto which one can
'lock' the laser, many of which depend on the specific internal structure of
the reference or are sensitive to laser intensity noise. We present a novel
method using the frequency modulation of an acousto-optic modulator's carrier
(drive) signal to generate two spatially separated beams, with a frequency
difference of only a few MHz. These beams are used to probe a narrow absorption
feature and the difference in their detected signals leads to a dispersion-like
feature suitable for wavelength stabilization of a diode laser. This simple and
versatile method only requires a narrow absorption line and is therefore
suitable for both atomic and cavity based stabilization schemes. To demonstrate
the suitability of this method we lock an external cavity diode laser near the
$^{85}\mathrm{Rb}\,5S_{1/2}\rightarrow5P_{3/2}, F=3\rightarrow F^{\prime}=4$
using sub-Doppler pump probe spectroscopy and also demonstrate excellent
agreement between the measured signal and a theoretical model.
",physics
"  Let $Y$ be the complement of a plane quartic curve $D$ defined over a number
field. Our main theorem confirms the Lang-Vojta conjecture for $Y$ when $D$ is
a generic smooth quartic curve, by showing that its integral points are
confined in a curve except for a finite number of exceptions. The required
finiteness will be obtained by reducing it to the Shafarevich conjecture for K3
surfaces. Some variants of our method confirm the same conjecture when $D$ is a
reducible generic quartic curve which consists of four lines, two lines and a
conic, or two conics.
",mathematics
"  Based on ab initio evolutionary crystal structure search computation, we
report a new phase of phosphorus called green phosphorus ({\lambda}-P), which
exhibits the direct band gaps ranging from 0.7 to 2.4 eV and the strong
anisotropy in optical and transport properties. Free energy calculations show
that a single-layer form, termed green phosphorene, is energetically more
stable than blue phosphorene and a phase transition from black to green
phosphorene can occur at temperatures above 87 K. Due to its buckled structure,
green phosphorene can be synthesized on corrugated metal surfaces rather than
clean surfaces.
",physics
"  A ranking is an ordered sequence of items, in which an item with higher
ranking score is more preferred than the items with lower ranking scores. In
many information systems, rankings are widely used to represent the preferences
over a set of items or candidates. The consensus measure of rankings is the
problem of how to evaluate the degree to which the rankings agree. The
consensus measure can be used to evaluate rankings in many information systems,
as quite often there is not ground truth available for evaluation.
This paper introduces a novel approach for consensus measure of rankings by
using graph representation, in which the vertices or nodes are the items and
the edges are the relationship of items in the rankings. Such representation
leads to various algorithms for consensus measure in terms of different aspects
of rankings, including the number of common patterns, the number of common
patterns with fixed length and the length of the longest common patterns. The
proposed measure can be adopted for various types of rankings, such as full
rankings, partial rankings and rankings with ties. This paper demonstrates how
the proposed approaches can be used to evaluate the quality of rank aggregation
and the quality of top-$k$ rankings from Google and Bing search engines.
",computer-science
"  Gravitational waves (GWs) generated by axisymmetric rotating collapse,
bounce, and early postbounce phases of a galactic core-collapse supernova will
be detectable by current-generation gravitational wave observatories. Since
these GWs are emitted from the quadrupole-deformed nuclear-density core, they
may encode information on the uncertain nuclear equation of state (EOS). We
examine the effects of the nuclear EOS on GWs from rotating core collapse and
carry out 1824 axisymmetric general-relativistic hydrodynamic simulations that
cover a parameter space of 98 different rotation profiles and 18 different EOS.
We show that the bounce GW signal is largely independent of the EOS and
sensitive primarily to the ratio of rotational to gravitational energy, and at
high rotation rates, to the degree of differential rotation. The GW frequency
of postbounce core oscillations shows stronger EOS dependence that can be
parameterized by the core's EOS-dependent dynamical frequency
$\sqrt{G\bar{\rho}_c}$. We find that the ratio of the peak frequency to the
dynamical frequency follows a universal trend that is obeyed by all EOS and
rotation profiles and that indicates that the nature of the core oscillations
changes when the rotation rate exceeds the dynamical frequency. We find that
differences in the treatments of low-density nonuniform nuclear matter, of the
transition from nonuniform to uniform nuclear matter, and in the description of
nuclear matter up to around twice saturation density can mildly affect the GW
signal. We find that approximations and uncertainties in electron capture rates
can lead to variations in the GW signal that are of comparable magnitude to
those due to different nuclear EOS. This emphasizes the need for reliable
nuclear electron capture rates and for self-consistent multi-dimensional
neutrino radiation-hydrodynamic simulations of rotating core collapse.
",physics
"  Background. Test resources are usually limited and therefore it is often not
possible to completely test an application before a release. To cope with the
problem of scarce resources, development teams can apply defect prediction to
identify fault-prone code regions. However, defect prediction tends to low
precision in cross-project prediction scenarios.
Aims. We take an inverse view on defect prediction and aim to identify
methods that can be deferred when testing because they contain hardly any
faults due to their code being ""trivial"". We expect that characteristics of
such methods might be project-independent, so that our approach could improve
cross-project predictions.
Method. We compute code metrics and apply association rule mining to create
rules for identifying methods with low fault risk. We conduct an empirical
study to assess our approach with six Java open-source projects containing
precise fault data at the method level.
Results. Our results show that inverse defect prediction can identify approx.
32-44% of the methods of a project to have a low fault risk; on average, they
are about six times less likely to contain a fault than other methods. In
cross-project predictions with larger, more diversified training sets,
identified methods are even eleven times less likely to contain a fault.
Conclusions. Inverse defect prediction supports the efficient allocation of
test resources by identifying methods that can be treated with less priority in
testing activities and is well applicable in cross-project prediction
scenarios.
",computer-science
"  Off-diagonal Aubry-André (AA) model has recently attracted a great deal
of attention as they provide condensed matter realization of topological
phases. We numerically study a generalized off-diagonal AA model with p-wave
superfluid pairing in the presence of both commensurate and incommensurate
hopping modulations. The phase diagram as functions of the modulation strength
of incommensurate hopping and the strength of the p-wave pairing is obtained by
using the multifractal analysis. We show that with the appearance of the p-wave
pairing, the system exhibits mobility-edge phases and critical phases with
various number of topologically protected zero-energy modes. Predicted
topological nature of these exotic phases can be realized in a cold atomic
system of incommensurate bichromatic optical lattice with induced p-wave
superfluid pairing by using a Raman laser in proximity to a molecular
Bose-Einstein condensation.
",physics
"  We classify the ergodic invariant random subgroups of block-diagonal limits
of symmetric groups in the cases when the groups are simple and the associated
dimension groups have finite dimensional state spaces. These block-diagonal
limits arise as the transformation groups (full groups) of Bratteli diagrams
that preserve the cofinality of infinite paths in the diagram. Given a simple
full group $G$ admitting only a finite number of ergodic measures on the
path-space $X$ of the associated Bratteli digram, we prove that every non-Dirac
ergodic invariant random subgroup of $G$ arises as the stabilizer distribution
of the diagonal action on $X^n$ for some $n\geq 1$. As a corollary, we
establish that every group character $\chi$ of $G$ has the form $\chi(g) =
Prob(g\in K)$, where $K$ is a conjugation-invariant random subgroup of $G$.
",mathematics
"  The deformation of disordered solids relies on swift and localised
rearrangements of particles. The inspection of soft vibrational modes can help
predict the locations of these rearrangements, while the strain that they
actually redistribute mediates collective effects. Here, we study soft modes
and strain redistribution in a two-dimensional continuous mesoscopic model
based on a Ginzburg-Landau free energy for perfect solids, supplemented with a
plastic disorder potential that accounts for shear softening and
rearrangements. Regardless of the disorder strength, our numerical simulations
show soft modes that are always sharply peaked at the softest point of the
material (unlike what happens for the depinning of an elastic interface).
Contrary to widespread views, the deformation halo around this peak does not
always have a quadrupolar (Eshelby-like) shape. Instead, for finite and
narrowly-distributed disorder, it looks like a fracture, with a strain field
that concentrates along some easy directions. These findings are rationalised
with analytical calculations in the case where the plastic disorder is confined
to a point-like `impurity'. In this case, we unveil a continuous family of
elastic propagators, which are identical for the soft modes and for the
equilibrium configurations. This family interpolates between the standard
quadrupolar propagator and the fracture-like one as the anisotropy of the
elastic medium is increased. Therefore, we expect to see a fracture-like
propagator when extended regions on the brink of failure have already softened
along the shear direction and thus rendered the material anisotropic, but not
failed yet. We speculate that this might be the case in carefully aged glasses
just before macroscopic failure.
",physics
"  The class of selfdecomposable distributions in free probability theory was
introduced by Barndorff-Nielsen and the third named author. It constitutes a
fairly large subclass of the freely infinitely divisible distributions, but so
far specific examples have been limited to Wigner's semicircle distributions,
the free stable distributions, two kinds of free gamma distributions and a few
other examples. In this paper, we prove that the (classical) normal
distributions are freely selfdecomposable. More generally it is established
that the Askey-Wimp-Kerov distribution $\mu_c$ is freely selfdecomposable for
any $c$ in $[-1,0]$. The main ingredient in the proof is a general
characterization of the freely selfdecomposable distributions in terms of the
derivative of their free cumulant transform.
",mathematics
"  Classical CTL temporal logics are built over systems with interleaving model
concurrency. Many attempts are made to fight a state space explosion problem
(for instance, compositional model checking). There are some methods of
reduction of a state space based on independence of actions. However, in CSM
model, which is based on coincidences rather than on interleaving, independence
of actions cannot be defined. Therefore a state space reduction basing on
identical temporal consequences rather than on independence of action is
proposed. The new reduction is not as good as for interleaving systems, because
all successors of a state (in depth of two levels) must be obtained before a
reduction may be applied. This leads to reduction of space required for
representation of a state space, but not in time of state space construction.
Yet much savings may occur in regular state spaces for CSM systems.
",computer-science
"  The present paper introduces the initial implementation of a software
exploration tool targeting graphical user interface (GUI) driven applications.
GUITracer facilitates the comprehension of GUI-driven applications by starting
from their most conspicuous artefact - the user interface itself. The current
implementation of the tool can be used with any Java-based target application
that employs one of the AWT, Swing or SWT toolkits. The tool transparently
instruments the target application and provides real time information about the
GUI events fired. For each event, call relations within the application are
displayed at method, class or package level, together with detailed coverage
information. The tool facilitates feature location, program comprehension as
well as GUI test creation by revealing the link between the application's GUI
and its underlying code. As such, GUITracer is intended for software
practitioners developing or maintaining GUI-driven applications. We believe our
tool to be especially useful for entry-level practitioners as well as students
seeking to understand complex GUI-driven software systems. The present paper
details the rationale as well as the technical implementation of the tool. As a
proof-of-concept implementation, we also discuss further development that can
lead to our tool's integration into a software development workflow.
",computer-science
"  For a positive parameter $\beta$, the $\beta$-bounded distance between a pair
of vertices $u,v$ in a weighted undirected graph $G = (V,E,\omega)$ is the
length of the shortest $u-v$ path in $G$ with at most $\beta$ edges, aka {\em
hops}. For $\beta$ as above and $\epsilon>0$, a {\em $(\beta,\epsilon)$-hopset}
of $G = (V,E,\omega)$ is a graph $G' =(V,H,\omega_H)$ on the same vertex set,
such that all distances in $G$ are $(1+\epsilon)$-approximated by
$\beta$-bounded distances in $G\cup G'$.
Hopsets are a fundamental graph-theoretic and graph-algorithmic construct,
and they are widely used for distance-related problems in a variety of
computational settings. Currently existing constructions of hopsets produce
hopsets either with $\Omega(n \log n)$ edges, or with a hopbound
$n^{\Omega(1)}$. In this paper we devise a construction of {\em linear-size}
hopsets with hopbound $(\log n)^{\log^{(3)}n+O(1)}$. This improves the previous
bound almost exponentially.
We also devise efficient implementations of our construction in PRAM and
distributed settings. The only existing PRAM algorithm \cite{EN16} for
computing hopsets with a constant (i.e., independent of $n$) hopbound requires
$n^{\Omega(1)}$ time. We devise a PRAM algorithm with polylogarithmic running
time for computing hopsets with a constant hopbound, i.e., our running time is
exponentially better than the previous one. Moreover, these hopsets are also
significantly sparser than their counterparts from \cite{EN16}.
We use our hopsets to devise a distributed routing scheme that exhibits
near-optimal tradeoff between individual memory requirement
$\tilde{O}(n^{1/k})$ of vertices throughout preprocessing and routing phases of
the algorithm, and stretch $O(k)$, along with a near-optimal construction time
$\approx D + n^{1/2 + 1/k}$, where $D$ is the hop-diameter of the input graph.
",computer-science
"  One of the remaining obstacles to approaching the theoretical efficiency
limit of crystalline silicon (c-Si) solar cells is the exceedingly high
interface recombination loss for minority carriers at the Ohmic contacts. In
ultra-thin-film c-Si solar cells, this contact recombination loss is far more
severe than for traditional thick cells due to the smaller volume and higher
minority carrier concentration of the former. This paper presents a novel
design of an electron passing (Ohmic) contact to n-type Si that is
hole-blocking with significantly reduced hole recombination. This contact is
formed by depositing a thin titanium dioxide (TiO2) layer to form a silicon
metal-insulator-semiconductor (MIS) contact. A 2 {\mu}m thick Si cell with this
TiO2 MIS contact achieved an open circuit voltage (Voc) of 645 mV, which is 10
mV higher than that of an ultra-thin cell with a metal contact. This MIS
contact demonstrates a new path for ultra-thin-film c-Si solar cells to achieve
high efficiencies as high as traditional thick cells, and enables the
fabrication of high-efficiency c-Si solar cells at a lower cost.
",physics
"  The actions of an autonomous vehicle on the road affect and are affected by
those of other drivers, whether overtaking, negotiating a merge, or avoiding an
accident. This mutual dependence, best captured by dynamic game theory, creates
a strong coupling between the vehicle's planning and its predictions of other
drivers' behavior, and constitutes an open problem with direct implications on
the safety and viability of autonomous driving technology. Unfortunately,
dynamic games are too computationally demanding to meet the real-time
constraints of autonomous driving in its continuous state and action space. In
this paper, we introduce a novel game-theoretic trajectory planning algorithm
for autonomous driving, that enables real-time performance by hierarchically
decomposing the underlying dynamic game into a long-horizon ""strategic"" game
with simplified dynamics and full information structure, and a short-horizon
""tactical"" game with full dynamics and a simplified information structure. The
value of the strategic game is used to guide the tactical planning, implicitly
extending the planning horizon, pushing the local trajectory optimization
closer to global solutions, and, most importantly, quantitatively accounting
for the autonomous vehicle and the human driver's ability and incentives to
influence each other. In addition, our approach admits non-deterministic models
of human decision-making, rather than relying on perfectly rational
predictions. Our results showcase richer, safer, and more effective autonomous
behavior in comparison to existing techniques.
",computer-science
"  We study the problems of clustering with outliers in high dimension. Though a
number of methods have been developed in the past decades, it is still quite
challenging to design quality guaranteed algorithms with low complexities for
the problems. Our idea is inspired by the greedy method, Gonzalez's algorithm,
for solving the problem of ordinary $k$-center clustering. Based on some novel
observations, we show that this greedy strategy actually can handle
$k$-center/median/means clustering with outliers efficiently, in terms of
qualities and complexities. We further show that the greedy approach yields
small coreset for the problem in doubling metrics, so as to reduce the time
complexity significantly. Moreover, a by-product is that the coreset
construction can be applied to speedup the popular density-based clustering
approach DBSCAN.
",computer-science
"  A `flutter machine' is introduced for the investigation of a singular
interface between the classical and reversible Hopf bifurcations that is
theoretically predicted to be generic in nonconservative reversible systems
with vanishing dissipation. In particular, such a singular interface exists for
the Pflüger viscoelastic column moving in a resistive medium, which is proven
by means of the perturbation theory of multiple eigenvalues with the Jordan
block. The laboratory setup, consisting of a cantilevered viscoelastic rod
loaded by a positional force with non-zero curl produced by dry friction,
demonstrates high sensitivity of the classical Hopf bifurcation onset {to the
ratio between} the weak air drag and Kelvin-Voigt damping in the Pflüger
column. Thus, the Whitney umbrella singularity is experimentally confirmed,
responsible for discontinuities accompanying dissipation-induced instabilities
in a broad range of physical contexts.
",physics
"  We consider inverse dynamic and spectral problems for the one dimensional
Dirac system on a finite tree. Our aim will be to recover the topology of a
tree (lengths and connectivity of edges) as well as matrix potentials on each
edge. As inverse data we use the Weyl-Titchmarsh matrix function or the dynamic
response operator.
",mathematics
"  The distribution of metals in the intra-cluster medium encodes important
information about the enrichment history and formation of galaxy clusters. Here
we explore the metal content of clusters in IllustrisTNG - a new suite of
galaxy formation simulations building on the Illustris project. Our cluster
sample contains 20 objects in TNG100 - a ~(100 Mpc)^3 volume simulation with
2x1820^3 resolution elements, and 370 objects in TNG300 - a ~(300 Mpc)^3 volume
simulation with 2x2500^3 resolution elements. The z=0 metallicity profiles
agree with observations, and the enrichment history is consistent with
observational data going beyond z~1, showing nearly no metallicity evolution.
The abundance profiles vary only minimally within the cluster samples,
especially in the outskirts with a relative scatter of ~15%. The average
metallicity profile flattens towards the center, where we find a logarithmic
slope of -0.1 compared to -0.5 in the outskirts. Cool core clusters have more
centrally peaked metallicity profiles (~0.8 solar) compared to non-cool core
systems (~0.5 solar), similar to observational trends. Si/Fe and O/Fe radial
profiles follow positive gradients. The outer abundance profiles do not evolve
below z~2, whereas the inner profiles flatten towards z=0. More than ~80% of
the metals in the intra-cluster medium have been accreted from the
proto-cluster environment, which has been enriched to ~0.1 solar already at
z~2. We conclude that the intra-cluster metal distribution is uniform among our
cluster sample, nearly time-invariant in the outskirts for more than 10 Gyr,
and forms through a universal enrichment history.
",physics
"  We present measurements of the hyperfine splitting in the Yb-173
$6s6p~^1P_1^{\rm o} (F^{\prime}=3/2,7/2)$ states that disagree significantly
with those measured previously by Das and Natarajan [Phys. Rev. A 76, 062505
(2007)]. We point out inconsistencies in their measurements and suggest that
their error is due to optical pumping and improper determination of the atomic
line center. Our measurements are made using an optical frequency comb. We use
an optical pumping scheme to improve the signal-to-background ratio for the
$F^{\prime}=3/2$ component.
",physics
"  Let $L/K$ be a tame and Galois extension of number fields with group $G$. It
is well-known that any ambiguous ideal in $L$ is locally free over
$\mathcal{O}_KG$ (of rank one), and so it defines a class in the locally free
class group of $\mathcal{O}_KG$, where $\mathcal{O}_K$ denotes the ring of
integers of $K$. In this paper, we shall study the relationship among the
classes arising from the ring of integers $\mathcal{O}_L$ of $L$, the inverse
different $\mathfrak{D}_{L/K}^{-1}$ of $L/K$, and the square root of the
inverse different $A_{L/K}$ of $L/K$ (if it exists), in the case that $G$ is
abelian. They are naturally related because $A_{L/K}^2 =
\mathfrak{D}_{L/K}^{-1} = \mathcal{O}_L^*$, and $A_{L/K}$ is special because
$A_{L/K} = A_{L/K}^*$, where $*$ denotes dual with respect to the trace of
$L/K$.
",mathematics
"  Recent data indicate one or more moderately nearby supernovae in the early
Pleistocene, with additional events likely in the Miocene. This has motivated
more detailed computations, using new information about the nature of
supernovae and the distances of these events to describe in more detail the
sorts of effects that are indicated at the Earth. This short
communication/review is designed to describe some of these effects so that they
may possibly be related to changes in the biota around these times.
",physics
"  Traditional supervised learning makes the closed-world assumption that the
classes appeared in the test data must have appeared in training. This also
applies to text learning or text classification. As learning is used
increasingly in dynamic open environments where some new/test documents may not
belong to any of the training classes, identifying these novel documents during
classification presents an important problem. This problem is called open-world
classification or open classification. This paper proposes a novel deep
learning based approach. It outperforms existing state-of-the-art techniques
dramatically.
",computer-science
"  Atomistic effective Hamiltonian simulations are used to investigate
electrocaloric (EC) effects in the lead-free Ba(Zr$_{0.5}$Ti$_{0.5}$)O$_{3}$
(BZT) relaxor ferroelectric. We find that the EC coefficient varies
non-monotonically with the field at any temperature, presenting a maximum that
can be traced back to the behavior of BZT's polar nanoregions. We also
introduce a simple Landau-based model that reproduces the EC behavior of BZT as
a function of field and temperature, and which is directly applicable to other
compounds. Finally, we confirm that, for low temperatures (i.e., in non-ergodic
conditions), the usual indirect approach to measure the EC response provides an
estimate that differs quantitatively from a direct evaluation of the
field-induced temperature change.
",physics
"  The problem of three-user multiple-access channel (MAC) with noiseless
feedback is investigated. A new coding strategy is presented. The coding scheme
builds upon the natural extension of the Cover-Leung (CL) scheme; and uses
quasi-linear codes. A new single-letter achievable rate region is derived. The
new achievable region strictly contains the CL region. This is shown through an
example. In this example, the coding scheme achieves optimality in terms of
transmission rates. It is shown that any optimality achieving scheme for this
example must have a specific algebraic structure. Particularly, the codebooks
must be closed under binary addition.
",computer-science
"  To resolve conflicts among norms, various nonmonotonic formalisms can be used
to perform prioritized normative reasoning. Meanwhile, formal argumentation
provides a way to represent nonmonotonic logics. In this paper, we propose a
representation of prioritized normative reasoning by argumentation. Using
hierarchical abstract normative systems, we define three kinds of prioritized
normative reasoning approaches, called Greedy, Reduction, and Optimization.
Then, after formulating an argumentation theory for a hierarchical abstract
normative system, we show that for a totally ordered hierarchical abstract
normative system, Greedy and Reduction can be represented in argumentation by
applying the weakest link and the last link principles respectively, and
Optimization can be represented by introducing additional defeats capturing the
idea that for each argument that contains a norm not belonging to the maximal
obeyable set then this argument should be rejected.
",computer-science
"  I present a family of algorithms to reduce noise in astrophysical im- ages
and image sequences, preserving more information from the original data than is
retained by conventional techniques. The family uses locally adaptive filters
(""noise gates"") in the Fourier domain, to separate coherent image structure
from background noise based on the statistics of local neighborhoods in the
image. Processing of solar data limited by simple shot noise or by additive
noise reveals image structure not easily visible in the originals, preserves
photometry of observable features, and reduces shot noise by a factor of 10 or
more with little to no apparent loss of resolution, revealing faint features
that were either not directly discernible or not sufficiently strongly detected
for quantitative analysis. The method works best on image sequences containing
related subjects, for example movies of solar evolution, but is also applicable
to single images provided that there are enough pixels. The adaptive filter
uses the statistical properties of noise and of local neighborhoods in the
data, to discriminate between coherent features and incoherent noise without
reference to the specific shape or evolution of the those features. The
technique can potentially be modified in a straightforward way to exploit
additional a priori knowledge about the functional form of the noise.
",physics
"  Resting-state functional Arterial Spin Labeling (rs-fASL) in clinical daily
practice and academic research stay discreet compared to resting-state BOLD.
However, by giving direct access to cerebral blood flow maps, rs-fASL leads to
significant clinical subject scaled application as CBF can be considered as a
biomarker in common neuropathology. Our work here focuses on the link between
overall quality of rs-fASL and duration of acquisition. To this end, we
consider subject self-Default Mode Network (DMN), and assess DMN quality
depletion compared to a gold standard DMN depending on the duration of
acquisition.
",quantitative-biology
"  The demand for single photon sources at $\lambda~=~1.54~\mu$m, which follows
from the consistent development of quantum networks based on commercial optical
fibers, makes Er:O$_x$ centers in Si still a viable resource thanks to the
optical transition of $Er^{3+}~:~^4I_{13/2}~\rightarrow~^4I_{15/2}$. Yet, to
date, the implementation of such system remains hindered by its extremely low
emission rate. In this Letter, we explore the room-temperature
photoluminescence (PL) at the telecomm wavelength of very low implantation
doses of $Er:O_x$ in $Si$. The emitted photons, excited by a $\lambda~=~792~nm$
laser in both large areas and confined dots of diameter down to $5~\mu$m, are
collected by an inverted confocal microscope. The lower-bound number of
detectable emission centers within our diffraction-limited illumination spot is
estimated to be down to about 10$^4$, corresponding to an emission rate per
individual ion of about $4~\times~10^{3}$ photons/s.
",physics
"  The total mass M_GCS in the globular cluster (GC) system of a galaxy is
empirically a near-constant fraction of the total mass M_h = M_bary + M_dark of
the galaxy, across a range of 10^5 in galaxy mass. This trend is radically
unlike the strongly nonlinear behavior of total stellar mass M_star versus M_h.
We discuss extensions of this trend to two more extreme situations: (a) entire
clusters of galaxies, and (b) the Ultra-Diffuse Galaxies (UDGs) recently
discovered in Coma and elsewhere. Our calibration of the ratio \eta_M = M_GCS /
M_h from normal galaxies, accounting for new revisions in the adopted
mass-to-light ratio for GCs, now gives \eta_M = 2.9 \times 10^{-5} as the mean
absolute mass fraction. We find that the same ratio appears valid for galaxy
clusters and UDGs. Estimates of \eta_M in the four clusters we examine tend to
be slightly higher than for individual galaxies, butmore data and better
constraints on the mean GC mass in such systems are needed to determine if this
difference is significant. We use the constancy of \eta_M to estimate total
masses for several individual cases; for example, the total mass of the Milky
Way is calculated to be M_h = 1.1 \times 10^{12} M_sun. Physical explanations
for the uniformity of \eta_M are still descriptive, but point to a picture in
which massive, dense star clusters in their formation stages were relatively
immune to the feedback that more strongly influenced lower-density regions
where most stars form.
",physics
"  Bandit based optimisation has a remarkable advantage over gradient based
approaches due to their global perspective, which eliminates the danger of
getting stuck at local optima. However, for continuous optimisation problems or
problems with a large number of actions, bandit based approaches can be
hindered by slow learning. Gradient based approaches, on the other hand,
navigate quickly in high-dimensional continuous spaces through local
optimisation, following the gradient in fine grained steps. Yet, apart from
being susceptible to local optima, these schemes are less suited for online
learning due to their reliance on extensive trial-and-error before the optimum
can be identified. In this paper, we propose a Bayesian approach that unifies
the above two paradigms in one single framework, with the aim of combining
their advantages. At the heart of our approach we find a stochastic linear
approximation of the function to be optimised, where both the gradient and
values of the function are explicitly captured. This allows us to learn from
both noisy function and gradient observations, and predict these properties
across the action space to support optimisation. We further propose an
accompanying bandit driven exploration scheme that uses Bayesian credible
bounds to trade off exploration against exploitation. Our empirical results
demonstrate that by unifying bandit and gradient based learning, one obtains
consistently improved performance across a wide spectrum of problem
environments. Furthermore, even when gradient feedback is unavailable, the
flexibility of our model, including gradient prediction, still allows us
outperform competing approaches, although with a smaller margin. Due to the
pervasiveness of bandit based optimisation, our scheme opens up for improved
performance both in meta-optimisation and in applications where gradient
related information is readily available.
",computer-science
"  We construct examples of flat fiber bundles over the Hopf surface such that
the total spaces have no pseudoconvex neighborhood basis, admit a complete
Kähler metric, or are hyperconvex but have no nonconstant holomorphic
functions. For any compact Riemannian surface of positive genus, we construct a
flat $\mathbb P^1$ bundle over it and a Stein domain with real analytic bundary
in it whose closure does not have pseudoconvex neighborhood basis. For a
compact complex manifold with positive first Betti number, we construct a flat
disc bundle over it such that the total space is hyperconvex but admits no
nonconstant holomorphic functions.
",mathematics
"  The Short-Baseline Neutrino (SBN) Program is a short-baseline neutrino
oscillation experiment in the Booster Neutrino Beam-line (BNB) at Fermilab. It
consists of three Liquid Argon Time Projection Chambers (LArTPCs) from the
Short-Baseline Near Detector (SBND), Micro Booster Neutrino Experiment
(MicroBooNE), and Imaging Cosmic And Rare Underground Signals (ICARUS)
experiments. The SBN Program will definitively search for short-baseline
neutrino oscillations in the 1 eV mass range, make precision neutrino-argon
interaction measurements, and further develop the LArTPC technology. The
physics program and current status of the program, and its constituent
experiments, are presented.
",physics
"  Autonomous agents must often detect affordances: the set of behaviors enabled
by a situation. Affordance detection is particularly helpful in domains with
large action spaces, allowing the agent to prune its search space by avoiding
futile behaviors. This paper presents a method for affordance extraction via
word embeddings trained on a Wikipedia corpus. The resulting word vectors are
treated as a common knowledge database which can be queried using linear
algebra. We apply this method to a reinforcement learning agent in a text-only
environment and show that affordance-based action selection improves
performance most of the time. Our method increases the computational complexity
of each learning step but significantly reduces the total number of steps
needed. In addition, the agent's action selections begin to resemble those a
human would choose.
",computer-science
"  We present a projectively invariant description of planar linear 3-webs and
construct a counterexample to Gronwall's conjecture.
",mathematics
"  Travel time on a route varies substantially by time of day and from day to
day. It is critical to understand to what extent this variation is correlated
with various factors, such as weather, incidents, events or travel demand level
in the context of dynamic networks. This helps a better decision making for
infrastructure planning and real-time traffic operation. We propose a
data-driven approach to understand and predict highway travel time using
spatio-temporal features of those factors, all of which are acquired from
multiple data sources. The prediction model holistically selects the most
related features from a high-dimensional feature space by correlation analysis,
principle component analysis and LASSO. We test and compare the performance of
several regression models in predicting travel time 30 min in advance via two
case studies: (1) a 6-mile highway corridor of I-270N in D.C. region, and (2) a
2.3-mile corridor of I-376E in Pittsburgh region. We found that some
bottlenecks scattered in the network can imply congestion on those corridors at
least 30 minutes in advance, including those on the alternative route to the
corridors of study. In addition, real-time travel time is statistically related
to incidents on some specific locations, morning/afternoon travel demand,
visibility, precipitation, wind speed/gust and the weather type. All those
spatio-temporal information together help improve prediction accuracy,
comparing to using only speed data. In both case studies, random forest shows
the most promise, reaching a root-mean-squared error of 16.6\% and 17.0\%
respectively in afternoon peak hours for the entire year of 2014.
",statistics
"  For VSLAM (Visual Simultaneous Localization and Mapping), localization is a
challenging task, especially for some challenging situations: textureless
frames, motion blur, etc.. To build a robust exploration and localization
system in a given space or environment, a submap-based VSLAM system is proposed
in this paper. Our system uses a submap back-end and a visual front-end. The
main advantage of our system is its robustness with respect to tracking
failure, a common problem in current VSLAM algorithms. The robustness of our
system is compared with the state-of-the-art in terms of average tracking
percentage. The precision of our system is also evaluated in terms of ATE
(absolute trajectory error) RMSE (root mean square error) comparing the
state-of-the-art. The ability of our system in solving the `kidnapped' problem
is demonstrated. Our system can improve the robustness of visual localization
in challenging situations.
",computer-science
"  Motivated by recent experiments with two-component Bose-Einstein condensates,
we study fully-connected spin models subject to an additional constraint. The
constraint is responsible for the Hilbert space dimension to scale only
linearly with the system size. We discuss the unconventional statistical
physical and thermodynamic properties of such a system, in particular the
absence of concentration of the underlying probability distributions. As a
consequence, expectation values are less suitable to characterize such systems,
and full distribution functions are required instead. Sharp signatures of phase
transitions do not occur in such a setting, but transitions from singly peaked
to doubly peaked distribution functions of an ""order parameter"" may be present.
",physics
"  No firm evidence has existed that the ancient Maya civilization recorded
specific occurrences of meteor showers or outbursts in the corpus of Maya
hieroglyphic inscriptions. In fact, there has been no evidence of any
pre-Hispanic civilization in the Western Hemisphere recording any observations
of any meteor showers on any specific dates.
The authors numerically integrated meteoroid-sized particles released by
Comet Halley as early as 1404 BC to identify years within the Maya Classic
Period, AD 250-909, when Eta Aquariid outbursts might have occurred. Outbursts
determined by computer model were then compared to specific events in the Maya
record to see if any correlation existed between the date of the event and the
date of the outburst. The model was validated by successfully explaining
several outbursts around the same epoch in the Chinese record. Some outbursts
observed by the Maya were due to recent revolutions of Comet Halley, within a
few centuries, and some to resonant behavior in older Halley trails, of the
order of a thousand years. Examples were found of several different Jovian mean
motion resonances as well as the 1:3 Saturnian resonance that have controlled
the dynamical evolution of meteoroids in apparently observed outbursts.
",physics
"  The mechanical failure of amorphous media is a ubiquitous phenomenon from
material engineering to geology. It has been noticed for a long time that the
phenomenon is ""scale-free"", indicating some type of criticality. In spite of
attempts to invoke ""Self-Organized Criticality"", the physical origin of this
criticality, and also its universal nature, being quite insensitive to the
nature of microscopic interactions, remained elusive. Recently we proposed that
the precise nature of this critical behavior is manifested by a spinodal point
of a thermodynamic phase transition. Moreover, at the spinodal point there
exists a divergent correlation length which is associated with the
system-spanning instabilities (known also as shear bands) which are typical to
the mechanical yield. Demonstrating this requires the introduction of an ""order
parameter"" that is suitable for distinguishing between disordered amorphous
systems, and an associated correlation function, suitable for picking up the
growing correlation length. The theory, the order parameter, and the
correlation functions used are universal in nature and can be applied to any
amorphous solid that undergoes mechanical yield. Critical exponents for the
correlation length divergence and the system size dependence are estimated. The
phenomenon is seen at its sharpest in athermal systems, as is explained below;
in this paper we extend the discussion also to thermal systems, showing that at
sufficiently high temperatures the spinodal phenomenon is destroyed by thermal
fluctuations.
",physics
"  The aim of this paper is to study a poset isomorphism between two support
$\tau$-tilting posets. We take several algebraic information from combinatorial
properties of support $\tau$-tilting posets. As an application, we treat a
certain class of basic algebras which contains preprojective algebras of type
$A$, Nakayama algebras, and generalized Brauer tree algebras. We provide a
necessary condition for that an algebra $\Lambda$ share the same support
$\tau$-tilting poset with a given algebra $\Gamma$ in this class. Furthermore,
we see that this necessary condition is also a sufficient condition if $\Gamma$
is either a preprojective algebra of type $A$, a Nakayama algebra, or a
generalized Brauer tree algebra.
",mathematics
"  We study the normal closure of a big power of one or several Dehn twists in a
Mapping Class Group. We prove that it has a presentation whose relators
consists only of commutators between twists of disjoint support, thus answering
a question of Ivanov. Our method is to use the theory of projection complexes
of Bestvina Bromberg and Fujiwara, together with the theory of rotating
families, simultaneously on several spaces.
",mathematics
"  Often, more time is spent on finding a model that works well, rather than
tuning the model and working directly with the dataset. Our research began as
an attempt to improve upon a simple Recurrent Neural Network for answering
""simple"" first-order questions (QA-RNN), developed by Ferhan Ture and Oliver
Jojic, from Comcast Labs, using the SimpleQuestions dataset. Their baseline
model, a bidirectional, 2-layer LSTM RNN and a GRU RNN, have accuracies of 0.94
and 0.90, for entity detection and relation prediction, respectively. We fine
tuned these models by doing substantial hyper-parameter tuning, getting
resulting accuracies of 0.70 and 0.80, for entity detection and relation
prediction, respectively. An accuracy of 0.984 was obtained on entity detection
using a 1-layer LSTM, where preprocessing was done by removing all words not
part of a noun chunk from the question. 100% of the dataset was available for
relation prediction, but only 20% of the dataset, was available for entity
detection, which we believe to be much of the reason for our initial
difficulties in replicating their result, despite the fact we were able to
improve on their entity detection results.
",statistics
"  In this work we investigate a one-dimensional parity-time (PT)-symmetric
magnetic metamaterial consisting of split-ring dimers having gain or loss.
Employing a Melnikov analysis we study the existence of localized travelling
waves, i.e. homoclinic or heteroclinic solutions. We find conditions under
which the homoclinic or heteroclinic orbits persist. Our analytical results are
found to be in good agreement with direct numerical computations. For the
particular nonlinearity admitting travelling kinks, numerically we observe
homoclinic snaking in the bifurcation diagram. The Melnikov analysis yields a
good approximation to one of the boundaries of the snaking profile.
",physics
"  Using cohomological methods, we prove a criterion for the embedding of a
group extension with abelian kernel into the split extension of a co-induced
module. This generalises some earlier similar results. We also prove an
assertion about the conjugacy of complements in split extensions of co-induced
modules. Both results follow from a relation between homomorphisms of certain
cohomology groups.
",mathematics
"  We show that certain orderable groups admit no isolated left orders. The
groups we consider are cyclic amalgamations of a free group with a general
orderable group, the HNN extensions of free groups over cyclic subgroups, and a
particular class of one-relator groups. In order to prove the results about
orders, we develop perturbation techniques for actions of these groups on the
line.
",mathematics
"  Steady State Superconducting Tokamak (SST-1) at the Institute for Plasma
Research (IPR) is an operational device and is the first superconducting
Tokamak in India. Superconducting Magnets System (SCMS) in SST-1 comprises of
sixteen Toroidal field (TF) magnets and nine Poloidal Field (PF) magnets
manufactured using NbTi/Cu based cable-in-conduit-conductor (CICC) concept.
SST-1, superconducting TF magnets are operated in a Cryo-stable manner being
cooled with two-phase (TP) flow helium. The typical operating pressure of the
TP helium is 1.6 bar (a) at corresponding saturation temperature. The SCMS has
a typical cool-down time of about 14 days from 300 K down to 4.5 K using Helium
plant of equivalent cooling capacity of 1350 W at 4.5 K. Using the onset of
experimental data from the HRL, we estimated the vapor quality for the input
heat load on to the TF magnets system. In this paper, we report the
characteristics of two-phase flow for given thermo-hydraulic conditions during
long steady state operation of the SST-1 TF magnets. Finally, the
experimentally obtained results have been compared with the well-known
correlations of two-phase flow.
",physics
"  While the Bayesian Information Criterion (BIC) and Akaike Information
Criterion (AIC) are powerful tools for model selection in linear regression,
they are built on different prior assumptions and thereby apply to different
data generation scenarios. We show that in the finite-dimensional case their
respective assumptions can be unified within an augmented model-plus-noise
space and construct a prior in this space which inherits the beneficial
properties of both AIC and BIC. This allows us to adapt the BIC to be robust
against misspecified models where the signal to noise ratio is low.
",statistics
"  Despite their impressive performance, Deep Neural Networks (DNNs) typically
underperform Gradient Boosting Trees (GBTs) on many tabular-dataset learning
tasks. We propose that applying a different regularization coefficient to each
weight might boost the performance of DNNs by allowing them to make more use of
the more relevant inputs. However, this will lead to an intractable number of
hyperparameters. Here, we introduce Regularization Learning Networks (RLNs),
which overcome this challenge by introducing an efficient hyperparameter tuning
scheme which minimizes a new Counterfactual Loss. Our results show that RLNs
significantly improve DNNs on tabular datasets, and achieve comparable results
to GBTs, with the best performance achieved with an ensemble that combines GBTs
and RLNs. RLNs produce extremely sparse networks, eliminating up to 99.8% of
the network edges and 82% of the input features, thus providing more
interpretable models and reveal the importance that the network assigns to
different inputs. RLNs could efficiently learn a single network in datasets
that comprise both tabular and unstructured data, such as in the setting of
medical imaging accompanied by electronic health records. An open source
implementation of RLN can be found at
this https URL.
",statistics
"  In this paper, we proposed a non-uniform power delivery network (PDN)
synthesis methodology. It first constructs initial PDN using uniform approach.
Then preliminary power integrity analysis is performed to derive IR-safe
candidate window. Congestion map is obtained based global route congestion
estimation. A self-adaptive non-uniform PDN synthesis is then performed to
globally and locally optimize PDN over selected regions. The PDN synthesis is
congestion-driven and IR- guarded. Experimental results show significant timing
important in trade-off small PDN length reduction with no EM/IR impact. We
further explored potential power savings using our non-uniform PDN synthesis
methodology.
",computer-science
"  In the late 1980s, Premet conjectured that the nilpotent variety of any
finite dimensional restricted Lie algebra over an algebraically closed field of
characteristic $p>0$ is irreducible. This conjecture remains open, but it is
known to hold for a large class of simple restricted Lie algebras, e.g. for Lie
algebras of connected reductive algebraic groups, and for Cartan series $W, S$
and $H$. In this paper, with the assumption that $p>3$, we confirm this
conjecture for the minimal $p$-envelope $W(1;n)_p$ of the Zassenhaus algebra
$W(1;n)$ for all $n\geq 2$.
",mathematics
"  Let $\mathcal{V}_p(\lambda)$ be the collection of all functions $f$ defined
in the unit disc $\ID$ having a simple pole at $z=p$ where $0<p<1$ and analytic
in $\ID\setminus\{p\}$ with $f(0)=0=f'(0)-1$ and satisfying the differential
inequality $|(z/f(z))^2 f'(z)-1|< \lambda $ for $z\in \ID$, $0<\lambda\leq 1$.
Each $f\in\mathcal{V}_p(\lambda)$ has the following Taylor expansion:
$$
f(z)=z+\sum_{n=2}^{\infty}a_n(f) z^n, \quad |z|<p.
$$
In \cite{BF-3}, we conjectured that
$$
|a_n(f)|\leq \frac{1-(\lambda p^2)^n}{p^{n-1}(1-\lambda p^2)}\quad
\mbox{for}\quad n\geq3. $$ In the present article, we first obtain a
representation formula for functions in the class $\mathcal{V}_p(\lambda)$.
Using this representation, we prove the aforementioned conjecture for $n=3,4,5$
whenever $p$ belongs to certain subintervals of $(0,1)$. Also we determine non
sharp bounds for $|a_n(f)|,\,n\geq 3$ and for $|a_{n+1}(f)-a_n(f)/p|,\,n\geq
2$.
",mathematics
"  Although a majority of the theoretical literature in high-dimensional
statistics has focused on settings which involve fully-observed data, settings
with missing values and corruptions are common in practice. We consider the
problems of estimation and of constructing component-wise confidence intervals
in a sparse high-dimensional linear regression model when some covariates of
the design matrix are missing completely at random. We analyze a variant of the
Dantzig selector [9] for estimating the regression model and we use a
de-biasing argument to construct component-wise confidence intervals. Our first
main result is to establish upper bounds on the estimation error as a function
of the model parameters (the sparsity level s, the expected fraction of
observed covariates $\rho_*$, and a measure of the signal strength
$\|\beta^*\|_2$). We find that even in an idealized setting where the
covariates are assumed to be missing completely at random, somewhat
surprisingly and in contrast to the fully-observed setting, there is a
dichotomy in the dependence on model parameters and much faster rates are
obtained if the covariance matrix of the random design is known. To study this
issue further, our second main contribution is to provide lower bounds on the
estimation error showing that this discrepancy in rates is unavoidable in a
minimax sense. We then consider the problem of high-dimensional inference in
the presence of missing data. We construct and analyze confidence intervals
using a de-biased estimator. In the presence of missing data, inference is
complicated by the fact that the de-biasing matrix is correlated with the pilot
estimator and this necessitates the design of a new estimator and a novel
analysis. We also complement our mathematical study with extensive simulations
on synthetic and semi-synthetic data that show the accuracy of our asymptotic
predictions for finite sample sizes.
",statistics
"  We compute the effects of generic short-range interactions on gapless
electrons residing at the quantum critical point separating a two-dimensional
Dirac semimetal (DSM) and a symmetry-preserving band insulator (BI). The
electronic dispersion at this critical point is anisotropic ($E_{\mathbf k}=\pm
\sqrt{v^2 k^2_x + b^2 k^{2n}_y}$ with $n=2$), which results in unconventional
scaling of physical observables. Due to the vanishing density of states
($\varrho(E) \sim |E|^{1/n}$), this anisotropic semimetal (ASM) is stable
against weak short-range interactions. However, for stronger interactions the
direct DSM-BI transition can either $(i)$ become a first-order transition, or
$(ii)$ get avoided by an intervening broken-symmetry phase (BSP). We perform a
renormalization group analysis by perturbing away from the one-dimensional
limit with the small parameter $\epsilon = 1/n$, augmented with a $1/n$
expansion (parametrically suppressing quantum fluctuations in higher
dimension). We identify charge density wave (CDW), antiferromagnet (AFM) and
singlet s-wave superconductor as the three dominant candidates for the BSP. The
onset of any such order at strong coupling $(\sim \epsilon)$ takes place
through a continuous quantum phase transition across multicritical point. We
also present the phase diagram of an extended Hubbard model for the ASM,
obtained via the controlled deformation of its counterpart in one dimension.
The latter displays spin-charge separation and instabilities to CDW, spin
density wave, and Luther-Emery liquid phases at arbitrarily weak coupling. The
spin density wave and Luther-Emery liquid phases deform into pseudospin
SU(2)-symmetric quantum critical points separating the ASM from the AFM and
superconducting orders, respectively. Our results can be germane for a
uniaxially strained honeycomb lattice or organic compound
$\alpha$-(BEDT-TTF)$_2\text{I}_3$.
",physics
"  This study presents systems submitted by the University of Texas at Dallas,
Center for Robust Speech Systems (UTD-CRSS) to the MGB-3 Arabic Dialect
Identification (ADI) subtask. This task is defined to discriminate between five
dialects of Arabic, including Egyptian, Gulf, Levantine, North African, and
Modern Standard Arabic. We develop multiple single systems with different
front-end representations and back-end classifiers. At the front-end level,
feature extraction methods such as Mel-frequency cepstral coefficients (MFCCs)
and two types of bottleneck features (BNF) are studied for an i-Vector
framework. As for the back-end level, Gaussian back-end (GB), and Generative
Adversarial Networks (GANs) classifiers are applied alternately. The best
submission (contrastive) is achieved for the ADI subtask with an accuracy of
76.94% by augmenting the randomly chosen part of the development dataset.
Further, with a post evaluation correction in the submitted system, final
accuracy is increased to 79.76%, which represents the best performance achieved
so far for the challenge on the test dataset.
",computer-science
"  Seasonal patterns associated with stress modulation, as evidenced by
earthquake occurrence, have been detected in regions characterized by present
day mountain building and glacial retreat in the Northern Hemisphere. In the
Himalaya and the Alps, seismicity is peaking in spring and summer; opposite
behaviour is observed in the Apennines. This diametrical behaviour, confirmed
by recent strong earthquakes, well correlates with the dominant tectonic
regime: peak in spring and summer in shortening areas, peak in fall and winter
in extensional areas. The analysis of the seasonal effect is extended to
several shortening (e.g. Zagros and Caucasus) and extensional regions, and
counter-examples from regions where no seasonal modulation is expected (e.g.
Tropical Atlantic Ridge). This study generalizes to different seismotectonic
settings the early observations made about short-term (seasonal) and long-term
(secular) modulation of seismicity and confirms, with some statistical
significance, that snow and ice thaw may cause crustal deformations that
modulate the occurrence of major earthquakes.
",physics
"  Music creation is typically composed of two parts: composing the musical
score, and then performing the score with instruments to make sounds. While
recent work has made much progress in automatic music generation in the
symbolic domain, few attempts have been made to build an AI model that can
render realistic music audio from musical scores. Directly synthesizing audio
with sound sample libraries often leads to mechanical and deadpan results,
since musical scores do not contain performance-level information, such as
subtle changes in timing and dynamics. Moreover, while the task may sound like
a text-to-speech synthesis problem, there are fundamental differences since
music audio has rich polyphonic sounds. To build such an AI performer, we
propose in this paper a deep convolutional model that learns in an end-to-end
manner the score-to-audio mapping between a symbolic representation of music
called the piano rolls and an audio representation of music called the
spectrograms. The model consists of two subnets: the ContourNet, which uses a
U-Net structure to learn the correspondence between piano rolls and
spectrograms and to give an initial result; and the TextureNet, which further
uses a multi-band residual network to refine the result by adding the spectral
texture of overtones and timbre. We train the model to generate music clips of
the violin, cello, and flute, with a dataset of moderate size. We also present
the result of a user study that shows our model achieves higher mean opinion
score (MOS) in naturalness and emotional expressivity than a WaveNet-based
model and two commercial sound libraries. We open our source code at
this https URL
",computer-science
"  When performing localization and mapping, working at the level of structure
can be advantageous in terms of robustness to environmental changes and
differences in illumination. This paper presents SegMap: a map representation
solution to the localization and mapping problem based on the extraction of
segments in 3D point clouds. In addition to facilitating the computationally
intensive task of processing 3D point clouds, working at the level of segments
addresses the data compression requirements of real-time single- and
multi-robot systems. While current methods extract descriptors for the single
task of localization, SegMap leverages a data-driven descriptor in order to
extract meaningful features that can also be used for reconstructing a dense 3D
map of the environment and for extracting semantic information. This is
particularly interesting for navigation tasks and for providing visual feedback
to end-users such as robot operators, for example in search and rescue
scenarios. These capabilities are demonstrated in multiple urban driving and
search and rescue experiments. Our method leads to an increase of area under
the ROC curve of 28.3% over current state of the art using eigenvalue based
features. We also obtain very similar reconstruction capabilities to a model
specifically trained for this task. The SegMap implementation will be made
available open-source along with easy to run demonstrations at
www.github.com/ethz-asl/segmap. A video demonstration is available at
this https URL.
",computer-science
"  Tomography has made a radical impact on diverse fields ranging from the study
of 3D atomic arrangements in matter to the study of human health in medicine.
Despite its very diverse applications, the core of tomography remains the same,
that is, a mathematical method must be implemented to reconstruct the 3D
structure of an object from a number of 2D projections. In many scientific
applications, however, the number of projections that can be measured is
limited due to geometric constraints, tolerable radiation dose and/or
acquisition speed. Thus it becomes an important problem to obtain the
best-possible reconstruction from a limited number of projections. Here, we
present the mathematical implementation of a tomographic algorithm, termed
GENeralized Fourier Iterative REconstruction (GENFIRE). By iterating between
real and reciprocal space, GENFIRE searches for a global solution that is
concurrently consistent with the measured data and general physical
constraints. The algorithm requires minimal human intervention and also
incorporates angular refinement to reduce the tilt angle error. We demonstrate
that GENFIRE can produce superior results relative to several other popular
tomographic reconstruction techniques by numerical simulations, and by
experimentally by reconstructing the 3D structure of a porous material and a
frozen-hydrated marine cyanobacterium. Equipped with a graphical user
interface, GENFIRE is freely available from our website and is expected to find
broad applications across different disciplines.
",physics
"  We present a quantization of an isomorphism of Mirković and Vybornov which
relates the intersection of a Slodowy slice and a nilpotent orbit closure in
$\mathfrak{gl}_N$ , to a slice between spherical Schubert varieties in the
affine Grassmannian of $PGL_n$ (with weights encoded by the Jordan types of the
nilpotent orbits). A quantization of the former variety is provided by a
parabolic W-algebra and of the latter by a truncated shifted Yangian. Building
on earlier work of Brundan and Kleshchev, we define an explicit isomorphism
between these non-commutative algebras, and show that its classical limit is a
variation of the original isomorphism of Mirković and Vybornov. As a
corollary, we deduce that the W-algebra is free as a left (or right) module
over its Gelfand-Tsetlin subalgebra, as conjectured by Futorny, Molev, and
Ovsienko.
",mathematics
"  Given a holomorphic principal bundle $Q\, \longrightarrow\, X$, the universal
space of holomorphic connections is a torsor $C_1(Q)$ for $\text{ad} Q \otimes
T^*X$ such that the pullback of $Q$ to $C_1(Q)$ has a tautological holomorphic
connection. When $X\,=\, G/P$, where $P$ is a parabolic subgroup of a complex
simple group $G$, and $Q$ is the frame bundle of an ample line bundle, we show
that $C_1(Q)$ may be identified with $G/L$, where $L\, \subset\, P$ is a Levi
factor. We use this identification to construct the twistor space associated to
a natural hyper-Kähler metric on $T^*(G/P)$, recovering Biquard's description
of this twistor space, but employing only finite-dimensional, Lie-theoretic
means.
",mathematics
"  The Greenberger-Horne-Zeilinger (GHZ) argument provides an all-or-nothing
contradiction between quantum mechanics and local-realistic theories. In its
original formulation, GHZ investigated three and four particles entangled in
two dimensions only. Very recently, higher dimensional contradictions
especially in three dimensions and three particles have been discovered but it
has remained unclear how to produce such states. In this article we
experimentally show how to generate a three-dimensional GHZ state from
two-photon orbital-angular-momentum entanglement. The first suggestion for a
setup which generates three-dimensional GHZ entanglement from these entangled
pairs came from using the computer algorithm Melvin. The procedure employs
novel concepts significantly beyond the qubit case. Our experiment opens up the
possibility of a truly high-dimensional test of the GHZ-contradiction which,
interestingly, employs non-Hermitian operators.
",physics
"  In a recent paper [A. Alberucci, C. Jisha, N. Smyth, and G. Assanto, Phys.
Rev. A 91, 013841 (2015)], Alberucci et al. have studied the propagation of
bright spatial solitary waves in highly nonlocal media. We find that the main
results in that and related papers, concerning soliton shape and dynamics,
based on the accessible soliton (AS) approximation, are incorrect; the correct
results have already been published by others. These and other inconsistencies
in the paper follow from the problems in applying the AS approximation in
earlier papers by the group that propagated to the later papers. The accessible
soliton theory cannot describe accurately the features and dynamics of solitons
in highly nonlocal media.
",physics
"  We perform a detailed analytical study of the Recent Fluid Deformation (RFD)
model for the onset of Lagrangian intermittency, within the context of the
Martin-Siggia-Rose-Janssen-de Dominicis (MSRJD) path integral formalism. The
model is based, as a key point, upon local closures for the pressure Hessian
and the viscous dissipation terms in the stochastic dynamical equations for the
velocity gradient tensor. We carry out a power counting hierarchical
classification of the several perturbative contributions associated to
fluctuations around the instanton-evaluated MSRJD action, along the lines of
the cumulant expansion. The most relevant Feynman diagrams are then integrated
out into the renormalized effective action, for the computation of velocity
gradient probability distribution functions (vgPDFs). While the subleading
perturbative corrections do not affect the global shape of the vgPDFs in an
appreciable qualitative way, it turns out that they have a significant role in
the accurate description of their non-Gaussian cores.
",physics
"  User-based Collaborative Filtering (CF) is one of the most popular approaches
to create recommender systems. This approach is based on finding the most
relevant k users from whose rating history we can extract items to recommend.
CF, however, suffers from data sparsity and the cold-start problem since users
often rate only a small fraction of available items. One solution is to
incorporate additional information into the recommendation process such as
explicit trust scores that are assigned by users to others or implicit trust
relationships that result from social connections between users. Such
relationships typically form a very sparse trust network, which can be utilized
to generate recommendations for users based on people they trust. In our work,
we explore the use of a measure from network science, i.e. regular equivalence,
applied to a trust network to generate a similarity matrix that is used to
select the k-nearest neighbors for recommending items. We evaluate our approach
on Epinions and we find that we can outperform related methods for tackling
cold-start users in terms of recommendation accuracy.
",computer-science
"  An equation-by-equation (EBE) method is proposed to solve a system of
nonlinear equations arising from the moment constrained maximum entropy problem
of multidimensional variables. The design of the EBE method combines ideas from
homotopy continuation and Newton's iterative methods. Theoretically, we
establish the local convergence under appropriate conditions and show that the
proposed method, geometrically, finds the solution by searching along the
surface corresponding to one component of the nonlinear problem. We will
demonstrate the robustness of the method on various numerical examples,
including: (1) A six-moment one-dimensional entropy problem with an explicit
solution that contains components of order $10^0-10^3$ in magnitude; (2)
Four-moment multidimensional entropy problems with explicit solutions where the
resulting systems to be solved ranging from $70-310$ equations; (3) Four- to
eight-moment of a two-dimensional entropy problem, which solutions correspond
to the densities of the two leading EOFs of the wind stress-driven large-scale
oceanic model. In this case, we find that the EBE method is more accurate
compared to the classical Newton's method, the MATLAB generic solver, and the
previously developed BFGS-based method, which was also tested on this problem.
(4) Four-moment constrained of up to five-dimensional entropy problems which
solutions correspond to multidimensional densities of the components of the
solutions of the Kuramoto-Sivashinsky equation. For the higher dimensional
cases of this example, the EBE method is superior because it automatically
selects a subset of the prescribed moment constraints from which the maximum
entropy solution can be estimated within the desired tolerance. This selection
feature is particularly important since the moment constrained maximum entropy
problems do not necessarily have solutions in general.
",mathematics
"  The RNiO$_3$ perovskites are known to order antiferromagnetically below a
material-dependent Néel temperature $T_\text{N}$. We report experimental
evidence indicating the existence of a second magnetically-ordered phase in
TlNiO$_3$ above $T_\text{N} = 104$ K, obtained using nuclear magnetic resonance
and muon spin rotation spectroscopy. The new phase, which persists up to a
temperature $T_\text{N}^* = 202$ K, is suppressed by the application of an
external magnetic field of approximately 1 T. It is not yet known if such a
phase also exists in other perovskite nickelates.
",physics
"  Graph games provide the foundation for modeling and synthesis of reactive
processes. Such games are played over graphs where the vertices are controlled
by two adversarial players. We consider graph games where the objective of the
first player is the conjunction of a qualitative objective (specified as a
parity condition) and a quantitative objective (specified as a mean-payoff
condition). There are two variants of the problem, namely, the threshold
problem where the quantitative goal is to ensure that the mean-payoff value is
above a threshold, and the value problem where the quantitative goal is to
ensure the optimal mean-payoff value; in both cases ensuring the qualitative
parity objective. The previous best-known algorithms for game graphs with $n$
vertices, $m$ edges, parity objectives with $d$ priorities, and maximal
absolute reward value $W$ for mean-payoff objectives, are as follows:
$O(n^{d+1} \cdot m \cdot W)$ for the threshold problem, and $O(n^{d+2} \cdot m
\cdot W)$ for the value problem. Our main contributions are faster algorithms,
and the running times of our algorithms are as follows: $O(n^{d-1} \cdot m
\cdot W)$ for the threshold problem, and $O(n^{d} \cdot m \cdot W \cdot \log
(n\cdot W))$ for the value problem. For mean-payoff parity objectives with two
priorities, our algorithms match the best-known bounds of the algorithms for
mean-payoff games (without conjunction with parity objectives). Our results are
relevant in synthesis of reactive systems with both functional requirement
(given as a qualitative objective) and performance requirement (given as a
quantitative objective).
",computer-science
"  Eliminating duplicate data in primary storage of clouds increases the
cost-efficiency of cloud service providers as well as reduces the cost of users
for using cloud services. Existing primary deduplication techniques either use
inline caching to exploit locality in primary workloads or use post-processing
deduplication running in system idle time to avoid the negative impact on I/O
performance. However, neither of them works well in the cloud servers running
multiple services or applications for the following two reasons: Firstly, the
temporal locality of duplicate data writes may not exist in some primary
storage workloads thus inline caching often fails to achieve good deduplication
ratio. Secondly, the post-processing deduplication allows duplicate data to be
written into disks, therefore does not provide the benefit of I/O deduplication
and requires high peak storage capacity. This paper presents HPDedup, a Hybrid
Prioritized data Deduplication mechanism to deal with the storage system shared
by applications running in co-located virtual machines or containers by fusing
an inline and a post-processing process for exact deduplication. In the inline
deduplication phase, HPDedup gives a fingerprint caching mechanism that
estimates the temporal locality of duplicates in data streams from different
VMs or applications and prioritizes the cache allocation for these streams
based on the estimation. HPDedup also allows different deduplication threshold
for streams based on their spatial locality to reduce the disk fragmentation.
The post-processing phase removes duplicates whose fingerprints are not able to
be cached due to the weak temporal locality from disks. Our experimental
results show that HPDedup clearly outperforms the state-of-the-art primary
storage deduplication techniques in terms of inline cache efficiency and
primary deduplication efficiency.
",computer-science
"  Search engines play an important role in our everyday lives by assisting us
in finding the information we need. When we input a complex query, however,
results are often far from satisfactory. In this work, we introduce a query
reformulation system based on a neural network that rewrites a query to
maximize the number of relevant documents returned. We train this neural
network with reinforcement learning. The actions correspond to selecting terms
to build a reformulated query, and the reward is the document recall. We
evaluate our approach on three datasets against strong baselines and show a
relative improvement of 5-20% in terms of recall. Furthermore, we present a
simple method to estimate a conservative upper-bound performance of a model in
a particular environment and verify that there is still large room for
improvements.
",computer-science
"  Participants enrolled into randomized controlled trials (RCTs) often do not
reflect real-world populations. Previous research in how best to translate RCT
results to target populations has focused on weighting RCT data to look like
the target data. Simulation work, however, has suggested that an outcome model
approach may be preferable. Here we describe such an approach using source data
from the 2x2 factorial NAVIGATOR trial which evaluated the impact of valsartan
and nateglinide on cardiovascular outcomes and new-onset diabetes in a
pre-diabetic population. Our target data consisted of people with pre-diabetes
serviced at our institution. We used Random Survival Forests to develop
separate outcome models for each of the 4 treatments, estimating the 5-year
risk difference for progression to diabetes and estimated the treatment effect
in our local patient populations, as well as sub-populations, and the results
compared to the traditional weighting approach. Our models suggested that the
treatment effect for valsartan in our patient population was the same as in the
trial, whereas for nateglinide treatment effect was stronger than observed in
the original trial. Our effect estimates were more efficient than the weighting
approach.
",statistics
"  We investigate the extent to which the weak equivalences in a model category
can be equipped with algebraic structure. We prove, for instance, that there
exists a monad T such that a morphism of topological spaces admits T-algebra
structure if and only it is a weak homotopy equivalence. Likewise for
quasi-isomorphisms and many other examples. The basic trick is to consider
injectivity in arrow categories. Using algebraic injectivity and cone
injectivity we obtain general results about the extent to which the weak
equivalences in a combinatorial model category can be equipped with algebraic
structure.
",mathematics
"  We present a model to generate power spectrum noise with intensity
proportional to 1/f as a function of frequency f. The model arises from a
broken-symmetry variable which corresponds to absolute pitch, where
fluctuations occur in an attempt to restore that symmetry, influenced by
interactions in the creation of musical melodies.
",physics
"  Conjunctivochalasis is a common cause of tear dysfunction due to the
conjunctiva becoming loose and wrinkly with age. The current solutions to this
disease include either surgical excision in the operating room, or
thermoreduction of the loose tissue with hot wire in the clinic. We developed a
near-infrared (NIR) laser thermal conjunctivoplasty (LTC) system, which gently
shrinks the redundant tissue. The NIR light is mainly absorbed by water, so the
heating is even and there is no bleeding. The system utilizes a 1460-nm
programmable laser diode system as a light source. A miniaturized handheld
probe delivers the laser light and focuses the laser into a 10x1 mm2 line. A
foot pedal is used to deliver a preset number of calibrated laser pulses. A
fold of loose conjunctiva is grasped by a pair of forceps. The infrared laser
light is delivered through an optical fiber and a laser line is focused exactly
on the conjunctival fold by a cylindrical lens. Ex vivo experiments using
porcine eye were performed with the optimal laser parameters. It was found that
up to 50% of conjunctiva shrinkage could be achieved.
",physics
"  One of the fundamental results in computability is the existence of
well-defined functions that cannot be computed. In this paper we study the
effects of data representation on computability; we show that, while for each
possible way of representing data there exist incomputable functions, the
computability of a specific abstract function is never an absolute property,
but depends on the representation used for the function domain. We examine the
scope of this dependency and provide mathematical criteria to favour some
representations over others. As we shall show, there are strong reasons to
suggest that computational enumerability should be an additional axiom for
computation models. We analyze the link between the techniques and effects of
representation changes and those of oracle machines, showing an important
connection between their hierarchies. Finally, these notions enable us to gain
a new insight on the Church-Turing thesis: its interpretation as the underlying
algebraic structure to which computation is invariant.
",computer-science
"  The magnetic anisotropy (MA) of Mo/Au/Co0.9Fe0.1/Au/MgO(0.7 - 3
nm)/Au/Co0.9Fe0.1/Au heterostructure has been investigated at room temperature
as a function of MgO layer thickness (tMgO). Our studies show that while the MA
of the top layer does not change its character upon variation of tMgO, the
uniaxial out-of-plane MA of the bottom one undergoes a spin reorientation
transition at tMgO of about 0.8 nm, switching to the regime where the
coexistence of in- and out-of-plane magnetization alignments is observed. The
magnitudes of the magnetic anisotropy constants have been determined from
ferromagnetic resonance and dc-magnetometry measurements. The origin of MA
evolution has been attributed to a presence of an interlayer exchange coupling
(IEC) between Co0.9Fe0.1 layers through the thin MgO film.
",physics
"  Accurate on-device keyword spotting (KWS) with low false accept and false
reject rate is crucial to customer experience for far-field voice control of
conversational agents. It is particularly challenging to maintain low false
reject rate in real world conditions where there is (a) ambient noise from
external sources such as TV, household appliances, or other speech that is not
directed at the device (b) imperfect cancellation of the audio playback from
the device, resulting in residual echo, after being processed by the Acoustic
Echo Cancellation (AEC) system. In this paper, we propose a data augmentation
strategy to improve keyword spotting performance under these challenging
conditions. The training set audio is artificially corrupted by mixing in music
and TV/movie audio, at different signal to interference ratios. Our results
show that we get around 30-45% relative reduction in false reject rates, at a
range of false alarm rates, under audio playback from such devices.
",statistics
"  Large amount of image denoising literature focuses on single channel images
and often experimentally validates the proposed methods on tens of images at
most. In this paper, we investigate the interaction between denoising and
classification on large scale dataset. Inspired by classification models, we
propose a novel deep learning architecture for color (multichannel) image
denoising and report on thousands of images from ImageNet dataset as well as
commonly used imagery. We study the importance of (sufficient) training data,
how semantic class information can be traded for improved denoising results. As
a result, our method greatly improves PSNR performance by 0.34 - 0.51 dB on
average over state-of-the art methods on large scale dataset. We conclude that
it is beneficial to incorporate in classification models. On the other hand, we
also study how noise affect classification performance. In the end, we come to
a number of interesting conclusions, some being counter-intuitive.
",computer-science
"  The exciton relaxation dynamics of photoexcited electronic states in
poly($p$-phenylenevinylene) (PPV) are theoretically investigated within a
coarse-grained model, in which both the exciton and nuclear degrees of freedom
are treated quantum mechanically. The Frenkel-Holstein Hamiltonian is used to
describe the strong exciton-phonon coupling present in the system, while
external damping of the internal nuclear degrees of freedom are accounted for
by a Lindblad master equation. Numerically, the dynamics are computed using the
time evolving block decimation (TEBD) and quantum jump trajectory techniques.
The values of the model parameters physically relevant to polymer systems
naturally lead to a separation of time scales, with the ultra-fast dynamics
corresponding to energy transfer from the exciton to the internal phonon modes
(i.e., the C-C bond oscillations), while the longer time dynamics correspond to
damping of these phonon modes by the external dissipation. Associated with
these time scales, we investigate the following processes that are indicative
of the system relaxing onto the emissive chromophores of the polymer: 1)
Exciton-polaron formation occurs on an ultra-fast time scale, with the
associated exciton-phonon correlations present within half a vibrational time
period of the C-C bond oscillations. 2) Exciton decoherence is driven by the
decay in the vibrational overlaps associated with exciton-polaron formation,
occurring on the same time scale. 3) Exciton density localization is driven by
the external dissipation, arising from `wavefunction collapse' occurring as a
result of the system-environment interactions. Finally, we show how
fluorescence anisotropy measurements can be used to investigate the exciton
decoherence process during the relaxation dynamics.
",physics
"  The work is devoted to constructing a wide class of differential-functional
dynamical systems, whose rich algebraic structure makes their integrability
analytically effective. In particular, there is analyzed in detail the operator
Lax type equations for factorized seed elements, there is proved an important
theorem about their operator factorization and the related analytical solution
scheme to the corresponding nonlinear differential-functional dynamical
systems.
",physics
"  We prove, by topological methods, new results on the existence of nonzero
positive weak solutions for a class of multi-parameter second order elliptic
systems subject to functional boundary conditions. The setting is fairly
general and covers the case of multi-point, integral and nonlinear boundary
conditions. We also present a non-existence result. We provide some examples to
illustrate the applicability our theoretical results.
",mathematics
"  In this article Hopf parametric adjunctions are defined and analysed within
the context of the 2-adjunction of the type $\mathbf{Adj}$-$\mathbf{Mnd}$. In
order to do so, the definition of adjoint objects in the 2-category of
adjunctions and in the 2-category of monads for $Cat$ are revised and
characterized. This article finalises with the application of the obtained
results on current categorical characterization of Hopf Monads.
",mathematics
"  The purpose of this article is to determine explicitly the complete surfaces
with parallel mean curvature vector, both in the complex projective plane and
the complex hyperbolic plane. The main results are as follows: When the
curvature of the ambient space is positive, there exists a unique such surface
up to rigid motions of the target space. On the other hand, when the curvature
of the ambient space is negative, there are `non-trivial' complete parallel
mean curvature surfaces generated by Jacobi elliptic functions and they exhaust
such surfaces.
",mathematics
"  We consider the spontaneous breaking of translational symmetry and identify
the associated Goldstone mode -- a longitudinal phonon -- in a holographic
model with Bianchi VII helical symmetry. For the first time in holography, we
observe the pinning of this mode after introducing a source for explicit
breaking compatible with the helical symmetry of our setup. We study the
dispersion relation of the resulting pseudo-Goldstone mode, uncovering how its
speed and mass gap depend on the amplitude of the source and temperature. In
addition, we extract the optical conductivity as a function of frequency, which
reveals a metal-insulator transition as a consequence of the pinning.
",physics
"  In this paper, we prove a mean value formula for bounded subharmonic
Hermitian matrix valued function on a complete Riemannian manifold with
nonnegative Ricci curvature. As its application, we obtain a Liouville type
theorem for the complex Monge-Ampère equation on product manifolds.
",mathematics
"  The current-driven domain wall motion in a ratchet memory due to spin-orbit
torques is studied from both full micromagnetic simulations and the one
dimensional model. Within the framework of this model, the integration of the
anisotropy energy contribution leads to a new term in the well known q-$\Phi$
equations, being this contribution responsible for driving the domain wall to
an equilibrium position. The comparison between the results drawn by the one
dimensional model and full micromagnetic simulations proves the utility of such
a model in order to predict the current-driven domain wall motion in the
ratchet memory. Additionally, since current pulses are applied, the paper shows
how the proper working of such a device requires the adequate balance of
excitation and relaxation times, being the latter longer than the former.
Finally, the current-driven regime of a ratchet memory is compared to the
field-driven regime described elsewhere, then highlighting the advantages of
this current-driven regime.
",physics
"  We give a general method of extending unital completely positive maps to
amalgamated free products of C*-algebras. As an application we give a dilation
theoretic proof of Boca's Theorem.
",mathematics
"  We present the first approach for 3D point-cloud to image translation based
on conditional Generative Adversarial Networks (cGAN). The model handles
multi-modal information sources from different domains, i.e. raw point-sets and
images. The generator is capable of processing three conditions, whereas the
point-cloud is encoded as raw point-set and camera projection. An image
background patch is used as constraint to bias environmental texturing. A
global approximation function within the generator is directly applied on the
point-cloud (Point-Net). Hence, the representative learning model incorporates
global 3D characteristics directly at the latent feature space. Conditions are
used to bias the background and the viewpoint of the generated image. This
opens up new ways in augmenting or texturing 3D data to aim the generation of
fully individual images. We successfully evaluated our method on the Kitti and
SunRGBD dataset with an outstanding object detection inception score.
",computer-science
"  Objective. The purpose of this work is to analyse the knowledge structure and
trends in scientific research in the Online Information Reviews journal by
bibliometric analysis of key words and social network analysis of co-words.
Methods. Key words included in a set of 758 papers included in the Web of
Science database from 2000 to 2014 were analysed. We conducted a subject
analysis considering the key words assigned to papers. A social network
analysis was also conducted to identify the number of co-occurrences between
key words (co-words). The Pajek software was used to create and graphically
visualize the networks. Results. Internet is the most frequent key word (n=219)
and the most central in the network of co-words, strongly associated with
Information retrieval, search engines, the World Wide Web, libraries and users
Conclusions. Information science, as represented by Online Information Review
in the present study, is an evolving discipline that draws on literature from a
relatively wide range of subjects. Although Online Information Review appears
to have well-defined and established research topics, the journal also changes
rapidly to embrace new lines of research.
",computer-science
"  Wireless engineers and business planners commonly raise the question on
where, when, and how millimeter-wave (mmWave) will be used in 5G and beyond.
Since the next generation network is not just a new radio access standard, but
instead an integration of networks for vertical markets with diverse
applications, answers to the question depend on scenarios and use cases to be
deployed. This paper gives four 5G mmWave deployment examples and describes in
chronological order the scenarios and use cases of their probable deployment,
including expected system architectures and hardware prototypes. The paper
starts with 28 GHz outdoor backhauling for fixed wireless access and moving
hotspots, which will be demonstrated at the PyeongChang winter Olympic games in
2018. The second deployment example is a 60 GHz unlicensed indoor access system
at the Tokyo-Narita airport, which is combined with Mobile Edge Computing (MEC)
to enable ultra-high speed content download with low latency. The third example
is mmWave mesh network to be used as a micro Radio Access Network ({\mu}-RAN),
for cost-effective backhauling of small-cell Base Stations (BSs) in dense urban
scenarios. The last example is mmWave based Vehicular-to-Vehicular (V2V) and
Vehicular-to-Everything (V2X) communications system, which enables automated
driving by exchanging High Definition (HD) dynamic map information between cars
and Roadside Units (RSUs). For 5G and beyond, mmWave and MEC will play
important roles for a diverse set of applications that require both ultra-high
data rate and low latency communications.
",computer-science
"  As mobile devices have become indispensable in modern life, mobile security
is becoming much more important. Traditional password or PIN-like
point-of-entry security measures score low on usability and are vulnerable to
brute force and other types of attacks. In order to improve mobile security, an
adaptive neuro-fuzzy inference system(ANFIS)-based implicit authentication
system is proposed in this paper to provide authentication in a continuous and
transparent manner.To illustrate the applicability and capability of ANFIS in
our implicit authentication system, experiments were conducted on behavioural
data collected for up to 12 weeks from different Android users. The ability of
the ANFIS-based system to detect an adversary is also tested with scenarios
involving an attacker with varying levels of knowledge. The results demonstrate
that ANFIS is a feasible and efficient approach for implicit authentication
with an average of 95% user recognition rate. Moreover, the use of ANFIS-based
system for implicit authentication significantly reduces manual tuning and
configuration tasks due to its selflearning capability.
",computer-science
"  The dynamic behavior of a capacitive micro-electro-mechanical (MEMS)
accelerometer is evaluated by using a theoretical approach which makes use of a
squeeze film damping (SFD) model and ideal gas approach. The study investigates
the performance of the device as a function of the temperature, from 228 K to
398 K, and pressure, from 20 to 1000 Pa, observing the damping gas trapped
inside de mechanical transducer. Thermoelastic properties of the silicon bulk
are considered for the entire range of temperature. The damping gases
considered are Air, Helium and Argon. The global behavior of the system is
evaluated considering the electro-mechanical sensitivity (SEM) as the main
figure of merit in frequency domain. The results show the behavior of the main
mechanism losses of SFD, as well as the dynamic sensitivity of the MEMS
transducer system, and are in good agreement with experimental dynamic results
behavior.
",physics
"  Pattern matching is a powerful tool which is part of many functional
programming languages as well as computer algebra systems such as Mathematica.
Among the existing systems, Mathematica offers the most expressive pattern
matching. Unfortunately, no open source alternative has comparable pattern
matching capabilities. Notably, these features include support for associative
and/or commutative function symbols and sequence variables. While those
features have individually been subject of previous research, their
comprehensive combination has not yet been investigated. Furthermore, in many
applications, a fixed set of patterns is matched repeatedly against different
subjects. This many-to-one matching can be sped up by exploiting similarities
between patterns. Discrimination nets are the state-of-the-art solution for
many-to-one matching. In this thesis, a generalized discrimination net which
supports the full feature set is presented. All algorithms have been
implemented as an open-source library for Python. In experiments on real world
examples, significant speedups of many-to-one over one-to-one matching have
been observed.
",computer-science
"  We introduce a novel generative formulation of deep probabilistic models
implementing ""soft"" constraints on their function dynamics. In particular, we
develop a flexible methodological framework where the modeled functions and
derivatives of a given order are subject to inequality or equality constraints.
We then characterize the posterior distribution over model and constraint
parameters through stochastic variational inference. As a result, the proposed
approach allows for accurate and scalable uncertainty quantification on the
predictions and on all parameters. We demonstrate the application of equality
constraints in the challenging problem of parameter inference in ordinary
differential equation models, while we showcase the application of inequality
constraints on the problem of monotonic regression of count data. The proposed
approach is extensively tested in several experimental settings, leading to
highly competitive results in challenging modeling applications, while offering
high expressiveness, flexibility and scalability.
",statistics
"  DZ Cha is a weak-lined T Tauri star (WTTS) surrounded by a bright
protoplanetary disc with evidence of inner disc clearing. Its narrow $\Ha$ line
and infrared spectral energy distribution suggest that DZ Cha may be a
photoevaporating disc. We aim to analyse the DZ Cha star + disc system to
identify the mechanism driving the evolution of this object. We have analysed
three epochs of high resolution optical spectroscopy, photometry from the UV up
to the sub-mm regime, infrared spectroscopy, and J-band imaging polarimetry
observations of DZ Cha. Combining our analysis with previous studies we find no
signatures of accretion in the $\Ha$ line profile in nine epochs covering a
time baseline of $\sim20$ years. The optical spectra are dominated by
chromospheric emission lines, but they also show emission from the forbidden
lines [SII] 4068 and [OI] 6300$\,\AA$ that indicate a disc outflow. The
polarized images reveal a dust depleted cavity of $\sim7$ au in radius and two
spiral-like features, and we derive a disc dust mass limit of
$M_\mathrm{dust}<3\MEarth$ from the sub-mm photometry. No stellar ($M_\star >
80 \MJup$) companions are detected down to $0\farcs07$ ($\sim 8$ au,
projected). The negligible accretion rate, small cavity, and forbidden line
emission strongly suggests that DZ Cha is currently at the initial stages of
disc clearing by photoevaporation. At this point the inner disc has drained and
the inner wall of the truncated outer disc is directly exposed to the stellar
radiation. We argue that other mechanisms like planet formation or binarity
cannot explain the observed properties of DZ Cha. The scarcity of objects like
this one is in line with the dispersal timescale ($\lesssim 10^5$ yr) predicted
by this theory. DZ Cha is therefore an ideal target to study the initial stages
of photoevaporation.
",physics
"  We generalize the chimney model by introducing nonlinear restoring and
gravitational forces for the purpose of modeling swaying of trees at high wind
speeds. Here we have restricted to the simplest case of a single element and
the governing equation we arrive at has not been studied so far. We study the
onset of fractal basin boundary of the two fixed points and also observe the
chaotic solutions. We also examine the need for considering the full sine term
in the gravitational force.
",physics
"  The dynamics along the particle trajectories for the 3D axisymmetric Euler
equations are considered. It is shown that if the inflow is rapidly increasing
(pushy) in time, the corresponding laminar profile of the incompressible Euler
flow is not (in some sense) stable provided that the swirling component is not
zero. It is also shown that if the vorticity on the axis is not zero (with some
extra assumptions), then there is no steady flow. We can rephrase these
instability to an instantaneous blow-up. In the proof, Frenet-Serret formulas
and orthonormal moving frame are essentially used.
",mathematics
"  The Riesz-Sobolev inequality provides an upper bound, in integral form, for
the convolution of indicator functions of subsets of Euclidean space. We
formulate and prove a sharper form of the inequality. This can be equivalently
phrased as a stability result, quantifying an inverse theorem of Burchard that
characterizes cases of equality.
",mathematics
"  We present transductive Boltzmann machines (TBMs), which firstly achieve
transductive learning of the Gibbs distribution. While exact learning of the
Gibbs distribution is impossible by the family of existing Boltzmann machines
due to combinatorial explosion of the sample space, TBMs overcome the problem
by adaptively constructing the minimum required sample space from data to avoid
unnecessary generalization. We theoretically provide bias-variance
decomposition of the KL divergence in TBMs to analyze its learnability, and
empirically demonstrate that TBMs are superior to the fully visible Boltzmann
machines and popularly used restricted Boltzmann machines in terms of
efficiency and effectiveness.
",statistics
"  In this work we discuss the related challenges and describe an approach
towards the fusion of state-of-the-art technologies from the Spoken Dialogue
Systems (SDS) and the Semantic Web and Information Retrieval domains. We
envision a dialogue system named LD-SDS that will support advanced, expressive,
and engaging user requests, over multiple, complex, rich, and open-domain data
sources that will leverage the wealth of the available Linked Data.
Specifically, we focus on: a) improving the identification, disambiguation and
linking of entities occurring in data sources and user input; b) offering
advanced query services for exploiting the semantics of the data, with
reasoning and exploratory capabilities; and c) expanding the typical
information seeking dialogue model (slot filling) to better reflect real-world
conversational search scenarios.
",computer-science
"  The use of functional brain imaging for research and diagnosis has benefitted
greatly from the recent advancements in neuroimaging technologies, as well as
the explosive growth in size and availability of fMRI data. While it has been
shown in literature that using multiple and large scale fMRI datasets can
improve reproducibility and lead to new discoveries, the computational and
informatics systems supporting the analysis and visualization of such fMRI big
data are extremely limited and largely under-discussed. We propose to address
these shortcomings in this work, based on previous success in using dictionary
learning method for functional network decomposition studies on fMRI data. We
presented a distributed dictionary learning framework based on rank-1 matrix
decomposition with sparseness constraint (D-r1DL framework). The framework was
implemented using the Spark distributed computing engine and deployed on three
different processing units: an in-house server, in-house high performance
clusters, and the Amazon Elastic Compute Cloud (EC2) service. The whole
analysis pipeline was integrated with our neuroinformatics system for data
management, user input/output, and real-time visualization. Performance and
accuracy of D-r1DL on both individual and group-wise fMRI Human Connectome
Project (HCP) dataset shows that the proposed framework is highly scalable. The
resulting group-wise functional network decompositions are highly accurate, and
the fast processing time confirm this claim. In addition, D-r1DL can provide
real-time user feedback and results visualization which are vital for
large-scale data analysis.
",computer-science
"  Neural networks with equal excitatory and inhibitory feedback show high
computational performance. They operate close to a critical point characterized
by the joint activation of large populations of neurons. Yet, in macaque motor
cortex we observe very different dynamics with weak fluctuations on the
population level. This suggests that motor cortex operates in a sub-optimal
regime. Here we show the opposite: the large dispersion of correlations across
neurons is a signature of a rich dynamical repertoire, hidden from macroscopic
brain signals, but essential for high performance in such concepts as reservoir
computing. Our findings suggest a refinement of the view on criticality in
neural systems: network topology and heterogeneity endow the brain with two
complementary substrates for critical dynamics of largely different
complexities.
",physics
"  Batyrev constructed a family of Calabi-Yau hypersurfaces dual to the first
Chern class in toric Fano varieties. Using this construction, we introduce a
family of Calabi-Yau manifolds whose SU-bordism classes generate the special
unitary bordism ring
$\varOmega^{SU}\otimes\mathbb{Z}[\frac{1}{2}]\cong\mathbb{Z}[\frac{1}{2}][y_{i}\colon
i\ge 2]$. We also describe explicit Calabi-Yau representatives for
multiplicative generators of the SU-bordism ring in low dimensions.
",mathematics
"  Long-range macrodimers formed by D-state cesium Rydberg atoms are studied in
experiments and in calculations. Cesium 62DJ-62DJ Rydberg-atom macrodimers,
bonded via long-range multipole interaction, are prepared by two-color
photo-association in a cesium atom trap. The first color (pulse A) resonantly
excites seed Rydberg atoms, while the second (pulse B, detuned by the molecular
binding energy) resonantly excites the Rydberg-atom macrodimer states below the
62DJ pair asymptotes. The Rydberg-atom molecules are measured by extraction of
auto-ionization products and Rydberg-atom electric-field ionization, and ion
detection. Molecular spectra are compared with calculations of adiabatic
molecular potentials. The lifetime of the molecules is obtained from
exponential fits to the dependence of the molecular signal on the detection
delay time; lifetimes of about 6 us are found.
",physics
"  Over the past decade asteroseismology has become a powerful method to
systematically characterize host stars and dynamical architectures of exoplanet
systems. In this contribution I review current key synergies between
asteroseismology and exoplanetary science such as the precise determination of
planet radii and ages, the measurement of orbital eccentricities, stellar
obliquities and their impact on hot Jupiter formation theories, and the
importance of asteroseismology on spectroscopic analyses of exoplanet hosts. I
also give an outlook on future synergies such as the characterization of
sub-Neptune-size planets orbiting solar-type stars, the study of planet
populations orbiting evolved stars, and the determination of ages of
intermediate-mass stars hosting directly imaged planets.
",physics
"  Data and knowledge representation are fundamental concepts in machine
learning. The quality of the representation impacts the performance of the
learning model directly. Feature learning transforms or enhances raw data to
structures that are effectively exploited by those models. In recent years,
several works have been using complex networks for data representation and
analysis. However, no feature learning method has been proposed for such
category of techniques. Here, we present an unsupervised feature learning
mechanism that works on datasets with binary features. First, the dataset is
mapped into a feature--sample network. Then, a multi-objective optimization
process selects a set of new vertices to produce an enhanced version of the
network. The new features depend on a nonlinear function of a combination of
preexisting features. Effectively, the process projects the input data into a
higher-dimensional space. To solve the optimization problem, we design two
metaheuristics based on the lexicographic genetic algorithm and the improved
strength Pareto evolutionary algorithm (SPEA2). We show that the enhanced
network contains more information and can be exploited to improve the
performance of machine learning methods. The advantages and disadvantages of
each optimization strategy are discussed.
",computer-science
"  We investigate the stability of the many-body localized (MBL) phase for a
system in contact with a single ergodic grain, modelling a Griffiths region
with low disorder. Our numerical analysis provides evidence that even a small
ergodic grain consisting of only 3 qubits can delocalize a localized chain, as
soon as the localization length exceeds a critical value separating localized
and extended regimes of the whole system. We present a simple theory,
consistent with the arguments in [Phys. Rev. B 95, 155129 (2017)], that assumes
a system to be locally ergodic unless the local relaxation time, determined by
Fermi's Golden Rule, is larger than the inverse level spacing. This theory
predicts a critical value for the localization length that is perfectly
consistent with our numerical calculations. We analyze in detail the behavior
of local operators inside and outside the ergodic grain, and find excellent
agreement of numerics and theory.
",physics
"  We study $p$-adic families of eigenforms for which the $p$-th Hecke
eigenvalue $a_p$ has constant $p$-adic valuation (""constant slope families"").
We prove two separate upper bounds for the size of such families. The first is
in terms of the logarithmic derivative of $a_p$ while the second depends only
on the slope of the family. We also investigate the numerical relationship
between our results and the former Gouvêa--Mazur conjecture.
",mathematics
"  This paper studies the synchronization of a finite number of Kuramoto
oscillators in a frequency-dependent bidirectional tree network. We assume that
the coupling strength of each link in each direction is equal to the product of
a common coefficient and the exogenous frequency of its corresponding head
oscillator. We derive a sufficient condition for the common coupling strength
in order to guarantee frequency synchronization in tree networks. Moreover, we
discuss the dependency of the obtained bound on both the graph structure and
the way that exogenous frequencies are distributed. Further, we present an
application of the obtained result by means of an event-triggered algorithm for
achieving frequency synchronization in a star network assuming that the common
coupling coefficient is given.
",computer-science
"  With the analysis of noise-induced synchronization of opinion dynamics with
bounded confidence (BC), a natural and fundamental question is what opinion
structures can be synchronized by noise. In the traditional Hegselmann-Krause
(HK) model, each agent examines the opinion values of all the other ones and
then choose neighbors to update its own opinion according to the BC scheme. In
reality, people are more likely to interchange opinions with only some
individuals, resulting in a predetermined local discourse relationship as
introduced by the DeGroot model. In this paper, we consider an opinion dynamics
that combines the schemes of BC and local discourse topology and investigate
its synchronization induced by noise. The new model endows the heterogeneous HK
model with a time-varying discourse topology. With the proposed definition of
noise-synchronizability, it is shown that the compound noisy model is almost
surely noise-synchronizable if and only if the time-varying discourse graph is
uniformly jointly connected, taking the noise-induced synchronization of the
classical heterogeneous HK model as a special case. As a natural implication,
the result for the first time builds the equivalence between the connectivity
of discourse graph and the beneficial effect of noise for opinion consensus.
",computer-science
"  We propose new positive definite kernels for permutations. First we introduce
a weighted version of the Kendall kernel, which allows to weight unequally the
contributions of different item pairs in the permutations depending on their
ranks. Like the Kendall kernel, we show that the weighted version is invariant
to relabeling of items and can be computed efficiently in $O(n \ln(n))$
operations, where $n$ is the number of items in the permutation. Second, we
propose a supervised approach to learn the weights by jointly optimizing them
with the function estimated by a kernel machine. Third, while the Kendall
kernel considers pairwise comparison between items, we extend it by considering
higher-order comparisons among tuples of items and show that the supervised
approach of learning the weights can be systematically generalized to
higher-order permutation kernels.
",statistics
"  In this paper, we are motivated by two important applications:
entropy-regularized optimal transport problem and road or IP traffic demand
matrix estimation by entropy model. Both of them include solving a special type
of optimization problem with linear equality constraints and objective given as
a sum of an entropy regularizer and a linear function. It is known that the
state-of-the-art solvers for this problem, which are based on Sinkhorn's method
(also known as RSA or balancing method), can fail to work, when the
entropy-regularization parameter is small. We consider the above optimization
problem as a particular instance of a general strongly convex optimization
problem with linear constraints. We propose a new algorithm to solve this
general class of problems. Our approach is based on the transition to the dual
problem. First, we introduce a new accelerated gradient method with adaptive
choice of gradient's Lipschitz constant. Then, we apply this method to the dual
problem and show, how to reconstruct an approximate solution to the primal
problem with provable convergence rate. We prove the rate $O(1/k^2)$, $k$ being
the iteration counter, both for the absolute value of the primal objective
residual and constraints infeasibility. Our method has similar to Sinkhorn's
method complexity of each iteration, but is faster and more stable numerically,
when the regularization parameter is small. We illustrate the advantage of our
method by numerical experiments for the two mentioned applications. We show
that there exists a threshold, such that, when the regularization parameter is
smaller than this threshold, our method outperforms the Sinkhorn's method in
terms of computation time.
",mathematics
"  The DArk Matter Particle Explorer (DAMPE) is one of the four satellites
within the Strategic Pioneer Research Program in Space Science of the Chinese
Academy of Science (CAS). The Silicon-Tungsten Tracker (STK), which is composed
of 768 singled-sided silicon microstrip detectors, is one of the four
subdetectors in DAMPE, providing track reconstruction and charge identification
for relativistic charged particles. The charge response of DAMPE silicon
microstrip detectors is complicated, depending on the incident angle and impact
position. A new charge reconstruction algorithm for the DAMPE silicon
microstrip detector is introduced in this paper. This algorithm can correct the
complicated charge response, and was proved applicable by the ion test beam.
",physics
"  Various experimental techniques, have revealed that the predominant intrinsic
point defects in BaF$_2$ are anion Frenkel defects. Their formation enthalpy
and entropy as well as the corresponding parameters for the fluorine vacancy
and fluorine interstitial motion have been determined. In addition, low
temperature dielectric relaxation measurements in BaF$_2$ doped with uranium
leads to the parameters {\tau}$_0$, E in the Arrhenius relation
{\tau}={\tau}$_0$exp(E/kBT) for the relaxation time {\tau}. For the relaxation
peak associated with a single tetravalent uranium, the migration entropy
deduced from the pre-exponential factor {\tau}$_0$, is smaller than the anion
Frenkel defect formation entropy by almost two orders of magnitude. We show
that, despite their great variation, the defect entropies and enthalpies are
interconnected through a model based on anharmonic properties of the bulk
material that have been recently studied by employing density-functional theory
and density-functional perturbation theory.
",physics
"  Recent developments of imaging techniques enable researchers to visualize
materials at the atomic resolution to better understand the microscopic
structures of materials. This paper aims at automatic and quantitative
characterization of potentially complicated microscopic crystal images,
providing feedback to tweak theories and improve synthesis in materials
science. As such, an efficient phase-space sketching method is proposed to
encode microscopic crystal images in a translation, rotation, illumination, and
scale invariant representation, which is also stable with respect to small
deformations. Based on the phase-space sketching, we generalize our previous
analysis framework for crystal images with simple structures to those with
complicated geometry.
",physics
"  For the quantum kinetic system modelling the Bose-Einstein Condensate that
accounts for interactions between condensate and excited atoms, we use the
Chapman-Enskog expansion to derive its hydrodynamic approximations, include
both Euler and Navier-Stokes approximations. The hydrodynamic approximations
describe not only the macroscopic behavior of the BEC but also its coupling
with the non-condensates, which agrees with Landau's two fluid theory.
",physics
"  We describe the first ever implementation of an emulsion multi-stage shifter
in an accelerator neutrino experiment. The system was installed in the neutrino
monitor building in J-PARC as a part of a test experiment T60 and stable
operation was maintained for a total of 126.6 days. By applying time
information to emulsion films, various results were obtained. Time resolutions
of 5.3 to 14.7 s were evaluated in an operation spanning 46.9 days (time
resolved numbers of 3.8--1.4$\times10^{5}$). By using timing and spatial
information, a reconstruction of coincident events that consisted of high
multiplicity events and vertex events, including neutrino events was performed.
Emulsion events were matched to events observed by INGRID, one of near
detectors of the T2K experiment, with high reliability (98.5\%) and hybrid
analysis was established via use of the multi-stage shifter. The results
demonstrate that the multi-stage shifter is feasible for use in neutrino
experiments.
",physics
"  We show that black-hole High-Mass X-ray Binaries (HMXBs) with O- or B-type
donor stars and relatively short orbital periods, of order one week to several
months may survive spiral in, to then form Wolf-Rayet (WR) X-ray binaries with
orbital periods of order a day to a few days; while in systems where the
compact star is a neutron star, HMXBs with these orbital periods never survive
spiral-in. We therefore predict that WR X-ray binaries can only harbor black
holes. The reason why black-hole HMXBs with these orbital periods may survive
spiral in is: the combination of a radiative envelope of the donor star, and a
high mass of the compact star. In this case, when the donor begins to overflow
its Roche lobe, the systems are able to spiral in slowly with stable Roche-lobe
overflow, as is shown by the system SS433. In this case the transferred mass is
ejected from the vicinity of the compact star (so-called ""isotropic
re-emission"" mass loss mode, or ""SS433-like mass loss""), leading to gradual
spiral-in. If the mass ratio of donor and black hole is $>3.5$, these systems
will go into CE evolution and are less likely to survive. If they survive, they
produce WR X-ray binaries with orbital periods of a few hours to one day.
Several of the well-known WR+O binaries in our Galaxy and the Magellanic
Clouds, with orbital periods in the range between a week and several months,
are expected to evolve into close WR-Black-Hole binaries,which may later
produce close double black holes. The galactic formation rate of double black
holes resulting from such systems is still uncertain, as it depends on several
poorly known factors in this evolutionary picture. It might possibly be as high
as $\sim 10^{-5}$ per year.
",physics
"  The collective magnetic excitations in the spin-orbit Mott insulator
(Sr$_{1-x}$La$_x$)$_2$IrO$_4$ ($x=0,\,0.01,\,0.04,\, 0.1$) were investigated by
means of resonant inelastic x-ray scattering. We report significant magnon
energy gaps at both the crystallographic and antiferromagnetic zone centers at
all doping levels, along with a remarkably pronounced momentum-dependent
lifetime broadening. The spin-wave gap is accounted for by a significant
anisotropy in the interactions between $J_\text{eff}=1/2$ isospins, thus
marking the departure of Sr$_2$IrO$_4$ from the essentially isotropic
Heisenberg model appropriate for the superconducting cuprates.
",physics
"  A fundamental characteristic of computer networks is their topological
structure. The question of the description of the structural characteristics of
computer networks represents a problem that is not completely solved. Search
methods for structures of computer networks, for which the values of the
selected parameters of their operation quality are extreme, have not been
completely developed. The construction of computer networks with optimum
indices of their operation quality is reduced to the solution of discrete
optimization problems over graphs. This paper describes in detail the
advantages of the practical use of k-geodetic graphs [2, 3] in the topological
design of computer networks as an alternative for the solution of the
fundamental problems mentioned above which, we believe, are still open. Also,
the topological analysis and synthesis of some classes of these networks have
been performed.
",computer-science
"  Let $\pi $ be an irreducible smooth complex representation of a general
linear $p$-adic group and let $\sigma $ be an irreducible complex supercuspidal
representation of a classical $p$-adic group of a given type, so that
$\pi\otimes\sigma $ is a representation of a standard Levi subgroup of a
$p$-adic classical group of higher rank. We show that the reducibility of the
representation of the appropriate $p$-adic classical group obtained by
(normalized) parabolic induction from $\pi\otimes\sigma $ does not depend on
$\sigma $, if $\sigma $ is ""separated"" from the supercuspidal support of $\pi
$. (Here, ""separated"" means that, for each factor $\rho $ of a representation
in the supercuspidal support of $\pi $, the representation parabolically
induced from $\rho\otimes\sigma $ is irreducible.) This was conjectured by E.
Lapid and M. Tadić. (In addition, they proved, using results of C. Jantzen,
that this induced representation is always reducible if the supercuspidal
support is not separated.)
More generally, we study, for a given set $I$ of inertial orbits of
supercuspidal representations of $p$-adic general linear groups, the category
$\CC _{I,\sigma}$ of smooth complex finitely generated representations of
classical $p$-adic groups of fixed type, but arbitrary rank, and supercuspidal
support given by $\sigma $ and $I$, show that this category is equivalent to a
category of finitely generated right modules over a direct sum of tensor
products of extended affine Hecke algebras of type $A$, $B$ and $D$ and
establish functoriality properties, relating categories with disjoint $I$'s. In
this way, we extend results of C. Jantzen who proved a bijection between
irreducible representations corresponding to these categories. The proof of the
above reducibility result is then based on Hecke algebra arguments, using
Kato's exotic geometry.
",mathematics
"  In this paper we present a neurally plausible model of robot reaching
inspired by human infant reaching that is based on embodied artificial
intelligence, which emphasizes the importance of the sensory-motor interaction
of an agent and the world. This model encompasses both learning sensory-motor
correlations through motor babbling and also arm motion planning using
spreading activation. This model is organized in three layers of neural maps
with parallel structures representing the same sensory-motor space. The motor
babbling period shapes the structure of the three neural maps as well as the
connections within and between them. We describe an implementation of this
model and an investigation of this implementation using a simple reaching task
on a humanoid robot. The robot has learned successfully to plan reaching
motions from a test set with high accuracy and smoothness.
",computer-science
"  As shown by McMullen in 1983, the coefficients of the Ehrhart polynomial of a
lattice polytope can be written as a weighted sum of facial volumes. The
weights in such a local formula depend only on the outer normal cones of faces,
but are far from being unique. In this paper, we develop an infinite class of
such local formulas. These are based on choices of fundamental domains in
sublattices and obtained by polyhedral volume computations. We hereby also give
a kind of geometric interpretation for the Ehrhart coefficients. Since our
construction gives us a great variety of possible local formulas, these can,
for instance, be chosen to fit well with a given polyhedral symmetry group. In
contrast to other constructions of local formulas, ours does not rely on
triangulations of rational cones into simplicial or even unimodular ones.
",mathematics
"  The CALICE collaboration is developing highly granular calorimeters for
experiments at a future lepton collider primarily to establish technologies for
particle flow event reconstruction. These technologies also find applications
elsewhere, such as detector upgrades for the LHC. Meanwhile, the large data
sets collected in an extensive series of beam tests have enabled detailed
studies of the properties of hadronic showers in calorimeter systems, resulting
in improved simulation models and development of sophisticated reconstruction
techniques. In this proceeding, highlights are included from studies of the
structure of hadronic showers and results on reconstruction techniques for
imaging calorimetry. In addition, current R&D activities within CALICE are
summarized, focusing on technological prototypes that address challenges from
full detector system integration and production techniques amenable to mass
production for electromagnetic and hadronic calorimeters based on silicon,
scintillator, and gas techniques.
",physics
"  In order to understand the exoplanet, you need to understand its parent star.
Astrophysical parameters of extrasolar planets are directly and indirectly
dependent on the properties of their respective host stars. These host stars
are very frequently the only visible component in the systems. This book
describes our work in the field of characterization of exoplanet host stars
using interferometry to determine angular diameters, trigonometric parallax to
determine physical radii, and SED fitting to determine effective temperatures
and luminosities. The interferometry data are based on our decade-long survey
using the CHARA Array. We describe our methods and give an update on the status
of the field, including a table with the astrophysical properties of all stars
with high-precision interferometric diameters out to 150 pc (status Nov 2016).
In addition, we elaborate in more detail on a number of particularly
significant or important exoplanet systems, particularly with respect to (1)
insights gained from transiting exoplanets, (2) the determination of system
habitable zones, and (3) the discrepancy between directly determined and
model-based stellar radii. Finally, we discuss current and future work
including the calibration of semi-empirical methods based on interferometric
data.
",physics
"  The convolution of galaxy images by the point-spread function (PSF) is the
dominant source of bias for weak gravitational lensing studies, and an accurate
estimate of the PSF is required to obtain unbiased shape measurements. The PSF
estimate for a galaxy depends on its spectral energy distribution (SED),
because the instrumental PSF is generally a function of the wavelength. In this
paper we explore various approaches to determine the resulting `effective' PSF
using broad-band data. Considering the Euclid mission as a reference, we find
that standard SED template fitting methods result in biases that depend on
source redshift, although this may be remedied if the algorithms can be
optimised for this purpose. Using a machine-learning algorithm we show that, at
least in principle, the required accuracy can be achieved with the current
survey parameters. It is also possible to account for the correlations between
photometric redshift and PSF estimates that arise from the use of the same
photometry. We explore the impact of errors in photometric calibration, errors
in the assumed wavelength dependence of the PSF model and limitations of the
adopted template libraries. Our results indicate that the required accuracy for
Euclid can be achieved using the data that are planned to determine photometric
redshifts.
",physics
"  We provide a microeconomic framework for decision trees: a popular machine
learning method. Specifically, we show how decision trees represent a
non-compensatory decision protocol known as disjunctions-of-conjunctions and
how this protocol generalizes many of the non-compensatory rules used in the
discrete choice literature so far. Additionally, we show how existing decision
tree variants address many economic concerns that choice modelers might have.
Beyond theoretical interpretations, we contribute to the existing literature of
two-stage, semi-compensatory modeling and to the existing decision tree
literature. In particular, we formulate the first bayesian model tree, thereby
allowing for uncertainty in the estimated non-compensatory rules as well as for
context-dependent preference heterogeneity in one's second-stage choice model.
Using an application of bicycle mode choice in the San Francisco Bay Area, we
estimate our bayesian model tree, and we find that it is over 1,000 times more
likely to be closer to the true data-generating process than a multinomial
logit model (MNL). Qualitatively, our bayesian model tree automatically finds
the effect of bicycle infrastructure investment to be moderated by travel
distance, socio-demographics and topography, and our model identifies
diminishing returns from bike lane investments. These qualitative differences
lead to bayesian model tree forecasts that directly align with the observed
bicycle mode shares in regions with abundant bicycle infrastructure such as
Davis, CA and the Netherlands. In comparison, MNL's forecasts are overly
optimistic.
",statistics
"  We report the detection of extended Halpha emission from the tip of the HI
disk of the nearby edge-on galaxy UGC 7321, observed with the Multi Unit
Spectroscopic Explorer (MUSE) instrument at the Very Large Telescope. The
Halpha surface brightness fades rapidly where the HI column density drops below
N(HI) = 10^19 cm^-2 , consistent with fluorescence arising at the ionisation
front from gas that is photoionized by the extragalactic ultraviolet background
(UVB). The surface brightness measured at this location is (1.2 +/- 0.5)x10^-19
erg/s/cm^2/arcsec^2, where the error is mostly systematic and results from the
proximity of the signal to the edge of the MUSE field of view, and from the
presence of a sky line next to the redshifted Halpha wavelength. By combining
the Halpha and the HI 21 cm maps with a radiative transfer calculation of an
exponential disk illuminated by the UVB, we derive a value for the HI
photoionization rate of Gamma ~ (6-8)x10^-14 1/s . This value is consistent
with transmission statistics of the Lyalpha forest and with recent models of a
UVB which is dominated by quasars.
",physics
"  Surface parameterizations have been widely applied to computer graphics and
digital geometry processing. In this paper, we propose a novel stretch energy
minimization (SEM) algorithm for the computation of equiareal parameterizations
of simply connected open surfaces with a very small area distortion and a
highly improved computational efficiency. In addition, the existence of
nontrivial limit points of the SEM algorithm is guaranteed under some mild
assumptions of the mesh quality. Numerical experiments indicate that the
efficiency, accuracy, and robustness of the proposed SEM algorithm outperform
other state-of-the-art algorithms. Applications of the SEM on surface remeshing
and surface registration for simply connected open surfaces are demonstrated
thereafter. Thanks to the SEM algorithm, the computations for these
applications can be carried out efficiently and robustly.
",computer-science
"  Tick is a statistical learning library for Python~3, with a particular
emphasis on time-dependent models, such as point processes, and tools for
generalized linear models and survival analysis. The core of the library is an
optimization module providing model computational classes, solvers and proximal
operators for regularization. tick relies on a C++ implementation and
state-of-the-art optimization algorithms to provide very fast computations in a
single node multi-core setting. Source code and documentation can be downloaded
from this https URL
",statistics
"  CMS-HF Calorimeters have been undergoing a major upgrade for the last couple
of years to alleviate the problems encountered during Run I, especially in the
PMT and the readout systems. In this poster, the problems caused by the old
PMTs installed in the detectors and their solutions will be explained.
Initially, regular PMTs with thicker windows, causing large Cherenkov
radiation, were used. Instead of the light coming through the fibers from the
detector, stray muons passing through the PMT itself produce Cherenkov
radiation in the PMT window, resulting in erroneously large signals. Usually,
large signals are the result of very high-energy particles in the calorimeter
and are tagged as important. As a result, these so-called window events
generate false triggers. Four-anode PMTs with thinner windows were selected to
reduce these window events. Additional channels also help eliminate such
remaining events through algorithms comparing the output of different PMT
channels. During the EYETS 16/17 period in the LHC operations, the final
components of the modifications to the readout system, namely the two-channel
front-end electronics cards, are installed. Complete upgrade of the HF
Calorimeter, including the preparations for the Run II will be discussed in
this poster, with possible effects on the eventual data taking.
",physics
"  We explore the simplification of widely used meta-generalized-gradient
approximation (mGGA) exchange-correlation functionals to the Laplacian level of
refinement by use of approximate kinetic energy density functionals (KEDFs).
Such deorbitalization is motivated by the prospect of reducing computational
cost while recovering a strictly Kohn-Sham local potential framework (rather
than the usual generalized Kohn-Sham treatment of mGGAs). A KEDF that has been
rather successful in solid simulations proves to be inadequate for
deorbitalization but we produce other forms which, with parametrization to
Kohn-Sham results (not experimental data) on a small training set, yield rather
good results on standard molecular test sets when used to deorbitalize the
meta-GGA made very simple, TPSS, and SCAN functionals. We also study the
difference between high-fidelity and best-performing deorbitalizations and
discuss possible implications for use in ab initio molecular dynamics
simulations of complicated condensed phase systems.
",physics
"  We present a vision-only model for gaming AI which uses a late integration
deep convolutional network architecture trained in a purely supervised
imitation learning context. Although state-of-the-art deep learning models for
video game tasks generally rely on more complex methods such as deep-Q
learning, we show that a supervised model which requires substantially fewer
resources and training time can already perform well at human reaction speeds
on the N64 classic game Super Smash Bros. We frame our learning task as a
30-class classification problem, and our CNN model achieves 80% top-1 and 95%
top-3 validation accuracy. With slight test-time fine-tuning, our model is also
competitive during live simulation with the highest-level AI built into the
game. We will further show evidence through network visualizations that the
network is successfully leveraging temporal information during inference to aid
in decision making. Our work demonstrates that supervised CNN models can
provide good performance in challenging policy prediction tasks while being
significantly simpler and more lightweight than alternatives.
",computer-science
"  For decades, conventional computers based on the von Neumann architecture
have performed computation by repeatedly transferring data between their
processing and their memory units, which are physically separated. As
computation becomes increasingly data-centric and as the scalability limits in
terms of performance and power are being reached, alternative computing
paradigms are searched for in which computation and storage are collocated. A
fascinating new approach is that of computational memory where the physics of
nanoscale memory devices are used to perform certain computational tasks within
the memory unit in a non-von Neumann manner. Here we present a large-scale
experimental demonstration using one million phase-change memory devices
organized to perform a high-level computational primitive by exploiting the
crystallization dynamics. Also presented is an application of such a
computational memory to process real-world data-sets. The results show that
this co-existence of computation and storage at the nanometer scale could be
the enabler for new, ultra-dense, low power, and massively parallel computing
systems.
",computer-science
"  We present clustering properties from 579,492 Lyman break galaxies (LBGs) at
z~4-6 over the 100 deg^2 sky (corresponding to a 1.4 Gpc^3 volume) identified
in early data of the Hyper Suprime-Cam (HSC) Subaru strategic program survey.
We derive angular correlation functions (ACFs) of the HSC LBGs with
unprecedentedly high statistical accuracies at z~4-6, and compare them with the
halo occupation distribution (HOD) models. We clearly identify significant ACF
excesses in 10""<$\theta$<90"", the transition scale between 1- and 2-halo terms,
suggestive of the existence of the non-linear halo bias effect. Combining the
HOD models and previous clustering measurements of faint LBGs at z~4-7, we
investigate dark-matter halo mass (Mh) of the z~4-7 LBGs and its correlation
with various physical properties including the star-formation rate (SFR), the
stellar-to-halo mass ratio (SHMR), and the dark matter accretion rate (dotMh)
over a wide-mass range of Mh/M$_\odot$=4x10^10-4x10^12. We find that the SHMR
increases from z~4 to 7 by a factor of ~4 at Mh~1x10^11 M$_\odot$, while the
SHMR shows no strong evolution in the similar redshift range at Mh~1x10^12
M$_\odot$. Interestingly, we identify a tight relation of SFR/dotMh-Mh showing
no significant evolution beyond 0.15 dex in this wide-mass range over z~4-7.
This weak evolution suggests that the SFR/dotMh-Mh relation is a fundamental
relation in high-redshift galaxy formation whose star formation activities are
regulated by the dark matter mass assembly. Assuming this fundamental relation,
we calculate the cosmic SFR densities (SFRDs) over z=0-10 (a.k.a. Madau-Lilly
plot). The cosmic SFRD evolution based on the fundamental relation agrees with
the one obtained by observations, suggesting that the cosmic SFRD increase from
z~10 to 4-2 (decrease from z~4-2 to 0) is mainly driven by the increase of the
halo abundance (the decrease of the accretion rate).
",physics
"  One of the key challenges for operations researchers solving real-world
problems is designing and implementing high-quality heuristics to guide their
search procedures. In the past, machine learning techniques have failed to play
a major role in operations research approaches, especially in terms of guiding
branching and pruning decisions. We integrate deep neural networks into a
heuristic tree search procedure to decide which branch to choose next and to
estimate a bound for pruning the search tree of an optimization problem. We
call our approach Deep Learning assisted heuristic Tree Search (DLTS) and apply
it to a well-known problem from the container terminals literature, the
container pre-marshalling problem (CPMP). Our approach is able to learn
heuristics customized to the CPMP solely through analyzing the solutions to
CPMP instances, and applies this knowledge within a heuristic tree search to
produce the highest quality heuristic solutions to the CPMP to date.
",computer-science
"  Soft Random Geometric Graphs (SRGGs) have been widely applied to various
models including those of wireless sensor, communication, social and neural
networks. SRGGs are constructed by randomly placing nodes in some space and
making pairwise links probabilistically using a connection function that is
system specific and usually decays with distance. In this paper we focus on the
application of SRGGs to wireless communication networks where information is
relayed in a multi hop fashion, although the analysis is more general and can
be applied elsewhere by using different distributions of nodes and/or
connection functions. We adopt a general non-uniform density which can model
the stationary distribution of different mobility models, with the interesting
case being when the density goes to zero along the boundaries. The global
connectivity properties of these non-uniform networks are likely to be
determined by highly isolated nodes, where isolation can be caused by the
spatial distribution or the local geometry (boundaries). We extend the analysis
to temporal-spatial networks where we fix the underlying non-uniform
distribution of points and the dynamics are caused by the temporal variations
in the link set, and explore the probability a node near the corner is isolated
at time $T$. This work allows for insight into how non-uniformity (caused by
mobility) and boundaries impact the connectivity features of temporal-spatial
networks. We provide a simple method for approximating these probabilities for
a range of different connection functions and verify them against simulations.
Boundary nodes are numerically shown to dominate the connectivity properties of
these finite networks with non-uniform measure.
",computer-science
"  A strong interaction is known to exist between edge-colored graphs (which
encode PL pseudo-manifolds of arbitrary dimension) and random tensor models (as
a possible approach to the study of Quantum Gravity). The key tool is the {\it
G-degree} of the involved graphs, which drives the {\it $1/N$ expansion} in the
tensor models context. In the present paper - by making use of combinatorial
properties concerning Hamiltonian decompositions of the complete graph - we
prove that, in any even dimension $d\ge 4$, the G-degree of all bipartite
graphs, as well as of all (bipartite or non-bipartite) graphs representing
singular manifolds, is an integer multiple of $(d-1)!$. As a consequence, in
even dimension, the terms of the $1/N$ expansion corresponding to odd powers of
$1/N$ are null in the complex context, and do not involve colored graphs
representing singular manifolds in the real context.
In particular, in the 4-dimensional case, where the G-degree is shown to
depend only on the regular genera with respect to an arbitrary pair of
""associated"" cyclic permutations, several results are obtained, relating the
G-degree or the regular genus of 5-colored graphs and the Euler characteristic
of the associated PL 4-manifolds.
",mathematics
"  High-mass stars are expected to form from dense prestellar cores. Their
precise formation conditions are widely discussed, including their virial
condition, which results in slow collapse for super-virial cores with strong
support by turbulence or magnetic fields, or fast collapse for sub-virial
sources. To disentangle their formation processes, measurements of the
deuterium fractions are frequently employed to approximately estimate the ages
of these cores and to obtain constraints on their dynamical evolution. We here
present 3D magneto-hydrodynamical simulations including for the first time an
accurate non-equilibrium chemical network with 21 gas-phase species plus dust
grains and 213 reactions. With this network we model the deuteration process in
fully depleted prestellar cores in great detail and determine its response to
variations in the initial conditions. We explore the dependence on the initial
gas column density, the turbulent Mach number, the mass-to-magnetic flux ratio
and the distribution of the magnetic field, as well as the initial
ortho-to-para ratio of H2. We find excellent agreement with recent observations
of deuterium fractions in quiescent sources. Our results show that deuteration
is rather efficient, even when assuming a conservative ortho-to-para ratio of 3
and highly sub-virial initial conditions, leading to large deuterium fractions
already within roughly a free-fall time. We discuss the implications of our
results and give an outlook to relevant future investigations.
",physics
"  We introduce a self-consistent multi-species kinetic theory based on the
structure of the narrow voltage-gated potassium channel. Transition rates
depend on a complete energy spectrum with contributions including the
dehydration amongst species, interaction with the dipolar charge of the filter
and, bulk solution properties. It displays high selectivity between species
coexisting with fast conductivity, and Coulomb blockade phenomena, and it fits
well to data.
",physics
"  We show how active transport of ions can be interpreted as an entropy
facilitated process. In this interpretation, the pore geometry through which
substrates are transported can give rise to a driving force. This gives a
direct link between the geometry and the changes in Gibbs energy required.
Quantifying the size of this effect for several proteins we find that the
entropic contribution from the pore geometry is significant and we discuss how
the effect can be used to interpret variations in the affinity at the binding
site.
",physics
"  The problem of machine learning with missing values is common in many areas.
A simple approach is to first construct a dataset without missing values simply
by discarding instances with missing entries or by imputing a fixed value for
each missing entry, and then train a prediction model with the new dataset. A
drawback of this naive approach is that the uncertainty in the missing entries
is not properly incorporated in the prediction. In order to evaluate prediction
uncertainty, the multiple imputation (MI) approach has been studied, but the
performance of MI is sensitive to the choice of the probabilistic model of the
true values in the missing entries, and the computational cost of MI is high
because multiple models must be trained. In this paper, we propose an
alternative approach called the Interval-based Prediction Uncertainty Bounding
(IPUB) method. The IPUB method represents the uncertainties due to missing
entries as intervals, and efficiently computes the lower and upper bounds of
the prediction results when all possible training sets constructed by imputing
arbitrary values in the intervals are considered. The IPUB method can be
applied to a wide class of convex learning algorithms including penalized
least-squares regression, support vector machine (SVM), and logistic
regression. We demonstrate the advantages of the IPUB method by comparing it
with an existing method in numerical experiment with benchmark datasets.
",statistics
"  The execution logs that are used for process mining in practice are often
obtained by querying an operational database and storing the result in a flat
file. Consequently, the data processing power of the database system cannot be
used anymore for this information, leading to constrained flexibility in the
definition of mining patterns and limited execution performance in mining large
logs. Enabling process mining directly on a database - instead of via
intermediate storage in a flat file - therefore provides additional flexibility
and efficiency. To help facilitate this ideal of in-database process mining,
this paper formally defines a database operator that extracts the 'directly
follows' relation from an operational database. This operator can both be used
to do in-database process mining and to flexibly evaluate process mining
related queries, such as: ""which employee most frequently changes the 'amount'
attribute of a case from one task to the next"". We define the operator using
the well-known relational algebra that forms the formal underpinning of
relational databases. We formally prove equivalence properties of the operator
that are useful for query optimization and present time-complexity properties
of the operator. By doing so this paper formally defines the necessary
relational algebraic elements of a 'directly follows' operator, which are
required for implementation of such an operator in a DBMS.
",computer-science
"  Categories of polymorphic lenses in computer science, and of open games in
compositional game theory, have a curious structure that is reminiscent of
compact closed categories, but differs in some crucial ways. Specifically they
have a family of morphisms that behave like the counits of a compact closed
category, but have no corresponding units; and they have a `partial' duality
that behaves like transposition in a compact closed category when it is
defined. We axiomatise this structure, which we refer to as a `teleological
category'. We precisely define a diagrammatic language suitable for these
categories, and prove a coherence theorem for them. This underpins the use of
diagrammatic reasoning in compositional game theory, which has previously been
used only informally.
",computer-science
"  We consider the refined topological vertex of Iqbal et al, as a function of
two parameters (x, y), and deform it by introducing Macdonald parameters (q,
t), as in the work of Vuletic on plane partitions, to obtain 'a Macdonald
refined topological vertex'. In the limit q -> t, we recover the refined
topological vertex of Iqbal et al. In the limit x -> y, we obtain a
qt-deformation of the topological vertex of Aganagic et al. Copies of the
vertex can be glued to obtain qt-deformed 5D instanton partition functions that
have well-defined 4D limits and, for generic values of (q, t), contain
infinite-towers of poles for every pole in the limit q -> t.
",mathematics
"  We present several formulae for the large-$t$ asymptotics of the modified
Hurwitz zeta function $\zeta_1(x,s),x>0,s=\sigma+it,0<\sigma\leq1,t>0,$ which
are valid to all orders. In the case of $x=0$, these formulae reduce to the
asymptotic expressions recently obtained for the Riemann zeta function, which
include the classical results of Siegel as a particular case.
",mathematics
"  Today digital sources supply an unprecedented component of human sensorimotor
data, the consumption of which is correlated with poorly understood maladies
such as Internet Addiction Disorder and Internet Gaming Disorder. This paper
offers a mathematical understanding of human sensorimotor processing as
multiscale, continuous-time vibratory interaction. We quantify human
informational needs using the signal processing metrics of entropy, noise,
dimensionality, continuity, latency, and bandwidth. Using these metrics, we
define the trust humans experience as a primitive statistical algorithm
processing finely grained sensorimotor data from neuromechanical interaction.
This definition of neuromechanical trust implies that artificial sensorimotor
inputs and interactions that attract low-level attention through frequent
discontinuities and enhanced coherence will decalibrate a brain's
representation of its world over the long term by violating the implicit
statistical contract for which self-calibration evolved. This approach allows
us to model addiction in general as the result of homeostatic regulation gone
awry in novel environments and digital dependency as a sub-case in which the
decalibration caused by digital sensorimotor data spurs yet more consumption of
them. We predict that institutions can use these sensorimotor metrics to
quantify media richness to improve employee well-being; that dyads and
family-size groups will bond and heal best through low-latency, high-resolution
multisensory interaction such as shared meals and reciprocated touch; and that
individuals can improve sensory and sociosensory resolution through deliberate
sensory reintegration practices. We conclude that we humans are the victims of
our own success, our hands so skilled they fill the world with captivating
things, our eyes so innocent they follow eagerly.
",physics
"  Analyzing the behaviour of a concurrent program is made difficult by the
number of possible executions. This problem can be alleviated by applying the
theory of Mazurkiewicz traces to focus only on the canonical representatives of
the equivalence classes of the possible executions of the program. This paper
presents a generic framework that allows to specify the possible behaviours of
the execution environment, and generate all Foata-normal executions of a
program, for that environment, by discarding abnormal executions during the
generation phase. The key ingredient of Mazurkiewicz trace theory, the
dependency relation, is used in the framework in two roles: first, as part of
the specification of which executions are allowed at all, and then as part of
the normality checking algorithm, which is used to discard the abnormal
executions. The framework is instantiated to the relaxed memory models of the
SPARC hierarchy.
",computer-science
"  This paper studies the eigenvalue problem on $\mathbb{R}^d$ for a class of
second order, elliptic operators of the form $\mathscr{L} =
a^{ij}\partial_{x_i}\partial_{x_j} + b^{i}\partial_{x_i} + f$, associated with
non-degenerate diffusions. We show that strict monotonicity of the principal
eigenvalue of the operator with respect to the potential function $f$ fully
characterizes the ergodic properties of the associated ground state diffusion,
and the unicity of the ground state, and we present a comprehensive study of
the eigenvalue problem from this point of view. This allows us to extend or
strengthen various results in the literature for a class of viscous
Hamilton-Jacobi equations of ergodic type with smooth coefficients to equations
with measurable drift and potential. In addition, we establish the strong
duality for the equivalent infinite dimensional linear programming formulation
of these ergodic control problems. We also apply these results to the study of
the infinite horizon risk-sensitive control problem for diffusions, and
establish existence of optimal Markov controls, verification of optimality
results, and the continuity of the controlled principal eigenvalue with respect
to stationary Markov controls.
",mathematics
"  In this work we focus on a novel completion of the well-known Brans-Dicke
theory that introduces an interaction between the dark energy and dark matter
sectors, known as complete Brans-Dicke (CBD) theory. We obtain viable
cosmological accelerating solutions that fit Supernovae observations with great
precision without any scalar potential $V(\phi)$. We use these solutions to
explore the impact of the CBD theory on the large scale structure by studying
the dynamics of its linear perturbations. We observe a growing behavior of the
lensing potential $\Phi_{+}$ at late-times, while the growth rate is actually
suppressed relatively to $\Lambda$CDM, which allows the CBD theory to provide a
competitive fit to current RSD measurements of $f\sigma_{8}$. However, we also
observe that the theory exhibits a pathological change of sign in the effective
gravitational constant concerning the perturbations on sub-horizon scales that
could pose a challenge to its validity.
",physics
"  For $d\geq1$, we study the simplicial structure of the chain complex
associated to the higher order Hochschild homology over the $d$-sphere. We
discuss $H_\bullet^{S^d}(A,M)$ by way of a bar-like resolution
$\mathcal{B}^d(A)$ in the context of simplicial modules. Besides the general
case, we give explicit detail corresponding to $S^3$. We also present a
description of what can replace these bar-like resolutions in order to aid with
computation. The cohomology version can be done following a similar
construction, of which we make mention.
",mathematics
"  The fusion of Iterative Closest Point (ICP) reg- istrations in existing state
estimation frameworks relies on an accurate estimation of their uncertainty. In
this paper, we study the estimation of this uncertainty in the form of a
covariance. First, we scrutinize the limitations of existing closed-form
covariance estimation algorithms over 3D datasets. Then, we set out to estimate
the covariance of ICP registrations through a data-driven approach, with over 5
100 000 registrations on 1020 pairs from real 3D point clouds. We assess our
solution upon a wide spectrum of environments, ranging from structured to
unstructured and indoor to outdoor. The capacity of our algorithm to predict
covariances is accurately assessed, as well as the usefulness of these
estimations for uncertainty estimation over trajectories. The proposed method
estimates covariances better than existing closed-form solutions, and makes
predictions that are consistent with observed trajectories.
",computer-science
"  The optimal learner for prediction modeling varies depending on the
underlying data-generating distribution. Super Learner (SL) is a generic
ensemble learning algorithm that uses cross-validation to select among a
""library"" of candidate prediction models. The SL is not restricted to a single
prediction model, but uses the strengths of a variety of learning algorithms to
adapt to different databases. While the SL has been shown to perform well in a
number of settings, it has not been thoroughly evaluated in large electronic
healthcare databases that are common in pharmacoepidemiology and comparative
effectiveness research. In this study, we applied and evaluated the performance
of the SL in its ability to predict treatment assignment using three electronic
healthcare databases. We considered a library of algorithms that consisted of
both nonparametric and parametric models. We also considered a novel strategy
for prediction modeling that combines the SL with the high-dimensional
propensity score (hdPS) variable selection algorithm. Predictive performance
was assessed using three metrics: the negative log-likelihood, area under the
curve (AUC), and time complexity. Results showed that the best individual
algorithm, in terms of predictive performance, varied across datasets. The SL
was able to adapt to the given dataset and optimize predictive performance
relative to any individual learner. Combining the SL with the hdPS was the most
consistent prediction method and may be promising for PS estimation and
prediction modeling in electronic healthcare databases.
",statistics
"  Model selection in mixed models based on the conditional distribution is
appropriate for many practical applications and has been a focus of recent
statistical research. In this paper we introduce the R-package cAIC4 that
allows for the computation of the conditional Akaike Information Criterion
(cAIC). Computation of the conditional AIC needs to take into account the
uncertainty of the random effects variance and is therefore not
straightforward. We introduce a fast and stable implementation for the
calculation of the cAIC for linear mixed models estimated with lme4 and
additive mixed models estimated with gamm4 . Furthermore, cAIC4 offers a
stepwise function that allows for a fully automated stepwise selection scheme
for mixed models based on the conditional AIC. Examples of many possible
applications are presented to illustrate the practical impact and easy handling
of the package.
",statistics
"  Given a tournament T and a positive integer k, the C_3-Pakcing-T problem asks
if there exists a least k (vertex-)disjoint directed 3-cycles in T. This is the
dual problem in tournaments of the classical minimal feedback vertex set
problem. Surprisingly C_3-Pakcing-T did not receive a lot of attention in the
literature. We show that it does not admit a PTAS unless P=NP, even if we
restrict the considered instances to sparse tournaments, that is tournaments
with a feedback arc set (FAS) being a matching. Focusing on sparse tournaments
we provide a (1+6/(c-1)) approximation algorithm for sparse tournaments having
a linear representation where all the backward arcs have ""length"" at least c.
Concerning kernelization, we show that C_3-Pakcing-T admits a kernel with O(m)
vertices, where m is the size of a given feedback arc set. In particular, we
derive a O(k) vertices kernel for C_3-Pakcing-T when restricted to sparse
instances. On the negative size, we show that C_3-Pakcing-T does not admit a
kernel of (total bit) size O(k^{2-\epsilon}) unless NP is a subset of coNP /
Poly. The existence of a kernel in O(k) vertices for C_3-Pakcing-T remains an
open question.
",computer-science
"  Deep convolution neural networks demonstrate impressive results in the
super-resolution domain. A series of studies concentrate on improving peak
signal noise ratio (PSNR) by using much deeper layers, which are not friendly
to constrained resources. Pursuing a trade-off between the restoration capacity
and the simplicity of models is still non-trivial. Recent contributions are
struggling to manually maximize this balance, while our work achieves the same
goal automatically with neural architecture search. Specifically, we handle
super-resolution with a multi-objective approach. We also propose an elastic
search tactic at both micro and macro level, based on a hybrid controller that
profits from evolutionary computation and reinforcement learning. Quantitative
experiments help us to draw a conclusion that our generated models dominate
most of the state-of-the-art methods with respect to the individual FLOPS.
",computer-science
"  We use the ab initio Bethe Ansatz dynamics to predict the dissociation of
one-dimensional cold-atom breathers that are created by a quench from a
fundamental soliton. We find that the dissociation is a robust quantum
many-body effect, while in the mean-field (MF) limit the dissociation is
forbidden by the integrability of the underlying nonlinear Schrödinger
equation. The analysis demonstrates the possibility to observe quantum
many-body effects without leaving the MF range of experimental parameters. We
find that the dissociation time is of the order of a few seconds for a typical
atomic-soliton setting.
",physics
"  Rydberg atoms have attracted considerable interest due to their huge
interaction among each other and with external fields. They demonstrate
characteristic scaling laws in dependence on the principal quantum number $n$
for features such as the magnetic field for level crossing. While bearing
striking similarities to Rydberg atoms, fundamentally new insights may be
obtained for Rydberg excitons, as the crystal environment gives easy optical
access to many states within an exciton multiplet. Here we study experimentally
and theoretically the scaling of several characteristic parameters of Rydberg
excitons with $n$. From absorption spectra in magnetic field we find for the
first crossing of levels with adjacent principal quantum numbers a $B_r \propto
n^{-4}$ dependence of the resonance field strength, $B_r$, due to the dominant
paramagnetic term unlike in the atomic case where the diamagnetic contribution
is decisive. By contrast, in electric field we find scaling laws just like for
Rydberg atoms. The resonance electric field strength scales as $E_r \propto
n^{-5}$. We observe anticrossings of the states belonging to multiplets with
different principal quantum numbers. The energy splittings at the avoided
crossings scale as $n^{-4}$ which we relate to the crystal specific deviation
of the exciton Hamiltonian from the hydrogen model. We observe the exciton
polarizability in the electric field to scale as $n^7$. In magnetic field the
crossover field strength from a hydrogen-like exciton to a magnetoexciton
dominated by electron and hole Landau level quantization scales as $n^{-3}$.
The ionization voltages demonstrate a $n^{-4}$ scaling as for atoms. The width
of the absorption lines remains constant before dissociation for high enough
$n$, while for small $n \lesssim 12$ an exponential increase with the field is
found. These results are in excellent agreement with theoretical calculations.
",physics
"  Random feature maps are ubiquitous in modern statistical machine learning,
where they generalize random projections by means of powerful, yet often
difficult to analyze nonlinear operators. In this paper, we leverage the
""concentration"" phenomenon induced by random matrix theory to perform a
spectral analysis on the Gram matrix of these random feature maps, here for
Gaussian mixture models of simultaneously large dimension and size. Our results
are instrumental to a deeper understanding on the interplay of the nonlinearity
and the statistics of the data, thereby allowing for a better tuning of random
feature-based techniques.
",statistics
"  A graph is said to be symmetric if its automorphism group is transitive on
its arcs. Guo et al. (Electronic J. Combin. 18, \#P233, 2011) and Pan et al.
(Electronic J. Combin. 20, \#P36, 2013) determined all pentavalent symmetric
graphs of order $4pq$. In this paper, we shall generalize this result by
determining all connected pentavalent symmetric graphs of order four times an
odd square-free integer. It is shown in this paper that, for each of such
graphs $\it\Gamma$, either the full automorphism group ${\sf Aut}\it\Gamma$ is
isomorphic to ${\sf PSL}(2,p)$, ${\sf PGL}(2,p)$, ${\sf
PSL}(2,p){\times}\mathbb{Z}_2$ or ${\sf PGL}(2,p){\times}\mathbb{Z}_2$, or
$\it\Gamma$ is isomorphic to one of 8 graphs.
",mathematics
"  Instance- and label-dependent label noise (ILN) is widely existed in
real-world datasets but has been rarely studied. In this paper, we focus on a
particular case of ILN where the label noise rates, representing the
probabilities that the true labels of examples flip into the corrupted labels,
have upper bounds. We propose to handle this bounded instance- and
label-dependent label noise under two different conditions. First,
theoretically, we prove that when the marginal distributions $P(X|Y=+1)$ and
$P(X|Y=-1)$ have non-overlapping supports, we can recover every noisy example's
true label and perform supervised learning directly on the cleansed examples.
Second, for the overlapping situation, we propose a novel approach to learn a
well-performing classifier which needs only a few noisy examples to be labeled
manually. Experimental results demonstrate that our method works well on both
synthetic and real-world datasets.
",statistics
"  We provide a compositional coalgebraic semantics for strategic games. In our
framework, like in the semantics of functional programming languages,
coalgebras represent the observable behaviour of systems derived from the
behaviour of the parts over an unobservable state space. We use coalgebras to
describe and program stage games, finitely and potentially infinitely repeated
hierarchical or parallel games with imperfect and incomplete information based
on deterministic, non-deterministic or probabilistic decisions of learning
agents in possibly endogenous networks. Our framework is compositional in that
arbitrarily complex network of games can be composed. The coalgebraic approach
allows to represent self-referential or reflexive structures like institutional
dynamics, strategic network formation from within the network, belief
formation, learning agents or other self-referential phenomena that
characterise complex social systems of cognitive agents. And finally our games
represent directly runnable code in functional programming languages that can
also be analysed by sophisticated verification and logical tools of software
engineering.
",computer-science
"  The aim of this work is to propose a first coarse-grained model of Bacillus
subtilis cell wall, handling explicitly the existence of multiple layers of
peptidoglycans. In this first work, we aim at the validation of the recently
proposed ""three under two"" principle.
",physics
"  The maximum speed at which a liquid can wet a solid is limited by the need to
displace gas lubrication films in front of the moving contact line. The
characteristic height of these films is often comparable to the mean free path
in the gas so that hydrodynamic models do not adequately describe the flow
physics. This Letter develops a model which incorporates kinetic effects in the
gas, via the Boltzmann equation, and can predict experimentally-observed
increases in the maximum speed of wetting when (a) the liquid's viscosity is
varied, (b) the ambient gas pressure is reduced or (c) the meniscus is
confined.
",physics
"  We establish lower bounds on the volume and the surface area of a geometric
body using the size of its slices along different directions. In the first part
of the paper, we derive volume bounds for convex bodies using generalized
subadditivity properties of entropy combined with entropy bounds for
log-concave random variables. In the second part, we investigate a new notion
of Fisher information which we call the $L_1$-Fisher information, and show that
certain superadditivity properties of the $L_1$-Fisher information lead to
lower bounds for the surface areas of polyconvex sets in terms of its slices.
",computer-science
"  For subspace estimation with an unknown colored noise, Factor Analysis (FA)
is a good candidate for replacing the popular eigenvalue decomposition (EVD).
Finding the unknowns in factor analysis can be done by solving a non-linear
least square problem. For this type of optimization problems, the Gauss-Newton
(GN) algorithm is a powerful and simple method. The most expensive part of the
GN algorithm is finding the direction of descent by solving a system of
equations at each iteration. In this paper we show that for FA, the matrices
involved in solving these systems of equations can be diagonalized in a closed
form fashion and the solution can be found in a computationally efficient way.
We show how the unknown parameters can be updated without actually constructing
these matrices. The convergence performance of the algorithm is studied via
numerical simulations.
",statistics
"  The Large Synoptic Survey Telescope (LSST) will generate light curves for
approximately 1 billion stars. Our previous work has demonstrated that, by the
end of the LSST 10 year mission, large numbers of transiting exoplanetary
systems could be recovered using the LSST ""deep drilling"" cadence. Here we
extend our previous work to examine how the recoverability of transiting
planets over a range of orbital periods and radii evolves per year of LSST
operation. As specific example systems we consider hot Jupiters orbiting
solar-type stars and hot Neptunes orbiting K-Dwarfs at distances from Earth of
several kpc, as well as super-Earths orbiting nearby low-mass M-dwarfs. The
detection of transiting planets increases steadily with the accumulation of
data over time, generally becoming large (greater than 10 percent) after 4 - 6
years of operation. However, we also find that short-period (less than 2 day)
hot Jupiters orbiting G-dwarfs and hot Neptunes orbiting K-dwarfs can already
be discovered within the first 1 - 2 years of LSST operation.
",physics
"  Thermodynamic potential of a neutral two-dimensional (2D) Cou\-lomb fluid,
confined to a large domain with a smooth boundary, exhibits at any (inverse)
temperature $\beta$ a logarithmic finite-size correction term whose universal
prefactor depends only on the Euler number of the domain and the conformal
anomaly number $c=-1$. A minimal free boson conformal field theory, which is
equivalent to the 2D symmetric two-component plasma of elementary $\pm e$
charges at coupling constant $\Gamma=\beta e^2$, was studied in the past. It
was shown that creating a non-neutrality by spreading out a charge $Q e$ at
infinity modifies the anomaly number to $c(Q,\Gamma) = - 1 + 3\Gamma Q^2$.
Here, we study the effect of non-neutrality on the finite-size expansion of the
free energy for another Coulomb fluid, namely the 2D one-component plasma
(jellium) composed of identical pointlike $e$-charges in a homogeneous
background surface charge density. For the disk geometry of the confining
domain we find that the non-neutrality induces the same change of the anomaly
number in the finite-size expansion. We derive this result first at the
free-fermion coupling $\Gamma\equiv\beta e^2=2$ and then, by using a mapping of
the 2D one-component plasma onto an anticommuting field theory formulated on a
chain, for an arbitrary coupling constant.
",physics
"  To gain control over magnetic order on ultrafast time scales, a fundamental
understanding of the way electron spins interact with the surrounding crystal
lattice is required. However, measurement and analysis even of basic collective
processes such as spin-phonon equilibration have remained challenging. Here, we
directly probe the flow of energy and angular momentum in the model insulating
ferrimagnet yttrium iron garnet. Following ultrafast resonant lattice
excitation, we observe that magnetic order reduces on distinct time scales of 1
ps and 100 ns. Temperature-dependent measurements, a spin-coupling analysis and
simulations show that the two dynamics directly reflect two stages of
spin-lattice equilibration. On the 1-ps scale, spins and phonons reach
quasi-equilibrium in terms of energy through phonon-induced modulation of the
exchange interaction. This mechanism leads to identical demagnetization of the
ferrimagnet's two spin-sublattices and a novel ferrimagnetic state of increased
temperature yet unchanged total magnetization. Finally, on the much slower,
100-ns scale, the excess of spin angular momentum is released to the crystal
lattice, resulting in full equilibrium. Our findings are relevant for all
insulating ferrimagnets and indicate that spin manipulation by phonons,
including the spin Seebeck effect, can be extended to antiferromagnets and into
the terahertz frequency range.
",physics
"  In contrast with the well-known methods of matching asymptotics and
multiscale (or compound) asymptotics, the "" functional analytic approach "" of
Lanza de Cristoforis (Analysis 28, 2008) allows to prove convergence of
expansions around interior small holes of size $\epsilon$ for solutions of
elliptic boundary value problems. Using the method of layer potentials, the
asymptotic behavior of the solution as $\epsilon$ tends to zero is described
not only by asymptotic series in powers of $\epsilon$, but by convergent power
series. Here we use this method to investigate the Dirichlet problem for the
Laplace operator where holes are collapsing at a polygonal corner of opening
$\omega$. Then in addition to the scale $\epsilon$ there appears the scale
$\eta = \epsilon^{\pi/\omega}$. We prove that when $\pi/\omega$ is irrational,
the solution of the Dirichlet problem is given by convergent series in powers
of these two small parameters. Due to interference of the two scales, this
convergence is obtained, in full generality, by grouping together integer
powers of the two scales that are very close to each other. Nevertheless, there
exists a dense subset of openings $\omega$ (characterized by Diophantine
approximation properties), for which real analyticity in the two variables
$\epsilon$ and $\eta$ holds and the power series converge unconditionally. When
$\pi/\omega$ is rational, the series are unconditionally convergent, but
contain terms in log $\epsilon$.
",mathematics
"  Given any Koszul algebra of finite global dimension one can define a new
algebra, which we call a higher zigzag algebra, as a twisted trivial extension
of the Koszul dual of our original algebra. If our original algebra is the path
algebra of a quiver whose underlying graph is a tree, this construction
recovers the zigzag algebras of Huerfano and Khovanov. We study examples of
higher zigzag algebras coming from Iyama's iterative construction of type A
higher representation finite algebras. We give presentations of these algebras
by quivers and relations, and describe relations between spherical twists
acting on their derived categories. We then make a connection to the McKay
correspondence in higher dimensions: if G is a finite abelian subgroup of the
special linear group acting on affine space, then the skew group algebra which
controls the category of G-equivariant sheaves is Koszul dual to a higher
zigzag algebra. Using this, we show that our relations between spherical twists
appear naturally in examples from algebraic geometry.
",mathematics
"  This paper is concerned with the detection of objects immersed in anisotropic
media from boundary measurements. We propose an accurate approach based on the
Kohn-Vogelius formulation and the topological sensitivity analysis method. The
inverse problem is formulated as a topology optimization one minimizing an
energy like functional. A topological asymptotic expansion is derived for the
anisotropic Laplace operator. The unknown object is reconstructed using a
level-set curve of the topological gradient. The efficiency and accuracy of the
proposed algorithm are illustrated by some numerical results. MOTS-CLÉS :
Problème inverse géométrique, Laplace anisotrope, formulation de
Kohn-Vogelius, analyse de sensibilité, optimisation topologique.
",mathematics
"  Let us say that an $n$-sided polygon is semi-regular if it is
circumscriptible and its angles are all equal but possibly one, which is then
larger than the rest. Regular polygons, in particular, are semi-regular. We
prove that semi-regular polygons are spectrally determined in the class of
convex piecewise smooth domains. Specifically, we show that if $\Omega$ is a
convex piecewise smooth planar domain, possibly with straight corners, whose
Dirichlet or Neumann spectrum coincides with that of an $n$-sided semi-regular
polygon $P_n$, then $\Omega$ is congruent to $P_n$.
",mathematics
"  We review some recent results on geometric equations on Lorentzian manifolds
such as the wave and Dirac equations. This includes well-posedness and
stability for various initial value problems, as well as results on the
structure of these equations on black-hole spacetimes (in particular, on the
Kerr solution), the index theorem for hyperbolic Dirac operators and properties
of the class of Green-hyperbolic operators.
",mathematics
"  We consider the problem of probabilistic projection of the total fertility
rate (TFR) for subnational regions. We seek a method that is consistent with
the UN's recently adopted Bayesian method for probabilistic TFR projections for
all countries, and works well for all countries. We assess various possible
methods using subnational TFR data for 47 countries. We find that the method
that performs best in terms of out-of-sample predictive performance and also in
terms of reproducing the within-country correlation in TFR is a method that
scales the national trajectory by a region-specific scale factor that is
allowed to vary slowly over time. This supports the hypothesis of Watkins
(1990, 1991) that within-country TFR converges over time in response to
country-specific factors, and extends the Watkins hypothesis to the last 50
years and to a much wider range of countries around the world.
",statistics
"  High Performance Computing is often performed on scarce and shared computing
resources. To ensure computers are used to their full capacity, administrators
often incentivize large workloads that are not possible on smaller systems.
Measurements in Lattice QCD frequently do not scale to machine-size workloads.
By bundling tasks together we can create large jobs suitable for gigantic
partitions. We discuss METAQ and mpi_jm, software developed to dynamically
group computational tasks together, that can intelligently backfill to consume
idle time without substantial changes to users' current workflows or
executables.
",physics
"  Many seminal results in Interactive Proofs (IPs) use algebraic techniques
based on low-degree polynomials, the study of which is pervasive in theoretical
computer science. Unfortunately, known methods for endowing such proofs with
zero knowledge guarantees do not retain this rich algebraic structure.
In this work, we develop algebraic techniques for obtaining zero knowledge
variants of proof protocols in a way that leverages and preserves their
algebraic structure. Our constructions achieve unconditional (perfect) zero
knowledge in the Interactive Probabilistically Checkable Proof (IPCP) model of
Kalai and Raz [KR08] (the prover first sends a PCP oracle, then the prover and
verifier engage in an Interactive Proof in which the verifier may query the
PCP).
Our main result is a zero knowledge variant of the sumcheck protocol [LFKN92]
in the IPCP model. The sumcheck protocol is a key building block in many IPs,
including the protocol for polynomial-space computation due to Shamir [Sha92],
and the protocol for parallel computation due to Goldwasser, Kalai, and
Rothblum [GKR15]. A core component of our result is an algebraic commitment
scheme, whose hiding property is guaranteed by algebraic query complexity lower
bounds [AW09,JKRS09]. This commitment scheme can then be used to considerably
strengthen our previous work [BCFGRS16] that gives a sumcheck protocol with
much weaker zero knowledge guarantees, itself using algebraic techniques based
on algorithms for polynomial identity testing [RS05,BW04].
We demonstrate the applicability of our techniques by deriving zero knowledge
variants of well-known protocols based on algebraic techniques, including the
protocols of Shamir and of Goldwasser, Kalai, and Rothblum, as well as the
protocol of Babai, Fortnow, and Lund [BFL91].
",computer-science
"  High-dose-rate brachytherapy is a tumor treatment method where a highly
radioactive source is brought in close proximity to the tumor. In this paper we
develop a simulated annealing algorithm to optimize the dwell times at
preselected dwell positions to maximize tumor coverage under dose-volume
constraints on the organs at risk. Compared to existing algorithms, our
algorithm has advantages in terms of speed and objective value and does not
require an expensive general purpose solver. Its success mainly depends on
exploiting the efficiency of matrix multiplication and a careful selection of
the neighboring states. In this paper we outline its details and make an
in-depth comparison with existing methods using real patient data.
",physics
"  Micro Aerial Vehicles (MAVs) are limited in their operation outdoors near
obstacles by their ability to withstand wind gusts. Currently widespread
position control methods such as Proportional Integral Derivative control do
not perform well under the influence of gusts. Incremental Nonlinear Dynamic
Inversion (INDI) is a sensor-based control technique that can control nonlinear
systems subject to disturbances. It was developed for the attitude control of
manned aircraft or MAVs. In this paper we generalize this method to the outer
loop control of MAVs under severe gust loads. Significant improvements over a
traditional Proportional Integral Derivative (PID) controller are demonstrated
in an experiment where the quadrotor flies in and out of a windtunnel exhaust
at 10 m/s. The control method does not rely on frequent position updates, as is
demonstrated in an outside experiment using a standard GPS module. Finally, we
investigate the effect of using a linearization to calculate thrust vector
increments, compared to a nonlinear calculation. The method requires little
modeling and is computationally efficient.
",computer-science
"  A repulsive Coulomb interaction between electrons in different orbitals in
correlated materials can give rise to bound quasiparticle states. We study the
non-hybridized two-orbital Hubbard model with intra (inter)-orbital interaction
$U$ ($U_{12}$) and different band widths using an improved dynamical mean field
theory numerical technique which leads to reliable spectra on the real energy
axis directly at zero temperature. We find that a finite density of states at
the Fermi energy in one band is correlated with the emergence of well defined
quasiparticle states at excited energies $\Delta=U-U_{12}$ in the other band.
These excitations are inter-band holon-doublon bound states. At the symmetric
point $U=U_{12}$, the quasiparticle peaks are located at the Fermi energy,
leading to a simultaneous and continuous Mott transition settling a
long-standing controversy.
",physics
"  We use plasmon rulers to follow the conformational dynamics of a single
protein for up to 24 h at a video rate. The plasmon ruler consists of two gold
nanospheres connected by a single protein linker. In our experiment, we follow
the dynamics of the molecular chaperone heat shock protein 90, which is known
to show open and closed conformations. Our measurements confirm the previously
known conformational dynamics with transition times in the second to minute
time scale and reveals new dynamics on the time scale of minutes to hours.
Plasmon rulers thus extend the observation bandwidth 3/4 orders of magnitude
with respect to single-molecule fluorescence resonance energy transfer and
enable the study of molecular dynamics with unprecedented precision.
",quantitative-biology
"  Mobile phones identification through their built in components has been
demonstrated in literature for various types of sensors including the camera,
microphones and accelerometers. The identification is performed by the
exploitation of the small but significant differences in the electronic
circuits generated during the production process. Thus, these differences
become an intrinsic property of the electronic components, which can be
detected and become an unique fingerprint of the component and of the mobile
phone. In this paper, we investigate the identification of mobile phones
through their builtin magnetometers, which has not been reported in literature
yet. Magnetometers are stimulated with different waveforms using a solenoid
connected to a computer s audio board. The identification is performed
analyzing the digital output of the magnetometer through the use of statistical
features and the Support Vector Machine (SVM) machine learning algorithm. We
prove that this technique can distinguish different models and brands with very
high accuracy but it can only distinguish phones of the same model with limited
accuracy.
",computer-science
"  We give a counter example to the new theorem that appeared in the survey
\cite{H} on Artin approximation. We then provide a correct statement and a
proof of it.
",mathematics
"  Remanufacturing is a significant factor in securing sustainability through a
circular economy. Sorting plays a significant role in remanufacturing
pre-processing inspections. Its significance can increase when remanufacturing
facilities encounter extreme situations, such as abnormally huge core arrivals.
Our main objective in this work is switching from less efficient to a more
efficient model and to characterize extreme behavior of core arrival in
remanufacturing and applying the developed model to triage cores. Central
tendency core flow models are not sufficient to handle extreme situations,
however, complementary Extreme Value (EV) approaches have shown to improve
model efficiency. Extreme core flows to remanufacturing facilities are rare but
still likely and can adversely affect remanufacturing business operations. In
this investigation, extreme end-of-use core flow is modelled by a threshold
approach using the Generalized Pareto Distribution (GPD). It is shown that GPD
has better performance than its maxima-block GEV counterpart from practical and
data efficiency perspectives. The model is validated by a synthesized big
dataset, tested by sophisticated statistical Anderson Darling (AD) test, and is
applied to a case of extreme flow to a valve shop in order to predict
probability of over-capacity arrivals that is critical in remanufacturing
business management. Finally, the GPD model combined with triage strategies is
used to initiate investigations into the efficacy of different triage methods
in remanufacturing operations.
",statistics
"  Many successful methods have been proposed for learning low dimensional
representations on large-scale networks, while almost all existing methods are
designed in inseparable processes, learning embeddings for entire networks even
when only a small proportion of nodes are of interest. This leads to great
inconvenience, especially on super-large or dynamic networks, where these
methods become almost impossible to implement. In this paper, we formalize the
problem of separated matrix factorization, based on which we elaborate a novel
objective function that preserves both local and global information. We further
propose SepNE, a simple and flexible network embedding algorithm which
independently learns representations for different subsets of nodes in
separated processes. By implementing separability, our algorithm reduces the
redundant efforts to embed irrelevant nodes, yielding scalability to
super-large networks, automatic implementation in distributed learning and
further adaptations. We demonstrate the effectiveness of this approach on
several real-world networks with different scales and subjects. With comparable
accuracy, our approach significantly outperforms state-of-the-art baselines in
running times on large networks.
",computer-science
"  We consider SIS contagion processes over networks where, a classical
assumption is that individuals' decisions to adopt a contagion are based on
their immediate neighbors. However, recent literature shows that some
attributes are more correlated between two-hop neighbors, a concept referred to
as monophily. This motivates us to explore monophilic contagion, the case where
a contagion (e.g. a product, disease) is adopted by considering two-hop
neighbors instead of immediate neighbors (e.g. you ask your friend about the
new iPhone and she recommends you the opinion of one of her friends). We show
that the phenomenon called friendship paradox makes it easier for the
monophilic contagion to spread widely. We also consider the case where the
underlying network stochastically evolves in response to the state of the
contagion (e.g. depending on the severity of a flu virus, people restrict their
interactions with others to avoid getting infected) and show that the dynamics
of such a process can be approximated by a differential equation whose
trajectory satisfies an algebraic constraint restricting it to a manifold. Our
results shed light on how graph theoretic consequences affect contagions and,
provide simple deterministic models to approximate the collective dynamics of
contagions over stochastic graph processes.
",computer-science
"  An interactive session of video-on-demand (VOD) streaming procedure deserves
smooth data transportation for the viewer, irrespective of their geographic
location. To access the required video, bandwidth management during the video
objects transportation at any interactive session is a mandatory prerequisite.
It has been observed in the domain likes movie on demand, electronic
encyclopedia, interactive games, and educational resources. The required data
is imported from the distributed storage servers through the high speed
backbone network. This paper presents the viewer driven session based
multi-user model with respect to the overlay mesh network. In virtue of
reality, the direct implication of this work elaborately shows the required
bandwidth is a causal part in the video on demand system. The analytic model of
session based single viewer bandwidth requirement model presents the bandwidth
requirement for any interactive session like, pause, move slow, rewind, skip
some number of frames, or move fast with some constant number of frames. This
work presents the bandwidth requirement model for any interactive session that
brings the trade-off in data-transportation and storage costs for different
system resources and also for the various system configurations.
",computer-science
"  We begin by summarizing the relevance and importance of inductive analytics
based on the geometry and topology of data and information. Contemporary issues
are then discussed. These include how sampling data for representativity is
increasingly to be questioned. While we can always avail of analytics from a
""bag of tools and techniques"", in the application of machine learning and
predictive analytics, nonetheless we present the case for Bourdieu and
Benzécri-based science of data, as follows. This is to construct bridges
between data sources and position-taking, and decision-making. There is summary
presentation of a few case studies, illustrating and exemplifying application
domains.
",computer-science
"  In this article, we provide a new algorithm for solving constraint
satisfaction problems over templates with few subpowers, by reducing the
problem to the combination of solvability of a polynomial number of systems of
linear equations over finite fields and reductions via absorbing subuniverses.
",mathematics
"  In this brief review we discuss the transient processes in solids under
irradiation with femtosecond X-ray free-electron-laser (FEL) pulses and
swift-heavy ions (SHI). Both kinds of irradiation produce highly excited
electrons in a target on extremely short timescales. Transfer of the excess
electronic energy into the lattice may lead to observable target modifications
such as phase transitions and damage formation. Transient kinetics of material
excitation and relaxation under FEL or SHI irradiation are comparatively
discussed. The same origin for the electronic and atomic relaxation in both
cases is demonstrated. Differences in these kinetics introduced by the
geometrical effects ({\mu}m-size of a laser spot vs nm-size of an ion track)
and initial irradiation (photoabsorption vs an ion impact) are analyzed. The
basic mechanisms of electron transport and electron-lattice coupling are
addressed. Appropriate models and their limitations are presented.
Possibilities of thermal and nonthermal melting of materials under FEL and SHI
irradiation are discussed.
",physics
"  Physicists at the Large Hadron Collider (LHC) rely on detailed simulations of
particle collisions to build expectations of what experimental data may look
like under different theory modeling assumptions. Petabytes of simulated data
are needed to develop analysis techniques, though they are expensive to
generate using existing algorithms and computing resources. The modeling of
detectors and the precise description of particle cascades as they interact
with the material in the calorimeter are the most computationally demanding
steps in the simulation pipeline. We therefore introduce a deep neural
network-based generative model to enable high-fidelity, fast, electromagnetic
calorimeter simulation. There are still challenges for achieving precision
across the entire phase space, but our current solution can reproduce a variety
of particle shower properties while achieving speed-up factors of up to
100,000$\times$. This opens the door to a new era of fast simulation that could
save significant computing time and disk space, while extending the reach of
physics searches and precision measurements at the LHC and beyond.
",statistics
"  Starting from a summary of detection statistics of our recent X-shooter
campaign, we review the major surveys, both space and ground based, for
emission counterparts of high-redshift damped Ly$\alpha$ absorbers (DLAs)
carried out since the first detection 25 years ago. We show that the detection
rates of all surveys are precisely reproduced by a simple model in which the
metallicity and luminosity of the galaxy associated to the DLA follow a
relation of the form, ${\rm M_{UV}} = -5 \times \left(\,[{\rm M/H}] + 0.3\,
\right) - 20.8$, and the DLA cross-section follows a relation of the form
$\sigma_{DLA} \propto L^{0.8}$. Specifically, our spectroscopic campaign
consists of 11 DLAs preselected based on their equivalent width of SiII
$\lambda1526$ to have a metallicity higher than [Si/H] > -1. The targets have
been observed with the X-shooter spectrograph at the Very Large Telescope to
search for emission lines around the quasars. We observe a high detection rate
of 64% (7/11), significantly higher than the typical $\sim$10% for random,
HI-selected DLA samples. We use the aforementioned model, to simulate the
results of our survey together with a range of previous surveys: spectral
stacking, direct imaging (using the `double DLA' technique), long-slit
spectroscopy, and integral field spectroscopy. Based on our model results, we
are able to reconcile all results. Some tension is observed between model and
data when looking at predictions of Ly$\alpha$ emission for individual targets.
However, the object to object variations are most likely a result of the
significant scatter in the underlying scaling relations as well as
uncertainties in the amount of dust which affects the emission.
",physics
"  We relate the minimax game of generative adversarial networks (GANs) to
finding the saddle points of the Lagrangian function for a convex optimization
problem, where the discriminator outputs and the distribution of generator
outputs play the roles of primal variables and dual variables, respectively.
This formulation shows the connection between the standard GAN training process
and the primal-dual subgradient methods for convex optimization. The inherent
connection does not only provide a theoretical convergence proof for training
GANs in the function space, but also inspires a novel objective function for
training. The modified objective function forces the distribution of generator
outputs to be updated along the direction according to the primal-dual
subgradient methods. A toy example shows that the proposed method is able to
resolve mode collapse, which in this case cannot be avoided by the standard GAN
or Wasserstein GAN. Experiments on both Gaussian mixture synthetic data and
real-world image datasets demonstrate the performance of the proposed method on
generating diverse samples.
",statistics
"  Cryptovirological augmentations present an immediate, incomparable threat.
Over the last decade, the substantial proliferation of crypto-ransomware has
had widespread consequences for consumers and organisations alike. Established
preventive measures perform well, however, the problem has not ceased. Reverse
engineering potentially malicious software is a cumbersome task due to platform
eccentricities and obfuscated transmutation mechanisms, hence requiring
smarter, more efficient detection strategies. The following manuscript presents
a novel approach for the classification of cryptographic primitives in compiled
binary executables using deep learning. The model blueprint, a DCNN, is
fittingly configured to learn from variable-length control flow diagnostics
output from a dynamic trace. To rival the size and variability of contemporary
data compendiums, hence feeding the model cognition, a methodology for the
procedural generation of synthetic cryptographic binaries is defined, utilising
core primitives from OpenSSL with multivariate obfuscation, to draw a vastly
scalable distribution. The library, CryptoKnight, rendered an algorithmic pool
of AES, RC4, Blowfish, MD5 and RSA to synthesis combinable variants which are
automatically fed in its core model. Converging at 91% accuracy, CryptoKnight
is successfully able to classify the sample algorithms with minimal loss.
",computer-science
"  In this paper, we consider a dataset comprising press releases about health
research from different universities in the UK along with a corresponding set
of news articles. First, we do an exploratory analysis to understand how the
basic information published in the scientific journals get exaggerated as they
are reported in these press releases or news articles. This initial analysis
shows that some news agencies exaggerate almost 60\% of the articles they
publish in the health domain; more than 50\% of the press releases from certain
universities are exaggerated; articles in topics like lifestyle and childhood
are heavily exaggerated. Motivated by the above observation we set the central
objective of this paper to investigate how exaggerated news spreads over an
online social network like Twitter. The LIWC analysis points to a remarkable
observation these late tweets are essentially laden in words from opinion and
realize categories which indicates that, given sufficient time, the wisdom of
the crowd is actually able to tell apart the exaggerated news. As a second step
we study the characteristics of the users who never or rarely post exaggerated
news content and compare them with those who post exaggerated news content more
frequently. We observe that the latter class of users have less retweets or
mentions per tweet, have significantly more number of followers, use more slang
words, less hyperbolic words and less word contractions. We also observe that
the LIWC categories like bio, health, body and negative emotion are more
pronounced in the tweets posted by the users in the latter class. As a final
step we use these observations as features and automatically classify the two
groups achieving an F1 score of 0.83.
",computer-science
"  This paper introduces PriMaL, a general PRIvacy-preserving MAchine-Learning
method for reducing the privacy cost of information transmitted through a
network. Distributed sensor networks are often used for automated
classification and detection of abnormal events in high-stakes situations, e.g.
fire in buildings, earthquakes, or crowd disasters. Such networks might
transmit privacy-sensitive information, e.g. GPS location of smartphones, which
might be disclosed if the network is compromised. Privacy concerns might slow
down the adoption of the technology, in particular in the scenario of social
sensing where participation is voluntary, thus solutions are needed which
improve privacy without compromising on the event detection accuracy. PriMaL is
implemented as a machine-learning layer that works on top of an existing event
detection algorithm. Experiments are run in a general simulation framework, for
several network topologies and parameter values. The privacy footprint of
state-of-the-art event detection algorithms is compared within the proposed
framework. Results show that PriMaL is able to reduce the privacy cost of a
distributed event detection algorithm below that of the corresponding
centralized algorithm, within the bounds of some assumptions about the
protocol. Moreover the performance of the distributed algorithm is not
statistically worse than that of the centralized algorithm.
",computer-science
"  We present a novel Affine-Gradient based Local Binary Pattern (AGLBP)
descriptor for texture classification. It is very hard to describe complicated
texture using single type information, such as Local Binary Pattern (LBP),
which just utilizes the sign information of the difference between the pixel
and its local neighbors. Our descriptor has three characteristics: 1) In order
to make full use of the information contained in the texture, the
Affine-Gradient, which is different from Euclidean-Gradient and invariant to
affine transformation is incorporated into AGLBP. 2) An improved method is
proposed for rotation invariance, which depends on the reference direction
calculating respect to local neighbors. 3) Feature selection method,
considering both the statistical frequency and the intraclass variance of the
training dataset, is also applied to reduce the dimensionality of descriptors.
Experiments on three standard texture datasets, Outex12, Outex10 and KTH-TIPS2,
are conducted to evaluate the performance of AGLBP. The results show that our
proposed descriptor gets better performance comparing to some state-of-the-art
rotation texture descriptors in texture classification.
",computer-science
"  We present a new paradigm for understanding optical absorption and hot
electron dynamics experiments in graphene. Our analysis pivots on assigning
proper importance to phonon assisted indirect processes and bleaching of direct
processes. We show indirect processes figure in the excess absorption in the UV
region. Experiments which were thought to indicate ultrafast relaxation of
electrons and holes, reaching a thermal distribution from an extremely
non-thermal one in under 5-10 fs, instead are explained by the nascent electron
and hole distributions produced by indirect transitions. These need no
relaxation or ad-hoc energy removal to agree with the observed emission spectra
and fast pulsed absorption spectra. The fast emission following pulsed
absorption is dominated by phonon assisted processes, which vastly outnumber
direct ones and are always available, connecting any electron with any hole any
time. Calculations are given, including explicitly calculating the magnitude of
indirect processes, supporting these views.
",physics
"  In this work we present a whole-body Nonlinear Model Predictive Control
approach for Rigid Body Systems subject to contacts. We use a full dynamic
system model which also includes explicit contact dynamics. Therefore, contact
locations, sequences and timings are not prespecified but optimized by the
solver. Yet, thorough numerical and software engineering allows for running the
nonlinear Optimal Control solver at rates up to 190 Hz on a quadruped for a
time horizon of half a second. This outperforms the state of the art by at
least one order of magnitude. Hardware experiments in form of periodic and
non-periodic tasks are applied to two quadrupeds with different actuation
systems. The obtained results underline the performance, transferability and
robustness of the approach.
",computer-science
"  An upgrade of the ATLAS experiment for the High Luminosity phase of LHC is
planned for 2024 and foresees the replacement of the present Inner Detector
(ID) with a new Inner Tracker (ITk) completely made of silicon devices.
Depleted active pixel sensors built with the High Voltage CMOS (HV-CMOS)
technology are investigated as an option to cover large areas in the outermost
layers of the pixel detector and are especially interesting for the development
of monolithic devices which will reduce the production costs and the material
budget with respect to the present hybrid assemblies. For this purpose the
H35DEMO, a large area HV-CMOS demonstrator chip, was designed by KIT, IFAE and
University of Liverpool, and produced in AMS 350 nm CMOS technology. It
consists of four pixel matrices and additional test structures. Two of the
matrices include amplifiers and discriminator stages and are thus designed to
be operated as monolithic detectors. In these devices the signal is mainly
produced by charge drift in a small depleted volume obtained by applying a bias
voltage of the order of 100 V. Moreover, to enhance the radiation hardness of
the chip, this technology allows to enclose the electronics in the same deep
N-WELLs which are also used as collecting electrodes. In this contribution the
characterisation of H35DEMO chips and results of the very first beam test
measurements of the monolithic CMOS matrices with high energetic pions at CERN
SPS will be presented.
",physics
"  The increasing practice of engaging crowds, where organizations use IT to
connect with dispersed individuals for explicit resource creation purposes, has
precipitated the need to measure the precise processes and benefits of these
activities over myriad different implementations. In this work, we seek to
address these salient and non-trivial considerations by laying a foundation of
theory, measures, and research methods that allow us to test crowd-engagement
efficacy across organizations, industries, technologies, and geographies. To do
so, we anchor ourselves in the Theory of Crowd Capital, a generalizable
framework for studying IT-mediated crowd-engagement phenomena, and put forth an
empirical apparatus of testable measures and generalizable methods to begin to
unify the field of crowd science.
",computer-science
"  We characterize operator-theoretic properties (boundedness, compactness, and
Schatten class membership) of Toeplitz operators with positive measure symbols
on Bergman spaces of holomorphic hermitian line bundles over Kähler
Cartan-Hadamard manifolds in terms of geometric or operator-theoretic
properties of measures.
",mathematics
"  We present a novel algorithm for learning the spectral density of large scale
networks using stochastic trace estimation and the method of maximum entropy.
The complexity of the algorithm is linear in the number of non-zero elements of
the matrix, offering a computational advantage over other algorithms. We apply
our algorithm to the problem of community detection in large networks. We show
state-of-the-art performance on both synthetic and real datasets.
",statistics
"  Spatially extended population dynamics models that incorporate intrinsic
noise serve as case studies for the role of fluctuations and correlations in
biological systems. Including spatial structure and stochastic noise in
predator-prey competition invalidates the deterministic Lotka-Volterra picture
of neutral population cycles. Stochastic models yield long-lived erratic
population oscillations stemming from a resonant amplification mechanism. In
spatially extended predator-prey systems, one observes noise-stabilized
activity and persistent correlations. Fluctuation-induced renormalizations of
the oscillation parameters can be analyzed perturbatively. The critical
dynamics and the non-equilibrium relaxation kinetics at the predator extinction
threshold are characterized by the directed percolation universality class.
Spatial or environmental variability results in more localized patches which
enhances both species densities. Affixing variable rates to individual
particles and allowing for trait inheritance subject to mutations induces fast
evolutionary dynamics for the rate distributions. Stochastic spatial variants
of cyclic competition with rock-paper-scissors interactions illustrate
connections between population dynamics and evolutionary game theory, and
demonstrate how space can help maintain diversity. In two dimensions,
three-species cyclic competition models of the May-Leonard type are
characterized by the emergence of spiral patterns whose properties are
elucidated by a mapping onto a complex Ginzburg-Landau equation. Extensions to
general food networks can be classified on the mean-field level, which provides
both a fundamental understanding of ensuing cooperativity and emergence of
alliances. Novel space-time patterns emerge as a result of the formation of
competing alliances, such as coarsening domains that each incorporate
rock-paper-scissors competition games.
",physics
"  Any acceptable quantum gravity theory must allow us to recover the classical
spacetime in the appropriate limit. Moreover, the spacetime geometrical notions
should be intrinsically tied to the behavior of the matter that probes them. We
consider some difficulties that would be confronted in attempting such an
enterprise. The problems we uncover seem to go beyond the technical level to
the point of questioning the overall feasibility of the project. The main issue
is related to the fact that, in the quantum theory, it is impossible to assign
a trajectory to a physical object, and, on the other hand, according to the
basic tenets of the geometrization of gravity, it is precisely the trajectories
of free localized objects that define the spacetime geometry. The insights
gained in this analysis should be relevant to those interested in the quest for
a quantum theory of gravity and might help refocus some of its goals.
",physics
"  While the polls have been the most trusted source for election predictions
for decades, in the recent presidential election they were called inaccurate
and biased. How inaccurate were the polls in this election and can social media
beat the polls as an accurate election predictor? Polls from several news
outlets and sentiment analysis on Twitter data were used, in conjunction with
the results of the election, to answer this question and outline further
research on the best method for predicting the outcome of future elections.
",computer-science
"  Today's Internet traffic is mostly dominated by multimedia content and the
prediction is that this trend will intensify in the future. Therefore, main
Internet players, such as ISPs, content delivery platforms (e.g. Youtube,
Bitorrent, Netflix, etc) or CDN operators, need to understand the evolution of
multimedia content availability and popularity in order to adapt their
infrastructures and resources to satisfy clients requirements while they
minimize their costs. This paper presents a thorough analysis on the evolution
of multimedia content available in BitTorrent. Specifically, we analyze the
evolution of four relevant metrics across different content categories: content
availability, content popularity, content size and user's feedback. To this end
we leverage a large-scale dataset formed by 4 snapshots collected from the most
popular BitTorrent portal, namely The Pirate Bay, between Nov. 2009 and Feb.
2012. Overall our dataset is formed by more than 160k content that attracted
more than 185M of download sessions.
",computer-science
"  We formulate an optimization problem to control a large number of automated
guided vehicles in a plant without collision. The formulation consists of
binary variables. A quadratic cost function over these variables enables us to
utilize certain solvers on digital computers and recently developed
purpose-specific hardware such as D-Wave 2000Q and the Fujitsu digital
annealer. In the present study, we consider an actual plant in Japan, in which
vehicles run, and assess efficiency of our formulation for optimizing the
vehicles via several solvers. We confirm that our formulation can be a powerful
approach for performing smooth control while avoiding collisions between
vehicles, as compared to a conventional method. In addition, comparative
experiments performed using several solvers reveal that D-Wave 2000Q can be
useful as a rapid solver for generating a plan for controlling the vehicles in
a short time although it deals only with a small number of vehicles, while a
digital computer can rapidly solve the corresponding optimization problem even
with a large number of binary variables.
",computer-science
"  The functional window is an experimentally observed property of the avian
compass that refers to its selectivity around the geomagnetic field strength.
We show that the radical-pair model, using biologically feasible hyperfine
parameters, can qualitatively explain the salient features of the avian compass
as observed from behavioral experiments: its functional window, as well as
disruption of the compass action by an RF field of specific frequencies.
Further, we show that adjustment of the hyperfine parameters can tune the
functional window, suggesting a possible mechanism for its observed
adaptability to field variation. While these lend strong support to the
radical-pair model, we find it impossible to explain quantitatively the
observed width of the functional window within this model, or even with simple
augmentations thereto. This suggests that a deeper generalization of this model
may be called for; we conjecture that environmental coupling may be playing a
subtle role here that has not been captured accurately. Lastly, we examine a
possible biological purpose to the functional window; assuming evolutionary
benefit from radical-pair magnetoreception, we conjecture that the functional
window is simply a corollary thereof and brings no additional advantage.
",physics
"  Consider a dihedral cover $f: Y\to X$ with $X$ and $Y$ four-manifolds and $f$
branched along an oriented surface embedded in $X$ with isolated cone
singularities. We prove that only a slice knot can arise as the unique
singularity on an irregular dihedral cover $f: Y\to S^4$ if $Y$ is homotopy
equivalent to $\mathbb{CP}^2$ and construct an explicit infinite family of such
covers with $Y$ diffeomorphic to $\mathbb{CP}^2$. An obstruction to a knot
being homotopically ribbon arises in this setting, and we describe a class of
potential counter-examples to the Slice-Ribbon Conjecture.
Our tools include lifting a trisection of a singularly embedded surface in a
four-manifold $X$ to obtain a trisection of the corresponding irregular
dihedral branched cover of $X$, when such a cover exists. We also develop a
combinatorial procedure to compute, using a formula by the second author, the
contribution to the signature of the covering manifold which results from the
presence of a singularity on the branching set.
",mathematics
"  In this paper we study the category LCA(2) of certain non-locally compact
abelian topological groups, and extend the notion of Weil index. As
applications we deduce some product formulas for curves over local fields and
arithmetic surfaces.
",mathematics
"  The emission properties of PbTe(111) single crystal have been extensively
investigated to demonstrate that PbTe(111) is a promising low root mean square
transverse momentum ({\Delta}p$_T$) and high brightness photocathode. The
density functional theory (DFT) based photoemission analysis successfully
elucidates that the 'hole-like' {\Lambda}$^+_6$ energy band in the $L$ valley
with low effective mass $m^*$ results in low {\Delta}p$_T$. Especially, as a
300K solid planar photocathode, Te-terminated PbTe(111) single crystal is
expected to be a potential 50K electron source.
",physics
"  This paper discusses a Metropolis-Hastings algorithm developed by
\citeA{MarsmanIsing}. The algorithm is derived from first principles, and it is
proven that the algorithm becomes more efficient with more data and meets the
growing demands of large scale educational measurement.
",statistics
"  Repair mechanisms are important within resilient systems to maintain the
system in an operational state after an error occurred. Usually, constraints on
the repair mechanisms are imposed, e.g., concerning the time or resources
required (such as energy consumption or other kinds of costs). For systems
modeled by Markov decision processes (MDPs), we introduce the concept of
resilient schedulers, which represent control strategies guaranteeing that
these constraints are always met within some given probability. Assigning
rewards to the operational states of the system, we then aim towards resilient
schedulers which maximize the long-run average reward, i.e., the expected mean
payoff. We present a pseudo-polynomial algorithm that decides whether a
resilient scheduler exists and if so, yields an optimal resilient scheduler. We
show also that already the decision problem asking whether there exists a
resilient scheduler is PSPACE-hard.
",computer-science
"  We present TriviaQA, a challenging reading comprehension dataset containing
over 650K question-answer-evidence triples. TriviaQA includes 95K
question-answer pairs authored by trivia enthusiasts and independently gathered
evidence documents, six per question on average, that provide high quality
distant supervision for answering the questions. We show that, in comparison to
other recently introduced large-scale datasets, TriviaQA (1) has relatively
complex, compositional questions, (2) has considerable syntactic and lexical
variability between questions and corresponding answer-evidence sentences, and
(3) requires more cross sentence reasoning to find answers. We also present two
baseline algorithms: a feature-based classifier and a state-of-the-art neural
network, that performs well on SQuAD reading comprehension. Neither approach
comes close to human performance (23% and 40% vs. 80%), suggesting that
TriviaQA is a challenging testbed that is worth significant future study. Data
and code available at -- this http URL
",computer-science
"  The recently introduced mixed time-averaging semiclassical initial value
representation molecular dynamics method for spectroscopic calculations [M.
Buchholz, F. Grossmann, and M. Ceotto, J. Chem. Phys. 144, 094102 (2016)] is
applied to systems with up to 61 dimensions, ruled by a condensed phase
Caldeira-Leggett model potential. By calculating the ground state as well as
the first few excited states of the system Morse oscillator, changes of both
the harmonic frequency and the anharmonicity are determined. The method
faithfully reproduces blueshift and redshift effects and the importance of the
counter term, as previously suggested by other methods. Differently from
previous methods, the present semiclassical method does not take advantage of
the specific form of the potential and it can represent a practical tool that
opens the route to direct ab initio semiclassical simulation of condensed phase
systems.
",physics
"  In order to find a way of measuring the degree of incompleteness of an
incomplete financial market, the rank of the vector price process of the traded
assets and the dimension of the associated acceptance set are introduced. We
show that they are equal and state a variety of consequences.
",quantitative-finance
"  We study the anomalous prevalence of integer percentages in the last
parliamentary (2016) and presidential (2018) Russian elections. We show how
this anomaly in Russian federal elections has evolved since 2000.
",statistics
"  Obtaining models that capture imaging markers relevant for disease
progression and treatment monitoring is challenging. Models are typically based
on large amounts of data with annotated examples of known markers aiming at
automating detection. High annotation effort and the limitation to a vocabulary
of known markers limit the power of such approaches. Here, we perform
unsupervised learning to identify anomalies in imaging data as candidates for
markers. We propose AnoGAN, a deep convolutional generative adversarial network
to learn a manifold of normal anatomical variability, accompanying a novel
anomaly scoring scheme based on the mapping from image space to a latent space.
Applied to new data, the model labels anomalies, and scores image patches
indicating their fit into the learned distribution. Results on optical
coherence tomography images of the retina demonstrate that the approach
correctly identifies anomalous images, such as images containing retinal fluid
or hyperreflective foci.
",computer-science
"  L1-norm Principal-Component Analysis (L1-PCA) of real-valued data has
attracted significant research interest over the past decade. However, L1-PCA
of complex-valued data remains to date unexplored despite the many possible
applications (e.g., in communication systems). In this work, we establish
theoretical and algorithmic foundations of L1-PCA of complex-valued data
matrices. Specifically, we first show that, in contrast to the real-valued case
for which an optimal polynomial-cost algorithm was recently reported by
Markopoulos et al., complex L1-PCA is formally NP-hard in the number of data
points. Then, casting complex L1-PCA as a unimodular optimization problem, we
present the first two suboptimal algorithms in the literature for its solution.
Our experimental studies illustrate the sturdy resistance of complex L1-PCA
against faulty measurements/outliers in the processed data.
",computer-science
"  Let {\Xi} be a function relating to the Riemann zeta function with . In this
paper, we construct a function containing and {\Xi} , and prove that satisfies
a nonadjoint boundary value problem to a nonsingular differential equation if
is any nontrivial zero of {\Xi} . Inspecting properties of and using known
results of nontrivial zeros of , we derive that nontrivial zeros of all have
real part equal to , which concludes that Riemann Hypothesis is true.
",mathematics
"  It is expected that progress toward true artificial intelligence will be
achieved through the emergence of a system that integrates representation
learning and complex reasoning (LeCun et al. 2015). In response to this
prediction, research has been conducted on implementing the symbolic reasoning
of a von Neumann computer in an artificial neural network (Graves et al. 2016;
Graves et al. 2014; Reed et al. 2015). However, these studies have many
limitations in realizing neural-symbolic integration (Jaeger. 2016). Here, we
present a new learning paradigm: a learning solving procedure (LSP) that learns
the procedure for solving complex problems. This is not accomplished merely by
learning input-output data, but by learning algorithms through a solving
procedure that obtains the output as a sequence of tasks for a given input
problem. The LSP neural network system not only learns simple problems of
addition and multiplication, but also the algorithms of complicated problems,
such as complex arithmetic expression, sorting, and Hanoi Tower. To realize
this, the LSP neural network structure consists of a deep neural network and
long short-term memory, which are recursively combined. Through
experimentation, we demonstrate the efficiency and scalability of LSP and its
validity as a mechanism of complex reasoning.
",computer-science
"  Nyquist ghost artifacts in EPI images are originated from phase mismatch
between the even and odd echoes. However, conventional correction methods using
reference scans often produce erroneous results especially in high-field MRI
due to the non-linear and time-varying local magnetic field changes. Recently,
it was shown that the problem of ghost correction can be transformed into
k-space data interpolation problem that can be solved using the annihilating
filter-based low-rank Hankel structured matrix completion approach (ALOHA).
Another recent discovery has shown that the deep convolutional neural network
is closely related to the data-driven Hankel matrix decomposition. By
synergistically combining these findings, here we propose a k-space deep
learning approach that immediately corrects the k-space phase mismatch without
a reference scan. Reconstruction results using 7T in vivo data showed that the
proposed reference-free k-space deep learning approach for EPI ghost correction
significantly improves the image quality compared to the existing methods, and
the computing time is several orders of magnitude faster.
",statistics
"  In hybrid normed ideal perturbations of $n$-tuples of operators, the normed
ideal is allowed to vary with the component operators. We begin extending to
this setting the machinery we developed for normed ideal perturbations based on
the modulus of quasicentral approximation and an adaptation of our
non-commutative generalization of the Weyl--von~Neumann theorem. For commuting
$n$-tuples of hermitian operators, the modulus of quasicentral approximation
remains essentially the same when $\cC_n^-$ is replaced by a hybrid $n$-tuple
$\cC_{p_1,\dots}^-,\dots,\cC^-_{p_n}$, $p_1^{-1} + \dots + p_n^{-1} = 1$. The
proof involves singular integrals of mixed homogeneity.
",mathematics
"  Galaxies in the local Universe are known to follow bimodal distributions in
the global stellar populations properties. We analyze the distribution of the
local average stellar-population ages of 654,053 sub-galactic regions resolved
on ~1-kpc scales in a volume-corrected sample of 394 galaxies, drawn from the
CALIFA-DR3 integral-field-spectroscopy survey and complemented by SDSS imaging.
We find a bimodal local-age distribution, with an old and a young peak
primarily due to regions in early-type galaxies and star-forming regions of
spirals, respectively. Within spiral galaxies, the older ages of bulges and
inter-arm regions relative to spiral arms support an internal age bimodality.
Although regions of higher stellar-mass surface-density, mu*, are typically
older, mu* alone does not determine the stellar population age and a bimodal
distribution is found at any fixed mu*. We identify an ""old ridge"" of regions
of age ~9 Gyr, independent of mu*, and a ""young sequence"" of regions with age
increasing with mu* from 1-1.5 Gyr to 4-5 Gyr. We interpret the former as
regions containing only old stars, and the latter as regions where the relative
contamination of old stellar populations by young stars decreases as mu*
increases. The reason why this bimodal age distribution is not inconsistent
with the unimodal shape of the cosmic-averaged star-formation history is that
i) the dominating contribution by young stars biases the age low with respect
to the average epoch of star formation, and ii) the use of a single average age
per region is unable to represent the full time-extent of the star-formation
history of ""young-sequence"" regions.
",physics
"  Interpretability has become incredibly important as machine learning is
increasingly used to inform consequential decisions. We propose to construct
global explanations of complex, blackbox models in the form of a decision tree
approximating the original model---as long as the decision tree is a good
approximation, then it mirrors the computation performed by the blackbox model.
We devise a novel algorithm for extracting decision tree explanations that
actively samples new training points to avoid overfitting. We evaluate our
algorithm on a random forest to predict diabetes risk and a learned controller
for cart-pole. Compared to several baselines, our decision trees are both
substantially more accurate and equally or more interpretable based on a user
study. Finally, we describe several insights provided by our interpretations,
including a causal issue validated by a physician.
",computer-science
"  We have investigated the crystal structure of LaOBiPbS3 using neutron
diffraction and synchrotron X-ray diffraction. From structural refinements, we
found that the two metal sites, occupied by Bi and Pb, were differently
surrounded by the sulfur atoms. Calculated bond valence sum suggested that one
metal site was nearly trivalent and the other was nearly divalent. Neutron
diffraction also revealed site selectivity of Bi and Pb in the LaOBiPbS3
structure. These results suggested that the crystal structure of LaOBiPbS3 can
be regarded as alternate stacks of the rock-salt-type Pb-rich sulfide layers
and the LaOBiS2-type Bi-rich layers. From band calculations for an ideal
(LaOBiS2)(PbS) system, we found that the S bands of the PbS layer were
hybridized with the Bi bands of the BiS plane at around the Fermi energy, which
resulted in the electronic characteristics different from that of LaOBiS2.
Stacking the rock-salt type sulfide (chalcogenide) layers and the BiS2-based
layered structure could be a new strategy to exploration of new BiS2-based
layered compounds, exotic two-dimensional electronic states, or novel
functionality.
",physics
"  We study the geometry and the singularities of the principal direction of the
Drinfeld-Lafforgue-Vinberg degeneration of the moduli space of G-bundles Bun_G
for an arbitrary reductive group G, and their relationship to the Langlands
dual group of G.
In the first part of the article we study the monodromy action on the nearby
cycles sheaf along the principal degeneration of Bun_G. We describe the
weight-monodromy filtration in terms of the combinatorics of the Langlands dual
group of G and generalizations of the Picard-Lefschetz oscillators found in
[Sch1]. Our proofs use certain local models for the principal degeneration
whose geometry is studied in the second part.
Our local models simultaneously provide two types of degenerations of the
Zastava spaces, which together equip the Zastava spaces with the geometric
analog of a Hopf algebra structure. The first degeneration corresponds to the
usual Beilinson-Drinfeld fusion of divisors on the curve. The second
degeneration is new and corresponds to what we call Vinberg fusion: It is
obtained not by degenerating divisors on the curve, but by degenerating the
group G via the Vinberg semigroup. On the level of cohomology the Vinberg
fusion gives rise to an algebra structure, while the Beilinson-Drinfeld fusion
gives rise to a coalgebra structure; the Hopf algebra axiom is a consequence of
the underlying geometry.
It is natural to conjecture that this Hopf algebra agrees with the universal
enveloping algebra of the positive part of the Langlands dual Lie algebra. The
above procedure would then yield a novel and highly geometric way to pass to
the Langlands dual side: Elements of the Langlands dual Lie algebra are
represented as cycles on the above moduli spaces, and the Lie bracket of two
elements is obtained by deforming the cartesian product cycle along the Vinberg
degeneration.
",mathematics
"  The seminal work of Morgan and Rubin (2012) considers rerandomization for all
the units at one time. In practice, however, experimenters may have to
rerandomize units sequentially. For example, a clinician studying a rare
disease may be unable to wait to perform an experiment until all the
experimental units are recruited. Our work offers a mathematical framework for
sequential rerandomization designs, where the experimental units are enrolled
in groups. We formulate an adaptive rerandomization procedure for balancing
treatment/control assignments over some continuous or binary covariates, using
Mahalanobis distance as the imbalance measure. We prove in our key result,
Theorem 3, that given the same number of rerandomizations (in expected value),
under certain mild assumptions, sequential rerandomization achieves better
covariate balance than rerandomization at one time.
",statistics
"  Mathematical modelling has shown that activity of the Geminid meteor shower
should rise with time, and that was confirmed by analysis of visual
observations 1985--2016. We do not expect any outburst activity of the Geminid
shower in 2017, even though the asteroid (3200) Phaethon has close approach to
Earth in December of 2017. A small probability to observe dust ejected at
perihelia 2009--2016 still exists.
",physics
"  Planning motions for two robot arms to move an object collaboratively is a
difficult problem, mainly because of the closed-chain constraint, which arises
whenever two robot hands simultaneously grasp a single rigid object. In this
paper, we propose a manipulation planning algorithm to bring an object from an
initial stable placement (position and orientation of the object on the support
surface) towards a goal stable placement. The key specificity of our algorithm
is that it is certified-complete: for a given object and a given environment,
we provide a certificate that the algorithm will find a solution to any
bimanual manipulation query in that environment whenever one exists. Moreover,
the certificate is constructive: at run-time, it can be used to quickly find a
solution to a given query. The algorithm is tested in software and hardware on
a number of large pieces of furniture.
",computer-science
"  We show that learning methods interpolating the training data can achieve
optimal rates for the problems of nonparametric regression and prediction with
square loss.
",statistics
"  Magnetotransport measurements in combination with molecular dynamics (MD)
simulations on two-dimensional disordered Lorentz gases in the classical regime
are reported. In quantitative agreement between experiment and simulation, the
magnetoconductivity displays a pronounced peak as a function of perpendicular
magnetic field $B$ which cannot be explained in the framework of existing
kinetic theories. We show that this peak is linked to the onset of a directed
motion of the electrons along the contour of the disordered obstacle matrix
when the cyclotron radius becomes smaller than the size of the obstacles. This
directed motion leads to transient superdiffusive motion and strong scaling
corrections in the vicinity of the insulator-to-conductor transitions of the
Lorentz gas.
",physics
"  In this paper, we analyzed parasitic coupling capacitance coming from dummy
metal fill and its impact on timing. Based on the modeling, we proposed two
approaches to minimize the timing impact from dummy metal fill. The first
approach applies more spacing between critical nets and metal fill, while the
second approach leverages the shielding effects of reference nets. Experimental
results show consistent improvement compared to traditional metal fill method.
",computer-science
"  The remoteness of the Sun and the harsh conditions prevailing in the solar
corona have so far limited the observational data used in the study of solar
physics to remote-sensing observations taken either from the ground or from
space. In contrast, the `solar wind laboratory' is directly measured in situ by
a fleet of spacecraft measuring the properties of the plasma and magnetic
fields at specific points in space. Since 2007, the solar-terrestrial relations
observatory (STEREO) has been providing images of the solar wind that flows
between the solar corona and spacecraft making in-situ measurements. This has
allowed scientists to directly connect processes imaged near the Sun with the
subsequent effects measured in the solar wind. This new capability prompted the
development of a series of tools and techniques to track heliospheric
structures through space. This article presents one of these tools, a web-based
interface called the 'Propagation Tool' that offers an integrated research
environment to study the evolution of coronal and solar wind structures, such
as Coronal Mass Ejections (CMEs), Corotating Interaction Regions (CIRs) and
Solar Energetic Particles (SEPs). These structures can be propagated from the
Sun outwards to or alternatively inwards from planets and spacecraft situated
in the inner and outer heliosphere. In this paper, we present the global
architecture of the tool, discuss some of the assumptions made to simulate the
evolution of the structures and show how the tool connects to different
databases.
",physics
"  We consider the $K$-User Multiple-Input-Single-Output (MISO) Broadcast
Channel (BC) where the transmitter, equipped with $M$ antennas, serves $K$
users, with $K \leq M$. The transmitter has access to a partial channel state
information of the users. This is modelled by letting the variance of the
Channel State Information at the Transmitter (CSIT) error of user $i$ scale as
$O(P^{-\alpha_i}$) for the Signal-to-Noise Ratio (SNR) $P$ and some constant
$\alpha_i \geq 0$. In this work we derive the optimal Degrees-of-Freedom (DoF)
region in such setting and we show that Rate-Splitting (RS) is the key scheme
to achieve such a region.
",computer-science
"  In the junction $\Omega$ of several semi-infinite cylindrical waveguides we
consider the Dirichlet Laplacian whose continuous spectrum is the ray
$[\lambda_\dagger, +\infty)$ with a positive cut-off value $\lambda_\dagger$.
We give two different criteria for the threshold resonance generated by
nontrivial bounded solutions to the Dirichlet problem for the Helmholtz
equation $-\Delta u=\lambda_\dagger u$ in $\Omega$. The first criterion is
quite simple and is convenient to disprove the existence of bounded solutions.
The second criterion is rather involved but can help to detect concrete shapes
supporting the resonance. Moreover, the latter distinguishes in a natural way
between stabilizing, i.e., bounded but non-descending solutions and trapped
modes with exponential decay at infinity.
",mathematics
"  Advanced persistent threats (APTs) are stealthy attacks which make use of
social engineering and deception to give adversaries insider access to
networked systems. Against APTs, active defense technologies aim to create and
exploit information asymmetry for defenders. In this paper, we study a scenario
in which a powerful defender uses honeynets for active defense in order to
observe an attacker who has penetrated the network. Rather than immediately
eject the attacker, the defender may elect to gather information. We introduce
an undiscounted, infinite-horizon Markov decision process on a continuous state
space in order to model the defender's problem. We find a threshold of
information that the defender should gather about the attacker before ejecting
him. Then we study the robustness of this policy using a Stackelberg game.
Finally, we simulate the policy for a conceptual network. Our results provide a
quantitative foundation for studying optimal timing for attacker engagement in
network defense.
",computer-science
"  Real-time instrument tracking is a crucial requirement for various
computer-assisted interventions. In order to overcome problems such as specular
reflections and motion blur, we propose a novel method that takes advantage of
the interdependency between localization and segmentation of the surgical tool.
In particular, we reformulate the 2D instrument pose estimation as heatmap
regression and thereby enable a concurrent, robust and near real-time
regression of both tasks via deep learning. As demonstrated by our experimental
results, this modeling leads to a significantly improved performance than
directly regressing the tool position and allows our method to outperform the
state of the art on a Retinal Microsurgery benchmark and the MICCAI EndoVis
Challenge 2015.
",computer-science
"  Context. Considering the importance of software testing to the development of
high quality and reliable software systems, this paper aims to investigate how
can work-related factors influence the motivation of software testers. Method.
We applied a questionnaire that was developed using a previous theory of
motivation and satisfaction of software engineers to conduct a survey-based
study to explore and understand how professional software testers perceive and
value work-related factors that could influence their motivation at work.
Results. With a sample of 80 software testers we observed that software testers
are strongly motivated by variety of work, creative tasks, recognition for
their work, and activities that allow them to acquire new knowledge, but in
general the social impact of this activity has low influence on their
motivation. Conclusion. This study discusses the difference of opinions among
software testers, regarding work-related factors that could impact their
motivation, which can be relevant for managers and leaders in software
engineering practice.
",computer-science
"  In this paper, we obtain some formulae for harmonic sums, alternating
harmonic sums and Stirling number sums by using the method of integral
representations of series. As applications of these formulae, we give explicit
formula of several quadratic and cubic Euler sums through zeta values and
linear sums. Furthermore, some relationships between harmonic numbers and
Stirling numbers of the first kind are established.
",mathematics
"  This paper proposes Power Slow Feature Analysis, a gradient-based method to
extract temporally-slow features from a high-dimensional input stream that
varies on a faster time-scale, and a variant of Slow Feature Analysis (SFA).
While displaying performance comparable to hierarchical extensions to the SFA
algorithm, such as Hierarchical Slow Feature Analysis, for a small number of
output-features, our algorithm allows end-to-end training of arbitrary
differentiable approximators (e.g., deep neural networks). We provide
experimental evidence that PowerSFA is able to extract meaningful and
informative low-dimensional features in the case of a) synthetic
low-dimensional data, b) visual data, and also for c) a general dataset for
which symmetric non-temporal relations between points can be defined.
",statistics
"  We study the spectral properties of curl, a linear differential operator of
first order acting on differential forms of appropriate degree on an
odd-dimensional closed oriented Riemannian manifold. In three dimensions its
eigenvalues are the electromagnetic oscillation frequencies in vacuum without
external sources. In general, the spectrum consists of the eigenvalue 0 with
infinite multiplicity and further real discrete eigenvalues of finite
multiplicity. We compute the Weyl asymptotics and study the zeta-function. We
give a sharp lower eigenvalue bound for positively curved manifolds and analyze
the equality case. Finally, we compute the spectrum for flat tori, round
spheres and 3-dimensional spherical space forms.
",mathematics
"  A parameterised Boolean equation system (PBES) is a set of equations that
defines sets as the least and/or greatest fixed-points that satisfy the
equations. This system is regarded as a declarative program defining functions
that take a datum and returns a Boolean value. The membership problem of PBESs
is a problem to decide whether a given element is in the defined set or not,
which corresponds to an execution of the program. This paper introduces reduced
proof graphs, and studies a technique to solve the membership problem of PBESs,
which is undecidable in general, by transforming it into a reduced proof graph.
A vertex X(v) in a proof graph represents that the data v is in the set X, if
the graph satisfies conditions induced from a given PBES. Proof graphs are,
however, infinite in general. Thus we introduce vertices each of which stands
for a set of vertices of the original ones, which possibly results in a finite
graph. For a subclass of disjunctive PBESs, we clarify some conditions which
reduced proof graphs should satisfy. We also show some examples having no
finite proof graph except for reduced one. We further propose a reduced
dependency space, which contains reduced proof graphs as sub-graphs if a proof
graph exists. We provide a procedure to construct finite reduced dependency
spaces, and show the soundness and completeness of the procedure.
",computer-science
"  In this paper we use the classical electrodynamics to show that the Lorenz
gauge can be incompatible with some particular solutions of the d Alembert
equations for electromagnetic potentials. In its turn, the d Alembert equations
for the elec- tromagnetic potentials is the result of application of the Lorenz
gauge to general equations for the potentials. The last ones is the
straightforward consequence of Maxwell equations. Since the d Alembert
equations and the electromagnetic poten- tials are necessary for quantum
electrodynamics formulation, one should oblige to satisfy these equations also
in classical case. The solution of d Alembert equations, which modifies
longitudinal electric field is found. The requirement of this modifi- cation
follows from the necessity to satisfy the physical condition of impossibility
of instantaneous transferring of interaction in space.
",physics
"  Microscopic artificial swimmers have recently become highly attractive due to
their promising potential for biomedical applications. The pioneering work of
Dreyfus et al (2005) has demonstrated the motion of a microswimmer with an
undulating chain of superparamagnetic beads, which is actuated by an
oscillating external magnetic field. Interestingly, it has also been
theoretically predicted that the swimming direction of this swimmer will
undergo a $90^\circ$-transition when the magnetic field's oscillations
amplitude is increased above a critical value of $\sqrt{2}$. In this work, we
further investigate this transition both theoretically and experimentally by
using numerical simulations and presenting a novel flexible microswimmer with a
superparamagnetic head. We realize the $90^\circ$-transition in swimming
direction, prove that this effect depends on both frequency and amplitude of
the oscillating magnetic field, and demonstrate the existence of an optimal
amplitude, under which, maximal swimming speed can be achieved. By
asymptotically analyzing the dynamic motion of microswimmer with a minimal
two-link model, we reveal that the stability transitions representing the
changes in the swimming direction are induced by the effect of nonlinear
parametric excitation.
",physics
"  The mechanism of ion bombardment induced magnetic patterning of exchange bias
layer systems for creating engineered magnetic stray field landscapes is still
unclear. We compare results from vectorial magneto-optic Kerr effect
measurements to a recently proposed model with time dependent rotatable
magnetic anisotropy. Results show massive reduction of rotational magnetic
anisotropy compared to all other magnetic anisotropies. We disprove the
assumption of comparable weakening of all magnetic anisotropies and show that
ion bombardment mainly influences smaller grains in the antiferromagnet.
",physics
"  In this paper, we introduce a new combinatorial curvature on triangulated
surfaces with inversive distance circle packing metrics. Then we prove that
this combinatorial curvature has global rigidity. To study the Yamabe problem
of the new curvature, we introduce a combinatorial Ricci flow, along which the
curvature evolves almost in the same way as that of scalar curvature along the
surface Ricci flow obtained by Hamilton \cite{Ham1}. Then we study the long
time behavior of the combinatorial Ricci flow and obtain that the existence of
a constant curvature metric is equivalent to the convergence of the flow on
triangulated surfaces with nonpositive Euler number. We further generalize the
combinatorial curvature to $\alpha$-curvature and prove that it is also
globally rigid, which is in fact a generalized Bower-Stephenson conjecture
\cite{BS}. We also use the combinatorial Ricci flow to study the corresponding
$\alpha$-Yamabe problem.
",mathematics
"  We map the phase-space trajectories of an external-cavity semiconductor laser
using phase portraits. This is both a visualization tool as well as a
thoroughly quantitative approach enabling unprecedented insight into the
dynamical regimes, from continuous-wave through coherence collapse as feedback
is increased. Namely, the phase portraits in the intensity versus laser-diode
terminal-voltage (serving as a surrogate for inversion) plane are mapped out.
We observe a route to chaos interrupted by two types of limit cycles, a
subharmonic regime and period-doubled dynamics at the edge of chaos. The
transition of the dynamics are analyzed utilizing bifurcation diagrams for both
the optical intensity and the laser-diode terminal voltage. These observations
provide visual insight into the dynamics in these systems.
",physics
"  In many modern machine learning applications, structures of underlying
mathematical models often yield nonconvex optimization problems. Due to the
intractability of nonconvexity, there is a rising need to develop efficient
methods for solving general nonconvex problems with certain performance
guarantee. In this work, we investigate the accelerated proximal gradient
method for nonconvex programming (APGnc). The method compares between a usual
proximal gradient step and a linear extrapolation step, and accepts the one
that has a lower function value to achieve a monotonic decrease. In specific,
under a general nonsmooth and nonconvex setting, we provide a rigorous argument
to show that the limit points of the sequence generated by APGnc are critical
points of the objective function. Then, by exploiting the
Kurdyka-{\L}ojasiewicz (\KL) property for a broad class of functions, we
establish the linear and sub-linear convergence rates of the function value
sequence generated by APGnc. We further propose a stochastic variance reduced
APGnc (SVRG-APGnc), and establish its linear convergence under a special case
of the \KL property. We also extend the analysis to the inexact version of
these methods and develop an adaptive momentum strategy that improves the
numerical performance.
",computer-science
"  Gossip protocols aim at arriving, by means of point-to-point or group
communications, at a situation in which all the agents know each other secrets.
Recently a number of authors studied distributed epistemic gossip protocols.
These protocols use as guards formulas from a simple epistemic logic, which
makes their analysis and verification substantially easier.
We study here common knowledge in the context of such a logic. First, we
analyze when it can be reduced to iterated knowledge. Then we show that the
semantics and truth for formulas without nested common knowledge operator are
decidable. This implies that implementability, partial correctness and
termination of distributed epistemic gossip protocols that use non-nested
common knowledge operator is decidable, as well. Given that common knowledge is
equivalent to an infinite conjunction of nested knowledge, these results are
non-trivial generalizations of the corresponding decidability results for the
original epistemic logic, established in (Apt & Wojtczak, 2016).
K. R. Apt & D. Wojtczak (2016): On Decidability of a Logic of Gossips. In
Proc. of JELIA 2016, pp. 18-33, doi:10.1007/ 978-3-319-48758-8_2.
",computer-science
"  In this paper, we address the problem of cross-view image geo-localization.
Specifically, we aim to estimate the GPS location of a query street view image
by finding the matching images in a reference database of geo-tagged bird's eye
view images, or vice versa. To this end, we present a new framework for
cross-view image geo-localization by taking advantage of the tremendous success
of deep convolutional neural networks (CNNs) in image classification and object
detection. First, we employ the Faster R-CNN to detect buildings in the query
and reference images. Next, for each building in the query image, we retrieve
the $k$ nearest neighbors from the reference buildings using a Siamese network
trained on both positive matching image pairs and negative pairs. To find the
correct NN for each query building, we develop an efficient multiple nearest
neighbors matching method based on dominant sets. We evaluate the proposed
framework on a new dataset that consists of pairs of street view and bird's eye
view images. Experimental results show that the proposed method achieves better
geo-localization accuracy than other approaches and is able to generalize to
images at unseen locations.
",computer-science
"  For an element $a$ of a monoid $H$, its set of lengths $\mathsf L (a) \subset
\mathbb N$ is the set of all positive integers $k$ for which there is a
factorization $a=u_1 \cdot \ldots \cdot u_k$ into $k$ atoms. We study the
system $\mathcal L (H) = \{\mathsf L (a) \mid a \in H \}$ with a focus on the
unions $\mathcal U_k (H) \subset \mathbb N$ which are the unions of all sets of
lengths containing a given $k \in \mathbb N$. The Structure Theorem for Unions
-- stating that for all sufficiently large $k$, the sets $\mathcal U_k (H)$ are
almost arithmetical progressions with the same difference and global bound --
has found much attention for commutative monoids and domains. We show that it
holds true for the not necessarily commutative monoids in the title satisfying
suitable algebraic finiteness conditions. Furthermore, we give an explicit
description of the system of sets of lengths of monoids $B_{n} = \langle a,b
\mid ba=b^{n} \rangle$ for $n \in \N_{\ge 2}$. Based on this description, we
show that the monoids $B_n$ are not transfer Krull, which implies that their
systems $\mathcal L (B_n)$ are distinct from systems of sets of lengths of
commutative Krull monoids and others.
",mathematics
"  We study the word and conjugacy problems in lacunary hyperbolic groups
(briefly, LHG). In particular, we describe a necessary and sufficient condition
for decidability of the word problem in LHG. Then, based on the graded
small-cancellation theory of Olshanskii, we develop a general framework which
allows us to construct lacunary hyperbolic groups with word and conjugacy
problems highly controllable and flexible both in terms of computability and
computational complexity.
As an application, we show that for any recursively enumerable subset
$\mathcal{L} \subseteq \mathcal{A}^*$, where $\mathcal{A}^*$ is the set of
words over arbitrarily chosen non-empty finite alphabet $\mathcal{A}$, there
exists a lacunary hyperbolic group $G_{\mathcal{L}}$ such that the membership
problem for $ \mathcal{L}$ is `almost' linear time equivalent to the conjugacy
problem in $G_{\mathcal{L}}$. Moreover, for the mentioned group the word and
individual conjugacy problems are decidable in `almost' linear time.
Another application is the construction of a lacunary hyperbolic group with
`almost' linear time word problem and with all the individual conjugacy
problems being undecidable except the word problem.
As yet another application of the developed framework, we construct infinite
verbally complete groups and torsion free Tarski monsters, i.e. infinite
torsion-free groups all of whose proper subgroups are cyclic, with `almost'
linear time word and polynomial time conjugacy problems. These groups are
constructed as quotients of arbitrarily given non-elementary torsion-free
hyperbolic groups and are lacunary hyperbolic.
Finally, as a consequence of the main results, we answer a few open
questions.
",mathematics
"  Modern deep transfer learning approaches have mainly focused on learning
generic feature vectors from one task that are transferable to other tasks,
such as word embeddings in language and pretrained convolutional features in
vision. However, these approaches usually transfer unary features and largely
ignore more structured graphical representations. This work explores the
possibility of learning generic latent relational graphs that capture
dependencies between pairs of data units (e.g., words or pixels) from
large-scale unlabeled data and transferring the graphs to downstream tasks. Our
proposed transfer learning framework improves performance on various tasks
including question answering, natural language inference, sentiment analysis,
and image classification. We also show that the learned graphs are generic
enough to be transferred to different embeddings on which the graphs have not
been trained (including GloVe embeddings, ELMo embeddings, and task-specific
RNN hidden unit), or embedding-free units such as image pixels.
",statistics
"  A practical, biologically motivated case of protein complexes (immunoglobulin
G and FcRII receptors) moving on the surface of mastcells, that are common
parts of an immunological system, is investigated. Proteins are considered as
nanomachines creating a nanonetwork. Accurate molecular models of the proteins
and the fluorophores which act as their nanoantennas are used to simulate the
communication between the nanomachines when they are close to each other. The
theory of diffusion-based Brownian motion is applied to model movements of the
proteins. It is assumed that fluorophore molecules send and receive signals
using the Forster Resonance Energy Transfer. The probability of the efficient
signal transfer and the respective bit error rate are calculated and discussed.
",quantitative-biology
"  The key component in forecasting demand and consumption of resources in a
supply network is an accurate prediction of real-valued time series. Indeed,
both service interruptions and resource waste can be reduced with the
implementation of an effective forecasting system. Significant research has
thus been devoted to the design and development of methodologies for short term
load forecasting over the past decades. A class of mathematical models, called
Recurrent Neural Networks, are nowadays gaining renewed interest among
researchers and they are replacing many practical implementation of the
forecasting systems, previously based on static methods. Despite the undeniable
expressive power of these architectures, their recurrent nature complicates
their understanding and poses challenges in the training procedures. Recently,
new important families of recurrent architectures have emerged and their
applicability in the context of load forecasting has not been investigated
completely yet. In this paper we perform a comparative study on the problem of
Short-Term Load Forecast, by using different classes of state-of-the-art
Recurrent Neural Networks. We test the reviewed models first on controlled
synthetic tasks and then on different real datasets, covering important
practical cases of study. We provide a general overview of the most important
architectures and we define guidelines for configuring the recurrent networks
to predict real-valued time series.
",computer-science
"  Collective motion is an intriguing phenomenon, especially considering that it
arises from a set of simple rules governing local interactions between
individuals. In theoretical models, these rules are normally \emph{assumed} to
take a particular form, possibly constrained by heuristic arguments. We propose
a new class of models, which describe the individuals as \emph{agents}, capable
of deciding for themselves how to act and learning from their experiences. The
local interaction rules do not need to be postulated in this model, since they
\emph{emerge} from the learning process. We apply this ansatz to a concrete
scenario involving marching locusts, in order to model the phenomenon of
density-dependent alignment. We show that our learning agent-based model can
account for a Fokker-Planck equation that describes the collective motion and,
most notably, that the agents can learn the appropriate local interactions,
requiring no strong previous assumptions on their form. These results suggest
that learning agent-based models are a powerful tool for studying a broader
class of problems involving collective motion and animal agency in general.
",statistics
"  Given $3 \leq k \leq s$, we say that a $k$-uniform hypergraph $C^k_s$ is a
tight cycle on $s$ vertices if there is a cyclic ordering of the vertices of
$C^k_s$ such that every $k$ consecutive vertices under this ordering form an
edge. We prove that if $k \ge 3$ and $s \ge 2k^2$, then every $k$-uniform
hypergraph on $n$ vertices with minimum codegree at least $(1/2 + o(1))n$ has
the property that every vertex is covered by a copy of $C^k_s$. Our result is
asymptotically best possible for infinitely many pairs of $s$ and $k$, e.g.
when $s$ and $k$ are coprime.
A perfect $C^k_s$-tiling is a spanning collection of vertex-disjoint copies
of $C^k_s$. When $s$ is divisible by $k$, the problem of determining the
minimum codegree that guarantees a perfect $C^k_s$-tiling was solved by a
result of Mycroft. We prove that if $k \ge 3$ and $s \ge 5k^2$ is not divisible
by $k$ and $s$ divides $n$, then every $k$-uniform hypergraph on $n$ vertices
with minimum codegree at least $(1/2 + 1/(2s) + o(1))n$ has a perfect
$C^k_s$-tiling. Again our result is asymptotically best possible for infinitely
many pairs of $s$ and $k$, e.g. when $s$ and $k$ are coprime with $k$ even.
",mathematics
"  Let $f$ be a holomorphic curve in $\mathbb{P}^n({\mathbb{C}})$ and let
$\mathcal{D}=\{D_1,\ldots,D_q\}$ be a family of moving hypersurfaces defined by
a set of homogeneous polynomials $\mathcal{Q}=\{Q_1,\ldots,Q_q\}$. For
$j=1,\ldots,q$, denote by
$Q_j=\sum\limits_{i_0+\cdots+i_n=d_j}a_{j,I}(z)x_0^{i_0}\cdots x_n^{i_n}$,
where $I=(i_0,\ldots,i_n)\in\mathbb{Z}_{\ge 0}^{n+1}$ and $a_{j,I}(z)$ are
entire functions on ${\mathbb{C}}$ without common zeros. Let
$\mathcal{K}_{\mathcal{Q}}$ be the smallest subfield of meromorphic function
field $\mathcal{M}$ which contains ${\mathbb{C}}$ and all
$\frac{a_{j,I'}(z)}{a_{j,I''}(z)}$ with $a_{j,I''}(z)\not\equiv 0$, $1\le j\le
q$. In previous known second main theorems for $f$ and $\mathcal{D}$, $f$ is
usually assumed to be algebraically nondegenerate over
$\mathcal{K}_{\mathcal{Q}}$. In this paper, we prove a second main theorem in
which $f$ is only assumed to be nonconstant. This result can be regarded as a
generalization of Cartan's conjecture for moving hypersurfaces.
",mathematics
"  Electrons that are confined to a single Landau level in a two dimensional
electron gas realize the effects of strong electron-electron repulsion in its
purest form. The kinetic energy of individual electrons is completely quenched
and all physical properties are dictated solely by many-body effects. A
remarkable consequence is the emergence of new quasiparticles with fractional
charge and exotic quantum statistics of which the most exciting ones are
non-Abelian quasiparticles. A non-integer quantized thermal Hall conductance
$\kappa_{xy}$ (in units of temperature times the universal constant $\pi^2
k_B^2 /3 h$; $h$ is the Planck constant and $k_B$ the Boltzmann constant)
necessitates the existence of such quasiparticles. It has been predicted, and
verified numerically, that such states are realized in the clean half-filled
first Landau level of electrons with Coulomb repulsion, with $\kappa_{xy}$
being either $3/2$ or $7/2$. Excitingly, a recent experiment has indeed
observed a half-integer value, which was measured, however, to be
$\kappa_{xy}=5/2$. We resolve this contradiction within a picture where smooth
disorder results in the formation of mesoscopic puddles with locally
$\kappa_{xy}=3/2$ or $7/2$. Interactions between these puddles generate a
coherent macroscopic state, which is reflected in an extended plateau with
quantized $\kappa_{xy}=5/2$. The topological properties of quasiparticles at
large distances are determined by the macroscopic phase, and not by the
microscopic puddle where they reside. In principle, the same mechanism might
also allow non-Abelian quasiparticles to emerge from a system comprised of
microscopic Abelian puddles.
",physics
"  In this paper, I study the isoparametric hypersurfaces in a Randers sphere
$(S^n,F)$ of constant flag curvature, with the navigation datum $(h,W)$. I
prove that an isoparametric hypersurface $M$ for the standard round sphere
$(S^n,h)$ which is tangent to $W$ remains isoparametric for $(S^n,F)$ after the
navigation process. This observation provides a special class of isoparametric
hypersurfaces in $(S^n,F)$, which can be equivalently described as the regular
level sets of isoparametric functions $f$ satisfying $-f$ is transnormal. I
provide a classification for these special isoparametric hypersurfaces $M$,
together with their ambient metric $F$ on $S^n$, except the case that $M$ is of
the OT-FKM type with the multiplicities $(m_1,m_2)=(8,7)$. I also give a
complete classificatoin for all homogeneous hypersurfaces in $(S^n,F)$. They
all belong to these special isoparametric hypersurfaces. Because of the extra
$W$, the number of distinct principal curvature can only be 1,2 or 4, i.e.
there are less homogeneous hypersurfaces for $(S^n,F)$ than those for
$(S^n,h)$.
",mathematics
"  It has been shown by McCoy that a right ideal of a polynomial ring with
several indeterminates has a non-trivial homogeneous right annihilator of
degree 0 provided its right annihilator is non-trivial to begin with. In this
note, it is documented that any $\mathbb{N}$-graded ring $R$ has a slightly
weaker property: the right annihilator of a right ideal contains a homogeneous
non-zero element, if it is non-trivial to begin with. If $R$ is a subring of a
$\mathbb{Z}^k$ -graded ring $S$ satisfying a certain non-annihilation property
(which is the case if $S$ is strongly graded, for example), then it is possible
to find annihilators of degree 0.
",mathematics
"  A probabilistic description is essential for understanding growth processes
far from equilibrium. In this paper, we compute time-dependent Probability
Density Functions (PDFs) in order to investigate stochastic logistic and
Gompertz models, which are two of the most popular growth models. We consider
different types of short-correlated internal (multiplicative) and external
(additive) stochastic noises and compare the time-dependent PDFs in the two
models, elucidating the effects of the additive and multiplicative noises on
the form of PDFs. We demonstrate an interesting transition from a unimodal to a
bimodal PDF as the multiplicative noise increases for a fixed value of the
additive noise. A much weaker (leaky) attractor in the Gompertz model leads to
a significant (singular) growth of the population of a very small size. We
point out the limitation of using stationary PDFs, mean value and variance in
understanding statistical properties of the growth far from equilibrium,
highlighting the importance of time-dependent PDFs. We further compare these
two models from the perspective of information change that occurs during the
growth process. Specifically, we define an infinitesimal distance at any time
by comparing two PDFs at times infinitesimally apart and sum these distances in
time. The total distance along the trajectory quantifies the total number of
different states that the system undergoes in time, and is called the
information length. We show that the time-evolution of the two models become
more similar when measured in units of the information length and point out the
merit of using the information length in unifying and understanding the dynamic
evolution of different growth processes.
",physics
"  An overview of research on laser-plasma based acceleration of ions is given.
The experimental state of the art is summarized and recent progress is
discussed. The basic acceleration processes are briefly reviewed with an
outlook on hybrid mechanisms and novel concepts. Finally, we put focus on the
development of engineered targets for enhanced acceleration and of all-optical
methods for beam post-acceleration and control.
",physics
"  We present the synthesis and a detailed investigation of structural and
magnetic properties of polycrystalline
NH$_4$[(V$_2$O$_3$)$_2$(4,4$^\prime$-$bpy$)$_2$(H$_2$PO$_4$)(PO$_4$)$_2$]$\cdot$0.5H$_2$O
by means of x-ray diffraction, magnetic susceptibility, electron spin
resonance, and $^{31}$P nuclear magnetic resonance measurements. Temperature
dependent magnetic susceptibility could be described well using a weakly
coupled spin-$1/2$ dimer model with an excitation gap $\Delta/k_{\rm B}\simeq
26.1$ K between the singlet ground state and triplet excited states and a weak
inter-dimer exchange coupling $J^\prime/k_{\rm B} \simeq 4.6$ K. A gapped chain
model also describes the data well with a gap of about 20 K. The ESR intensity
as a function of temperature traces the bulk susceptibility nicely. The
isotropic Land$\acute{\rm e}$ $g$-factor is estimated to be about $g \simeq
1.97$, at room temperature. We are able to resolve the $^{31}$P NMR signal as
coming from two inequivalent P-sites in the crystal structure. The hyperfine
coupling constant between $^{31}$P nucleus and V$^{4+}$ spins is calculated to
be $A_{\rm hf}(1) \simeq 2963$ Oe/$\mu_{\rm B}$ and $A_{\rm hf}(2) \simeq 1466$
Oe/$\mu_{\rm B}$ for the P(1) and P(2) sites, respectively. Our NMR shift and
spin-lattice relaxation rate for both the $^{31}$P sites show an activated
behaviour at low temperatures, further confirming the singlet ground state. The
estimated value of the spin gap from the NMR data measured in an applied field
of $H = 9.394$ T is consistent with the gap obtained from the magnetic
susceptibility analysis using the dimer model. Because of a relatively small
spin gap,
NH$_4$[(V$_2$O$_3$)$_2$(4,4$^\prime$-$bpy$)$_2$(H$_2$PO$_4$)(PO$_4$)$_2$]$\cdot$0.5H$_2$O
is a promising compound for further experimental studies under high magnetic
fields.
",physics
"  Optimization of energy cost determines average values of spatio-temporal gait
parameters such as step duration, step length or step speed. However, during
walking, humans need to adapt these parameters at every step to respond to
exogenous and/or endogenic perturbations. While some neurological mechanisms
that trigger these responses are known, our understanding of the fundamental
principles governing step-by-step adaptation remains elusive. We determined the
gait parameters of 20 healthy subjects with right-foot preference during
treadmill walking at speeds of 1.1, 1.4 and 1.7 m/s. We found that when the
value of the gait parameter was conspicuously greater (smaller) than the mean
value, it was either followed immediately by a smaller (greater) value of the
contralateral leg (interleg control), or the deviation from the mean value
decreased during the next movement of ipsilateral leg (intraleg control). The
selection of step duration and the selection of step length during such
transient control events were performed in unique ways. We quantified the
symmetry of short-term control of gait parameters and observed the significant
dominance of the right leg in short-term control of all three parameters at
higher speeds (1.4 and 1.7 m/s).
",physics
"  The Ward identities for the charge and heat currents are derived for
particle-particle and particle-hole pairs. They are the exact constraints on
the current-vertex functions imposed by conservation laws and should be
satisfied by consistent theories. While the Ward identity for the charge
current of electrons is well established, that for the heat current is not
understood correctly. Thus the correct interpretation is presented. On this
firm basis the Ward identities for pairs are discussed. As the application of
the identity we criticize some inconsistent results in the studies of the
superconducting fluctuation transport and the transport anomaly in the normal
state of high-Tc superconductors.
",physics
"  Non-conding RNAs play a key role in the post-transcriptional regulation of
mRNA translation and turnover in eukaryotes. miRNAs, in particular, interact
with their target RNAs through protein-mediated, sequence-specific binding,
giving rise to extended and highly heterogeneous miRNA-RNA interaction
networks. Within such networks, competition to bind miRNAs can generate an
effective positive coupling between their targets. Competing endogenous RNAs
(ceRNAs) can in turn regulate each other through miRNA-mediated crosstalk.
Albeit potentially weak, ceRNA interactions can occur both dynamically,
affecting e.g. the regulatory clock, and at stationarity, in which case ceRNA
networks as a whole can be implicated in the composition of the cell's
proteome. Many features of ceRNA interactions, including the conditions under
which they become significant, can be unraveled by mathematical and in silico
models. We review the understanding of the ceRNA effect obtained within such
frameworks, focusing on the methods employed to quantify it, its role in the
processing of gene expression noise, and how network topology can determine its
reach.
",quantitative-biology
"  In this paper, we introduce a simple, yet powerful pipeline for medical image
segmentation that combines Fully Convolutional Networks (FCNs) with Fully
Convolutional Residual Networks (FC-ResNets). We propose and examine a design
that takes particular advantage of recent advances in the understanding of both
Convolutional Neural Networks as well as ResNets. Our approach focuses upon the
importance of a trainable pre-processing when using FC-ResNets and we show that
a low-capacity FCN model can serve as a pre-processor to normalize medical
input data. In our image segmentation pipeline, we use FCNs to obtain
normalized images, which are then iteratively refined by means of a FC-ResNet
to generate a segmentation prediction. As in other fully convolutional
approaches, our pipeline can be used off-the-shelf on different image
modalities. We show that using this pipeline, we exhibit state-of-the-art
performance on the challenging Electron Microscopy benchmark, when compared to
other 2D methods. We improve segmentation results on CT images of liver
lesions, when contrasting with standard FCN methods. Moreover, when applying
our 2D pipeline on a challenging 3D MRI prostate segmentation challenge we
reach results that are competitive even when compared to 3D methods. The
obtained results illustrate the strong potential and versatility of the
pipeline by achieving highly accurate results on multi-modality images from
different anatomical regions and organs.
",computer-science
"  In this paper, Runge-Kutta-Gegenbauer (RKG) stability polynomials of
arbitrarily high order of accuracy are introduced in closed form. The stability
domain of RKG polynomials extends in the the real direction with the square of
polynomial degree, and in the imaginary direction as an increasing function of
Gegenbauer parameter. Consequently, the polynomials are naturally suited to the
construction of high order stabilized Runge-Kutta (SRK) methods for systems of
PDEs of mixed hyperbolic-parabolic type.
We present SRK methods composed of $L$ ordered forward Euler stages, with
complex-valued stepsizes derived from the roots of RKG stability polynomials of
degree $L$. Internal stability is maintained at large stage number through an
ordering algorithm which limits internal amplification factors to $10 L^2$.
Test results for mildly stiff nonlinear advection-diffusion-reaction problems
with moderate ($\lesssim 1$) mesh Péclet numbers are provided at second,
fourth, and sixth orders, with nonlinear reaction terms treated by complex
splitting techniques above second order.
",physics
"  The superconductivity of the 4-angstrom single-walled carbon nanotubes
(SWCNTs) was discovered more than a decade ago, and marked the breakthrough of
finding superconductivity in pure elemental undoped carbon compounds. The van
Hove singularities in the electronic density of states at the Fermi level in
combination with a large Debye temperature of the SWCNTs are expected to cause
an impressively large superconducting gap. We have developed an innovative
computational algorithm specially tailored for the investigation of
superconductivity in ultrathin SWCNTs. We predict the superconducting
transition temperature of various thin carbon nanotubes resulting from
electron-phonon coupling by an ab-initio method, taking into account the effect
of radial pressure, symmetry, chirality (N,M) and bond lengths. By optimizing
the geometry of the carbon nanotubes, a maximum Tc of 60K is found. We also use
our method to calculate the Tc of a linear carbon chain embedded in the center
of (5,0) SWCNTs. The strong curvature in the (5,0) carbon nanotubes in the
presence of the inner carbon chain provides an alternative path to increase the
Tc of this carbon composite by a factor of 2.2 with respect to the empty (5,0)
SWCNTs.
",physics
"  We propose a novel decoding approach for neural machine translation (NMT)
based on continuous optimisation. We convert decoding - basically a discrete
optimization problem - into a continuous optimization problem. The resulting
constrained continuous optimisation problem is then tackled using
gradient-based methods. Our powerful decoding framework enables decoding
intractable models such as the intersection of left-to-right and right-to-left
(bidirectional) as well as source-to-target and target-to-source (bilingual)
NMT models. Our empirical results show that our decoding framework is
effective, and leads to substantial improvements in translations generated from
the intersected models where the typical greedy or beam search is not feasible.
We also compare our framework against reranking, and analyse its advantages and
disadvantages.
",computer-science
"  Large inter-datacenter transfers are crucial for cloud service efficiency and
are increasingly used by organizations that have dedicated wide area networks
between datacenters. A recent work uses multicast forwarding trees to reduce
the bandwidth needs and improve completion times of point-to-multipoint
transfers. Using a single forwarding tree per transfer, however, leads to poor
performance because the slowest receiver dictates the completion time for all
receivers. Using multiple forwarding trees per transfer alleviates this
concern--the average receiver could finish early; however, if done naively,
bandwidth usage would also increase and it is apriori unclear how best to
partition receivers, how to construct the multiple trees and how to determine
the rate and schedule of flows on these trees. This paper presents QuickCast, a
first solution to these problems. Using simulations on real-world network
topologies, we see that QuickCast can speed up the average receiver's
completion time by as much as $10\times$ while only using $1.04\times$ more
bandwidth; further, the completion time for all receivers also improves by as
much as $1.6\times$ faster at high loads.
",computer-science
"  Cyber Physical Systems (CPS) are becoming ubiquitous and affect the physical
world, yet security is seldom at the forefront of their design. This is
especially true of robotic control algorithms which seldom consider the effect
of a cyber attack on mission objectives and success. This work presents a
secure optimal control algorithm in the face of a cyber attack on a robot's
knowledge of the environment. This work focuses on cyber attack, but the
results generalize to incomplete or outdated information of an environment.
This work fuses ideas from robust control, optimal control, and sensor based
planning to provide a generalization of stopping distance in 3D. The planner is
implemented in simulation and its properties are analyzed.
",computer-science
"  A new type of absorbing boundary conditions for molecular dynamics
simulations are presented. The exact boundary conditions for crystalline solids
with harmonic approximation are expressed as a dynamic Dirichlet- to-Neumann
(DtN) map. It connects the displacement of the atoms at the boundary to the
traction on these atoms. The DtN map is valid for a domain with general
geometry. To avoid evaluating the time convo- lution of the dynamic DtN map, we
approximate the associated kernel function by rational functions in the Laplace
domain. The parameters in the approximations are determined by interpolations.
The explicit forms of the zeroth, first, and second order approximations will
be presented. The stability of the molecular dynamics model, supplemented with
these absorbing boundary conditions is established. Two numerical simulations
are performed to demonstrate the effectiveness of the methods.
",physics
"  Wireless communication plays a vital role in the promising performance of
connected and automated vehicle (CAV) technology. This paper proposes a
Vissim-based microscopic traffic simulation framework with an analytical
dedicated short-range communication (DSRC) module for packet reception. Being
derived from ns-2, a packet-level network simulator, the DSRC probability
module takes into account the imperfect wireless communication that occurs in
real-world deployment. Four managed lane deployment strategies are evaluated
using the proposed framework. While the average packet reception rate is above
93\% among all tested scenarios, the results reveal that the reliability of the
vehicle-to-vehicle (V2V) communication can be influenced by the deployment
strategies. Additionally, the proposed framework exhibits desirable scalability
for traffic simulation and it is able to evaluate transportation-network-level
deployment strategies in the near future for CAV technologies.
",computer-science
"  AMI observations towards CIZA J2242+5301, in comparison with observations of
weak gravitational lensing and X-ray emission from the literature, are used to
investigate the behaviour of non-baryonic dark matter (NBDM) and gas during the
merger. Analysis of the Sunyaev-Zel'dovich (SZ) signal indicates the presence
of high pressure gas elongated perpendicularly to the X-ray and weak-lensing
morphologies which, given the merger-axis constraints in the literature,
implies that high pressure gas is pushed out into a linear structure during
core passing. Simulations in the literature closely matching the inferred
merger scenario show the formation of gas density and temperature structures
perpendicular to the merger axis. These SZ observations are challenging for
modified gravity theories in which NBDM is not the dominant contributor to
galaxy-cluster gravity.
",physics
"  Formal models of games help us account for and predict behavior, leading to
more robust and innovative designs. While the games research community has
proposed many formalisms for both the ""game half"" (game models, game
description languages) and the ""human half"" (player modeling) of a game
experience, little attention has been paid to the interface between the two,
particularly where it concerns the player expressing her intent toward the
game. We describe an analytical and computational toolbox based on programming
language theory to examine the phenomenon sitting between control schemes and
game rules, which we identify as a distinct player intent language for each
game.
",computer-science
"  We propose to use neural networks for simultaneous detection and localization
of multiple sound sources in human-robot interaction. In contrast to
conventional signal processing techniques, neural network-based sound source
localization methods require fewer strong assumptions about the environment.
Previous neural network-based methods have been focusing on localizing a single
sound source, which do not extend to multiple sources in terms of detection and
localization. In this paper, we thus propose a likelihood-based encoding of the
network output, which naturally allows the detection of an arbitrary number of
sources. In addition, we investigate the use of sub-band cross-correlation
information as features for better localization in sound mixtures, as well as
three different network architectures based on different motivations.
Experiments on real data recorded from a robot show that our proposed methods
significantly outperform the popular spatial spectrum-based approaches.
",computer-science
"  The implementation of the algebraic Bethe ansatz for the XXZ Heisenberg spin
chain, of arbitrary spin-$s$, in the case, when both reflection matrices have
the upper-triangular form is analyzed. The general form of the Bethe vectors is
studied. In the particular form, Bethe vectors admit the recurrent procedure,
with an appropriate modification, used previously in the case of the XXX
Heisenberg chain. As expected, these Bethe vectors yield the strikingly simple
expression for the off-shell action of the transfer matrix of the chain as well
as the spectrum of the transfer matrix and the corresponding Bethe equations.
As in the XXX case, the so-called quasi-classical limit gives the off-shell
action of the generating function of the corresponding trigonometric Gaudin
Hamiltonians with boundary terms.
",physics
"  Material mixing induced by a Rayleigh-Taylor instability occurs ubiquitously
in either nature or engineering when a light fluid pushes against a heavy
fluid, accompanying with the formation and evolution of chaotic bubbles. Its
general evolution involves two mechanisms: bubble-merge and bubble-competition.
The former obeys a universa1 evolution law and has been well-studied, while the
latter depends on many factors and has not been well-recognized. In this paper,
we establish a theory for the latter to clarify and quantify the longstanding
open question: the dependence of bubbles evolution on the dominant factors of
arbitrary density ratio, broadband initial perturbations and various material
properties (e.g., viscosity, miscibility, surface tensor). Evolution of the
most important characteristic quantities, i.e., the diameter of dominant bubble
$D$ and the height of bubble zone $h$, is derived: (i) the $D$ expands
self-similarly with steady aspect ratio $\beta \equiv D/h \thickapprox (1{\rm{
+ }}A)/4$, depending only on dimensionless density ratio $A$, and (ii) the $h$
grows quadratically with constant growth coefficient $\alpha \equiv h/(Ag{t^2})
\thickapprox [2\phi/{\ln}(2{\eta _{\rm{0}}})]^2$, depending on both
dimensionless initial perturbation amplitude ${\eta _{\rm{0}}}$ and
material-property-associated linear growth rate ratio
$\phi\equiv\Gamma_{actual}/\Gamma_{ideal}\leqslant1$. The theory successfully
explains the continued puzzle about the widely varying $\alpha\in (0.02,0.12)$
in experiments and simulations, conducted at all value of $A \in (0,1)$ and
widely varying value of ${\eta _{\rm{0}}} \in [{10^{ - 7}},{10^{ - 2}}]$ with
different materials. The good agreement between theory and experiments implies
that majority of actual mixing depends on initial perturbations and material
properties, to which more attention should be paid in either natural or
engineering problems.
",physics
"  By building up on the recent theory that established the connection between
implicit generative modeling and optimal transport, in this study, we propose a
novel parameter-free algorithm for learning the underlying distributions of
complicated datasets and sampling from them. The proposed algorithm is based on
a functional optimization problem, which aims at finding a measure that is
close to the data distribution as much as possible and also expressive enough
for generative modeling purposes. We formulate the problem as a gradient flow
in the space of probability measures. The connections between gradient flows
and stochastic differential equations let us develop a computationally
efficient algorithm for solving the optimization problem, where the resulting
algorithm resembles the recent dynamics-based Markov Chain Monte Carlo
algorithms. We provide formal theoretical analysis where we prove finite-time
error guarantees for the proposed algorithm. Our experimental results support
our theory and shows that our algorithm is able to capture the structure of
challenging distributions.
",statistics
"  Yttria-stabilized zirconia (YSZ), a ZrO2-Y2O3 solid solution that contains a
large population of oxygen vacancies, is widely used in energy and industrial
applications. Past computational studies correctly predicted the anion
diffusivity but not the cation diffusivity, which is important for material
processing and stability. One of the challenges lies in identifying a plausible
configuration akin to the ground state in a glassy landscape. This is unlikely
to come from random sampling of even a very large sample space, but the odds
are much improved by incorporating packing preferences revealed by a modest
sized configurational library established from empirical potential
calculations. Ab initio calculations corroborated these preferences, which
prove remarkably robust extending to the fifth cation-oxygen shell about 8
{\AA} away. Yet because of frustration there are still rampant violations of
packing preferences and charge neutrality in the ground state, and the approach
toward it bears a close analogy to glass relaxations. Fast relaxations proceed
by fast oxygen movement around cations, while slow relaxations require slow
cation diffusion. The latter is necessarily cooperative because of strong
coupling imposed by the long-range packing preferences.
",physics
"  For a field $k$, we prove that the $i$th homology of the groups $GL_n(k)$,
$SL_n(k)$, $Sp_{2n}(k)$, $SO_{n,n}(k)$, and $SO_{n,n+1}(k)$ with coefficients
in their Steinberg representations vanish for $n \geq 2i+2$.
",mathematics
"  In combinatorial auctions, a designer must decide how to allocate a set of
indivisible items amongst a set of bidders. Each bidder has a valuation
function which gives the utility they obtain from any subset of the items. Our
focus is specifically on welfare maximization, where the objective is to
maximize the sum of valuations that the bidders place on the items that they
were allocated (the valuation functions are assumed to be reported truthfully).
We analyze an online problem in which the algorithm is not given the set of
bidders in advance. Instead, the bidders are revealed sequentially in a
uniformly random order, similarly to secretary problems. The algorithm must
make an irrevocable decision about which items to allocate to the current
bidder before the next one is revealed. When the valuation functions lie in the
class $XOS$ (which includes submodular functions), we provide a black box
reduction from offline to online optimization. Specifically, given an
$\alpha$-approximation algorithm for offline welfare maximization, we show how
to create a $(0.199 \alpha)$-approximation algorithm for the online problem.
Our algorithm draws on connections to secretary problems; in fact, we show that
the online welfare maximization problem itself can be viewed as a particular
kind of secretary problem with nonuniform arrival order.
",computer-science
"  We report the results of the 2dF-VST ATLAS Cold Spot galaxy redshift survey
(2CSz) based on imaging from VST ATLAS and spectroscopy from 2dF AAOmega over
the core of the CMB Cold Spot. We sparsely surveyed the inner 5$^{\circ}$
radius of the Cold Spot to a limit of $i_{AB} \le 19.2$, sampling $\sim7000$
galaxies at $z<0.4$. We have found voids at $z=$ 0.14, 0.26 and 0.30 but they
are interspersed with small over-densities and the scale of these voids is
insufficient to explain the Cold Spot through the $\Lambda$CDM ISW effect.
Combining with previous data out to $z\sim1$, we conclude that the CMB Cold
Spot could not have been imprinted by a void confined to the inner core of the
Cold Spot. Additionally we find that our 'control' field GAMA G23 shows a
similarity in its galaxy redshift distribution to the Cold Spot. Since the GAMA
G23 line-of-sight shows no evidence of a CMB temperature decrement we conclude
that the Cold Spot may have a primordial origin rather than being due to
line-of-sight effects.
",physics
"  Sea-level rise (SLR) is magnifying the frequency and severity of coastal
flooding. The rate and amount of global mean sea-level (GMSL) rise is a
function of the trajectory of global mean surface temperature (GMST).
Therefore, temperature stabilization targets (e.g., 1.5 °C and 2.0 °C
of warming above pre-industrial levels, as from the Paris Agreement) have
important implications for coastal flood risk. Here, we assess differences in
the return periods of coastal floods at a global network of tide gauges between
scenarios that stabilize GMST warming at 1.5 °C, 2.0 °C, and 2.5
°C above pre-industrial levels. We employ probabilistic, localized SLR
projections and long-term hourly tide gauge records to construct estimates of
the return levels of current and future flood heights for the 21st and 22nd
centuries. By 2100, under 1.5 °C, 2.0 °C, and 2.5 °C GMST
stabilization, median GMSL is projected to rise 47 cm with a very likely range
of 28-82 cm (90% probability), 55 cm (very likely 30-94 cm), and 58 cm (very
likely 36-93 cm), respectively. As an independent comparison, a semi-empirical
sea level model calibrated to temperature and GMSL over the past two millennia
estimates median GMSL will rise within < 13% of these projections. By 2150,
relative to the 2.0 °C scenario, GMST stabilization of 1.5 °C
inundates roughly 5 million fewer inhabitants that currently occupy lands,
including 40,000 fewer individuals currently residing in Small Island
Developing States. Relative to a 2.0 °C scenario, the reduction in the
amplification of the frequency of the 100-yr flood arising from a 1.5 °C
GMST stabilization is greatest in the eastern United States and in Europe, with
flood frequency amplification being reduced by about half.
",physics
"  Conventional textbook treatments on electromagnetic wave propagation consider
the induced charge and current densities as ""bound"", and therefore absorb them
into a refractive index. In principle it must also be possible to treat the
medium as vacuum, but with explicit charge and current densities. This gives a
more direct, physical description. However, since the induced waves propagate
in vacuum in this picture, it is not straightforward to realize that the
wavelength becomes different compared to that in vacuum. We provide an
explanation, and also associated time-domain simulations. As an extra bonus the
results turn out to illuminate the behavior of metamaterials.
",physics
"  According to a result of Ehresmann, the torsions of integral homology of real
Grassmannian are all of order $2$. In this note, We compute the
$\mathbb{Z}_2$-dimensions of torsions in the integral homology and cohomology
of real Grassmannian.
",mathematics
"  We investigate the configuration space of the Delta-Manipulator, identify 24
points in the configuration space, where the Jacobian of the Constraint
Equations looses rank and show, that these are not manifold points of the Real
Algebraic Set, which is defined by the Constraint Equations.
",computer-science
"  3D-Polarized Light Imaging (3D-PLI) reconstructs nerve fibers in histological
brain sections by measuring their birefringence. This study investigates
another effect caused by the optical anisotropy of brain tissue -
diattenuation. Based on numerical and experimental studies and a complete
analytical description of the optical system, the diattenuation was determined
to be below 4 % in rat brain tissue. It was demonstrated that the diattenuation
effect has negligible impact on the fiber orientations derived by 3D-PLI. The
diattenuation signal, however, was found to highlight different anatomical
structures that cannot be distinguished with current imaging techniques, which
makes Diattenuation Imaging a promising extension to 3D-PLI.
",physics
"  We investigate the ground state properties of ultracold atoms with long range
interactions trapped in a two leg ladder configuration in the presence of an
artificial magnetic field. Using a Gross-Pitaevskii approach and a mean field
Gutzwiller variational method, we explore both the weakly interacting and
strongly interacting regime, respectively. We calculate the boundaries between
the density-wave/supersolid and the Mott-insulator/superfluid phases as a
function of magnetic flux and uncover regions of supersolidity. The mean-field
results are confirmed by numerical simulations using a cluster mean field
approach.
",physics
"  We realize scattering states in a lossy and chaotic two-dimensional microwave
cavity which follow bundles of classical particle trajectories. To generate
such particlelike scattering states we measure the system's transmission matrix
and apply an adapted Wigner-Smith time-delay formalism to it. The necessary
shaping of the incident wave is achieved in situ using phase and amplitude
regulated microwave antennas. Our experimental findings pave the way for
establishing spatially confined communication channels that avoid possible
intruders or obstacles in wave-based communication systems.
",physics
"  The Doppler effect is a shift in the frequency of waves emitted from an
object moving relative to the observer. By observing and analysing the Doppler
shift in electromagnetic waves from astronomical objects, astronomers gain
greater insight into the structure and operation of our universe. In this
paper, a simple technique is described for teaching the basics of the Doppler
effect to undergraduate astrophysics students using acoustic waves. An
advantage of the technique is that it produces a visual representation of the
acoustic Doppler shift. The equipment comprises a 40 kHz acoustic transmitter
and a microphone. The sound is bounced off a computer fan and the signal
collected by a DrDAQ ADC and processed by a spectrum analyser. Widening of the
spectrum is observed as the fan power supply potential is increased from 4 to
12 V.
",physics
"  The Reidemeister number of an endomorphism of a group is the number of
twisted conjugacy classes determined by that endomorphism. The collection of
all Reidemeister numbers of all automorphisms of a group $G$ is called the
Reidemeister spectrum of $G$. In this paper, we determine the Reidemeister
spectra of all fundamental groups of solvmanifolds up to Hirsch length 4.
",mathematics
"  Nonlinear modal decoupling (NMD) was recently proposed to nonlinearly
transform a multi-oscillator system into a number of decoupled oscillators
which together behave the same as the original system in an extended
neighborhood of the equilibrium. Each oscillator has just one degree of freedom
and hence can easily be analyzed to infer the stability of the original system
associated with one electromechanical mode. As the first attempt of applying
the NMD methodology to realistic power system models, this paper proposes an
NMD-based transient stability analysis approach. For a multi-machine power
system, the approach first derives decoupled nonlinear oscillators by a
coordinates transformation, and then applies Lyapunov stability analysis to
oscillators to assess the stability of the original system. Nonlinear modal
interaction is also considered. The approach can be efficiently applied to a
large-scale power grid by conducting NMD regarding only selected modes. Case
studies on a 3-machine 9-bus system and an NPCC 48-machine 140-bus system show
the potentials of the approach in transient stability analysis for
multi-machine systems.
",computer-science
"  A numerical analysis of heat conduction through the cover plate of a heat
pipe is carried out to determine the temperature of the working substance,
average temperature of heating and cooling surfaces, heat spread in the
transmitter, and the heat bypass through the cover plate. Analysis has been
extended for the estimation of heat transfer requirements at the outer surface
of the con- denser under different heat load conditions using Genetic
Algorithm. This paper also presents the estimation of an average heat transfer
coefficient for the boiling and condensation of the working substance inside
the microgrooves corresponding to a known temperature of the heat source. The
equation of motion of the working fluid in the meniscus of an equilateral
triangular groove has been presented from which a new term called the minimum
surface tension required for avoiding the dry out condition is defined.
Quantitative results showing the effect of thickness of cover plate, heat load,
angle of inclination and viscosity of the working fluid on the different
aspects of the heat transfer, minimum surface tension required to avoid dry
out, velocity distribution of the liquid, and radius of liquid meniscus inside
the micro-grooves have been presented and discussed.
",physics
"  Fitting stochastic kinetic models represented by Markov jump processes within
the Bayesian paradigm is complicated by the intractability of the observed data
likelihood. There has therefore been considerable attention given to the design
of pseudo-marginal Markov chain Monte Carlo algorithms for such models.
However, these methods are typically computationally intensive, often require
careful tuning and must be restarted from scratch upon receipt of new
observations. Sequential Monte Carlo (SMC) methods on the other hand aim to
efficiently reuse posterior samples at each time point. Despite their appeal,
applying SMC schemes in scenarios with both dynamic states and static
parameters is made difficult by the problem of particle degeneracy. A
principled approach for overcoming this problem is to move each parameter
particle through a Metropolis-Hastings kernel that leaves the target invariant.
This rejuvenation step is key to a recently proposed SMC$^2$ algorithm, which
can be seen as the pseudo-marginal analogue of an idealised scheme known as
iterated batch importance sampling. Computing the parameter weights in SMC$^2$
requires running a particle filter over dynamic states to unbiasedly estimate
the intractable observed data likelihood contributions at each time point. In
this paper, we propose to use an auxiliary particle filter inside the SMC$^2$
scheme. Our method uses two recently proposed constructs for sampling
conditioned jump processes and we find that the resulting inference schemes
typically require fewer state particles than when using a simple bootstrap
filter. Using two applications, we compare the performance of the proposed
approach with various competing methods, including two global MCMC schemes.
",statistics
"  We raise a question on the existence of continuous roots of families of monic
polynomials (by the root of a family of polynomials we mean a function of the
coefficients of polynomials of a given family that maps each tuple of
coefficients to a root of the polynomial with these coefficients). We prove
that the family of monic second-degree polynomials with complex coefficients
and the families of monic fourth-degree and fifth-degree polynomials with real
coefficients have no continuous root. We also prove that the family of monic
second-degree polynomials with real coefficients has continuous roots and we
describe the set of all such roots.
",mathematics
"  RoboJam is a machine-learning system for generating music that assists users
of a touchscreen music app by performing responses to their short
improvisations. This system uses a recurrent artificial neural network to
generate sequences of touchscreen interactions and absolute timings, rather
than high-level musical notes. To accomplish this, RoboJam's network uses a
mixture density layer to predict appropriate touch interaction locations in
space and time. In this paper, we describe the design and implementation of
RoboJam's network and how it has been integrated into a touchscreen music app.
A preliminary evaluation analyses the system in terms of training, musical
generation and user interaction.
",computer-science
"  This paper is a survey of recent results on the adaptive robust non
parametric methods for the continuous time regression model with the semi -
martingale noises with jumps. The noises are modeled by the Lévy processes,
the Ornstein -- Uhlenbeck processes and semi-Markov processes. We represent the
general model selection method and the sharp oracle inequalities methods which
provide the robust efficient estimation in the adaptive setting. Moreover, we
present the recent results on the improved model selection methods for the
nonparametric estimation problems.
",mathematics
"  This study deals with content-based musical playlists generation focused on
Songs and Instrumentals. Automatic playlist generation relies on collaborative
filtering and autotagging algorithms. Autotagging can solve the cold start
issue and popularity bias that are critical in music recommender systems.
However, autotagging remains to be improved and cannot generate satisfying
music playlists. In this paper, we suggest improvements toward better
autotagging-generated playlists compared to state-of-the-art. To assess our
method, we focus on the Song and Instrumental tags. Song and Instrumental are
two objective and opposite tags that are under-studied compared to genres or
moods, which are subjective and multi-modal tags. In this paper, we consider an
industrial real-world musical database that is unevenly distributed between
Songs and Instrumentals and bigger than databases used in previous studies. We
set up three incremental experiments to enhance automatic playlist generation.
Our suggested approach generates an Instrumental playlist with up to three
times less false positives than cutting edge methods. Moreover, we provide a
design of experiment framework to foster research on Songs and Instrumentals.
We give insight on how to improve further the quality of generated playlists
and to extend our methods to other musical tags. Furthermore, we provide the
source code to guarantee reproducible research.
",computer-science
"  A whole-body torque control framework adapted for balancing and walking tasks
is presented in this paper. In the proposed approach, centroidal momentum terms
are excluded in favor of a hierarchy of high-priority position and orientation
tasks and a low-priority postural task. More specifically, the controller
stabilizes the position of the center of mass, the orientation of the pelvis
frame, as well as the position and orientation of the feet frames. The
low-priority postural task provides reference positions for each joint of the
robot. Joint torques and contact forces to stabilize tasks are obtained through
quadratic programming optimization. Besides the exclusion of centroidal
momentum terms, part of the novelty of the approach lies in the definition of
control laws in SE(3) which do not require the use of Euler parameterization.
Validation of the framework was achieved in a scenario where the robot kept
balance while walking in place. Experiments have been conducted with the iCub
robot, in simulation and in real-world experiments.
",computer-science
"  The ever-increasing architectural complexity in contemporary ASIC projects
turns Design Verification (DV) into a highly advanced endeavor. Pressing needs
for short time-to-market has made automation a key solution in DV. However,
recurring execution of large regression suites inevitably leads to challenging
amounts of test results. Following the design science paradigm, we present an
action research study to introduce visual analytics in a commercial ASIC
project. We develop a cityscape visualization tool using the game engine Unity.
Initial evaluations are promising, suggesting that the tool offers a novel
approach to identify error-prone parts of the design, as well as coverage
holes.
",computer-science
"  We propose and experimentally demonstrate the enhancement in the filtering
quality (Q) factor of an integrated micro-ring resonator (MRR) by embedding it
in an integrated Fabry-Perot (FP) cavity formed by cascaded Sagnac loop
reflectors (SLRs). By utilizing coherent interference within the FP cavity to
reshape the transmission spectrum of the MRR, both the Q factor and the
extinction ratio (ER) can be significantly improved. The device is
theoretically analyzed, and practically fabricated on a silicon-on-insulator
(SOI) wafer. Experimental results show that up to 11-times improvement in Q
factor, together with an 8-dB increase in ER, can be achieved via our proposed
method. The impact of varying structural parameters on the device performance
is also investigated and verified by the measured spectra of the fabricated
devices with different structural parameters.
",physics
"  This paper is concerned with radially symmetric solutions of systems of the
form \[ u_t = -\nabla V(u) + \Delta_x u \] where space variable $x$ and and
state-parameter $u$ are multidimensional, and the potential $V$ is coercive at
infinity. For such systems, under generic assumptions on the potential, the
asymptotic behaviour of solutions ""stable at infinity"", that is approaching a
spatially homogeneous equilibrium when $|x|$ approaches $+\infty$, is
investigated. It is proved that every such solutions approaches a stacked
family of radially symmetric bistable fronts travelling to infinity. This
behaviour is similar to the one of bistable solutions for gradient systems in
one unbounded spatial dimension, described in a companion paper. It is expected
(but unfortunately not proved at this stage) that behind these travelling
fronts the solution again behaves as in the one-dimensional case (that is, the
time derivative approaches zero and the solution approaches a pattern of
stationary solutions).
",mathematics
"  We construct a new family of high genus examples of free boundary minimal
surfaces in the Euclidean unit 3-ball by desingularizing the intersection of a
coaxial pair of a critical catenoid and an equatorial disk. The surfaces are
constructed by singular perturbation methods and have three boundary
components. They are the free boundary analogue of the Costa-Hoffman-Meeks
surfaces and the surfaces constructed by Kapouleas by desingularizing coaxial
catenoids and planes. It is plausible that the minimal surfaces we constructed
here are the same as the ones obtained recently by Ketover using the min-max
method.
",mathematics
"  The article analysis was carried out within the confines of the replication
project of the telescope, which was used by Mikhail Lomonosov at observation
the transit of Venus in 1761. At that time he discovered the Venusian
atmosphere. It is known that Lomonosov used Dollond 4.5 feet long achromatic
telescope. The investigation revealed significant faults in the description of
the approximation method, which most likely was used by J. Dollond & Son during
manufacturing of the early achromatic lenses.
",physics
"  Side channel attacks are a major class of attacks to crypto-systems.
Attackers collect and analyze timing behavior, I/O data, or power consumption
in these systems to undermine their effectiveness in protecting sensitive
information. In this work, we propose a new cache architecture, called Janus,
to enable crypto-systems to introduce randomization and uncertainty in their
runtime timing behavior and power utilization profile. In the proposed cache
architecture, each data block is equipped with an on-off flag to enable/disable
the data block. The Janus architecture has two special instructions in its
instruction set to support the on-off flag. Beside the analytical evaluation of
the proposed cache architecture, we deploy it in an ARM-7 processor core to
study its feasibility and practicality. Results show a significant variation in
the timing behavior across all the benchmarks. The new secure processor
architecture has minimal hardware overhead and significant improvement in
protecting against power analysis and timing behavior attacks.
",computer-science
"  Magnetic skyrmions are localized nanometric spin textures with quantized
winding numbers as the topological invariant. Rapidly increasing attention has
been paid to the investigations of skyrmions since their experimental discovery
in 2009, due both to the fundamental properties and the promising potential in
spintronics based applications. However, controlled creation of skyrmions
remains a pivotal challenge towards technological applications. Here, we report
that skyrmions can be created locally by electric field in the magnetoelectric
helimagnet Cu$\mathsf{_2}$OSeO$\mathsf{_3}$. Using Lorentz transmission
electron microscopy, we successfully write skyrmions in situ from a helical
spin background. Our discovery is highly coveted since it implies that
skyrmionics can be integrated into contemporary field effect transistor based
electronic technology, where very low energy dissipation can be achieved, and
hence realizes a large step forward to its practical applications.
",physics
"  This paper proposes a segmentation-free, automatic and efficient procedure to
detect general geometric quadric forms in point clouds, where clutter and
occlusions are inevitable. Our everyday world is dominated by man-made objects
which are designed using 3D primitives (such as planes, cones, spheres,
cylinders, etc.). These objects are also omnipresent in industrial
environments. This gives rise to the possibility of abstracting 3D scenes
through primitives, thereby positions these geometric forms as an integral part
of perception and high level 3D scene understanding.
As opposed to state-of-the-art, where a tailored algorithm treats each
primitive type separately, we propose to encapsulate all types in a single
robust detection procedure. At the center of our approach lies a closed form 3D
quadric fit, operating in both primal & dual spaces and requiring as low as 4
oriented-points. Around this fit, we design a novel, local null-space voting
strategy to reduce the 4-point case to 3. Voting is coupled with the famous
RANSAC and makes our algorithm orders of magnitude faster than its conventional
counterparts. This is the first method capable of performing a generic
cross-type multi-object primitive detection in difficult scenes. Results on
synthetic and real datasets support the validity of our method.
",computer-science
"  Detection, tracking, and pose estimation of surgical instruments are crucial
tasks for computer assistance during minimally invasive robotic surgery. In the
majority of cases, the first step is the automatic segmentation of surgical
tools. Prior work has focused on binary segmentation, where the objective is to
label every pixel in an image as tool or background. We improve upon previous
work in two major ways. First, we leverage recent techniques such as deep
residual learning and dilated convolutions to advance binary-segmentation
performance. Second, we extend the approach to multi-class segmentation, which
lets us segment different parts of the tool, in addition to background. We
demonstrate the performance of this method on the MICCAI Endoscopic Vision
Challenge Robotic Instruments dataset.
",computer-science
"  We consider finite point subsets (distributions) in compact metric spaces. In
the case of general rectifiable metric spaces, non-trivial bounds for sums of
distances between points of distributions and for discrepancies of
distributions in metric balls are given (Theorem 1.1). We generalize
Stolarsky's invariance principle to distance-invariant spaces (Theorem 2.1).
For arbitrary metric spaces, we prove a probabilistic invariance principle
(Theorem 3.1). Furthermore, we construct equal-measure partitions of general
rectifiable compact metric spaces into parts of small average diameter (Theorem
4.1). This version of the paper will be published in Mathematika
",mathematics
"  Dielectric lined waveguides are under extensive study as accelerating
structures that can be excited by electron beams. Rectangular dielectric
structures are used both in proof of principle experiments for new accelerating
schemes and for studying the electronic properties of the structure loading
material. Analysis of Cherenkov radiation generated by high current
relativistic electron bunch passing through a rectangular waveguide with
transversal isotropic dielectric loading has been carried out. Some of the
materials used for the waveguide loading of accelerating structures (sapphire,
ceramic films) possess significant anisotropic properties. In turn, it can
influence excitation parameters of the wakefields generated by an electron
beam. General solutions for the fields generated by a relativistic electron
beam propagating in a rectangular dielectric waveguide have been derived using
the orthogonal eigenmode decomposition method for the transverse operators of
the Helmholtz equation. The analytical expression for the combined Cherenkov
and Coulomb fields in terms of a superposition of LSM and LSE-modes of
rectangular waveguide with transversal isotropic dielectric loading has been
obtained. Numerical modelling of the longitudinal and transverse (deflecting)
wakefields has been carried out as well. It is shown that the dielectric
anisotropy causes frequency shift in comparison to the dielectric-lined
waveguide with the isotropic dielectric loading.
",physics
"  In the present work, we aim at taking a step towards the spectral stability
analysis of Peregrine solitons, i.e., wave structures that are used to emulate
extreme wave events. Given the space-time localized nature of Peregrine
solitons, this is a priori a non-trivial task. Our main tool in this effort
will be the study of the spectral stability of the periodic generalization of
the Peregrine soliton in the evolution variable, namely the Kuznetsov--Ma
breather. Given the periodic structure of the latter, we compute the
corresponding Floquet multipliers, and examine them in the limit where the
period of the orbit tends to infinity. This way, we extrapolate towards the
stability of the limiting structure, namely the Peregrine soliton. We find that
multiple unstable modes of the background are enhanced, yet no additional
unstable eigenmodes arise as the Peregrine limit is approached. We explore the
instability evolution also in direct numerical simulations.
",physics
"  The major system is a mnemonic system that can be used to memorize sequences
of numbers. In this work, we present a method to automatically generate
sentences that encode a given number. We propose several encoding models and
compare the most promising ones in a password memorability study. The results
of the study show that a model combining part-of-speech sentence templates with
an $n$-gram language model produces the most memorable password
representations.
",computer-science
"  A procedure for the design of fixed-gain tracking filters, using an
augmented-state observer with signal and interference subspaces, is proposed.
The signal subspace incorporates an integrating Newtonian model and a
second-order maneuver model that is matched to a sustained constant-g turn; the
deterministic interference model creates a Nyquist null for smoother track
estimates. The selected models provide a simple means of shaping and analyzing
the (transient and steady-state) response of tracking-filters of elevated
order.
",computer-science
"  Designing analog sub-threshold neuromorphic circuits in deep sub-micron
technologies e.g. 28 nm can be a daunting task due to the problem of excessive
leakage current. We propose novel energy-efficient hybrid CMOS-nano
electro-mechanical switches (NEMS) Leaky Integrate and Fire (LIF) neuron and
synapse circuits and investigate the impact of NEM switches on the leakage
power and overall energy consumption. We analyze the performance of
biologically-inspired neuron circuit in terms of leakage power consumption and
present new energy-efficient neural circuits that operate with biologically
plausible firing rates. Our results show the proposed CMOS-NEMS neuron circuit
is, on average, 35% more energy-efficient than its CMOS counterpart with same
complexity in 28 nm process. Moreover, we discuss how NEM switches can be
utilized to further improve the scalability of mixed-signal neuromorphic
circuits.
",computer-science
"  Winograd-based convolution has quickly gained traction as a preferred
approach to implement convolutional neural networks (ConvNet) on various
hardware platforms because it requires fewer floating point operations than
FFT-based or direct convolutions.
This paper compares three highly optimized implementations (regular FFT--,
Gauss--FFT--, and Winograd--based convolutions) on modern multi-- and
many--core CPUs. Although all three implementations employed the same
optimizations for modern CPUs, our experimental results with two popular
ConvNets (VGG and AlexNet) show that the FFT--based implementations generally
outperform the Winograd--based approach, contrary to the popular belief.
To understand the results, we use a Roofline performance model to analyze the
three implementations in detail, by looking at each of their computation phases
and by considering not only the number of floating point operations, but also
the memory bandwidth and the cache sizes. The performance analysis explains
why, and under what conditions, the FFT--based implementations outperform the
Winograd--based one, on modern CPUs.
",computer-science
"  Models and observations suggest that ice-particle aggregation at and beyond
the snowline dominates the earliest stages of planet-formation, which therefore
is subject to many laboratory studies. However, the pressure-temperature
gradients in proto-planetary disks mean that the ices are constantly processed,
undergoing phase changes between different solid phases and the gas phase. Open
questions remain as to whether the properties of the icy particles themselves
dictate collision outcomes and therefore how effectively collision experiments
reproduce conditions in pro- toplanetary environments. Previous experiments
often yielded apparently contradictory results on collision outcomes, only
agreeing in a temperature dependence setting in above $\approx$ 210 K. By
exploiting the unique capabilities of the NIMROD neutron scattering instrument,
we characterized the bulk and surface structure of icy particles used in
collision experiments, and studied how these structures alter as a function of
temperature at a constant pressure of around 30 mbar. Our icy grains, formed
under liquid nitrogen, undergo changes in the crystalline ice-phase,
sublimation, sintering and surface pre-melting as they are heated from 103 to
247 K. An increase in the thickness of the diffuse surface layer from $\approx$
10 to $\approx$ 30 {\AA} ($\approx$ 2.5 to 12 bilayers) proves increased
molecular mobility at temperatures above $\approx$ 210 K. As none of the other
changes tie-in with the temperature trends in collisional outcomes, we conclude
that the surface pre-melting phenomenon plays a key role in collision
experiments at these temperatures. Consequently, the pressure-temperature
environment, may have a larger influence on collision outcomes than previously
thought.
",physics
"  We discuss the derivation of a low-energy effective field theory of phase
(Goldstone) and amplitude (Higgs) modes of the pairing field from a microscopic
theory of attractive fermions. The coupled equations for Goldstone and Higgs
fields are critically analyzed in the Bardeen-Cooper-Schrieffer (BCS) to
Bose-Einstein condensate (BEC) crossover both in three spatial dimensions and
in two spatial dimensions. The crucial role of pair fluctuations is
investigated, and the beyond-mean-field Gaussian theory of the BCS-BEC
crossover is compared with available experimental data of the two-dimensional
ultracold Fermi superfluid.
",physics
"  In [Mas82] and [Vee78] it was proved independently that almost every interval
exchange transformation is uniquely ergodic. The Birkhoff ergodic theorem
implies that these maps mainly have uniformly distributed orbits. This raises
the question under which conditions the orbits yield low-discrepancy sequences.
The case of $n=2$ intervals corresponds to circle rotation, where conditions
for low-discrepancy are well-known. In this paper, we give corresponding
conditions in the case $n=3$. Furthermore, we construct infinitely many
interval exchange transformations with low-discrepancy orbits for $n \geq 4$.
We also show that these examples do not coincide with $LS$-sequences if $S \geq
2$.
",mathematics
"  We review the physics of GRB production by relativistic jets that start
highly opaque near the central source and then expand to transparency. We
discuss dissipative and radiative processes in the jet and how radiative
transfer shapes the observed nonthermal spectrum released at the photosphere. A
comparison of recent detailed models with observations gives estimates for
important parameters of GRB jets, such as the Lorentz factor and magnetization.
We also discuss predictions for GRB polarization and neutrino emission.
",physics
"  We analysed the flux-flow region of isofield magneto resistivity data
obtained on three crystals of BaFe$_{2-x}$Ni$_x$As$_2$ with $T_c$$\sim$20 K for
three different geometries relative to the angle formed between the applied
magnetic field and the c-axis of the crystals. The field dependent activation
energy, $U_0$, was obtained from the TAFF and modified vortex-glass models,
which were compared with the values of $U_0$ obtained from flux-creep available
in the literature. We observed that the $U_0$ obtained from the TAFF model show
deviations among the different crystals, while the correspondent glass lines
obtained from the vortex glass model are virtually coincident. It is shown that
the data is well explained by the modified vortex glass model, allowing to
extract values of $T_g$, the glass transition temperature, and $T^*$, a
temperature which scales with the mean field critical temperature $T_c(H)$. The
resulting glass lines obey the anisotropic Ginzburg-Landau theory and are well
fitted by a theory developed in the literature by considering the effect of
disorder.
",physics
"  Tracking and controlling the shape of continuum dexterous manipulators (CDM)
in constraint environments is a challenging task. The imposed constraints and
interaction with unknown obstacles may conform the CDM's shape and therefore
demands for shape sensing methods which do not rely on direct line of sight. To
address these issues, we integrate a novel Fiber Bragg Grating (FBG) shape
sensing unit into a CDM, reconstruct the shape in real-time, and develop an
optimization-based control algorithm using FBG tip position feedback. The CDM
is designed for less-invasive treatment of osteolysis (bone degradation). To
evaluate the performance of the feedback control algorithm when the CDM
interacts with obstacles, we perform a set of experiments similar to the real
scenario of the CDM interaction with soft and hard lesions during the treatment
of osteolysis. In addition, we propose methods for identification of the CDM
collisions with soft or hard obstacles using the jacobian information. Results
demonstrate successful control of the CDM tip based on the FBG feedback and
indicate repeatability and robustness of the proposed method when interacting
with unknown obstacles.
",computer-science
"  Ages and masses of young stars are often estimated by comparing their
luminosities and effective temperatures to pre-main sequence stellar evolution
tracks, but magnetic fields and starspots complicate both the observations and
evolution. To understand their influence, we study the heavily-spotted
weak-lined T-Tauri star LkCa 4 by searching for spectral signatures of
radiation originating from the starspot or starspot groups. We introduce a new
methodology for constraining both the starspot filling factor and the spot
temperature by fitting two-temperature stellar atmosphere models constructed
from Phoenix synthetic spectra to a high-resolution near-IR IGRINS spectrum.
Clearly discernable spectral features arise from both a hot photospheric
component $T_{\mathrm{hot}} \sim4100$ K and to a cool component
$T_{\mathrm{cool}} \sim2700-3000$ K, which covers $\sim80\%$ of the visible
surface. This mix of hot and cool emission is supported by analyses of the
spectral energy distribution, rotational modulation of colors and of TiO band
strengths, and features in low-resolution optical/near-IR spectroscopy.
Although the revised effective temperature and luminosity make LkCa 4 appear
much younger and lower mass than previous estimates from unspotted stellar
evolution models, appropriate estimates will require the production and
adoption of spotted evolutionary models. Biases from starspots likely afflict
most fully convective young stars and contribute to uncertainties in ages and
age spreads of open clusters. In some spectral regions starspots act as a
featureless veiling continuum owing to high rotational broadening and heavy
line-blanketing in cool star spectra. Some evidence is also found for an
anti-correlation between the velocities of the warm and cool components.
",physics
"  The significance of topological phases has been widely recognized in the
community of condensed matter physics. The well controllable quantum systems
provide an artificial platform to probe and engineer various topological
phases. The adiabatic trajectory of a quantum state describes the change of the
bulk Bloch eigenstates with the momentum, and this adiabatic simulation method
is however practically limited due to quantum dissipation. Here we apply the
`shortcut to adiabaticity' (STA) protocol to realize fast adiabatic evolutions
in the system of a superconducting phase qubit. The resulting fast adiabatic
trajectories illustrate the change of the bulk Bloch eigenstates in the
Su-Schrieffer-Heeger (SSH) model. A sharp transition is experimentally
determined for the topological invariant of a winding number. Our experiment
helps identify the topological Chern number of a two-dimensional toy model,
suggesting the applicability of the fast adiabatic simulation method for
topological systems.
",physics
"  As a natural extension of compressive sensing and the requirement of some
practical problems, Phaseless Compressed Sensing (PCS) has been introduced and
studied recently. Many theoretical results have been obtained for PCS with the
aid of its convex relaxation. Motivated by successful applications of nonconvex
relaxed methods for solving compressive sensing, in this paper, we try to
investigate PCS via its nonconvex relaxation. Specifically, we relax PCS in the
real context by the corresponding $\ell_p$-minimization with $p\in (0,1)$. We
show that there exists a constant $p^\ast\in (0,1]$ such that for any fixed
$p\in(0, p^\ast)$, every optimal solution to the $\ell_p$-minimization also
solves the concerned problem; and derive an expression of such a constant
$p^\ast$ by making use of the known data and the sparsity level of the
concerned problem. These provide a theoretical basis for solving this class of
problems via the corresponding $\ell_p$-minimization.
",mathematics
"  Android has been the most popular smartphone system, with multiple platform
versions (e.g., KITKAT and Lollipop) active in the market. To manage the
application's compatibility with one or more platform versions, Android allows
apps to declare the supported platform SDK versions in their manifest files. In
this paper, we make a first effort to study this modern software mechanism. Our
objective is to measure the current practice of the declared SDK versions
(which we term as DSDK versions afterwards) in real apps, and the consistency
between the DSDK versions and their app API calls. To this end, we perform a
three-dimensional analysis. First, we parse Android documents to obtain a
mapping between each API and their corresponding platform versions. We then
analyze the DSDK-API consistency for over 24K apps, among which we pre-exclude
1.3K apps that provide different app binaries for different Android versions
through Google Play analysis. Besides shedding light on the current DSDK
practice, our study quantitatively measures the two side effects of
inappropriate DSDK versions: (i) around 1.8K apps have API calls that do not
exist in some declared SDK versions, which causes runtime crash bugs on those
platform versions; (ii) over 400 apps, due to claiming the outdated targeted
DSDK versions, are potentially exploitable by remote code execution. These
results indicate the importance and difficulty of declaring correct DSDK, and
our work can help developers fulfill this goal.
",computer-science
"  High-resolution imaging reveals a large morphological variety of
protoplanetary disks. To date, no constraints on their global evolution have
been found from this census. An evolutionary classification of disks was
proposed based on their IR spectral energy distribution, with the Group I
sources showing a prominent cold component ascribed to an earlier stage of
evolution than Group II. Disk evolution can be constrained from the comparison
of disks with different properties. A first attempt of disk taxonomy is now
possible thanks to the increasing number of high-resolution images of Herbig
Ae/Be stars becoming available. Near-IR images of six Group II disks in
scattered light were obtained with VLT/NACO in Polarimetric Differential
Imaging, which is the most efficient technique to image the light scattered by
the disk material close to the stars. We compare the stellar/disk properties of
this sample with those of well-studied Group I sources available from the
literature. Three Group II disks are detected. The brightness distribution in
the disk of HD163296 indicates the presence of a persistent ring-like structure
with a possible connection with the CO snowline. A rather compact (less than
100 AU) disk is detected around HD142666 and AK Sco. A taxonomic analysis of 17
Herbig Ae/Be sources reveals that the difference between Group I and Group II
is due to the presence or absence of a large disk cavity (larger than 5 AU).
There is no evidence supporting the evolution from Group I to Group II. Group
II are not evolved version of the Group I. Within the Group II disks, very
different geometries (both self-shadowed and compact) exist. HD163296 could be
the primordial version of a typical Group I. Other Group II, like AK Sco and
HD142666, could be smaller counterpart of Group I unable to open cavities as
large as those of Group I.
",physics
"  Automata expressiveness is an essential feature in understanding which of the
formalisms available should be chosen for modelling a particular problem.
Probabilistic and stochastic automata are suitable for modelling systems
exhibiting probabilistic behavior and their expressiveness has been studied
relative to non-probabilistic transition systems and Markov chains. In this
paper, we consider previous formalisms of Timed, Probabilistic and Stochastic
Timed Automata, we present our new model of Timed Automata with Polynomial
Delay, we introduce a measure of expressiveness for automata we call trace
expressiveness and we characterize the expressiveness of these models relative
to each other under this new measure.
",computer-science
"  Simulation of wave propagation in a microearthquake environment is often
challenging due to small-scale structural and material heterogeneities. We
simulate wave propagation in three different real microearthquake environments
using a spectral-element method. In the first example, we compute the full
wavefield in 2D and 3D models of an underground ore mine, namely the Pyhaesalmi
mine in Finland. In the second example, we simulate wave propagation in a
homogeneous velocity model including the actual topography of an unstable rock
slope at Aaknes in western Norway. Finally, we compute the full wavefield for a
weakly anisotropic cylindrical sample at laboratory scale, which was used for
an acoustic emission experiment under triaxial loading. We investigate the
characteristic features of wave propagation in those models and compare
synthetic waveforms with observed waveforms wherever possible. We illustrate
the challenges associated with the spectral-element simulation in those models.
",physics
"  We establish a natural connection of the $q$-Virasoro algebra $D_{q}$
introduced by Belov and Chaltikian with affine Kac-Moody Lie algebras. More
specifically, for each abelian group $S$ together with a one-to-one linear
character $\chi$, we define an infinite-dimensional Lie algebra $D_{S}$ which
reduces to $D_{q}$ when $S=\mathbb{Z}$. Guided by the theory of equivariant
quasi modules for vertex algebras, we introduce another Lie algebra
${\mathfrak{g}}_{S}$ with $S$ as an automorphism group and we prove that
$D_{S}$ is isomorphic to the $S$-covariant algebra of the affine Lie algebra
$\widehat{\mathfrak{g}_{S}}$. We then relate restricted $D_{S}$-modules of
level $\ell\in \mathbb{C}$ to equivariant quasi modules for the vertex algebra
$V_{\widehat{\mathfrak{g}_{S}}}(\ell,0)$ associated to
$\widehat{\mathfrak{g}_{S}}$ with level $\ell$. Furthermore, we show that if
$S$ is a finite abelian group of order $2l+1$, $D_{S}$ is isomorphic to the
affine Kac-Moody algebra of type $B^{(1)}_{l}$.
",mathematics
"  The Alvarez-Macovski method [Alvarez, R. E and Macovski, A.,
""Energy-selective reconstructions in X-ray computerized tomography"", Phys. Med.
Biol. (1976), 733--44] requires the inversion of the transformation from the
line integrals of the basis set coefficients to measurements with multiple
x-ray spectra. Analytical formulas for invertibility of the transformation from
two measurements to two line integrals are derived. It is found that
non-invertible systems have near zero Jacobian determinants on a nearly
straight line in the line integrals plane. Formulas are derived for the points
where the line crosses the axes, thus determining the line. Additional formulas
are derived for the values of the terms of the Jacobian determinant at the
endpoints of the line of non-invertibility. The formulas are applied to a set
of spectra including one suggested by Levine that is not invertible as well as
similar spectra that are invertible and voltage switched x-ray tube spectra
that are also invertible. An iterative inverse transformation algorithm
exhibits large errors with non-invertible spectra.
",physics
"  We address the important question of whether the newly discovered exoplanet,
Proxima Centauri b (PCb), is capable of retaining an atmosphere over long
periods of time. This is done by adapting a sophisticated multi-species MHD
model originally developed for Venus and Mars, and computing the ion escape
losses from PCb. The results suggest that the ion escape rates are about two
orders of magnitude higher than the terrestrial planets of our Solar system if
PCb is unmagnetized. In contrast, if the planet does have an intrinsic dipole
magnetic field, the rates are lowered for certain values of the stellar wind
dynamic pressure, but they are still higher than the observed values for our
Solar system's terrestrial planets. These results must be interpreted with due
caution, since most of the relevant parameters for PCb remain partly or wholly
unknown.
",physics
"  Electromagnetic properties of single crystal terbium gallium garnet (TGG) are
characterised from room down to millikelvin temperatures using the whispering
gallery mode method. Microwave spectroscopy is performed at low powers
equivalent to a few photons in energy and conducted as functions of the
magnetic field and temperature. A phase transition is detected close to the
temperature of 3.5 K. This is observed for multiple whispering gallery modes
causing an abrupt negative frequency shift and a change in transmission due to
extra losses in the new phase caused by a change in complex magnetic
susceptibility.
",physics
"  A parametrization of irreducible unitary representations associated with the
regular adjoint orbits of a hyperspecial compact subgroup of a reductive group
over a non-dyadic non-archimedean local filed is presented. The parametrization
is given by means of (a subset of) the character group of certain finite
abelian groups arising from the reductive group. Our method is based upon
Cliffod's theory and Weil representations over finite fields. It works under an
assumption of the triviality of certain Schur multipliers defined for an
algebraic group over a finite field. The assumption of the triviality has good
evidences in the case of general linear groups and highly probable in general.
",mathematics
"  The paper conducts a second-order variational analysis for an important class
of nonpolyhedral conic programs generated by the so-called
second-order/Lorentz/ice-cream cone $Q$. From one hand, we prove that the
indicator function of $Q$ is always twice epi-differentiable and apply this
result to characterizing the uniqueness of Lagrange multipliers at stationary
points together with an error bound estimate in the general second-order cone
setting involving ${\cal C}^2$-smooth data. On the other hand, we precisely
calculate the graphical derivative of the normal cone mapping to $Q$ under the
weakest metric subregularity constraint qualification and then give an
application of the latter result to a complete characterization of isolated
calmness for perturbed variational systems associated with second-order cone
programs. The obtained results seem to be the first in the literature in these
directions for nonpolyhedral problems without imposing any nondegeneracy
assumptions.
",mathematics
"  We have studied the critical properties of the contact process on a square
lattice with quenched site dilution by Monte Carlo simulations. This was
achieved by generating in advance the percolating cluster, through the use of
an appropriate epidemic model, and then by the simulation of the contact
process on the top of the percolating cluster. The dynamic critical exponents
were calculated by assuming an activated scaling relation and the static
exponents by the usual power law behavior. Our results are in agreement with
the prediction that the quenched diluted contact process belongs to the
universality class of the random transverse-field Ising model. We have also
analyzed the model and determined the phase diagram by the use of a mean-field
theory that takes into account the correlation between neighboring sites.
",physics
"  Distributed algorithms for solving additive or consensus optimization
problems commonly rely on first-order or proximal splitting methods. These
algorithms generally come with restrictive assumptions and at best enjoy a
linear convergence rate. Hence, they can require many iterations or
communications among agents to converge. In many cases, however, we do not seek
a highly accurate solution for consensus problems. Based on this we propose a
controlled relaxation of the coupling in the problem which allows us to compute
an approximate solution, where the accuracy of the approximation can be
controlled by the level of relaxation. The relaxed problem can be efficiently
solved in a distributed way using a combination of primal-dual interior-point
methods (PDIPMs) and message-passing. This algorithm purely relies on
second-order methods and thus requires far fewer iterations and communications
to converge. This is illustrated in numerical experiments, showing its superior
performance compared to existing methods.
",mathematics
"  Thermal gradients induce concentration gradients in alkali halide solutions,
and the salt migrates towards hot or cold regions depending on the average
temperature of the solution. This effect has been interpreted using the heat of
transport, which provides a route to rationalize thermophoretic phenomena.
Early theories provide estimates of the heat of transport at infinite dilution.
These values are used to interpret thermodiffusion (Soret) and thermoelectric
(Seebeck) effects. However, accessing heats of transport of individual ions at
finite concentration remains an outstanding question both theoretically and
experimentally. Here we discuss a computational approach to calculate heats of
transport of aqueous solutions at finite concentrations, and apply our method
to study lithium chloride solutions at concentrations $>0.5$~M. The heats of
transport are significantly different for Li$^+$ and Cl$^-$ ions, unlike what
is expected at infinite dilution. We find theoretical evidence for the
existence of minima in the Soret coefficient of LiCl, where the magnitude of
the heat of transport is maximized. The Seebeck coefficient obtained from the
ionic heats of transport varies significantly with temperature and
concentration. We identify thermodynamic conditions leading to a maximization
of the thermoelectric response of aqueous solutions.
",physics
"  The motion of electrons and nuclei in photochemical events often involve
conical intersections, degeneracies between electronic states. They serve as
funnels for nuclear relaxation - on the femtosecond scale - in processes where
the electrons and nuclei couple nonadiabatically. Accurate ab initio quantum
chemical models are essential for interpreting experimental measurements of
such phenomena. In this paper we resolve a long-standing problem in coupled
cluster theory, presenting the first formulation of the theory that correctly
describes conical intersections between excited electronic states of the same
symmetry. This new development demonstrates that the highly accurate coupled
cluster theory can be applied to describe dynamics on excited electronic states
involving conical intersections.
",physics
"  The quantum anomalous Hall (QAH) phase is a novel topological state of matter
characterized by a nonzero quantized Hall conductivity without an external
magnetic field. The realizations of QAH effect, however, are experimentally
challengeable. Based on ab initio calculations, here we propose an intrinsic
QAH phase in DCA Kagome lattice. The nontrivial topology in Kagome bands are
confirmed by the nonzero chern number, quantized Hall conductivity, and gapless
chiral edge states of Mn-DCA lattice. A tight-binding (TB) model is further
constructed to clarify the origin of QAH effect. Furthermore, its Curie
temperature, estimated to be ~ 253 K using Monte-Carlo simulation, is
comparable with room temperature and higher than most of two-dimensional
ferromagnetic thin films. Our findings present a reliable material platform for
the observation of QAH effect in covalent-organic frameworks.
",physics
"  Decentralized machine learning is a promising emerging paradigm in view of
global challenges of data ownership and privacy. We consider learning of linear
classification and regression models, in the setting where the training data is
decentralized over many user devices, and the learning algorithm must run
on-device, on an arbitrary communication network, without a central
coordinator. We propose COLA, a new decentralized training algorithm with
strong theoretical guarantees and superior practical performance. Our framework
overcomes many limitations of existing methods, and achieves communication
efficiency, scalability, elasticity as well as resilience to changes in data
and participating devices.
",statistics
"  A particularly promising pathway to enhance the efficiency of thermoelectric
materials lies in the use of resonant states, as suggested by experimentalists
and theorists alike. In this paper, we go over the mechanisms used in the
literature to explain how resonant levels affect the thermoelectric properties,
and we suggest that the effects of hybridization are crucial yet
ill-understood. In order to get a good grasp of the physical picture and to
draw guidelines for thermoelectric enhancement, we use a tight-binding model
containing a conduction band hybridized with a flat band. We find that the
conductivity is suppressed in a wide energy range near the resonance, but that
the Seebeck coefficient can be boosted for strong enough hybridization, thus
allowing for a significant increase of the power factor. The Seebeck
coefficient can also display a sign change as the Fermi level crosses the
resonance. Our results suggest that in order to boost the power factor, the
hybridization strength must not be too low, the resonant level must not be too
close to the conduction (or valence) band edge, and the Fermi level must be
located around, but not inside, the resonant peak.
",physics
"  We study the heat trace for both the drifting Laplacian as well as
Schrödinger operators on compact Riemannian manifolds. In the case of a
finite regularity potential or weight function, we prove the existence of a
partial (six term) asymptotic expansion of the heat trace for small times as
well as a suitable remainder estimate. We also demonstrate that the more
precise asymptotic behavior of the remainder is determined by and conversely
distinguishes higher (Sobolev) regularity on the potential or weight function.
In the case of a smooth weight function, we determine the full asymptotic
expansion of the heat trace for the drifting Laplacian for small times. We then
use the heat trace to study the asymptotics of the eigenvalue counting
function. In both cases the Weyl law coincides with the Weyl law for the
Riemannian manifold with the standard Laplace-Beltrami operator. We conclude by
demonstrating isospectrality results for the drifting Laplacian on compact
manifolds.
",mathematics
"  Consider a nilpotent element e in a simple complex Lie algebra. The Springer
fibre corresponding to e admits a discretization (discrete analogue) introduced
by the author in 1999. In this paper we propose a conjectural description of
that discretization which is more amenable to computation.
",mathematics
"  We propose a new A* CCG parsing model in which the probability of a tree is
decomposed into factors of CCG categories and its syntactic dependencies both
defined on bi-directional LSTMs. Our factored model allows the precomputation
of all probabilities and runs very efficiently, while modeling sentence
structures explicitly via dependencies. Our model achieves the state-of-the-art
results on English and Japanese CCG parsing.
",computer-science
"  We present the combined Chandra and Swift-BAT spectral analysis of seven
Seyfert 2 galaxies selected from the Swift-BAT 100-month catalog. We selected
nearby (z<=0.03) sources lacking of a ROSAT counterpart and never previously
observed with Chandra in the 0.3-10 keV energy range, and targeted these
objects with 10 ks Chandra ACIS-S observations. The X-ray spectral fitting over
the 0.3-150 keV energy range allows us to determine that all the objects are
significantly obscured, having NH>=1E23 cm^(-2) at a >99% confidence level.
Moreover, one to three sources are candidate Compton thick Active Galactic
Nuclei (CT-AGN), i.e., have NH>=1E24 cm^(-2). We also test the recent ""spectral
curvature"" method developed by Koss et al. (2016) to find candidate CT-AGN,
finding a good agreement between our results and their predictions. Since the
selection criteria we adopted have been effective in detecting highly obscured
AGN, further observations of these and other Seyfert 2 galaxies selected from
the Swift-BAT 100-month catalog will allow us to create a statistically
significant sample of highly obscured AGN, therefore better understanding the
physics of the obscuration processes.
",physics
"  We investigate the time evolution towards the asymptotic steady state of a
one dimensional interacting system after a quantum quench. We show that at
finite time the latter induces entanglement between right- and left- moving
density excitations, encoded in their cross-correlators, which vanishes in the
long-time limit. This behavior results in a universal time-decay in system
spectral properties $ \propto t^{-2} $, in addition to non-universal power-law
contributions typical of Luttinger liquids. Importantly, we argue that the
presence of quench-induced entanglement clearly emerges in transport
properties, such as charge and energy currents injected in the system from a
biased probe, and determines their long-time dynamics. In particular, energy
fractionalization phenomenon turns out to be a promising platform to observe
the universal power-law decay $ \propto t^{-2} $ induced by entanglement and
represents a novel way to study the corresponding relaxation mechanism.
",physics
"  Large-area ($\sim$cm$^2$) films of vertical heterostructures formed by
alternating graphene and transition-metal dichalcogenide(TMD) alloys are
obtained by wet chemical routes followed by a thermal treatment at low
temperature (300 $^\circ$C). In particular, we synthesized stacked graphene and
W$_x$Mo$_{1-x}$S$_2$ alloy phases that were used as hydrogen evolution
catalysts. We observed a Tafel slope of 38.7 mV dec$^{-1}$ and 96 mV onset
potential (at current density of 10 mA cm$^{-2}$) when the heterostructure
alloy is annealed at 300 $^o$C. These results indicate that heterostructure
formed by graphene and W$_{0.4}$Mo$_{0.6}$S$_2$ alloys are far more efficient
than WS$_2$ and MoS$_2$ by at least a factor of two, and it is superior than
any other reported TMD system. This strategy offers a cheap and low temperature
synthesis alternative able to replace Pt in the hydrogen evolution reaction
(HER). Furthermore, the catalytic activity of the alloy is stable over time,
i.e. the catalytic activity does not experience a significant change even after
1000 cycles. Using density functional theory calculations, we found that this
enhanced hydrogen evolution in the W$_x$Mo$_{1-x}$S$_2$ alloys is mainly due to
the lower energy barrier created by a favorable overlap of the d-orbitals from
the transition metals and the s-orbitals of H$_2$, with the lowest energy
barrier occurring for W$_{0.4}$Mo$_{0.6}$S$_2$ alloy. Thus, it is now possible
to further improve the performance of the ""inert"" TMD basal plane via metal
alloying, in addition to the previously reported strategies of creation of
point defects, vacancies and edges. The synthesis of
graphene/W$_{0.4}$Mo$_{0.6}$S$_2$ produced at relatively low temperatures is
scalable and could be used as an effective low cost Pt-free catalyst.
",physics
"  Understanding semantic similarity among images is the core of a wide range of
computer vision applications. An important step towards this goal is to collect
and learn human perceptions. Interestingly, the semantic context of images is
often ambiguous as images can be perceived with emphasis on different aspects,
which may be contradictory to each other.
In this paper, we present a method for learning the semantic similarity among
images, inferring their latent aspects and embedding them into multi-spaces
corresponding to their semantic aspects.
We consider the multi-embedding problem as an optimization function that
evaluates the embedded distances with respect to the qualitative clustering
queries. The key idea of our approach is to collect and embed qualitative
measures that share the same aspects in bundles. To ensure similarity aspect
sharing among multiple measures, image classification queries are presented to,
and solved by users. The collected image clusters are then converted into
bundles of tuples, which are fed into our bundle optimization algorithm that
jointly infers the aspect similarity and multi-aspect embedding. Extensive
experimental results show that our approach significantly outperforms
state-of-the-art multi-embedding approaches on various datasets, and scales
well for large multi-aspect similarity measures.
",computer-science
"  Power flow in a low voltage direct current grid (LVDC) is a non-linear
problem just as its counterpart ac. This paper demonstrates that, unlike in ac
grids, convergence and uniqueness of the solution can be guaranteed in this
type of grids. The result is not a linearization nor an approximation, but an
analysis of the set of non-linear algebraic equations, which is valid for any
LVDC grid regardless its size, topology or load condition. Computer simulation
corroborate the theoretical analysis.
",mathematics
"  Conditional generators learn the data distribution for each class in a
multi-class scenario and generate samples for a specific class given the right
input from the latent space. In this work, a method known as ""Versatile
Auxiliary Classifier with Generative Adversarial Network"" for multi-class
scenarios is presented. In this technique, the Generative Adversarial Networks
(GAN)'s generator is turned into a conditional generator by placing a
multi-class classifier in parallel with the discriminator network and
backpropagate the classification error through the generator. This technique is
versatile enough to be applied to any GAN implementation. The results on two
databases and comparisons with other method are provided as well.
",statistics
"  Recently, wind Riemannian structures (WRS) have been introduced as a
generalization of Randers and Kropina metrics. They are constructed from the
natural data for Zermelo navigation problem, namely, a Riemannian metric $g_R$
and a vector field $W$ (the wind), where, now, the restriction of mild wind
$g_R(W,W)<1$ is dropped.
Here, the models of WRS spaceforms of constant flag curvature are determined.
Indeed, the celebrated classification of Randers metrics of constant flag
curvature by Bao, Robles and Shen, extended to the Kropina case in the works by
Yoshikawa, Okubo and Sabau, can be used to obtain the local classification. For
the global one, a suitable result on completeness for WRS yields the complete
simply connected models. In particular, any of the local models in the Randers
classification does admit an extension to a unique model of wind Riemannian
structure, even if it cannot be extended as a complete Finslerian manifold.
Thus, WRS's emerge as the natural framework for the analysis of Randers
spaceforms and, prospectively, wind Finslerian structures would become
important for other global problems too. For the sake of completeness, a brief
overview about WRS (including a useful link with the conformal geometry of a
class of relativistic spacetimes) is also provided.
",mathematics
"  This paper is a continuation of the second author's previous work. We
investigate the isoperimetric problem in the 2-dimensional Finsler space form
$(F_B, B^2(1))$ with $k=0$ by using the Holmes-Thompson area and prove that the
circle centered the origin achieves the local maximum area of the isoperimetric
problem.
",mathematics
"  Differentiable systems in this paper means systems of equations that are
described by differentiable real functions in real matrix variables. This paper
proposes algorithms for finding minimal rank solutions to such systems over
(arbitrary and/or several structured) matrices by using the Levenberg-Marquardt
method (LM-method) for solving least squares problems. We then apply these
algorithms to solve several engineering problems such as the low-rank matrix
completion problem and the low-dimensional Euclidean embedding one. Some
numerical experiments illustrate the validity of the approach.
On the other hand, we provide some further properties of low rank solutions
to systems linear matrix equations. This is useful when the differentiable
function is linear or quadratic.
",mathematics
"  A detailed characterization of the particle induced background is fundamental
for many of the scientific objectives of the Athena X-ray telescope, thus an
adequate knowledge of the background that will be encountered by Athena is
desirable. Current X-ray telescopes have shown that the intensity of the
particle induced background can be highly variable. Different regions of the
magnetosphere can have very different environmental conditions, which can, in
principle, differently affect the particle induced background detected by the
instruments. We present results concerning the influence of the magnetospheric
environment on the background detected by EPIC instrument onboard XMM-Newton
through the estimate of the variation of the in-Field-of-View background excess
along the XMM-Newton orbit. An important contribution to the XMM background,
which may affect the Athena background as well, comes from soft proton flares.
Along with the flaring component a low-intensity component is also present. We
find that both show modest variations in the different magnetozones and that
the soft proton component shows a strong trend with the distance from Earth.
",physics
"  We present bounds for the finite sample error of sequential Monte Carlo
samplers on static spaces. Our approach explicitly relates the performance of
the algorithm to properties of the chosen sequence of distributions and mixing
properties of the associated Markov kernels. This allows us to give the first
finite sample comparison to other Monte Carlo schemes. We obtain bounds for the
complexity of sequential Monte Carlo approximations for a variety of target
distributions including finite spaces, product measures, and log-concave
distributions including Bayesian logistic regression. The bounds obtained are
within a logarithmic factor of similar bounds obtainable for Markov chain Monte
Carlo.
",statistics
"  We suggest a model of a multi-agent society of decision makers taking
decisions being based on two criteria, one is the utility of the prospects and
the other is the attractiveness of the considered prospects. The model is the
generalization of quantum decision theory, developed earlier for single
decision makers realizing one-step decisions, in two principal aspects. First,
several decision makers are considered simultaneously, who interact with each
other through information exchange. Second, a multistep procedure is treated,
when the agents exchange information many times. Several decision makers
exchanging information and forming their judgement, using quantum rules, form a
kind of a quantum information network, where collective decisions develop in
time as a result of information exchange. In addition to characterizing
collective decisions that arise in human societies, such networks can describe
dynamical processes occurring in artificial quantum intelligence composed of
several parts or in a cluster of quantum computers. The practical usage of the
theory is illustrated on the dynamic disjunction effect for which three
quantitative predictions are made: (i) the probabilistic behavior of decision
makers at the initial stage of the process is described; (ii) the decrease of
the difference between the initial prospect probabilities and the related
utility factors is proved; (iii) the existence of a common consensus after
multiple exchange of information is predicted. The predicted numerical values
are in very good agreement with empirical data.
",computer-science
"  The big graph database model provides strong modeling for complex
applications and efficient querying. However, it is still a big challenge to
find all exact matches of a query graph in a big graph database, which is known
as the subgraph isomorphism problem. The current subgraph isomorphism
approaches are built on Ullmann's idea of focusing on the strategy of pruning
out the irrelevant candidates. Nevertheless, the existing pruning techniques
need much more improvement to efficiently handle complex queries. Moreover,
many of those existing algorithms need large indices requiring extra memory
consumption. Motivated by these, we introduce a new subgraph isomorphism
algorithm, named as BB-Graph, for querying big graph databases efficiently
without requiring a large data structure to be stored in main memory. We test
and compare our proposed BB-Graph algorithm with two popular existing
approaches, GraphQL and Cypher. Our experiments are done on three different
data sets; (1) a very big graph database of a real-life population database,
(2) a graph database of a simulated bank database, and (3) the publicly
available World Cup big graph database. We show that our solution performs
better than those algorithms mentioned here for most of the query types
experimented on these big databases.
",computer-science
"  Using state-of-the-art techniques combining imaging methods and
high-throughput genomic mapping tools leaded to the significant progress in
detailing chromosome architecture of various organisms. However, a gap still
remains between the rapidly growing structural data on the chromosome folding
and the large-scale genome organization. Could a part of information on the
chromosome folding be obtained directly from underlying genomic DNA sequences
abundantly stored in the databanks? To answer this question, we developed an
original discrete double Fourier transform (DDFT). DDFT serves for the
detection of large-scale genome regularities associated with domains/units at
the different levels of hierarchical chromosome folding. The method is
versatile and can be applied to both genomic DNA sequences and corresponding
physico-chemical parameters such as base-pairing free energy. The latter
characteristic is closely related to the replication and transcription and can
also be used for the assessment of temperature or supercoiling effects on the
chromosome folding. We tested the method on the genome of Escherichia coli K-12
and found good correspondence with the annotated domains/units established
experimentally. As a brief illustration of further abilities of DDFT, the study
of large-scale genome organization for bacteriophage PHIX174 and bacterium
Caulobacter crescentus was also added. The combined experimental, modeling, and
bioinformatic DDFT analysis should yield more complete knowledge on the
chromosome architecture and genome organization.
",physics
"  This survey article is dedicated to some families of fractals that were
introduced and studied during the last decade, more precisely, families of
Sierpiński carpets: limit net sets, generalised Sierpiński carpets and
labyrinth fractals. We give a unifying approach of these fractals and several
of their topological and geometrical properties, by using the framework of
planar patterns.
",mathematics
"  We study the problem of policy evaluation and learning from batched
contextual bandit data when treatments are continuous, going beyond previous
work on discrete treatments. Previous work for discrete treatment/action spaces
focuses on inverse probability weighting (IPW) and doubly robust (DR) methods
that use a rejection sampling approach for evaluation and the equivalent
weighted classification problem for learning. In the continuous setting, this
reduction fails as we would almost surely reject all observations. To tackle
the case of continuous treatments, we extend the IPW and DR approaches to the
continuous setting using a kernel function that leverages treatment proximity
to attenuate discrete rejection. Our policy estimator is consistent and we
characterize the optimal bandwidth. The resulting continuous policy optimizer
(CPO) approach using our estimator achieves convergent regret and approaches
the best-in-class policy for learnable policy classes. We demonstrate that the
estimator performs well and, in particular, outperforms a discretization-based
benchmark. We further study the performance of our policy optimizer in a case
study on personalized dosing based on a dataset of Warfarin patients, their
covariates, and final therapeutic doses. Our learned policy outperforms
benchmarks and nears the oracle-best linear policy.
",statistics
"  In this paper, we prove the pointwise convergence and the rate of pointwise
convergence for a family of singular integral operators in two-dimensional
setting in the following form: \begin{equation*} L_{\lambda }\left(
f;x,y\right) =\underset{D}{\iint }f\left( t,s\right) K_{\lambda }\left(
t-x,s-y\right) dsdt,\text{ }\left( x,y\right) \in D, \end{equation*} where
$D=\left \langle a,b\right \rangle \times \left \langle c,d\right \rangle $ is
an arbitrary closed, semi-closed or open rectangle in $\mathbb{R}^{2}$ and $%
\lambda \in \Lambda ,$ $\Lambda $ is a set of non-negative indices with
accumulation point $\lambda_{0}$. Also, we provide an example to support these
theoretical results. In contrast to previous works, the kernel function
$K_{\lambda }\left( t,s\right) $ does not have to be even, positive or 2$\pi
-$periodic.
",mathematics
"  We propose a novel Metropolis-Hastings algorithm to sample uniformly from the
space of correlation matrices. Existing methods in the literature are based on
elaborated representations of a correlation matrix, or on complex
parametrizations of it. By contrast, our method is intuitive and simple, based
the classical Cholesky factorization of a positive definite matrix and Markov
chain Monte Carlo theory. We perform a detailed convergence analysis of the
resulting Markov chain, and show how it benefits from fast convergence, both
theoretically and empirically. Furthermore, in numerical experiments our
algorithm is shown to be significantly faster than the current alternative
approaches, thanks to its simple yet principled approach.
",statistics
"  Glass corrosion is a crucial problem in keeping and conservation of beadworks
in museums. All kinds of glass beads undergo deterioration but blue-green
lead-potassium glass beads of the 19th century are subjected to the destruction
to the greatest extent. Blue-green lead-potassium glass beads of the 19th
century obtained from exhibits kept in Russian museums were studied with the
purpose to determine the causes of the observed phenomenon. For the comparison,
yellow lead beads of the 19th century were also explored. Both kinds of beads
contain Sb but yellow ones are stable. Using scanning electron microscopy,
energy dispersive X-ray microspectrometry, electron backscatter diffraction,
transmission electron microscopy and X-ray powder analysis, we have registered
the presence of crystallites of orthorhombic KSbOSiO$_4$ and cubic
Pb$_2$Sb$_{1.5}$Fe$_{0.5}$O$_{6.5}$ in glass matrix of blue-green and yellow
beads, respectively. Both compounds form at rather high temperatures obviously
during glass melting and/or melt cooling. We suppose that the crystallites
generate internal tensile strain in glass during its cooling which causes
formation of multiple microcracks in inner domains of blue-green beads. We
suggest that the deterioration degree depends on quantity of the precipitates,
their sizes and their temperature coefficients of linear expansion. In
blue-green beads, the crystallites are distributed in their sizes from
$\sim\,$200 nm to several tens of $\mu$m and tend to gather in large colonies.
The sizes of crystallites in yellow beads are several hundreds of nm and their
clusters contain few crystallites. This explains the difference in corrosion of
these kinds of beads containing crystals of Sb compounds.
",physics
"  The complex Lie superalgebras $\mathfrak{g}$ of type $D(2,1;a)$ - also
denoted by $\mathfrak{osp}(4,2;a) $ - are usually considered for ""non-singular""
values of the parameter $a$, for which they are simple. In this paper we
introduce five suitable integral forms of $\mathfrak{g}$, that are well-defined
at singular values too, giving rise to ""singular specializations"" that are no
longer simple: this extends the family of simple objects of type $D(2,1;a)$ in
five different ways. The resulting five families coincide for general values of
$a$, but are different at ""singular"" ones: here they provide non-simple Lie
superalgebras, whose structure we describe explicitly. We also perform the
parallel construction for complex Lie supergroups and describe their singular
specializations (or ""degenerations"") at singular values of $a$. Although one
may work with a single complex parameter $a$, in order to stress the overall
$\mathfrak{S}_3$-symmetry of the whole situation, we shall work (following
Kaplansky) with a two-dimensional parameter $\boldsymbol{\sigma} =
(\sigma_1,\sigma_2,\sigma_3)$ ranging in the complex affine plane $\sigma_1 +
\sigma_2 + \sigma_3 = 0$.
",mathematics
"  This volume contains the proceedings of F-IDE 2016, the third international
workshop on Formal Integrated Development Environment, which was held as an FM
2016 satellite event, on November 8, 2016, in Limassol (Cyprus). High levels of
safety, security and also privacy standards require the use of formal methods
to specify and develop compliant software (sub)systems. Any standard comes with
an assessment process, which requires a complete documentation of the
application in order to ease the justification of design choices and the review
of code and proofs. Thus tools are needed for handling specifications, program
constructs and verification artifacts. The aim of the F-IDE workshop is to
provide a forum for presenting and discussing research efforts as well as
experience returns on design, development and usage of formal IDE aiming at
making formal methods ""easier"" for both specialists and non-specialists.
",computer-science
"  This paper introduces pyRecLab, a software library written in C++ with Python
bindings which allows to quickly train, test and develop recommender systems.
Although there are several software libraries for this purpose, only a few let
developers to get quickly started with the most traditional methods, permitting
them to try different parameters and approach several tasks without a
significant loss of performance. Among the few libraries that have all these
features, they are available in languages such as Java, Scala or C#, what is a
disadvantage for less experienced programmers more used to the popular Python
programming language. In this article we introduce details of pyRecLab, showing
as well performance analysis in terms of error metrics (MAE and RMSE) and
train/test time. We benchmark it against the popular Java-based library LibRec,
showing similar results. We expect programmers with little experience and
people interested in quickly prototyping recommender systems to be benefited
from pyRecLab.
",computer-science
"  In this paper, metric reduction in generalized geometry is investigated. We
show how the Bismut connections on the quotient manifold are obtained from
those on the original manifold. The result facilitates the analysis of
generalized K$\ddot{a}$hler reduction, which motivates the concept of metric
generalized principal bundles and our approach to construct a family of
generalized holomorphic line bundles over $\mathbb{C}P^2$ equipped with some
non-trivial generalized K$\ddot{a}$hler structures.
",mathematics
"  Air-showers measured by the Pierre Auger Observatory were analyzed in order
to extract the depth of maximum (Xmax).The results allow the analysis of the
Xmax distributions as a function of energy ($> 10^{17.8}$ eV). The Xmax
distributions, their mean and standard deviation are analyzed with the help of
shower simulations with the aim of interpreting the mass composition. The mean
and standard deviation were used to derive <ln A> and its variance as a
function of energy. The fraction of four components (p, He, N and Fe) were fit
to the Xmax distributions. Regardless of the hadronic model used the data is
better described by a mix of light, intermediate and heavy primaries. Also,
independent of the hadronic models, a decrease of the proton flux with energy
is observed. No significant contribution of iron nuclei is derived in the
entire energy range studied.
",physics
"  Collaborations are an integral part of scientific research and publishing. In
the past, access to large-scale corpora has limited the ways in which questions
about collaborations could be investigated. However, with improvements in
data/metadata quality and access, it is possible to explore the idea of
research collaboration in ways beyond the traditional definition of multiple
authorship. In this paper, we examine scientific works through three different
lenses of collaboration: across multiple authors, multiple institutions, and
multiple departments. We believe this to be a first look at multiple
departmental collaborations as we employ extensive data curation to
disambiguate authors' departmental affiliations for nearly 70,000 scientific
papers. We then compare citation metrics across the different definitions of
collaboration and find that papers defined as being collaborative were more
frequently cited than their non-collaborative counterparts, regardless of the
definition of collaboration used. We also share preliminary results from
examining the relationship between co-citation and co-authorship by analyzing
the extent to which similar fields (as determined by co-citation) are
collaborating on works (as determined by co-authorship). These preliminary
results reveal trends of compartmentalization with respect to
intra-institutional collaboration and show promise in being expanded.
",computer-science
"  The graphene/MoS2 heterojunction formed by joining the two components
laterally in a single plane promises to exhibit a low-resistance contact
according to the Schottky-Mott rule. Here we provide an atomic-scale
description of the structural, electronic, and magnetic properties of this type
of junction. We first identify the energetically favorable structures in which
the preference of forming C-S or C-Mo bonds at the boundary depends on the
chemical conditions. We find that significant charge transfer between graphene
and MoS2 is localized at the boundary. We show that the abundant 1D boundary
states substantially pin the Fermi level in the lateral contact between
graphene and MoS2, in close analogy to the effect of 2D interfacial states in
the contacts between 3D materials. Furthermore, we propose specific ways in
which these effects can be exploited to achieve spin-polarized currents.
",physics
"  Consider a set of categorical variables $\mathcal{P}$ where at least one,
denoted by $Y$, is binary. The log-linear model that describes the counts in
the resulting contingency table implies a specific logistic regression model,
with the binary variable as the outcome. Extending results in Christensen
(1997), by also considering the case where factors present in the contingency
table disappear from the logistic regression model, we prove that the Maximum
Likelihood Estimate (MLE) for the parameters of the logistic regression equals
the MLE for the corresponding parameters of the log-linear model. We prove
that, asymptotically, standard errors for the two sets of parameters are also
equal. Subsequently, Wald confidence intervals are asymptotically equal. These
results demonstrate the extent to which inferences from the log-linear
framework can be translated to inferences within the logistic regression
framework, on the magnitude of main effects and interactions. Finally, we prove
that the deviance of the log-linear model is equal to the deviance of the
corresponding logistic regression, provided that the latter is fitted to a
dataset where no cell observations are merged when one or more factors in
$\mathcal{P} \setminus \{ Y \}$ become obsolete. We illustrate the derived
results with the analysis of a real dataset.
",statistics
"  In 1998, R. Gompf defined a homotopy invariant $\theta_G$ of oriented 2-plane
fields in 3-manifolds. This invariant is defined for oriented 2-plane fields
$\xi$ in a closed oriented 3-manifold $M$ when the first Chern class $c_1(\xi)$
is a torsion element of $H^2(M;\mathbb{Z})$. In this article, we define an
extension of the Gompf invariant for all compact oriented 3-manifolds with
boundary and we study its iterated variations under Lagrangian-preserving
surgeries. It follows that the extended Gompf invariant is a degree two
invariant with respect to a suitable finite type invariant theory.
",mathematics
"  Ordinary differential operators with periodic coefficients analytic in a
strip act on a Hardy-Hilbert space of analytic functions with inner product
defined by integration over a period on the boundary of the strip. Simple
examples show that eigenfunctions may form a complete set for a narrow strip,
but completeness may be lost for a wide strip. Completeness of the
eigenfunctions in the Hardy-Hilbert space is established for regular second
order operators with matrix-valued coefficients when the leading coefficient
satisfies a positive real part condition throughout the strip.
",mathematics
"  Controlling nanocircuits at the single electron spin level is a possible
route for large-scale quantum information processing. In this context,
individual electron spins have been identified as versatile quantum information
carriers to interconnect different nodes of a spin-based semiconductor quantum
circuit. Despite important experimental efforts to control the electron
displacement over long distances, keeping the electron spin coherence after
transfer remained up to now elusive. Here we demonstrate that individual
electron spins can be displaced coherently over a distance of 5 micrometers.
This displacement is realized on a closed path made of three tunnel-coupled
lateral quantum dots. Using fast quantum dot control, the electrons tunnel from
one dot to another at a speed approaching 100 m/s. We find that the spin
coherence length is 8 times longer than expected from the electron spin
coherence without displacement. Such an enhanced spin coherence points at a
process similar to motional narrowing observed in nuclear magnetic resonance
experiments6. The demonstrated coherent displacement will enable long-range
interaction between distant spin-qubits and will open the route towards
non-abelian and holonomic manipulation of a single electron spin.
",physics
"  The Greek aperitif Ouzo is not only famous for its specific anise-flavored
taste, but also for its ability to turn from a transparent miscible liquid to a
milky-white colored emulsion when water is added. Recently, it has been shown
that this so-called Ouzo effect, i.e. the spontaneous emulsification of oil
microdroplets, can also be triggered by the preferential evaporation of ethanol
in an evaporating sessile Ouzo drop, leading to an amazingly rich drying
process with multiple phase transitions [H. Tan et al., Proc. Natl. Acad. Sci.
USA 113(31) (2016) 8642]. Due to the enhanced evaporation near the contact
line, the nucleation of oil droplets starts at the rim which results in an oil
ring encircling the drop. Furthermore, the oil droplets are advected through
the Ouzo drop by a fast solutal Marangoni flow. In this article, we investigate
the evaporation of mixture droplets in more detail, by successively increasing
the mixture complexity from pure water over a binary water-ethanol mixture to
the ternary Ouzo mixture (water, ethanol and anise oil). In particular,
axisymmetric and full three-dimensional finite element method simulations have
been performed on these droplets to discuss thermal effects and the complicated
flow in the droplet driven by an interplay of preferential evaporation,
evaporative cooling and solutal and thermal Marangoni flow. By using image
analysis techniques and micro-PIV measurements, we are able to compare the
numerically predicted volume evolutions and velocity fields with experimental
data. The Ouzo droplet is furthermore investigated by confocal microscopy. It
is shown that the oil ring predominantly emerges due to coalescence.
",physics
"  Distributed Generation (DG) units are increasingly installed in the power
systems. Distribution Companies (DisCo) can opt to purchase the electricity
from DG in an energy purchase contract to supply the customer demand and reduce
energy loss. This paper proposes a framework for optimal contract pricing of
independent dispatchable DG units considering competition among them. While DG
units tend to increase their profit from the energy purchase contract, DisCo
minimizes the demand supply cost. Multi-leader follower game theory concept is
used to analyze the situation in which competing DG units offer the energy
price to DisCo and DisCo determines the DG generation. A bi-level approach is
used to formulate the competition in which each DG problem is the upper-level
problem and the DisCo problem is considered as the lower-level one. Combining
the optimality conditions ofall upper-level problems with the lower level
problem results in a multi-DG equilibrium problem formulated as an equilibrium
problem with equilibrium constraints (EPEC). Using a nonlinear approach, the
EPEC problem is reformulated as a single nonlinear optimization model which is
simultaneously solved for all independent DG units. The proposed framework was
applied to the Modified IEEE 34-Bus Distribution Test System. Performance and
robustness of the proposed framework in determining econo-technically fare DG
contract price has been demonstrated through a series of analyses.
",computer-science
"  Detecting activities in untrimmed videos is an important but challenging
task. The performance of existing methods remains unsatisfactory, e.g., they
often meet difficulties in locating the beginning and end of a long complex
action. In this paper, we propose a generic framework that can accurately
detect a wide variety of activities from untrimmed videos. Our first
contribution is a novel proposal scheme that can efficiently generate
candidates with accurate temporal boundaries. The other contribution is a
cascaded classification pipeline that explicitly distinguishes between
relevance and completeness of a candidate instance. On two challenging temporal
activity detection datasets, THUMOS14 and ActivityNet, the proposed framework
significantly outperforms the existing state-of-the-art methods, demonstrating
superior accuracy and strong adaptivity in handling activities with various
temporal structures.
",computer-science
"  This paper presents our work on developing parallel computational methods for
two-phase flow on modern parallel computers, where techniques for linear
solvers and nonlinear methods are studied and the standard and inexact Newton
methods are investigated. A multi-stage preconditioner for two-phase flow is
applied and advanced matrix processing strategies are studied. A local
reordering method is developed to speed the solution of linear systems.
Numerical experiments show that these computational methods are effective and
scalable, and are capable of computing large-scale reservoir simulation
problems using thousands of CPU cores on parallel computers. The nonlinear
techniques, preconditioner and matrix processing strategies can also be applied
to three-phase black oil, compositional and thermal models.
",computer-science
"  We demonstrate an InAlN/GaN-on-Si HEMT based UV detector with photo to dark
current ratio > 107. Ti/Al/Ni/Au metal stack was evaporated and rapid thermal
annealed for Ohmic contacts to the 2D electron gas (2DEG) at the InAlN/GaN
interface while the channel + barrier was recess etched to a depth of 20 nm to
pinch-off the 2DEG between Source-Drain pads. Spectral responsivity (SR) of 34
A/W at 367 nm was measured at 5 V in conjunction with very high photo to dark
current ratio of > 10^7. The photo to dark current ratio at a fixed bias was
found to be decreasing with increase in recess length of the PD. The fabricated
devices were found to exhibit a UV-to-visible rejection ratio of >103 with a
low dark current < 32 pA at 5 V. Transient measurements showed rise and fall
times in the range of 3-4 ms. The gain mechanism was investigated and carrier
lifetimes were estimated which matched well with those reported elsewhere.
",physics
"  In a scalar reaction-diffusion equation, it is known that the stability of a
steady state can be determined from the Maslov index, a topological invariant
that counts the state's critical points. In particular, this implies that pulse
solutions are unstable. We extend this picture to pulses in reaction-diffusion
systems with gradient nonlinearity. In particular, we associate a Maslov index
to any asymptotically constant state, generalizing existing definitions of the
Maslov index for homoclinic orbits. It is shown that this index equals the
number of unstable eigenvalues for the linearized evolution equation. Finally,
we use a symmetry argument to show that any pulse solution must have nonzero
Maslov index, and hence be unstable.
",mathematics
"  Cosmologies including strongly Coupled (SC) Dark Energy (DE) and Warm dark
matter (SCDEW) are based on a conformally invariant (CI) attractor solution
modifying the early radiative expansion. Then, aside of radiation, a kinetic
field $\Phi$ and a DM component account for a stationary fraction, $\sim 1\,
\%$, of the total energy. Most SCDEW predictions are hardly distinguishable
from LCDM, while SCDEW alleviates quite a few LCDM conceptual problems, as well
as its difficulties to meet data below the average galaxy scale. The CI
expansion begins at the inflation end, when $\Phi$ (future DE) possibly plays a
role in reheating, and ends at the Higgs' scale. Afterwards, a number of viable
options is open, allowing for the transition from the CI expansion to the
present Universe. In this paper: (i) We show how the attractor is recovered
when the spin degrees of freedom decreases. (ii) We perform a detailed
comparison of CMB anisotropy and polarization spectra for SCDEW and LCDM,
including tensor components, finding negligible discrepancies. (iii) Linear
spectra exhibit a greater parameter dependence at large $k$'s, but are still
consistent with data for suitable parameter choices. (iv) We also compare
previous simulation results with fresh data on galaxy concentration. Finally,
(v) we outline numerical difficulties at high $k$. This motivates a second
related paper, where such problems are treated in a quantitative way.
",physics
"  Multi robot systems have the potential to be utilized in a variety of
applications. In most of the previous works, the trajectory generation for
multi robot systems is implemented in known environments. To overcome that we
present an online trajectory optimization algorithm that utilizes communication
of robots' current states to account to the other robots while using local
object based maps for identifying obstacles. Based upon this data, we predict
the trajectory expected to be traversed by the robots and utilize that to avoid
collisions by formulating regions of free space that the robot can be without
colliding with other robots and obstacles. A trajectory is optimized
constraining the robot to remain within this region.The proposed method is
tested in simulations on Gazebo using ROS.
",computer-science
"  We give a survey on some results covering the last 60 years concerning
Jeśmanowicz' conjecture. Moreover, we conclude the survey with a new result
by showing that the special Diophantine equation $$(20k)^x+(99k)^y=(101k)^z$$
has no solution other than $(x,y,z)=(2,2,2)$.
",mathematics
"  In this paper, an enthalpy-based multiple-relaxation-time (MRT) lattice
Boltzmann (LB) method is developed for solid-liquid phase change heat transfer
in metal foams under local thermal non-equilibrium (LTNE) condition. The
enthalpy-based MRT-LB method consists of three different MRT-LB models: one for
flow field based on the generalized non-Darcy model, and the other two for
phase change material (PCM) and metal foam temperature fields described by the
LTNE model. The moving solid-liquid phase interface is implicitly tracked
through the liquid fraction, which is simultaneously obtained when the energy
equations of PCM and metal foam are solved. The present method has several
distinctive features. First, as compared with previous studies, the present
method avoids the iteration procedure, thus it retains the inherent merits of
the standard LB method and is superior over the iteration method in terms of
accuracy and computational efficiency. Second, a volumetric LB scheme instead
of the bounce-back scheme is employed to realize the no-slip velocity condition
in the interface and solid phase regions, which is consistent with the actual
situation. Last but not least, the MRT collision model is employed, and with
additional degrees of freedom, it has the ability to reduce the numerical
diffusion across phase interface induced by solid-liquid phase change.
Numerical tests demonstrate that the present method can be served as an
accurate and efficient numerical tool for studying metal foam enhanced
solid-liquid phase change heat transfer in latent heat storage. Finally,
comparisons and discussions are made to offer useful information for practical
applications of the present method.
",physics
"  Keywords are important for information retrieval. They are used to classify
and sort papers. However, these terms can also be used to study trends within
and across fields. We want to explore the lifecycle of new keywords. How often
do new terms come into existence and how long till they fade out? In this
paper, we present our preliminary analysis where we measure the burstiness of
keywords within the field of AI. We examine 150k keywords in approximately 100k
journal and conference papers. We find that nearly 80\% of the keywords die off
before year one for both journals and conferences but that terms last longer in
journals versus conferences. We also observe time periods of thematic bursts in
AI -- one where the terms are more neuroscience inspired and one more oriented
to computational optimization. This work shows promise of using author keywords
to better understand dynamics of buzz within science.
",computer-science
"  The paper presents a solution to the Boltzmann kinetic equation based on the
construction of its discrete conservative model. Discrete analogue of the
collision integral is presented as a contraction of a tensor, which is
independent from the initial distribution function, colliding with a tensor
composed of medium densities in the cells. Numerical implementation of the
discrete model is demonstrated on the example of the isotropic gas relaxation
problem applied to the hard spheres model. The key feature of the method is
independence of the collision tensor components from the distribution function.
Consequently the components of the collision tensor are calculated once for
various initial distribution functions, which substantially increases
performance of the suggested method.
",physics
"  We study a photonic analog of the chiral magnetic (vortical) effect. We
discuss that the vector component of magnetoelectric tensors plays a role of
""vector potential,"" and its rotation is understood as ""magnetic field"" of a
light. Using the geometrical optics approximation, we show that ""magnetic
fields"" cause an anomalous shift of a wave packet of a light through an
interplay with the Berry curvature of photons. The mechanism is the same as
that of the chiral magnetic (vortical) effect of a chiral fermion, so that we
term the anomalous shift ""chiral magnetic effect of a light."" We further study
the chiral magnetic effect of a light beyond geometric optics by directly
solving the transmission problem of a wave packet at a surface of a
magnetoelectric material. We show that the experimental signal of the chiral
magnetic effect of a light is the nonvanishing of transverse displacements for
the beam normally incident to a magnetoelectric material.
",physics
"  In this paper, we study Prandtl's boundary layer asymptotic expansion for
incompressible fluids on the half-space in the inviscid limit. In \cite{Gr1},
E. Grenier proved that Prandtl's Ansatz is false for data with Sobolev
regularity near Rayleigh's unstable shear flows. In this paper, we show that
this Ansatz is also false for Rayleigh's stable shear flows. Namely we
construct unstable solutions near arbitrary stable monotonic boundary layer
profiles. Such shear flows are stable for Euler equations, but not for
Navier-Stokes equations: adding a small viscosity destabilizes the flow.
",mathematics
"  We conducted a search for an exotic spin- and velocity-dependent interaction
for polarized electrons with an experimental approach based on a
high-sensitivity spin-exchange relaxation-free (SERF) magnetometer, which
serves as both a source of polarized electrons and a magnetic-field sensor. The
experiment aims to sensitively detect magnetic-fieldlike effects from the
exotic interaction between the polarized electrons in a SERF vapor cell and
unpolarized nucleons of a closely located solid-state mass. We report
experimental results on the interaction with 82 h of data averaging, which sets
an experimental limit on the coupling strength around $10^{-19}$ for the axion
mass $m_a \lesssim 10^{-3}$ eV, within the important axion window.
",physics
"  Quantitative nuclear magnetic resonance imaging (MRI) shifts more and more
into the focus of clinical research. Especially determination of relaxation
times without/and with contrast agents becomes the foundation of tissue
characterization, e.g. in cardiac MRI for myocardial fibrosis. Techniques which
assess longitudinal relaxation times rely on repetitive application of readout
modules, which are interrupted by free relaxation periods, e.g. the Modified
Look-Locker Inversion Recovery = MOLLI sequence. These discontinuous sequences
reveal an apparent relaxation time, and, by techniques extrapolated from
continuous readout sequences, the real T1 is determined. What is missing is a
rigorous analysis of the dependence of the apparent relaxation time on its real
partner, readout sequence parameters and biological parameters as heart rate.
This is provided in this paper for the discontinuous balanced steady state free
precession (bSSFP) and spoiled gradient echo readouts. It turns out that the
apparente longitudinal relaxation rate is the time average of the relaxation
rates during the readout module, and free relaxation period. Knowing the heart
rate our results vice versa allow to determine the real T1 from its measured
apparent partner.
",physics
"  In this paper we investigate the convection phenomenon in the intracluster
medium (the weakly-collisional magnetized inhomogeneous plasma permeating
galaxy clusters) where the concentration gradient of the Helium ions is not
ignorable. To this end, we build upon the general machinery employed to study
the salt finger instability found in the oceans. The salt finger instability is
a form of double diffusive convection where the diffusions of two physical
quantities---heat and salt concentrations---occur with different diffusion
rates. The analogous instability in the intracluster medium may result owing to
the magnetic field mediated anisotropic diffusions of the heat and the Helium
ions (in the sea of the Hydrogen ions and the free electrons). These two
diffusions have inherently different diffusion rates. Hence the convection
caused by the onset of this instability is an example of double diffusive
convection in the astrophysical settings. A consequence of this instability is
the formation of the vertical filamentary structures having more concentration
of the Helium ions with respect to the immediate neighbourhoods of the
filaments. We term these structures as Helium fingers in analogy with the salt
fingers found in the case of the salt finger instability. Here we show that the
width of a Helium finger scales as one-fourth power of the radius of the inner
region of the intracluster medium in the supercritical regime. We also
determine the explicit mathematical expression of the criterion for the onset
of the heat-flux-driven buoyancy instability modified by the presence of
inhomogeneously distributed Helium ions.
",physics
"  We study the hyperplane arrangements associated, via the minimal model
programme, to symplectic quotient singularities. We show that this hyperplane
arrangement equals the arrangement of CM-hyperplanes coming from the
representation theory of restricted rational Cherednik algebras. We explain
some of the interesting consequences of this identification for the
representation theory of restricted rational Cherednik algebras. We also show
that the Calogero-Moser space is smooth if and only if the Calogero-Moser
families are trivial. We describe the arrangements of CM-hyperplanes associated
to several exceptional complex reflection groups, some of which are free.
",mathematics
"  The high-performance computing resources and the constant improvement of both
numerical simulation accuracy and the experimental measurements with which they
are confronted, bring a new compulsory step to strengthen the credence given to
the simulation results: uncertainty quantification. This can have different
meanings, according to the requested goals (rank uncertainty sources, reduce
them, estimate precisely a critical threshold or an optimal working point) and
it could request mathematical methods with greater or lesser complexity. This
paper introduces the Uranie platform, an Open-source framework which is
currently developed at the Alternative Energies and Atomic Energy Commission
(CEA), in the nuclear energy division, in order to deal with uncertainty
propagation, surrogate models, optimisation issues, code calibration... This
platform benefits from both its dependencies, but also from personal
developments, to offer an efficient data handling model, a C++ and Python
interpreter, advanced graphical tools, several parallelisation solutions...
These methods are very generic and can then be applied to many kinds of code
(as Uranie considers them as black boxes) so to many fields of physics as well.
In this paper, the example of thermal exchange between a plate-sheet and a
fluid is introduced to show how Uranie can be used to perform a large range of
analysis. The code used to produce the figures of this paper can be found in
this https URL along with the sources of the
platform.
",statistics
"  Using the method of Elias-Hogancamp and combinatorics of toric braids we give
an explicit formula for the triply graded Khovanov-Rozansky homology of an
arbitrary torus knot, thereby proving some of the conjectures of
Aganagic-Shakirov, Cherednik, Gorsky-Negut and Oblomkov-Rasmussen-Shende.
",mathematics
"  In this paper, we first propose two types of concepts of almost automorphic
functions on the quantum time scale. Secondly, we study some basic properties
of almost automorphic functions on the quantum time scale. Then, we introduce a
transformation between functions defined on the quantum time scale and
functions defined on the set of generalized integer numbers, by using this
transformation we give equivalent definitions of almost automorphic functions
on the quantum time scale. Finally, as an application of our results, we
establish the existence of almost automorphic solutions of linear and
semilinear dynamic equations on the quantum time scale.
",mathematics
"  We study the classification problems over string data for hypotheses
specified by formulas of monadic second-order logic MSO. The goal is to design
learning algorithms that run in time polynomial in the size of the training
set, independently of or at least sublinear in the size of the whole data set.
We prove negative as well as positive results. If the data set is an
unprocessed string to which our algorithms have local access, then learning in
sublinear time is impossible even for hypotheses definable in a small fragment
of first-order logic. If we allow for a linear time pre-processing of the
string data to build an index data structure, then learning of MSO-definable
hypotheses is possible in time polynomial in the size of the training set,
independently of the size of the whole data set.
",computer-science
"  A new characterization of CMO(R^n) is established by the local mean
oscillation. Some characterizations of iterated compact commutators on weighted
Lebesgue spaces are given, which are new even in the unweighted setting for the
first order commutators.
",mathematics
"  An Electronic Health Record (EHR) is designed to store diverse data
accurately from a range of health care providers and to capture the status of a
patient by a range of health care providers across time. Realising the numerous
benefits of the system, EHR adoption is growing globally and many countries
invest heavily in electronic health systems. In Australia, the Government
invested $467 million to build key components of the Personally Controlled
Electronic Health Record (PCEHR) system in July 2012. However, in the last
three years, the uptake from individuals and health care providers has not been
satisfactory. Unauthorised access of the PCEHR was one of the major barriers.
We propose an improved access control model for the PCEHR system to resolve the
unauthorised access issue. We discuss the unauthorised access issue with real
examples and present a potential solution to overcome the issue to make the
PCEHR system a success in Australia.
",computer-science
"  This paper describes our approach for the triple scoring task at the WSDM Cup
2017. The task required participants to assign a relevance score for each pair
of entities and their types in a knowledge base in order to enhance the ranking
results in entity retrieval tasks. We propose an approach wherein the outputs
of multiple neural network classifiers are combined using a supervised machine
learning model. The experimental results showed that our proposed method
achieved the best performance in one out of three measures (i.e., Kendall's
tau), and performed competitively in the other two measures (i.e., accuracy and
average score difference).
",computer-science
"  We consider multivariate $\mathbb{L}_2$-approximation in reproducing kernel
Hilbert spaces which are tensor products of weighted Walsh spaces and weighted
Korobov spaces. We study the minimal worst-case error
$e^{\mathbb{L}_2-\mathrm{app},\Lambda}(N,d)$ of all algorithms that use $N$
information evaluations from the class $\Lambda$ in the $d$-dimensional case.
The two classes $\Lambda$ considered in this paper are the class $\Lambda^{\rm
all}$ consisting of all linear functionals and the class $\Lambda^{\rm std}$
consisting only of function evaluations.
The focus lies on the dependence of
$e^{\mathbb{L}_2-\mathrm{app},\Lambda}(N,d)$ on the dimension $d$. The main
results are conditions for weak, polynomial, and strong polynomial
tractability.
",mathematics
"  Pre-training of models in pruning algorithms plays an important role in
pruning decision-making. We find that excessive pre-training is not necessary
for pruning algorithms. According to this idea, we propose a pruning
algorithm---Incremental pruning based on less training (IPLT). Compared with
the traditional pruning algorithm based on a large number of pre-training, IPLT
has competitive compression effect than the traditional pruning algorithm under
the same simple pruning strategy. On the premise of ensuring accuracy, IPLT can
achieve 8x-9x compression for VGG-19 on CIFAR-10 and only needs to pre-train
few epochs. For VGG-19 on CIFAR-10, we can not only achieve 10 times test
acceleration, but also about 10 times training acceleration. At present, the
research mainly focuses on the compression and acceleration in the application
stage of the model, while the compression and acceleration in the training
stage are few. We newly proposed a pruning algorithm that can compress and
accelerate in the training stage. It is novel to consider the amount of
pre-training required by pruning algorithm. Our results have implications: Too
much pre-training may be not necessary for pruning algorithms.
",computer-science
"  Multi-source transfer learning has been proven effective when within-target
labeled data is scarce. Previous work focuses primarily on exploiting domain
similarities and assumes that source domains are richly or at least comparably
labeled. While this strong assumption is never true in practice, this paper
relaxes it and addresses challenges related to sources with diverse labeling
volume and diverse reliability. The first challenge is combining domain
similarity and source reliability by proposing a new transfer learning method
that utilizes both source-target similarities and inter-source relationships.
The second challenge involves pool-based active learning where the oracle is
only available in source domains, resulting in an integrated active transfer
learning framework that incorporates distribution matching and uncertainty
sampling. Extensive experiments on synthetic and two real-world datasets
clearly demonstrate the superiority of our proposed methods over several
baselines including state-of-the-art transfer learning methods.
",statistics
"  Over the last decade, both the neural network and kernel adaptive filter have
successfully been used for nonlinear signal processing. However, they suffer
from high computational cost caused by their complex/growing network
structures. In this paper, we propose two random Euler filters for
complex-valued nonlinear filtering problem, i.e., linear random Euler
complex-valued filter (LRECF) and its widely-linear version (WLRECF), which
possess a simple and fixed network structure. The transient and steady-state
performances are studied in a non-stationary environment. The analytical
minimum mean square error (MSE) and optimum step-size are derived. Finally,
numerical simulations on complex-valued nonlinear system identification and
nonlinear channel equalization are presented to show the effectiveness of the
proposed methods.
",statistics
"  For well-generated complex reflection groups, Chapuy and Stump gave a simple
product for a generating function counting reflection factorizations of a
Coxeter element by their length. This is refined here to record the number of
reflections used from each orbit of hyperplanes. The proof is case-by-case via
the classification of well-generated groups. It implies a new expression for
the Coxeter number, expressed via data coming from a hyperplane orbit; a
case-free proof of this due to J. Michel is included.
",mathematics
"  During the last decade, the information technology industry has adopted a
data-driven culture, relying on online metrics to measure and monitor business
performance. Under the setting of big data, the majority of such metrics
approximately follow normal distributions, opening up potential opportunities
to model them directly without extra model assumptions and solve big data
problems via closed-form formulas using distributed algorithms at a fraction of
the cost of simulation-based procedures like bootstrap. However, certain
attributes of the metrics, such as their corresponding data generating
processes and aggregation levels, pose numerous challenges for constructing
trustworthy estimation and inference procedures. Motivated by four real-life
examples in metric development and analytics for large-scale A/B testing, we
provide a practical guide to applying the Delta method, one of the most
important tools from the classic statistics literature, to address the
aforementioned challenges. We emphasize the central role of the Delta method in
metric analytics by highlighting both its classic and novel applications.
",statistics
"  Given $n$ vectors $\mathbf{x}_i\in \mathbb{R}^d$, we want to fit a linear
regression model for noisy labels $y_i\in\mathbb{R}$. The ridge estimator is a
classical solution to this problem. However, when labels are expensive, we are
forced to select only a small subset of vectors $\mathbf{x}_i$ for which we
obtain the labels $y_i$. We propose a new procedure for selecting the subset of
vectors, such that the ridge estimator obtained from that subset offers strong
statistical guarantees in terms of the mean squared prediction error over the
entire dataset of $n$ labeled vectors. The number of labels needed is
proportional to the statistical dimension of the problem which is often much
smaller than $d$. Our method is an extension of a joint subsampling procedure
called volume sampling. A second major contribution is that we speed up volume
sampling so that it is essentially as efficient as leverage scores, which is
the main i.i.d. subsampling procedure for this task. Finally, we show
theoretically and experimentally that volume sampling has a clear advantage
over any i.i.d. sampling when labels are expensive.
",computer-science
"  Structural nested mean models (SNMMs) are among the fundamental tools for
inferring causal effects of time-dependent exposures from longitudinal studies.
With binary outcomes, however, current methods for estimating multiplicative
and additive SNMM parameters suffer from variation dependence between the
causal SNMM parameters and the non-causal nuisance parameters. Estimating
methods for logistic SNMMs do not suffer from this dependence. Unfortunately,
in contrast with the multiplicative and additive models, unbiased estimation of
the causal parameters of a logistic SNMM rely on additional modeling
assumptions even when the treatment probabilities are known. These difficulties
have hindered the uptake of SNMMs in epidemiological practice, where binary
outcomes are common. We solve the variation dependence problem for the binary
multiplicative SNMM by a reparametrization of the non-causal nuisance
parameters. Our novel nuisance parameters are variation independent of the
causal parameters, and hence allows the fitting of a multiplicative SNMM by
unconstrained maximum likelihood. It also allows one to construct true (i.e.
congenial) doubly robust estimators of the causal parameters. Along the way, we
prove that an additive SNMM with binary outcomes does not admit a variation
independent parametrization, thus explaining why we restrict ourselves to the
multiplicative SNMM.
",statistics
"  The electronic structure and energetic stability of A$_2$BX$_6$ halide
compounds with the cubic and tetragonal variants of the perovskite-derived
K$_2$PtCl$_6$ prototype structure are investigated computationally within the
frameworks of density-functional-theory (DFT) and hybrid (HSE06) functionals.
The HSE06 calculations are undertaken for seven known A$_2$BX$_6$ compounds
with A = K, Rb and Cs, and B = Sn, Pd, Pt, Te, and X = I. Trends in band gaps
and energetic stability are identified, which are explored further employing
DFT calculations over a larger range of chemistries, characterized by A = K,
Rb, Cs, B = Si, Ge, Sn, Pb, Ni, Pd, Pt, Se and Te and X = Cl, Br, I. For the
systems investigated in this work, the band gap increases from iodide to
bromide to chloride. Further, variations in the A site cation influences the
band gap as well as the preferred degree of tetragonal distortion. Smaller A
site cations such as K and Rb favor tetragonal structural distortions,
resulting in a slightly larger band gap. For variations in the B site in the
(Ni, Pd, Pt) group and the (Se, Te) group, the band gap increases with
increasing cation size. However, no observed chemical trend with respect to
cation size for band gap was found for the (Si, Sn, Ge, Pb) group. The findings
in this work provide guidelines for the design of halide A$_2$BX$_6$ compounds
for potential photovoltaic applications.
",physics
"  In this paper, energy efficient power allocation for downlink massive MIMO
systems is investigated. A constrained non-convex optimization problem is
formulated to maximize the energy efficiency (EE), which takes into account the
quality of service (QoS) requirements. By exploiting the properties of
fractional programming and the lower bound of the user data rate, the
non-convex optimization problem is transformed into a convex optimization
problem. The Lagrangian dual function method is utilized to convert the
constrained convex problem into an unconstrained convex one. Due to the
multi-variable coupling problem caused by the intra-user interference, it is
intractable to derive an explicit solution to the above optimization problem.
Exploiting the standard interference function, we propose an implicit iterative
algorithm to solve the unconstrained convex optimization problem and obtain the
optimal power allocation scheme. Simulation results show that the proposed
iterative algorithm converges in just a few iterations, and demonstrate the
impact of the number of users and the number of antennas on the EE.
",computer-science
"  In a recent work, the degenerate Stirling polynomials of the second kind were
studied by T. Kim. In this paper, we investigate the extended degenerate
Stirling numbers of the second kind and the extended degenerate Bell
polynomials associated with them. As results, we give some expressions,
identities and properties about the extended degener- ate Stirling numbers of
the second kind and the extended degenerate Bell polynomials.
",mathematics
"  We study, with the help of a computer program, the Polish Algorithm for
finite terms satisfying various algebraic laws, e.g., left distributivity a(bc)
= (ab)(ac). While the termination of the algorithm for left distributivity
remains open in general, we can establish some partial results, which might be
useful towards a positive solution. In contrast, we show the divergence of the
algorithm for the laws a(bc) = (ab)(cc) and a(bc) = (ab)(a(ac)).
",mathematics
"  For the quantification of QoE, subjects often provide individual rating
scores on certain rating scales which are then aggregated into Mean Opinion
Scores (MOS). From the observed sample data, the expected value is to be
estimated. While the sample average only provides a point estimator, confidence
intervals (CI) are an interval estimate which contains the desired expected
value with a given confidence level. In subjective studies, the number of
subjects performing the test is typically small, especially in lab
environments. The used rating scales are bounded and often discrete like the
5-point ACR rating scale. Therefore, we review statistical approaches in the
literature for their applicability in the QoE domain for MOS interval
estimation (instead of having only a point estimator, which is the MOS). We
provide a conservative estimator based on the SOS hypothesis and binomial
distributions and compare its performance (CI width, outlier ratio of CI
violating the rating scale bounds) and coverage probability with well known CI
estimators. We show that the provided CI estimator works very well in practice
for MOS interval estimators, while the commonly used studentized CIs suffer
from a positive outlier ratio, i.e., CIs beyond the bounds of the rating scale.
As an alternative, bootstrapping, i.e., random sampling of the subjective
ratings with replacement, is an efficient CI estimator leading to typically
smaller CIs, but lower coverage than the proposed estimator.
",computer-science
"  Unmanned Aerial Vehicles (UAVs) equipped with bioradars are a life-saving
technology that can enable identification of survivors under collapsed
buildings in the aftermath of natural disasters such as earthquakes or gas
explosions. However, these UAVs have to be able to autonomously land on debris
piles in order to accurately locate the survivors. This problem is extremely
challenging as the structure of these debris piles is often unknown and no
prior knowledge can be leveraged. In this work, we propose a computationally
efficient system that is able to reliably identify safe landing sites and
autonomously perform the landing maneuver. Specifically, our algorithm computes
costmaps based on several hazard factors including terrain flatness, steepness,
depth accuracy and energy consumption information. We first estimate dense
candidate landing sites from the resulting costmap and then employ clustering
to group neighboring sites into a safe landing region. Finally, a minimum-jerk
trajectory is computed for landing considering the surrounding obstacles and
the UAV dynamics. We demonstrate the efficacy of our system using experiments
from a city scale hyperrealistic simulation environment and in real-world
scenarios with collapsed buildings.
",computer-science
"  Human societies around the world interact with each other by developing and
maintaining social norms, and it is critically important to understand how such
norms emerge and change. In this work, we define an evolutionary game-theoretic
model to study how norms change in a society, based on the idea that different
strength of norms in societies translate to different game-theoretic
interaction structures and incentives. We use this model to study, both
analytically and with extensive agent-based simulations, the evolutionary
relationships of the need for coordination in a society (which is related to
its norm strength) with two key aspects of norm change: cultural inertia
(whether or how quickly the population responds when faced with conditions that
make a norm change desirable), and exploration rate (the willingness of agents
to try out new strategies). Our results show that a high need for coordination
leads to both high cultural inertia and a low exploration rate, while a low
need for coordination leads to low cultural inertia and high exploration rate.
This is the first work, to our knowledge, on understanding the evolutionary
causal relationships among these factors.
",computer-science
"  This article deals with a Markov process related to the fundamental solution
of a heat equation on the direct product ring Q_S, where Q_S is a finite direct
product of p-adic fields. The techniques developed here are different from the
well known ones: they are geometrical and very simple. As a result, the
techniques developed here provides a general framework of these problems on
other related ultrametric groups.
",mathematics
"  The Giornata Sesta about the Force of Percussion is a relatively less known
Chapter from the Galileo's masterpiece ""Discourse about Two New Sciences"". It
was first published lately (1718), long after the first edition of the Two New
Sciences (1638) and Galileo's death (1642). The Giornata Sesta focuses on how
to quantify the percussion force caused by a body in movement, and describes a
very interesting experiment known as ""the two-bucket experiment"". In this
paper, we review this experiment reported by Galileo, develop a steady-state
theoretical model, and solve its transient form numerically; additionally, we
report the results from one real simplified analogous experiment. Finally, we
discuss the conclusions drawn by Galileo -- correct, despite a probably
unnoticeable imbalance --, showing that he did not report the thrust force
component in his setup -- which would be fundamental for the correct
calculation of the percussion force.
",physics
"  We propose a novel approach to allocating resources for expensive simulations
of high fidelity models when used in a multifidelity framework. Allocation
decisions that distribute computational resources across several simulation
models become extremely important in situations where only a small number of
expensive high fidelity simulations can be run. We identify this allocation
decision as a problem in optimal subset selection, and subsequently regularize
this problem so that solutions can be computed. Our regularized formulation
yields a type of group lasso problem that has been studied in the literature to
accomplish subset selection. Our numerical results compare performance of
algorithms that solve the group lasso problem for algorithmic allocation
against a variety of other strategies, including those based on classical
linear algebraic pivoting routines and those derived from more modern machine
learning-based methods. We demonstrate on well known synthetic problems and
more difficult real-world simulations that this group lasso solution to the
relaxed optimal subset selection problem performs better than the alternatives.
",computer-science
"  For an effect algebra $A$, we examine the category of all morphisms from
finite Boolean algebras into $A$. This category can be described as a category
of elements of a presheaf $R(A)$ on the category of finite Boolean algebras. We
prove that some properties (being an orthoalgebra, the Riesz decomposition
property, being a Boolean algebra) of an effect algebra $A$ can be
characterized by properties of the category of elements of the presheaf $R(A)$.
We prove that the tensor product of of effect algebras arises as a left Kan
extension of the free product of finite Boolean algebras along the inclusion
functor. As a consequence, the tensor product of effect algebras can be
expressed by means of the Day convolution of presheaves on finite Boolean
algebras.
",mathematics
"  We conduct an in depth study on the performance of deep learning based radio
signal classification for radio communications signals. We consider a rigorous
baseline method using higher order moments and strong boosted gradient tree
classification and compare performance between the two approaches across a
range of configurations and channel impairments. We consider the effects of
carrier frequency offset, symbol rate, and multi-path fading in simulation and
conduct over-the-air measurement of radio classification performance in the lab
using software radios and compare performance and training strategies for both.
Finally we conclude with a discussion of remaining problems, and design
considerations for using such techniques.
",computer-science
"  The ""Planning in the Early Medieval Landscape"" project (PEML)
<this http URL>,
funded by the Leverhulme Trust, has organized and collated a substantial
quantity of images, and has used this as evidence to support the hypothesis
that Anglo-Saxon building construction was based on grid-like planning
structures based on fixed modules or quanta of measurement. We report on the
development of some statistical contributions to the debate concerning this
hypothesis. In practice the PEML images correspond to data arising in a wide
variety of different forms. It does not seem feasible to produce a single
automatic method which can be applied uniformly to all such images; even the
initial chore of cleaning up an image (removing extraneous material such as
legends and physical features which do not bear on the planning hypothesis)
typically presents a separate and demanding challenge for each different image.
Moreover care must be taken, even in the relatively straightforward cases of
clearly defined ground-plans (for example for large ecclesiastical buildings of
the period), to consider exactly what measurements might be relevant. We report
on pilot statistical analyses concerning three different situations. These
establish not only the presence of underlying structure (which indeed is often
visually obvious), but also provide an account of the numerical evidence
supporting the deduction that such structure is present. We contend that
statistical methodology thus contributes to the larger historical debate and
provides useful input to the wide and varied range of evidence that has to be
debated.
",statistics
"  In the quasi-1D heavy-fermion system YbNi$_4$(P$_{1-x}$As$_x$)$_2$ the
presence of a ferromagnetic (FM) quantum critical point (QCP) at $x_c$ $\approx
0.1$ with unconventional quantum critical exponents in the thermodynamic
properties has been recently reported. Here, we present muon-spin relaxation
($\mu$SR) experiments on polycrystals of this series to study the magnetic
order and the low energy 4$f$-electronic spin dynamics across the FM QCP. The
zero field $\mu$SR measurements on pure YbNi$_4$(P$_{2}$ proved static long
range magnetic order and suggested a strongly reduced ordered Yb moment of
about 0.04$\mu_B$. With increasing As substitution the ordered moment is
reduced by half at $x = 0.04$ and to less than 0.005 $\mu_B$ at $x=0.08$. The
dynamic behavior in the $\mu$SR response show that magnetism remains
homogeneous upon As substitution, without evidence for disorder effect. In the
paramagnetic state across the FM QCP the dynamic muon-spin relaxation rate
follows 1/$T_{1}T\propto T^{-n}$ with $1.01 \pm 0.04 \leq n \leq 1.13 \pm
0.06$. The critical fluctuations are very slow and are even becoming slower
when approaching the QCP.
",physics
"  A new Short-Orbit Spectrometer (SOS) has been constructed and installed
within the experimental facility of the A1 collaboration at Mainz Microtron
(MAMI), with the goal to detect low-energy pions. It is equipped with a
Browne-Buechner magnet and a detector system consisting of two helium-ethane
based drift chambers and a scintillator telescope made of five layers. The
detector system allows detection of pions in the momentum range of 50 - 147
MeV/c, which corresponds to 8.7 - 63 MeV kinetic energy. The spectrometer can
be placed at a distance range of 54 - 66 cm from the target center. Two
collimators are available for the measurements, one having 1.8 msr aperture and
the other having 7 msr aperture. The Short-Orbit Spectrometer has been
successfully calibrated and used in coincidence measurements together with the
standard magnetic spectrometers of the A1 collaboration.
",physics
"  In prior work, we addressed the problem of optimally controlling on line
connected and automated vehicles crossing two adjacent intersections in an
urban area to minimize fuel consumption while achieving maximal throughput
without any explicit traffic signaling and without considering left and right
turns. In this paper, we extend the solution of this problem to account for
left and right turns under hard safety constraints. Furthermore, we formulate
and solve another optimization problem to minimize a measure of passenger
discomfort while the vehicle turns at the intersection and we investigate the
associated tradeoff between minimizing fuel consumption and passenger
discomfort.
",mathematics
"  This paper presents a study of the metaphorism pattern of relational
specification, showing how it can be refined into recursive programs.
Metaphorisms express input-output relationships which preserve relevant
information while at the same time some intended optimization takes place. Text
processing, sorting, representation changers, etc., are examples of
metaphorisms. The kind of metaphorism refinement studied in this paper is a
strategy known as change of virtual data structure. By framing metaphorisms in
the class of (inductive) regular relations, sufficient conditions are given for
such implementations to be calculated using relation algebra. The strategy is
illustrated with examples including the derivation of the quicksort and
mergesort algorithms, showing what they have in common and what makes them
different from the very start of development.
",computer-science
"  Self-nested trees present a systematic form of redundancy in their subtrees
and thus achieve optimal compression rates by DAG compression. A method for
quantifying the degree of self-similarity of plants through self-nested trees
has been introduced by Godin and Ferraro in 2010. The procedure consists in
computing a self-nested approximation, called the nearest embedding self-nested
tree, that both embeds the plant and is the closest to it. In this paper, we
propose a new algorithm that computes the nearest embedding self-nested tree
with a smaller overall complexity, but also the nearest embedded self-nested
tree. We show from simulations that the latter is mostly the closest to the
initial data, which suggests that this better approximation should be used as a
privileged measure of the degree of self-similarity of plants.
",computer-science
"  A hierarchical scheme for clustering data is presented which applies to
spaces with a high number of dimension ($N_{_{D}}>3$). The data set is first
reduced to a smaller set of partitions (multi-dimensional bins). Multiple
clustering techniques are used, including spectral clustering, however, new
techniques are also introduced based on the path length between partitions that
are connected to one another. A Line-Of-Sight algorithm is also developed for
clustering. A test bank of 12 data sets with varying properties is used to
expose the strengths and weaknesses of each technique. Finally, a robust
clustering technique is discussed based on reaching a consensus among the
multiple approaches, overcoming the weaknesses found individually.
",computer-science
"  Caching popular contents at the edge of cellular networks has been proposed
to reduce the load, and hence the cost of backhaul links. It is significant to
decide which files should be cached and where to cache them. In this paper, we
propose a distributed caching scheme considering the tradeoff between the
diversity and redundancy of base stations' cached contents. Whether it is
better to cache the same or different contents in different base stations? To
find out this, we formulate an optimal redundancy caching problem. Our goal is
to minimize the total transmission cost of the network, including cost within
the radio access network (RAN) and cost incurred by transmission to the core
network via backhaul links. The optimal redundancy ratio under given system
configuration is obtained with adapted particle swarm optimization (PSO)
algorithm. We analyze the impact of important system parameters through
Monte-Carlo simulation. Results show that the optimal redundancy ratio is
mainly influenced by two parameters, which are the backhaul to RAN unit cost
ratio and the steepness of file popularity distribution. The total cost can be
reduced by up to 54% at given unit cost ratio of backhaul to RAN when the
optimal redundancy ratio is selected. Under typical file request pattern, the
reduction amount can be up to 57%.
",computer-science
"  We initiate the algorithmic study of the following ""structured augmentation""
question: is it possible to increase the connectivity of a given graph G by
superposing it with another given graph H? More precisely, graph F is the
superposition of G and H with respect to injective mapping \phi: V(H)->V(G) if
every edge uv of F is either an edge of G, or \phi^{-1}(u)\phi^{-1}(v) is an
edge of H. We consider the following optimization problem. Given graphs G,H,
and a weight function \omega assigning non-negative weights to pairs of
vertices of V(G), the task is to find \varphi of minimum weight
\omega(\phi)=\sum_{xy\in E(H)}\omega(\phi(x)\varphi(y)) such that the edge
connectivity of the superposition F of G and H with respect to \phi is higher
than the edge connectivity of G. Our main result is the following ""dichotomy""
complexity classification. We say that a class of graphs C has bounded
vertex-cover number, if there is a constant t depending on C only such that the
vertex-cover number of every graph from C does not exceed t. We show that for
every class of graphs C with bounded vertex-cover number, the problems of
superposing into a connected graph F and to 2-edge connected graph F, are
solvable in polynomial time when H\in C. On the other hand, for any hereditary
class C with unbounded vertex-cover number, both problems are NP-hard when H\in
C. For the unweighted variants of structured augmentation problems, i.e. the
problems where the task is to identify whether there is a superposition of
graphs of required connectivity, we provide necessary and sufficient
combinatorial conditions on the existence of such superpositions. These
conditions imply polynomial time algorithms solving the unweighted variants of
the problems.
",computer-science
"  Online advertising is progressively moving towards a programmatic model in
which ads are matched to actual interests of individuals collected as they
browse the web. Letting the huge debate around privacy aside, a very important
question in this area, for which little is known, is: How much do advertisers
pay to reach an individual? In this study, we develop a first of its kind
methodology for computing exactly that -- the price paid for a web user by the
ad ecosystem -- and we do that in real time. Our approach is based on tapping
on the Real Time Bidding (RTB) protocol to collect cleartext and encrypted
prices for winning bids paid by advertisers in order to place targeted ads. Our
main technical contribution is a method for tallying winning bids even when
they are encrypted. We achieve this by training a model using as ground truth
prices obtained by running our own ""probe"" ad-campaigns. We design our
methodology through a browser extension and a back-end server that provides it
with fresh models for encrypted bids. We validate our methodology using a one
year long trace of 1600 mobile users and demonstrate that it can estimate a
user's advertising worth with more than 82% accuracy.
",computer-science
"  Let $\sigma$ be arc-length measure on $S^1\subset \mathbb R^2$ and $\Theta$
denote rotation by an angle $\theta \in (0, \pi]$. Define a model bilinear
generalized Radon transform, $$B_{\theta}(f,g)(x)=\int_{S^1} f(x-y)g(x-\Theta
y)\, d\sigma(y),$$ an analogue of the linear generalized Radon transforms of
Guillemin and Sternberg \cite{GS} and Phong and Stein (e.g.,
\cite{PhSt91,St93}). Operators such as $B_\theta$ are motivated by problems in
geometric measure theory and combinatorics. For $\theta<\pi$, we show that
$B_{\theta}: L^p({\Bbb R}^2) \times L^q({\Bbb R}^2) \to L^r({\Bbb R}^2)$ if
$\left(\frac{1}{p},\frac{1}{q},\frac{1}{r}\right)\in Q$, the polyhedron with
the vertices $(0,0,0)$, $(\frac{2}{3}, \frac{2}{3}, 1)$, $(0, \frac{2}{3},
\frac{1}{3})$, $(\frac{2}{3},0,\frac{1}{3})$, $(1,0,1)$, $(0,1,1)$ and
$(\frac{1}{2},\frac{1}{2},\frac{1}{2})$, except for $\left(
\frac{1}{2},\frac{1}{2},\frac{1}{2} \right)$, where we obtain a restricted
strong type estimate. For the degenerate case $\theta=\pi$, a more restrictive
set of exponents holds. In the scale of normed spaces, $p,q,r \ge 1$, the type
set $Q$ is sharp. Estimates for the same exponents are also proved for a class
of bilinear generalized Radon transforms in $\mathbb R^2$ of the form $$
B(f,g)(x)=\int \int \delta(\phi_1(x,y)-t_1)\delta(\phi_2(x,z)-t_2)
\delta(\phi_3(y,z)-t_3) f(y)g(z) \psi(y,z) \, dy\, dz, $$ where $\delta$
denotes the Dirac distribution, $t_1,t_2,t_3\in\mathbb R$, $\psi$ is a smooth
cut-off and the defining functions $\phi_j$ satisfy some natural geometric
assumptions.
",mathematics
"  Strain engineering has attracted great attention, particularly for epitaxial
films grown on a different substrate. Residual strains of SiC have been widely
employed to form ultra-high frequency and high Q factor resonators. However, to
date the highest residual strain of SiC was reported to be limited to
approximately 0.6%. Large strains induced into SiC could lead to several
interesting physical phenomena, as well as significant improvement of resonant
frequencies. We report an unprecedented nano strain-amplifier structure with an
ultra-high residual strain up to 8% utilizing the natural residual stress
between epitaxial 3C SiC and Si. In addition, the applied strain can be tuned
by changing the dimensions of the amplifier structure. The possibility of
introducing such a controllable and ultra-high strain will open the door to
investigating the physics of SiC in large strain regimes, and the development
of ultra sensitive mechanical sensors.
",physics
"  Vagueness is something everyone is familiar with. In fact, most people think
that vagueness is closely related to language and exists only there. However,
vagueness is a property of the physical world. Quantum computers harness
superposition and entanglement to perform their computational tasks. Both
superposition and entanglement are vague processes. Thus quantum computers,
which process exact data without ""exploiting"" vagueness, are actually vague
computers.
",computer-science
"  Making sense of a dataset in an automatic and unsupervised fashion is a
challenging problem in statistics and AI. Classical approaches for density
estimation are usually not flexible enough to deal with the uncertainty
inherent to real-world data: they are often restricted to fixed latent
interaction models and homogeneous likelihoods; they are sensitive to missing,
corrupt and anomalous data; moreover, their expressiveness generally comes at
the price of intractable inference. As a result, supervision from statisticians
is usually needed to find the right model for the data. However, as domain
experts do not necessarily have to be experts in statistics, we propose
Automatic Bayesian Density Analysis (ABDA) to make density estimation
accessible at large. ABDA automates the selection of adequate likelihood models
from arbitrarily rich dictionaries while modeling their interactions via a deep
latent structure adaptively learned from data as a sum-product network. ABDA
casts uncertainty estimation at these local and global levels into a joint
Bayesian inference problem, providing robust and yet tractable inference.
Extensive empirical evidence shows that ABDA is a suitable tool for automatic
exploratory analysis of heterogeneous tabular data, allowing for missing value
estimation, statistical data type and likelihood discovery, anomaly detection
and dependency structure mining, on top of providing accurate density
estimation.
",statistics
"  We study the eigenvalues of the semiclassical Witten Laplacian $\Delta_\phi$
associated to a potential $\phi$. We consider the case where the sequence of
Arrhenius numbers $S_1\leq \ldots\leq S_n$ associated to $\phi$ is degenerated,
that is the preceding inequality are not necessarily strict.
",mathematics
"  In this paper, we consider the problem of pursuit-evasion using multiple
Autonomous Underwater Vehicles (AUVs) in a 3D water volume, with and without
simple obstacles. Pursuit-evasion is a well studied topic in robotics, but the
results are mostly set in 2D environments, using unlimited line of sight
sensing. We propose an algorithm for range limited sensing in 3D environments
that captures a finite speed evader based on one single previous observation of
its location. The pursuers are first moved to form a maximal cage formation,
based on their number and sensor ranges, containing all of the possible evader
locations. The cage is then shrunk until every part of that volume is sensed,
thereby capturing the evader. The pursuers need only limited sensing range and
low bandwidth communication, making the algorithm well suited for an underwater
environment.
",computer-science
"  Direct experimental investigations of the low-energy electronic structure of
the Na$_2$IrO$_3$ iridate insulator are sparse and draw two conflicting
pictures. One relies on flat bands and a clear gap, the other involves
dispersive states approaching the Fermi level, pointing to surface metallicity.
Here, by a combination of angle-resolved photoemission, photoemission electron
microscopy, and x-ray absorption, we show that the correct picture is more
complex and involves an anomalous band, arising from charge transfer from Na
atoms to Ir-derived states. Bulk quasiparticles do exist, but in one of the two
possible surface terminations the charge transfer is smaller and they remain
elusive.
",physics
"  This paper is concerned with the online estimation of a nonlinear dynamic
system from a series of noisy measurements. The focus is on cases wherein
outliers are present in-between normal noises. We assume that the outliers
follow an unknown generating mechanism which deviates from that of normal
noises, and then model the outliers using a Bayesian nonparametric model called
Dirichlet process mixture (DPM). A sequential particle-based algorithm is
derived for posterior inference for the outlier model as well as the state of
the system to be estimated. The resulting algorithm is termed DPM based robust
PF (DPM-RPF). The nonparametric feature makes this algorithm allow the data to
""speak for itself"" to determine the complexity and structure of the outlier
model. Simulation results show that it performs remarkably better than two
state-of-the-art methods especially when outliers appear frequently along time.
",statistics
"  Our experience of the world is multimodal - we see objects, hear sounds, feel
texture, smell odors, and taste flavors. Modality refers to the way in which
something happens or is experienced and a research problem is characterized as
multimodal when it includes multiple such modalities. In order for Artificial
Intelligence to make progress in understanding the world around us, it needs to
be able to interpret such multimodal signals together. Multimodal machine
learning aims to build models that can process and relate information from
multiple modalities. It is a vibrant multi-disciplinary field of increasing
importance and with extraordinary potential. Instead of focusing on specific
multimodal applications, this paper surveys the recent advances in multimodal
machine learning itself and presents them in a common taxonomy. We go beyond
the typical early and late fusion categorization and identify broader
challenges that are faced by multimodal machine learning, namely:
representation, translation, alignment, fusion, and co-learning. This new
taxonomy will enable researchers to better understand the state of the field
and identify directions for future research.
",computer-science
"  We present an explicitly correlated formalism for the second-order
single-particle Green's function method (GF2-F12) that does not assume the
popular diagonal approximation, and describes the energy dependence of the
explicitly correlated terms. For small and medium organic molecules the basis
set errors of ionization potentials of GF2-F12 are radically improved relative
to GF2: the performance of GF2-F12/aug- cc-pVDZ is better than that of
GF2/aug-cc-pVQZ, at a significantly lower cost.
",physics
"  Electron cloud can lead to a fast instability in intense proton and positron
beams in circular accelerators. In the Fermilab Recycler the electron cloud is
confined within its combined function magnets. We show that the field of
combined function magnets traps the electron cloud, present the results of
analytical estimates of trapping, and compare them to numerical simulations of
electron cloud formation. The electron cloud is located at the beam center and
up to 1% of the particles can be trapped by the magnetic field. Since the
process of electron cloud build-up is exponential, once trapped this amount of
electrons significantly increases the density of the cloud on the next
revolution. In a Recycler combined function dipole this multi-turn accumulation
allows the electron cloud reaching final intensities orders of magnitude
greater than in a pure dipole. The multi-turn build-up can be stopped by
injection of a clearing bunch of $10^{10}$ p at any position in the ring.
",physics
"  Vibrational energy harvesters capture mechanical energy from ambient
vibrations and convert the mechanical energy into electrical energy to power
wireless electronic systems. Challenges exist in the process of capturing
mechanical energy from ambient vibrations. For example, resonant harvesters may
be used to improve power output near their resonance, but their narrow
bandwidth makes them less suitable for applications with varying vibrational
frequencies. Higher operating frequencies can increase harvesters power output,
but many vibrational sources are characterized by lower frequencies, such as
human motions. This paper provides a thorough review of state of the art energy
harvesters based on various energy sources such as solar, thermal,
electromagnetic and mechanical energy, as well as smart materials including
piezoelectric materials and carbon nanotubes. The paper will then focus on
vibrational energy harvesters to review harvesters using typical transduction
mechanisms and various techniques to address the challenges in capturing
mechanical energy and delivering it to the transducers.
",physics
"  Spin-spin correlation function response in the low electronic density regime
and externally applied electric field is evaluated for 2D metallic crystals
under Rashba-type coupling, fixed number of particles and two-fold energy band
structure. Intrinsic Zeeman-like effect on electron spin polarization, density
of states, Fermi surface topology and transverse magnetic susceptibility are
analyzed in the zero temperature limit. A possible magnetic state for Dirac
electrons depending on the zero field band gap magnitude under this conditions
is found.
",physics
"  Many phenomena in collisionless plasma physics require a kinetic description.
The evolution of the phase space density can be modeled by means of the Vlasov
equation, which has to be solved numerically in most of the relevant cases. One
of the problems that often arise in such simulations is the violation of
important physical conservation laws. Numerical diffusion in phase space
translates into unphysical heating, which can increase the overall energy
significantly, depending on the time scale and the plasma regime. In this
paper, a general and straightforward way of improving conservation properties
of Vlasov schemes is presented that can potentially be applied to a variety of
different codes. The basic idea is to use fluid models with good conservation
properties for correcting kinetic models. The higher moments that are missing
in the fluid models are provided by the kinetic codes, so that both kinetic and
fluid codes compensate the weaknesses of each other in a closed feedback loop.
",physics
"  This volume contains the proceedings of MARS 2017, the second workshop on
Models for Formal Analysis of Real Systems, held on April 29, 2017 in Uppala,
Sweden, as an affiliated workshop of ETAPS 2017, the European Joint Conferences
on Theory and Practice of Software.
The workshop emphasises modelling over verification. It aims at discussing
the lessons learned from making formal methods for the verification and
analysis of realistic systems. Examples are:
(1) Which formalism is chosen, and why?
(2) Which abstractions have to be made and why?
(3) How are important characteristics of the system modelled?
(4) Were there any complications while modelling the system?
(5) Which measures were taken to guarantee the accuracy of the model?
We invited papers that present full models of real systems, which may lay the
basis for future comparison and analysis. An aim of the workshop is to present
different modelling approaches and discuss pros and cons for each of them.
Alternative formal descriptions of the systems presented at this workshop are
encouraged, which should foster the development of improved specification
formalisms.
",computer-science
"  We study a supersymmetric version of the Gardner equation (both focusing and
defocusing) using the superbilinear formalism. This equation is new and cannot
be obtained from supersymmetric modified Korteweg-de Vries equation with a
nonzero boundary condition. We construct supersymmetric solitons and then by
passing to the long-wave limit in the focusing case obtain rational nonsingular
solutions. We also discuss the supersymmetric version of the defocusing
equation and the dynamics of its solutions.
",physics
"  We report on the observation of phase space modulations in the correlated
electron emission after strong field double ionization of helium using laser
pulses with a wavelength of 394~nm and an intensity of $3\cdot10^{14}$W/cm$^2$.
Those modulations are identified as direct results of quantum mechanical
selection rules predicted by many theoretical calculations. They only occur for
an odd number of absorbed photons. By that we attribute this effect to the
parity of the continuum wave function.
",physics
"  Let $(X, g^+)$ be an asymptotically hyperbolic manifold and $(M, [\hat{h}])$
its conformal infinity. Our primary aim in this paper is to introduce the
prescribed fractional scalar curvature problem on $M$ and provide solutions
under various geometric conditions on $X$ and $M$. We also obtain the existence
results for the fractional Yamabe problem in the endpoint case, e.g., $n = 3$,
$\gamma = 1/2$ and $M$ is non-umbilic, etc. Every solution we find turns out to
be smooth on $M$.
",mathematics
"  Previous secondary eclipse observations of the hot Jupiter Qatar-1b in the Ks
band suggest that it may have an unusually high day side temperature,
indicative of minimal heat redistribution. There have also been indications
that the orbit may be slightly eccentric, possibly forced by another planet in
the system. We investigate the day side temperature and orbital eccentricity
using secondary eclipse observations with Spitzer. We observed the secondary
eclipse with Spitzer/IRAC in subarray mode, in both 3.6 and 4.5 micron
wavelengths. We used pixel-level decorrelation to correct for Spitzer's
intra-pixel sensitivity variations and thereby obtain accurate eclipse depths
and central phases. Our 3.6 micron eclipse depth is 0.149 +/- 0.051% and the
4.5 micron depth is 0.273 +/- 0.049%. Fitting a blackbody planet to our data
and two recent Ks band eclipse depths indicates a brightness temperature of
1506 +/- 71K. Comparison to model atmospheres for the planet indicates that its
degree of longitudinal heat redistribution is intermediate between fully
uniform and day side only. The day side temperature of the planet is unlikely
to be as high (1885K) as indicated by the ground-based eclipses in the Ks band,
unless the planet's emergent spectrum deviates strongly from model atmosphere
predictions. The average central phase for our Spitzer eclipses is 0.4984 +/-
0.0017, yielding e cos(omega) = -0.0028 +/- 0.0027. Our results are consistent
with a circular orbit, and we constrain e cos(omega) much more strongly than
has been possible with previous observations.
",physics
"  Semi-Lagrangian methods are numerical methods designed to find approximate
solutions to particular time-dependent partial differential equations (PDEs)
that describe the advection process. We propose semi-Lagrangian one-step
methods for numerically solving initial value problems for two general systems
of partial differential equations. Along the characteristic lines of the PDEs,
we use ordinary differential equation (ODE) numerical methods to solve the
PDEs. The main benefit of our methods is the efficient achievement of high
order local truncation error through the use of Runge-Kutta methods along the
characteristics. In addition, we investigate the numerical analysis of
semi-Lagrangian methods applied to systems of PDEs: stability, convergence, and
maximum error bounds.
",mathematics
"  The difficulty of validating large-scale quantum devices, such as Boson
Samplers, poses a major challenge for any research program that aims to show
quantum advantages over classical hardware. To address this problem, we propose
a novel data-driven approach wherein models are trained to identify common
pathologies using unsupervised machine learning methods. We illustrate this
idea by training a classifier that exploits K-means clustering to distinguish
between Boson Samplers that use indistinguishable photons from those that do
not. We train the model on numerical simulations of small-scale Boson Samplers
and then validate the pattern recognition technique on larger numerical
simulations as well as on photonic chips in both traditional Boson Sampling and
scattershot experiments. The effectiveness of such method relies on
particle-type-dependent internal correlations present in the output
distributions. This approach performs substantially better on the test data
than previous methods and underscores the ability to further generalize its
operation beyond the scope of the examples that it was trained on.
",computer-science
"  We theoretically investigate an ultrastrongly-coupled micromaser based on
Rydberg atoms interacting with a superconducting LC resonator, where the common
rotating-wave approximation and slowly-varying-envelope approximation are no
longer applicable. The effect of counter-rotating terms on the masing dynamics
is studied in detail. We find that the intraresonator electric energy declines
and the microwave oscillation frequency shifts significantly in the regime of
ultrastrong coupling. Additionally, the micromaser phase fluctuation is
suppressed, resulting in a reduced spectral linewidth.
",physics
"  Spin-gapless semiconductors with their unique band structures have recently
attracted much attention due to their interesting transport properties that can
be utilized in spintronics applications. We have successfully deposited the
thin films of quaternary spin-gapless semiconductor CoFeMnSi Heusler alloy on
MgO (001) substrates using a pulsed laser deposition system. These films show
epitaxial growth along (001) direction and display uniform and smooth
crystalline surface. The magnetic properties reveal that the film is
ferromagnetically soft along the in-plane direction and its Curie temperature
is well above 400 K. The electrical conductivity of the film is low and
exhibits a nearly temperature independent semiconducting behaviour. The
estimated temperature coefficient of resistivity for the film is -7x10^-10
Ohm.m/K, which is comparable to the values reported for spin-gapless
semiconductors.
",physics
"  We present a new system S for handling uncertainty in a quantified modal
logic (first-order modal logic). The system is based on both probability theory
and proof theory. The system is derived from Chisholm's epistemology. We
concretize Chisholm's system by grounding his undefined and primitive (i.e.
foundational) concept of reasonablenes in probability and proof theory. S can
be useful in systems that have to interact with humans and provide
justifications for their uncertainty. As a demonstration of the system, we
apply the system to provide a solution to the lottery paradox. Another
advantage of the system is that it can be used to provide uncertainty values
for counterfactual statements. Counterfactuals are statements that an agent
knows for sure are false. Among other cases, counterfactuals are useful when
systems have to explain their actions to users. Uncertainties for
counterfactuals fall out naturally from our system.
Efficient reasoning in just simple first-order logic is a hard problem.
Resolution-based first-order reasoning systems have made significant progress
over the last several decades in building systems that have solved non-trivial
tasks (even unsolved conjectures in mathematics). We present a sketch of a
novel algorithm for reasoning that extends first-order resolution.
Finally, while there have been many systems of uncertainty for propositional
logics, first-order logics and propositional modal logics, there has been very
little work in building systems of uncertainty for first-order modal logics.
The work described below is in progress; and once finished will address this
lack.
",computer-science
"  The intricate interplay between optically dark and bright excitons governs
the light-matter interaction in transition metal dichalcogenide monolayers. We
have performed a detailed investigation of the ""spin-forbidden"" dark excitons
in WSe2 monolayers by optical spectroscopy in an out-of-plane magnetic field
Bz. In agreement with the theoretical predictions deduced from group theory
analysis, magneto-photoluminescence experiments reveal a zero field splitting
$\delta=0.6 \pm 0.1$ meV between two dark exciton states. The low energy state
being strictly dipole forbidden (perfectly dark) at Bz=0 while the upper state
is partially coupled to light with z polarization (""grey"" exciton). The first
determination of the dark neutral exciton lifetime $\tau_D$ in a transition
metal dichalcogenide monolayer is obtained by time-resolved photoluminescence.
We measure $\tau_D \sim 110 \pm 10$ ps for the grey exciton state, i.e. two
orders of magnitude longer than the radiative lifetime of the bright neutral
exciton at T=12 K.
",physics
"  We study flows on C*-algebras with the Rokhlin property. We show that every
Kirchberg algebra carries a unique Rokhlin flow up to cocycle conjugacy, which
confirms a long-standing conjecture of Kishimoto. We moreover present a
classification theory for Rokhlin flows on C*-algebras satisfying certain
technical properties, which hold for many C*-algebras covered by the Elliott
program. As a consequence, we obtain the following further classification
theorems for Rokhlin flows. Firstly, we extend the statement of Kishimoto's
conjecture to the non-simple case: Up to cocycle conjugacy, a Rokhlin flow on a
separable, nuclear, strongly purely infinite C*-algebra is uniquely determined
by its induced action on the prime ideal space. Secondly, we give a complete
classification of Rokhlin flows on simple classifiable $KK$-contractible
C*-algebras: Two Rokhlin flows on such a C*-algebra are cocycle conjugate if
and only if their induced actions on the cone of lower-semicontinuous traces
are affinely conjugate.
",mathematics
"  The relationship between communicating automata and session types is the
cornerstone of many diverse theories and tools, including type checking, code
generation, and runtime verification. A serious limitation of session types is
that, while endpoint programs interact asynchronously, the underlying property
which guarantees safety of session types is too synchronous: it requires a
one-to-one synchronisation between send and receive actions. This paper
proposes a sound procedure to verify properties of communicating session
automata (CSA), i.e., communicating automata that correspond to multiparty
session types. We introduce a new asynchronous compatibility property for CSA,
called k-multiparty compatibility (k-MC), which is a strict superset of the
synchronous multiparty compatibility proposed in the literature. It is
decomposed into two bounded properties: (i) a condition called k-safety which
guarantees that, within the bound, all sent messages can be received and each
automaton can make a move; and (ii) a condition called k-exhaustivity which
guarantees that all k-reachable send actions can be fired within the bound. We
show that k-exhaustive systems soundly and completely characterise systems
where each automaton behaves uniformly for any bound greater or equal to k. We
show that checking k-MC is PSPACE-complete, but can be done efficiently over
large systems by using partial order reduction techniques. We demonstrate that
several examples from the literature are k-MC, but not synchronous compatible.
",computer-science
"  This paper is concerned with the blowup phenomena for initial value problem
of semilinear wave equation with critical space-dependent damping term
(DW:$V$). The main result of the present paper is to give a solution of the
problem and to provide a sharp estimate for lifespan for such a solution when
$\frac{N}{N-1}<p\leq p_S(N+V_0)$, where $p_S(N)$ is the Strauss exponent for
(DW:$0$). The main idea of the proof is due to the technique of test functions
for (DW:$0$) originated by Zhou--Han (2014, MR3169791). Moreover, we find a new
threshold value $V_0=\frac{(N-1)^2}{N+1}$ for the coefficient of critical and
singular damping $|x|^{-1}$.
",mathematics
"  Static and dynamic properties of vortices in a two-component Bose-Einstein
condensate with Rashba spin-orbit coupling are investigated. The mass current
around a vortex core in the plane-wave phase is found to be deformed by the
spin-orbit coupling, and this makes the dynamics of the vortex pairs quite
different from those in a scalar Bose-Einstein condensate. The velocity of a
vortex-antivortex pair is much smaller than that without spin-orbit coupling,
and there exist stationary states. Two vortices with the same circulation move
away from each other or unite to form a stationary state.
",physics
"  Runtime enforcement can be effectively used to improve the reliability of
software applications. However, it often requires the definition of ad hoc
policies and enforcement strategies, which might be expensive to identify and
implement. This paper discusses how to exploit lifecycle events to obtain
useful enforcement strategies that can be easily reused across applications,
thus reducing the cost of adoption of the runtime enforcement technology. The
paper finally sketches how this idea can be used to define libraries that can
automatically overcome problems related to applications misusing them.
",computer-science
"  At the core of many important machine learning problems faced by online
streaming services is a need to model how users interact with the content.
These problems can often be reduced to a combination of 1) sequentially
recommending items to the user, and 2) exploiting the user's interactions with
the items as feedback for the machine learning model. Unfortunately, there are
no public datasets currently available that enable researchers to explore this
topic. In order to spur that research, we release the Music Streaming Sessions
Dataset (MSSD), which consists of approximately 150 million listening sessions
and associated user actions. Furthermore, we provide audio features and
metadata for the approximately 3.7 million unique tracks referred to in the
logs. This is the largest collection of such track metadata currently available
to the public. This dataset enables research on important problems including
how to model user listening and interaction behaviour in streaming, as well as
Music Information Retrieval (MIR), and session-based sequential
recommendations.
",computer-science
"  We study the problems of clustering locally asymptotically self-similar
stochastic processes, when the true number of clusters is priorly known. A new
covariance-based dissimilarity measure is introduced, from which the so-called
approximately asymptotically consistent clustering algorithms are obtained. In
a simulation study, clustering data sampled from multifractional Brownian
motions is performed to illustrate the approximated asymptotic consistency of
the proposed algorithms.
",statistics
"  The (torsion) complexity of a finite edge-weighted graph is defined to be the
order of the torsion subgroup of the abelian group presented by its Laplacian
matrix. When G is d-periodic (i.e., G has a free action of the rank-d free
abelian group by graph automorphisms, with finite quotient) the Mahler measure
of its Laplacian determinant polynomial is the growth rate of the complexity of
finite quotients of G. Lehmer's question, an open question about the roots of
monic integral polynomials, is equivalent to a question about the complexity
growth of edge-weighted 1-periodic graphs.
",mathematics
"  In this work, we define and solve the Fair Top-k Ranking problem, in which we
want to determine a subset of k candidates from a large pool of n >> k
candidates, maximizing utility (i.e., select the ""best"" candidates) subject to
group fairness criteria. Our ranked group fairness definition extends group
fairness using the standard notion of protected groups and is based on ensuring
that the proportion of protected candidates in every prefix of the top-k
ranking remains statistically above or indistinguishable from a given minimum.
Utility is operationalized in two ways: (i) every candidate included in the
top-$k$ should be more qualified than every candidate not included; and (ii)
for every pair of candidates in the top-k, the more qualified candidate should
be ranked above. An efficient algorithm is presented for producing the Fair
Top-k Ranking, and tested experimentally on existing datasets as well as new
datasets released with this paper, showing that our approach yields small
distortions with respect to rankings that maximize utility without considering
fairness criteria.
To the best of our knowledge, this is the first algorithm grounded in
statistical tests that can mitigate biases in the representation of an
under-represented group along a ranked list.
",computer-science
"  Stochastic user equilibrium is an important issue in the traffic assignment
problems, tradition models for the stochastic user equilibrium problem are
designed as mathematical programming problems. In this article, a
Physarum-inspired model for the probit-based stochastic user equilibrium
problem is proposed. There are two main contributions of our work. On the one
hand, the origin Physarum model is modified to find the shortest path in
traffic direction networks with the properties of two-way traffic
characteristic. On the other hand, the modified Physarum-inspired model could
get the equilibrium flows when traveller's perceived transportation cost
complies with normal distribution. The proposed method is constituted with a
two-step procedure. First, the modified Physarum model is applied to get the
auxiliary flows. Second, the auxiliary flows are averaged to obtain the
equilibrium flows. Numerical examples are conducted to illustrate the
performance of the proposed method, which is compared with the Method of
Successive Average method.
",computer-science
"  If $\mathcal{G}$ is the group (under composition) of diffeomorphisms $f :
{\bar{D}}(0;1) \rightarrow {\bar{D}}(0;1)$ of the closed unit disc
${\bar{D}}(0;1)$ which are the identity map $id : {\bar{D}}(0;1) \rightarrow
{\bar{D}}(0;1)$ on the closed unit circle and satisfy the condition $det(J(f))
> 0$, where $J(f)$ is the Jacobian matrix of $f$ or (equivalently) the
Fréchet derivative of $f$, then $\mathcal{G}$ equipped with the metric
$d_{\mathcal{G}}(f,g) = \Vert f-g \Vert_{\infty } + \Vert J(f) - J(g)
\Vert_{\infty }$, where $f$, $g$ range over $\mathcal{G}$, is a metric space in
which $d_{\mathcal{G}} \left( f_{t} , id \right) \rightarrow 0$ as $t
\rightarrow 1^{+}$, where $f_{t}(z) = \frac{ tz }{ 1 + (t-1) \vert z \vert }$,
whenever $z \in {\bar{D}}(0;1)$ and $t \geq 1$.
",mathematics
"  We revisit the algebraic description of shape invariance method in
one-dimensional quantum mechanics. In this note we focus on four particular
examples: the Kepler problem in flat space, the Kepler problem in spherical
space, the Kepler problem in hyperbolic space, and the Rosen-Morse potential
problem. Following the prescription given by Gangopadhyaya et al., we first
introduce certain nonlinear algebraic systems. We then show that, if the model
parameters are appropriately quantized, the bound-state problems can be solved
solely by means of representation theory.
",physics
"  We open a new field on how one can define means on infinite sets. We
investigate many different ways on how such means can be constructed. One
method is based on sequences of ideals, other deals with accumulation points,
one uses isolated points, other deals with average using integral, other with
limit of average on surroundings and one deals with evenly distributed samples.
We study various properties of such means and their relations to each other.
",mathematics
"  Chest radiography is an extremely powerful imaging modality, allowing for a
detailed inspection of a patient's thorax, but requiring specialized training
for proper interpretation. With the advent of high performance general purpose
computer vision algorithms, the accurate automated analysis of chest
radiographs is becoming increasingly of interest to researchers. However, a key
challenge in the development of these techniques is the lack of sufficient
data. Here we describe MIMIC-CXR, a large dataset of 371,920 chest x-rays
associated with 227,943 imaging studies sourced from the Beth Israel Deaconess
Medical Center between 2011 - 2016. Each imaging study can pertain to one or
more images, but most often are associated with two images: a frontal view and
a lateral view. Images are provided with 14 labels derived from a natural
language processing tool applied to the corresponding free-text radiology
reports. All images have been de-identified to protect patient privacy. The
dataset is made freely available to facilitate and encourage a wide range of
research in medical computer vision.
",computer-science
"  Deep learning (DL) defines a new data-driven programming paradigm that
constructs the internal system logic of a crafted neuron network through a set
of training data. We have seen wide adoption of DL in many safety-critical
scenarios. However, a plethora of studies have shown that the state-of-the-art
DL systems suffer from various vulnerabilities which can lead to severe
consequences when applied to real-world applications. Currently, the testing
adequacy of a DL system is usually measured by the accuracy of test data.
Considering the limitation of accessible high quality test data, good accuracy
performance on test data can hardly provide confidence to the testing adequacy
and generality of DL systems. Unlike traditional software systems that have
clear and controllable logic and functionality, the lack of interpretability in
a DL system makes system analysis and defect detection difficult, which could
potentially hinder its real-world deployment. In this paper, we propose
DeepGauge, a set of multi-granularity testing criteria for DL systems, which
aims at rendering a multi-faceted portrayal of the testbed. The in-depth
evaluation of our proposed testing criteria is demonstrated on two well-known
datasets, five DL systems, and with four state-of-the-art adversarial attack
techniques against DL. The potential usefulness of DeepGauge sheds light on the
construction of more generic and robust DL systems.
",statistics
"  One of the long-standing challenges in Artificial Intelligence for learning
goal-directed behavior is to build a single agent which can solve multiple
tasks. Recent progress in multi-task learning for goal-directed sequential
problems has been in the form of distillation based learning wherein a student
network learns from multiple task-specific expert networks by mimicking the
task-specific policies of the expert networks. While such approaches offer a
promising solution to the multi-task learning problem, they require supervision
from large expert networks which require extensive data and computation time
for training. In this work, we propose an efficient multi-task learning
framework which solves multiple goal-directed tasks in an on-line setup without
the need for expert supervision. Our work uses active learning principles to
achieve multi-task learning by sampling the harder tasks more than the easier
ones. We propose three distinct models under our active sampling framework. An
adaptive method with extremely competitive multi-tasking performance. A
UCB-based meta-learner which casts the problem of picking the next task to
train on as a multi-armed bandit problem. A meta-learning method that casts the
next-task picking problem as a full Reinforcement Learning problem and uses
actor critic methods for optimizing the multi-tasking performance directly. We
demonstrate results in the Atari 2600 domain on seven multi-tasking instances:
three 6-task instances, one 8-task instance, two 12-task instances and one
21-task instance.
",computer-science
"  The first order magneto-structural transition ($T_t\simeq95$ K) and
magnetocaloric effect in MnNiGe$_{0.9}$Ga$_{0.1}$ are studied via powder x-ray
diffraction and magnetization measurements. Temperature dependent x-ray
diffraction measurements reveal that the magneto-structural transition remains
incomplete down to 23 K, resulting in a coexistence of antiferromagnetic and
ferromagnetic phases at low temperatures. The fraction of the high temperature
Ni$_2$In-type hexagonal ferromagnetic and low temperature TiNiSi-type
orthorhombic antiferromagnetic phases is estimated to be $\sim 40\%$ and $\sim
60\%$, respectively at 23 K. The ferromagnetic phase fraction increases with
increasing field which is found to be in non-equilibrium state and gives rise
to a weak re-entrant transition while warming under field-cooled condition. It
shows a large inverse magnetocaloric effect across the magneto-structural
transition and a conventional magnetocaloric effect across the second order
paramagnetic to ferromagnetic transition. The relative cooling power which
characterizes the performance of a magnetic refrigerant material is found to be
reasonably high compared to the other reported magnetocaloric alloys.
",physics
"  This work presents a new tool to verify the correctness of cryptographic
implementations with respect to cache attacks. Our methodology discovers
vulnerabilities that are hard to find with other techniques, observed as
exploitable leakage. The methodology works by identifying secret dependent
memory and introducing forced evictions inside potentially vulnerable code to
obtain cache traces that are analyzed using Mutual Information. If dependence
is observed, the cryptographic implementation is classified as to leak
information.
We demonstrate the viability of our technique in the design of the three main
cryptographic primitives, i.e., AES, RSA and ECC, in eight popular up to date
cryptographic libraries, including OpenSSL, Libgcrypt, Intel IPP and NSS. Our
results show that cryptographic code designers are far away from incorporating
the appropriate countermeasures to avoid cache leakages, as we found that 50%
of the default implementations analyzed leaked information that lead to key
extraction. We responsibly notified the designers of all the leakages found and
suggested patches to solve these vulnerabilities.
",computer-science
"  Three separation properties for a closed subgroup $H$ of a locally compact
group $G$ are studied: (1) the existence of a bounded approximate indicator for
$H$, (2) the existence of a completely bounded invariant projection of
$VN\left(G\right)$ onto $VN_{H}\left(G\right)$, and (3) the approximability of
the characteristic function $\chi_{H}$ by functions in $M_{cb}A\left(G\right)$
with respect to the weak$^{*}$ topology of $M_{cb}A\left(G_{d}\right)$. We show
that the $H$-separation property of Kaniuth and Lau is characterized by the
existence of certain bounded approximate indicators for $H$ and that a
discretized analogue of the $H$-separation property is equivalent to (3).
Moreover, we give a related characterization of amenability of $H$ in terms of
any group $G$ containing $H$ as a closed subgroup. The weak amenability of $G$
or that $G_{d}$ satisfies the approximation property, in combination with the
existence of a natural projection (in the sense of Lau and Ülger), are shown
to suffice to conclude (3). Several consequences of (2) involving the
cb-multiplier completion of $A\left(G\right)$ are given. Finally, a convolution
technique for averaging over the closed subgroup $H$ is developed and used to
weaken a condition for the existence of a bounded approximate indicator for
$H$.
",mathematics
"  A systematic design of adaptive waveform for Wireless Power Transfer (WPT)
has recently been proposed and shown through simulations to lead to significant
performance benefits compared to traditional non-adaptive and heuristic
waveforms. In this study, we design the first prototype of a closed-loop
wireless power transfer system with adaptive waveform optimization based on
Channel State Information acquisition. The prototype consists of three
important blocks, namely the channel estimator, the waveform optimizer, and the
energy harvester. Software Defined Radio (SDR) prototyping tools are used to
implement a wireless power transmitter and a channel estimator, and a voltage
doubler rectenna is designed to work as an energy harvester. A channel adaptive
waveform with 8 sinewaves is shown through experiments to improve the average
harvested DC power at the rectenna output by 9.8% to 36.8% over a non-adaptive
design with the same number of sinewaves.
",computer-science
"  Bayesian optimization is proposed for automatic learning of optimal
controller parameters from experimental data. A probabilistic description (a
Gaussian process) is used to model the unknown function from controller
parameters to a user-defined cost. The probabilistic model is updated with
data, which is obtained by testing a set of parameters on the physical system
and evaluating the cost. In order to learn fast, the Bayesian optimization
algorithm selects the next parameters to evaluate in a systematic way, for
example, by maximizing information gain about the optimum. The algorithm thus
iteratively finds the globally optimal parameters with only few experiments.
Taking throttle valve control as a representative industrial control example,
the proposed auto-tuning method is shown to outperform manual calibration: it
consistently achieves better performance with a low number of experiments. The
proposed auto-tuning framework is flexible and can handle different control
structures and objectives.
",computer-science
"  We develop the theory of hydrodynamic charge and heat transport in strongly
interacting quasi-relativistic systems on manifolds with inhomogeneous spatial
curvature. In solid-state physics, this is analogous to strain disorder in the
underlying lattice. In the hydrodynamic limit, we find that the thermal and
electrical conductivities are dominated by viscous effects, and that the
thermal conductivity is most sensitive to this disorder. We compare the effects
of inhomogeneity in the spatial metric to inhomogeneity in the chemical
potential, and discuss the extent to which our hydrodynamic theory is relevant
for experimentally realizable condensed matter systems, including suspended
graphene at the Dirac point.
",physics
"  We consider a novel stochastic multi-armed bandit problem called {\em good
arm identification} (GAI), where a good arm is defined as an arm with expected
reward greater than or equal to a given threshold. GAI is a pure-exploration
problem that a single agent repeats a process of outputting an arm as soon as
it is identified as a good one before confirming the other arms are actually
not good. The objective of GAI is to minimize the number of samples for each
process. We find that GAI faces a new kind of dilemma, the {\em
exploration-exploitation dilemma of confidence}, which is different difficulty
from the best arm identification. As a result, an efficient design of
algorithms for GAI is quite different from that for the best arm
identification. We derive a lower bound on the sample complexity of GAI that is
tight up to the logarithmic factor $\mathrm{O}(\log \frac{1}{\delta})$ for
acceptance error rate $\delta$. We also develop an algorithm whose sample
complexity almost matches the lower bound. We also confirm experimentally that
our proposed algorithm outperforms naive algorithms in synthetic settings based
on a conventional bandit problem and clinical trial researches for rheumatoid
arthritis.
",statistics
"  Given p independent normal populations, we consider the problem of estimating
the mean of those populations, that based on the observed data, give the
strongest signals. We explicitly condition on the ranking of the sample means,
and consider a constrained conditional maximum likelihood (CCMLE) approach,
avoiding the use of any priors and of any sparsity requirement between the
population means. Our results show that if the observed means are too close
together, we should in fact use the grand mean to estimate the mean of the
population with the larger sample mean. If they are separated by more than a
certain threshold, we should shrink the observed means towards each other. As
intuition suggests, it is only if the observed means are far apart that we
should conclude that the magnitude of separation and consequent ranking are not
due to chance. Unlike other methods, our approach does not need to pre-specify
the number of selected populations and the proposed CCMLE is able to perform
simultaneous inference. Our method, which is conceptually straightforward, can
be easily adapted to incorporate other selection criteria.
Selected populations, Maximum likelihood, Constrained MLE, Post-selection
inference
",statistics
"  The Fault Detection and Isolation Tools (FDITOOLS) is a collection of MATLAB
functions for the analysis and solution of fault detection and model detection
problems. The implemented functions are based on the computational procedures
described in the Chapters 5, 6 and 7 of the book: ""A. Varga, Solving Fault
Diagnosis Problems - Linear Synthesis Techniques, Springer, 2017"". This
document is the User's Guide for the version V1.0 of FDITOOLS. First, we
present the mathematical background for solving several basic exact and
approximate synthesis problems of fault detection filters and model detection
filters. Then, we give in-depth information on the command syntax of the main
analysis and synthesis functions. Several examples illustrate the use of the
main functions of FDITOOLS.
",computer-science
"  Given a statistical model for the request frequencies and sizes of data
objects in a caching system, we derive the probability density of the size of
the file that accounts for the largest amount of data traffic. This is
equivalent to finding the required size of the cache for a caching placement
that maximizes the expected byte hit ratio for given file size and popularity
distributions. The file that maximizes the expected byte hit ratio is the file
for which the product of its size and popularity is the highest -- thus, it is
the file that incurs the greatest load on the network. We generalize this
theoretical problem to cover factors and addends of arbitrary order statistics
for given parent distributions. Further, we study the asymptotic behavior of
these distributions. We give several factor and addend densities of widely-used
distributions, and verify our results by extensive computer simulations.
",computer-science
"  We discuss the existence of ground state solutions for the Choquard equation
$$-\Delta u=(I_\alpha*F(u))F'(u)\quad\quad\quad\text{in }\mathbb R^N.$$ We
prove the existence of solutions under general hypotheses, investigating in
particular the case of a homogeneous nonlinearity $F(u)=\frac{|u|^p}p$. The
cases $N=2$ and $N\ge3$ are treated differently in some steps. The solutions
are found through a variational mountain pass strategy. The result presented
are contained in the papers with arXiv ID 1212.2027 and 1604.03294
",mathematics
"  The Discrete Truncated Wigner Approximation (DTWA) is a semi-classical phase
space method useful for the exploration of Many-body quantum dynamics. In this
work we investigate Many-Body Localization (MBL) and thermalization using DTWA
and compare its performance to exact numerical solutions. By taking as a
benchmark case a 1D random field Heisenberg spin chain with short range
interactions, and by comparing to numerically exact techniques, we show that
DTWA is able to reproduce dynamical signatures that characterize both the
thermal and the MBL phases. It exhibits the best quantitative agreement at
short times deep in each of the phases and larger mismatches close to the phase
transition. The DTWA captures the logarithmic growth of entanglement in the MBL
phase, even though a pure classical mean-field analysis would lead to no
dynamics at all. Our results suggest the DTWA can become a useful method to
investigate MBL and thermalization in experimentally relevant settings
intractable with exact numerical techniques, such as systems with long range
interactions and/or systems in higher dimensions.
",physics
"  Supervised object detection and semantic segmentation require object or even
pixel level annotations. When there exist image level labels only, it is
challenging for weakly supervised algorithms to achieve accurate predictions.
The accuracy achieved by top weakly supervised algorithms is still
significantly lower than their fully supervised counterparts. In this paper, we
propose a novel weakly supervised curriculum learning pipeline for multi-label
object recognition, detection and semantic segmentation. In this pipeline, we
first obtain intermediate object localization and pixel labeling results for
the training images, and then use such results to train task-specific deep
networks in a fully supervised manner. The entire process consists of four
stages, including object localization in the training images, filtering and
fusing object instances, pixel labeling for the training images, and
task-specific network training. To obtain clean object instances in the
training images, we propose a novel algorithm for filtering, fusing and
classifying object instances collected from multiple solution mechanisms. In
this algorithm, we incorporate both metric learning and density-based
clustering to filter detected object instances. Experiments show that our
weakly supervised pipeline achieves state-of-the-art results in multi-label
image classification as well as weakly supervised object detection and very
competitive results in weakly supervised semantic segmentation on MS-COCO,
PASCAL VOC 2007 and PASCAL VOC 2012.
",statistics
"  Around year 2000 the centenary of Planck's thermal radiation formula awakened
interest in the origins of quantum theory, traditionally traced back to the
Planck's conference on 14 December 1900 at the Berlin Academy of Sciences. A
lot of more accurate historical reconstructions, conducted under the stimulus
of that recurrence, placed the birth date of quantum theory in March 1905 when
Einstein advanced his light quantum hypothesis. Both interpretations are yet
controversial, but science historians agree on one point: the emergence of
quantum theory from a presumed ""crisis"" of classical physics is a myth with
scarce adherence to the historical truth. This article, written in Italian
language, was originally presented in connection with the celebration of the
World Year of Phyics 2005 with the aim of bringing these scholarly theses to a
wider audience.
---
Tradizionalmente la nascita della teoria quantistica viene fatta risalire al
14 dicembre 1900, quando Planck presentò all'Accademia delle Scienze di
Berlino la dimostrazione della formula della radiazione termica. Numerose
ricostruzioni storiche più accurate, effettuate nel periodo intorno al 2000
sotto lo stimolo dell'interesse per il centenario di quell'avvenimento,
collocano invece la nascita della teoria quantistica nel marzo del 1905, quando
Einstein avanzò l'ipotesi dei quanti di luce. Entrambe le interpretazioni
sono tuttora controverse, ma gli storici della scienza concordano su un punto:
l'emergere della teoria quantistica da una presunta ""crisi"" della fisica
classica è un mito con scarsa aderenza alla verità storica. Con questo
articolo in italiano, presentato originariamente in occasione delle
celebrazioni per il World Year of Phyics 2005, si è inteso portare a un più
largo pubblico queste tesi già ben note agli specialisti.
",physics
"  Multidimensional item response theory is widely used in education and
psychology for measuring multiple latent traits. However, exploratory analysis
of large-scale item response data with many items, respondents, and latent
traits is still a challenge. In this paper, we consider a high-dimensional
setting that both the number of items and the number of respondents grow to
infinity. A constrained joint maximum likelihood estimator is proposed for
estimating both item and person parameters, which yields good theoretical
properties and computational advantage. Specifically, we derive error bounds
for parameter estimation and develop an efficient algorithm that can scale to
very large datasets. The proposed method is applied to a large scale
personality assessment data set from the Synthetic Aperture Personality
Assessment (SAPA) project. Simulation studies are conducted to evaluate the
proposed method.
",statistics
"  We investigate the theoretical foundations of the simulated tempering method
and use our findings to design efficient algorithms. Employing a large
deviation argument first used for replica exchange molecular dynamics [Plattner
et al., J. Chem. Phys. 135:134111 (2011)], we demonstrate that the most
efficient approach to simulated tempering is to vary the temperature infinitely
rapidly. In this limit, we can replace the equations of motion for the
temperature and physical variables by averaged equations for the latter alone,
with the forces rescaled according to a position-dependent function defined in
terms of temperature weights. The averaged equations are similar to those used
in Gao's integrated-over-temperature method, except that we show that it is
better to use a continuous rather than a discrete set of temperatures. We give
a theoretical argument for the choice of the temperature weights as the
reciprocal partition function, thereby relating simulated tempering to
Wang-Landau sampling. Finally, we describe a self-consistent algorithm for
simultaneously sampling the canonical ensemble and learning the weights during
simulation. This algorithm is tested on a system of harmonic oscillators as
well as a continuous variant of the Curie-Weiss model, where it is shown to
perform well and to accurately capture the second-order phase transition
observed in this model.
",statistics
"  We present a comprehensive account of the proton radiation hardness of Eljen
Technology's EJ-500 optical cement used in the construction of experiment
detectors. The cement was embedded into five plastic scintillator tiles which
were each exposed to one of five different levels of radiation by a 50 MeV
proton beam produced at the 88-Inch Cyclotron at Lawrence Berkeley National
Laboratory. A cosmic ray telescope setup was used to measure signal amplitudes
before and after irradiation. Another post-radiation measurement was taken four
months after the experiment to investigate whether the radiation damage to the
cement recovers after a short amount of time. We verified that the radiation
damage to the tiles increased with increasing dose but showed significant
improvement after the four months time interval.
",physics
"  Selecting a representative vector for a set of vectors is a very common
requirement in many algorithmic tasks. Traditionally, the mean or median vector
is selected. Ontology classes are sets of homogeneous instance objects that can
be converted to a vector space by word vector embeddings. This study proposes a
methodology to derive a representative vector for ontology classes whose
instances were converted to the vector space. We start by deriving five
candidate vectors which are then used to train a machine learning model that
would calculate a representative vector for the class. We show that our
methodology out-performs the traditional mean and median vector
representations.
",computer-science
"  We revisit the low energy physics of one dimensional spinless fermion
liquids, showing that with sufficiently strong interactions the conventional
Luttinger liquid can give way to a strong pairing phase. While the density
fluctuations in both phases are described by a gapless Luttinger liquid, single
fermion excitations are gapped only in the strong pairing phase. Smooth spatial
Interfaces between the two phases lead to topological degeneracies in the
ground state and low energy phonon spectrum. Using a concrete microscopic
model, with both single particle and pair hopping, we show that the strong
pairing state is established through emergence of a new low energy fermionic
mode. We characterize the two phases with numerical calculations using the
density matrix renormalization group. In particular we find enhancement of the
central charge from $c=1$ in the two Luttinger liquid phases to $c=3/2$ at the
critical point, which gives direct evidence for an emergent critical Majorana
mode. Finally, we confirm the existence of topological degeneracies in the low
energy phonon spectrum, associated with spatial interfaces between the two
phases.
",physics
"  This paper describes our submission to the 2017 BioASQ challenge. We
participated in Task B, Phase B which is concerned with biomedical question
answering (QA). We focus on factoid and list question, using an extractive QA
model, that is, we restrict our system to output substrings of the provided
text snippets. At the core of our system, we use FastQA, a state-of-the-art
neural QA system. We extended it with biomedical word embeddings and changed
its answer layer to be able to answer list questions in addition to factoid
questions. We pre-trained the model on a large-scale open-domain QA dataset,
SQuAD, and then fine-tuned the parameters on the BioASQ training set. With our
approach, we achieve state-of-the-art results on factoid questions and
competitive results on list questions.
",computer-science
"  Achieving high spatial resolution in contact sensing for robotic manipulation
often comes at the price of increased complexity in fabrication and
integration. One traditional approach is to fabricate a large number of taxels,
each delivering an individual, isolated response to a stimulus. In contrast, we
propose a method where the sensor simply consists of a continuous volume of
piezoresistive elastomer with a number of electrodes embedded inside. We
measure piezoresistive effects between all pairs of electrodes in the set, and
count on this rich signal set containing the information needed to pinpoint
contact location with high accuracy using regression algorithms. In our
validation experiments, we demonstrate submillimeter median accuracy in
locating contact on a 10mm by 16mm sensor using only four electrodes (creating
six unique pairs). In addition to extracting more information from fewer wires,
this approach lends itself to simple fabrication methods and makes no
assumptions about the underlying geometry, simplifying future integration on
robot fingers.
",computer-science
"  In this paper, we address the basic problem of recognizing moving objects in
video images using Visual Vocabulary model and Bag of Words and track our
object of interest in the subsequent video frames using species inspired PSO.
Initially, the shadow free images are obtained by background modelling followed
by foreground modeling to extract the blobs of our object of interest.
Subsequently, we train a cubic SVM with human body datasets in accordance with
our domain of interest for recognition and tracking. During training, using the
principle of Bag of Words we extract necessary features of certain domains and
objects for classification. Subsequently, matching these feature sets with
those of the extracted object blobs that are obtained by subtracting the shadow
free background from the foreground, we detect successfully our object of
interest from the test domain. The performance of the classification by cubic
SVM is satisfactorily represented by confusion matrix and ROC curve reflecting
the accuracy of each module. After classification, our object of interest is
tracked in the test domain using species inspired PSO. By combining the
adaptive learning tools with the efficient classification of description, we
achieve optimum accuracy in recognition of the moving objects. We evaluate our
algorithm benchmark datasets: iLIDS, VIVID, Walking2, Woman. Comparative
analysis of our algorithm against the existing state-of-the-art trackers shows
very satisfactory and competitive results.
",computer-science
"  This paper, the third in a series, completes our description of all (radial)
solutions on C* of the tt*-Toda equations, using a combination of methods from
p.d.e., isomonodromic deformations (Riemann-Hilbert method), and loop groups.
We place these global solutions into the broader context of solutions which are
smooth near 0. For such solutions, we compute explicitly the Stokes data and
connection matrix of the associated meromorphic system, in the resonant cases
as well as the non-resonant case. This allows us to give a complete picture of
the monodromy data of the global solutions.
",mathematics
"  Parametric resonance is among the most efficient phenomena generating
gravitational waves (GWs) in the early Universe. The dynamics of parametric
resonance, and hence of the GWs, depend exclusively on the resonance parameter
$q$. The latter is determined by the properties of each scenario: the initial
amplitude and potential curvature of the oscillating field, and its coupling to
other species. Previous works have only studied the GW production for fixed
value(s) of $q$. We present an analytical derivation of the GW amplitude
dependence on $q$, valid for any scenario, which we confront against numerical
results. By running lattice simulations in an expanding grid, we study for a
wide range of $q$ values, the production of GWs in post-inflationary preheating
scenarios driven by parametric resonance. We present simple fits for the final
amplitude and position of the local maxima in the GW spectrum. Our
parametrization allows to predict the location and amplitude of the GW
background today, for an arbitrary $q$. The GW signal can be rather large, as
$h^2\Omega_{\rm GW}(f_p) \lesssim 10^{-11}$, but it is always peaked at high
frequencies $f_p \gtrsim 10^{7}$ Hz. We also discuss the case of
spectator-field scenarios, where the oscillatory field can be e.g.~a curvaton,
or the Standard Model Higgs.
",physics
"  We determine the composition factors of the tensor product $S(E)\otimes S(E)$
of two copies of the symmetric algebra of the natural module $E$ of a general
linear group over an algebraically closed field of positive characteristic. Our
main result may be regarded as a substantial generalisation of the tensor
product theorem of Krop and Sullivan, on composition factors of $S(E)$. We
earlier answered the question of which polynomially injective modules are
infinitesimally injective in terms of the ""divisibility index"". We are now able
to give an explicit description of the divisibility index for polynomial
modules for general linear groups of degree at most $3$.
",mathematics
"  Recently a new fault tolerant and simple mechanism was designed for solving
commit consensus problem. It is based on replicated validation of messages sent
between transaction participants and a special dispatcher validator manager
node. This paper presents a correctness, safety proofs and performance analysis
of this algorithm.
",computer-science
"  Sleep condition is closely related to an individual's health. Poor sleep
conditions such as sleep disorder and sleep deprivation affect one's daily
performance, and may also cause many chronic diseases. Many efforts have been
devoted to monitoring people's sleep conditions. However, traditional
methodologies require sophisticated equipment and consume a significant amount
of time. In this paper, we attempt to develop a novel way to predict
individual's sleep condition via scrutinizing facial cues as doctors would.
Rather than measuring the sleep condition directly, we measure the
sleep-deprived fatigue which indirectly reflects the sleep condition. Our
method can predict a sleep-deprived fatigue rate based on a selfie provided by
a subject. This rate is used to indicate the sleep condition. To gain deeper
insights of human sleep conditions, we collected around 100,000 faces from
selfies posted on Twitter and Instagram, and identified their age, gender, and
race using automatic algorithms. Next, we investigated the sleep condition
distributions with respect to age, gender, and race. Our study suggests among
the age groups, fatigue percentage of the 0-20 youth and adolescent group is
the highest, implying that poor sleep condition is more prevalent in this age
group. For gender, the fatigue percentage of females is higher than that of
males, implying that more females are suffering from sleep issues than males.
Among ethnic groups, the fatigue percentage in Caucasian is the highest
followed by Asian and African American.
",computer-science
"  A recent result characterizes the fully order reversing operators acting on
the class of lower semicontinuous proper convex functions in a real Banach
space as certain linear deformations of the Legendre-Fenchel transform.
Motivated by the Hilbert space version of this result and by the well-known
result saying that this convex conjugation transform has a unique fixed point
(namely, the normalized energy function), we investigate the fixed point
equation in which the involved operator is fully order reversing and acts on
the above-mentioned class of functions. It turns out that this nonlinear
equation is very sensitive to the involved parameters and can have no solution,
a unique solution, or several (possibly infinitely many) ones. Our analysis
yields a few by-products, such as results related to positive definite
operators, and to functional equations and inclusions involving monotone
operators.
",mathematics
"  The formalism of partial information decomposition provides independent or
non-overlapping components constituting total information content provided by a
set of source variables about the target variable. These components are
recognised as unique information, synergistic information and, redundant
information. The metric of net synergy, conceived as the difference between
synergistic and redundant information, is capable of detecting synergy,
redundancy and, information independence among stochastic variables. And it can
be quantified, as it is done here, using appropriate combinations of different
Shannon mutual information terms. Utilisation of such a metric in network
motifs with the nodes representing different biochemical species, involved in
information sharing, uncovers rich store for interesting results. In the
current study, we make use of this formalism to obtain a comprehensive
understanding of the relative information processing mechanism in a diamond
motif and two of its sub-motifs namely bifurcation and integration motif
embedded within the diamond motif. The emerging patterns of synergy and
redundancy and their effective contribution towards ensuring high fidelity
information transmission are duly compared in the sub-motifs and independent
motifs (bifurcation and integration). In this context, the crucial roles played
by various time scales and activation coefficients in the network topologies
are especially emphasised. We show that the origin of synergy and redundancy in
information transmission can be physically justified by decomposing diamond
motif into bifurcation and integration motif.
",physics
"  We analyze the dynamics of inflationary models with a coupling of the
inflaton $\phi$ to gauge fields of the form $\phi F \tilde{F}/f$, as in the
case of axions. It is known that this leads to an instability, with exponential
amplification of gauge fields, controlled by the parameter $\xi=
\dot{\phi}/(2fH)$, which can strongly affect the generation of cosmological
perturbations and even the background. We show that scattering rates involving
gauge fields can become larger than the expansion rate $H$, due to the very
large occupation numbers, and create a thermal bath of particles of temperature
$T$ during inflation. In the thermal regime, energy is transferred to smaller
scales, radically modifying the predictions of this scenario. We thus argue
that previous constraints on $\xi$ are alleviated. If the gauge fields have
Standard Model interactions, which naturally provides reheating, they
thermalize already at $\xi\gtrsim2.9$, before perturbativity constraints and
also before backreaction takes place. In absence of SM interactions (i.e. for a
dark photon), we find that gauge fields and inflaton perturbations thermalize
if $\xi\gtrsim3.4$; however, observations require $\xi\gtrsim6$, which is above
the perturbativity and backreaction bounds and so a dedicated study is
required. After thermalization, though, the system should evolve non-trivially
due to the competition between the instability and the gauge field thermal
mass. If the thermal mass and the instabilities equilibrate, we expect an
equilibrium temperature of $T_{eq} \simeq \xi H/\bar{g}$ where $\bar{g}$ is the
effective gauge coupling. Finally, we estimate the spectrum of perturbations if
$\phi$ is thermal and find that the tensor to scalar ratio is suppressed by
$H/(2T)$, if tensors do not thermalize.
",physics
"  A detailed thermal analysis of a Niobium (Nb) based superconducting radio
frequency (SRF) cavity in a liquid helium bath is presented, taking into
account the temperature and magnetic field dependence of the surface resistance
and thermal conductivity in the superconducting state of the starting Nb
material (for SRF cavity fabrication) with different impurity levels. The drop
in SRF cavity quality factor (Q_0) in the high acceleration gradient regime
(before ultimate breakdown of the SRF cavity) is studied in details. It is
argued that the high field Q_0-drop in SRF cavity is considerably influenced by
the intrinsic material parameters such as electrical conductivity, and thermal
diffusivity. The detail analysis also shows that the current specification on
the purity of niobium material for SRF cavity fabrication is somewhat over
specified. Niobium material with a relatively low purity can very well serve
the purpose for the accelerators dedicated for spallation neutron source (SNS)
or accelerator driven sub-critical system (ADSS) applications, where the
required accelerating gradient is typically up to 20 MV/m,. This information
will have important implication towards the cost reduction of superconducting
technology based particle accelerators for various applications.
",physics
"  The interaction between proteins and DNA is a key driving force in a
significant number of biological processes such as transcriptional regulation,
repair, recombination, splicing, and DNA modification. The identification of
DNA-binding sites and the specificity of target proteins in binding to these
regions are two important steps in understanding the mechanisms of these
biological activities. A number of high-throughput technologies have recently
emerged that try to quantify the affinity between proteins and DNA motifs.
Despite their success, these technologies have their own limitations and fall
short in precise characterization of motifs, and as a result, require further
downstream analysis to extract useful and interpretable information from a
haystack of noisy and inaccurate data. Here we propose MotifMark, a new
algorithm based on graph theory and machine learning, that can find binding
sites on candidate probes and rank their specificity in regard to the
underlying transcription factor. We developed a pipeline to analyze
experimental data derived from compact universal protein binding microarrays
and benchmarked it against two of the most accurate motif search methods. Our
results indicate that MotifMark can be a viable alternative technique for
prediction of motif from protein binding microarrays and possibly other related
high-throughput techniques.
",computer-science
"  In this article we study automorphisms of Toeplitz subshifts. Such groups are
abelian and any finitely generated torsion subgroup is finite and cyclic. When
the complexity is non superlinear, we prove that the automorphism group is,
modulo a finite cyclic group, generated by a unique root of the shift. In the
subquadratic complexity case, we show that the automorphism group modulo the
torsion is generated by the roots of the shift map and that the result of the
non superlinear case is optimal. Namely, for any $\varepsilon > 0$ we construct
examples of minimal Toeplitz subshifts with complexity bounded by $C
n^{1+\epsilon}$ whose automorphism groups are not finitely generated. Finally,
we observe the coalescence and the automorphism group give no restriction on
the complexity since we provide a family of coalescent Toeplitz subshifts with
positive entropy such that their automorphism groups are arbitrary finitely
generated infinite abelian groups with cyclic torsion subgroup (eventually
restricted to powers of the shift).
",mathematics
"  This paper addresses the task of learning an image clas-sifier when some
categories are defined by semantic descriptions only (e.g. visual attributes)
while the others are defined by exemplar images as well. This task is often
referred to as the Zero-Shot classification task (ZSC). Most of the previous
methods rely on learning a common embedding space allowing to compare visual
features of unknown categories with semantic descriptions. This paper argues
that these approaches are limited as i) efficient discrimi-native classifiers
can't be used ii) classification tasks with seen and unseen categories
(Generalized Zero-Shot Classification or GZSC) can't be addressed efficiently.
In contrast , this paper suggests to address ZSC and GZSC by i) learning a
conditional generator using seen classes ii) generate artificial training
examples for the categories without exemplars. ZSC is then turned into a
standard supervised learning problem. Experiments with 4 generative models and
5 datasets experimentally validate the approach, giving state-of-the-art
results on both ZSC and GZSC.
",computer-science
"  In this paper, we report on the visualization capabilities of an Explainable
AI Planning (XAIP) agent that can support human in the loop decision making.
Imposing transparency and explainability requirements on such agents is
especially important in order to establish trust and common ground with the
end-to-end automated planning system. Visualizing the agent's internal
decision-making processes is a crucial step towards achieving this. This may
include externalizing the ""brain"" of the agent -- starting from its sensory
inputs, to progressively higher order decisions made by it in order to drive
its planning components. We also show how the planner can bootstrap on the
latest techniques in explainable planning to cast plan visualization as a plan
explanation problem, and thus provide concise model-based visualization of its
plans. We demonstrate these functionalities in the context of the automated
planning components of a smart assistant in an instrumented meeting space.
",computer-science
"  There does not exist a general positive correlation between important
life-supporting properties and the entropy production rate. The simple reason
is that nondissipative and time-symmetric kinetic aspects are also relevant for
establishing optimal functioning. In fact those aspects are even crucial in the
nonlinear regimes around equilibrium where we find biological processing on
mesoscopic scales. We make these claims specific via examples of molecular
motors, of circadian cycles and of sensory adaptation, whose performance in
some regimes is indeed spoiled by increasing the dissipated power. We use the
relation between dissipation and the amount of time-reversal breaking to keep
the discussion quantitative also in effective models where the physical entropy
production is not clearly identifiable.
",physics
"  The resolvent Krylov subspace method builds approximations to operator
functions $f(A)$ times a vector $v$. For the semigroup and related operator
functions, this method is proved to possess the favorable property that the
convergence is automatically faster when the vector $v$ is smoother. The user
of the method does not need to know the presented theory and alterations of the
method are not necessary in order to adapt to the (possibly unknown) smoothness
of $v$. The findings are illustrated by numerical experiments.
",mathematics
"  This article is the second of a pair of articles about the Goldman symplectic
form on the PSL(V )-Hitchin component. We show that any ideal triangulation on
a closed connected surface of genus at least 2, and any compatible bridge
system determine a symplectic trivialization of the tangent bundle to the
Hitchin component. Using this, we prove that a large class of flows defined in
the companion paper [SWZ17] are Hamiltonian. We also construct an explicit
collection of Hamiltonian vector fields on the Hitchin component that give a
symplectic basis at every point. These are used in the companion paper to
compute explicit global Darboux coordinates for the Hitchin component.
",mathematics
"  On the worldwide web, not only are webpages connected but source code is too.
Software development is becoming more accessible to everyone and the licensing
for software remains complicated. We need to know if software licenses are
being maintained properly throughout their reuse and evolution. This motivated
the development of the Sourcerer's Apprentice, a webservice that helps track
clone relicensing, because software typically employ software licenses to
describe how their software may be used and adapted. But most developers do not
have the legal expertise to sort out license conflicts. In this paper we put
the Apprentice to work on empirical studies that demonstrate there is much
sharing between StackOverflow code and Python modules and Python documentation
that violates the licensing of the original Python modules and documentation:
software snippets shared through StackOverflow are often being relicensed
improperly to CC-BY-SA 3.0 without maintaining the appropriate attribution. We
show that many snippets on StackOverflow are inappropriately relicensed by
StackOverflow users, jeopardizing the status of the software built by companies
and developers who reuse StackOverflow snippets.
",computer-science
"  The rise of user-contributed Open Source Software (OSS) ecosystems
demonstrate their prevalence in the software engineering discipline. Libraries
work together by depending on each other across the ecosystem. From these
ecosystems emerges a minimized library called a micro-package. Micro- packages
become problematic when breaks in a critical ecosystem dependency ripples its
effects to unsuspecting users. In this paper, we investigate the impact of
micro-packages in the npm JavaScript ecosystem. Specifically, we conducted an
empirical in- vestigation with 169,964 JavaScript npm packages to understand
(i) the widespread phenomena of micro-packages, (ii) the size dependencies
inherited by a micro-package and (iii) the developer usage cost (ie., fetch,
install, load times) of using a micro-package. Results of the study find that
micro-packages form a significant portion of the npm ecosystem. Apart from the
ease of readability and comprehension, we show that some micro-packages have
long dependency chains and incur just as much usage costs as other npm
packages. We envision that this work motivates the need for developers to be
aware of how sensitive their third-party dependencies are to critical changes
in the software ecosystem.
",computer-science
"  Observations with powerful X-ray telescopes, such as XMM-Newton and Chandra,
significantly advance our understanding of massive stars. Nearly all early-type
stars are X-ray sources. Studies of their X-ray emission provide important
diagnostics of stellar winds. High-resolution X-ray spectra of O-type stars are
well explained when stellar wind clumping is taking into account, providing
further support to a modern picture of stellar winds as non-stationary,
inhomogeneous outflows. X-ray variability is detected from such winds, on time
scales likely associated with stellar rotation. High-resolution X-ray
spectroscopy indicates that the winds of late O-type stars are predominantly in
a hot phase. Consequently, X-rays provide the best observational window to
study these winds. X-ray spectroscopy of evolved, Wolf-Rayet type, stars allows
to probe their powerful metal enhanced winds, while the mechanisms responsible
for the X-ray emission of these stars are not yet understood.
",physics
"  We consider the characterization as well as the construction of quantum codes
that allow to transmit both quantum and classical information, which we refer
to as `hybrid codes'. We construct hybrid codes $[\![n,k{: }m,d]\!]_q$ with
length $n$ and distance $d$, that simultaneously transmit $k$ qudits and $m$
symbols from a classical alphabet of size $q$. Many good codes such as
$[\![7,1{: }1,3]\!]_2$, $[\![9,2{: }2,3]\!]_2$, $[\![10,3{: }2,3]\!]_2$,
$[\![11,4{: }2,3]\!]_2$, $[\![11,1{: }2,4]\!]_2$, $[\![13,1{: }4,4]\!]_2$,
$[\![13,1{: }1,5]\!]_2$, $[\![14,1{: }2,5]\!]_2$, $[\![15,1{: }3,5]\!]_2$,
$[\![19,9{: }1,4]\!]_2$, $[\![20,9{: }2,4]\!]_2$, $[\![21,9{: }3,4]\!]_2$,
$[\![22,9{: }4,4]\!]_2$ have been found. All these codes have better parameters
than hybrid codes obtained from the best known stabilizer quantum codes.
",computer-science
"  We develop refined Strichartz estimates at $L^2$ regularity for a class of
time-dependent Schrödinger operators. Such refinements begin to
characterize the near-optimizers of the Strichartz estimate, and play a pivotal
part in the global theory of mass-critical NLS. On one hand, the harmonic
analysis is quite subtle in the $L^2$-critical setting due to an enormous group
of symmetries, while on the other hand, the spacetime Fourier analysis employed
by the existing approaches to the constant-coefficient equation are not adapted
to nontranslation-invariant situations, especially with potentials as large as
those considered in this article.
Using phase space techniques, we reduce to proving certain analogues of
(adjoint) bilinear Fourier restriction estimates. Then we extend Tao's bilinear
restriction estimate for paraboloids to more general Schrödinger operators.
As a particular application, the resulting inverse Strichartz theorem and
profile decompositions constitute a key harmonic analysis input for studying
large data solutions to the $L^2$-critical NLS with a harmonic oscillator
potential in dimensions $\ge 2$. This article builds on recent work of Killip,
Visan, and the author in one space dimension.
",mathematics
"  This article is based on a series of lectures on toric varieties given at
RIMS, Kyoto. We start by introducing toric varieties, their basic properties
and later pass to more advanced topics relating mostly to combinatorics.
",mathematics
"  Geodesic Monte Carlo (gMC) is a powerful algorithm for Bayesian inference on
non-Euclidean manifolds. The original gMC algorithm was cleverly derived in
terms of its progenitor, the Riemannian manifold Hamiltonian Monte Carlo
(RMHMC). Here, it is shown that alternative and theoretically simpler
derivations are available in which the original algorithm is a special case of
two general classes of algorithms characterized by non-trivial mass matrices.
The proposed derivations work entirely in embedding coordinates and thus
clarify the original algorithm as applied to manifolds embedded in Euclidean
space.
",statistics
"  Targeted attacks against network infrastructure are notoriously difficult to
guard against. In the case of communication networks, such attacks can leave
users vulnerable to censorship and surveillance, even when cryptography is
used. Much of the existing work on network fault-tolerance focuses on random
faults and does not apply to adversarial faults (attacks). Centralized networks
have single points of failure by definition, leading to a growing popularity in
decentralized architectures and protocols for greater fault-tolerance. However,
centralized network structure can arise even when protocols are decentralized.
Despite their decentralized protocols, the Internet and World-Wide Web have
been shown both theoretically and historically to be highly susceptible to
attack, in part due to emergent structural centralization. When single points
of failure exist, they are potentially vulnerable to non-technological (i.e.,
coercive) attacks, suggesting the importance of a structural approach to
attack-tolerance. We show how the assumption of partial trust transitivity,
while more realistic than the assumption underlying webs of trust, can be used
to quantify the effective redundancy of a network as a function of trust
transitivity. We also prove that the effective redundancy of the wrap-around
butterfly topology increases exponentially with trust transitivity and describe
a novel concurrent multipath routing algorithm for constructing paths to
utilize that redundancy. When portions of network structure can be dictated our
results can be used to create scalable, attack-tolerant infrastructures. More
generally, our results provide a theoretical formalism for evaluating the
effects of network structure on adversarial fault-tolerance.
",computer-science
"  We answer the question to what extent homotopy (co)limits in categories with
weak equivalences allow for a Fubini-type interchange law. The main obstacle is
that we do not assume our categories with weak equivalences to come equipped
with a calculus for homotopy (co)limits, such as a derivator.
",mathematics
"  Recurrent neural networks have been the dominant models for many speech and
language processing tasks. However, we understand little about the behavior and
the class of functions recurrent networks can realize. Moreover, the heuristics
used during training complicate the analyses. In this paper, we study recurrent
networks' ability to learn long-term dependency in the context of speech
recognition. We consider two decoding approaches, online and batch decoding,
and show the classes of functions to which the decoding approaches correspond.
We then draw a connection between batch decoding and a popular training
approach for recurrent networks, truncated backpropagation through time.
Changing the decoding approach restricts the amount of past history recurrent
networks can use for prediction, allowing us to analyze their ability to
remember. Empirically, we utilize long-term dependency in subphonetic states,
phonemes, and words, and show how the design decisions, such as the decoding
approach, lookahead, context frames, and consecutive prediction, characterize
the behavior of recurrent networks. Finally, we draw a connection between
Markov processes and vanishing gradients. These results have implications for
studying the long-term dependency in speech data and how these properties are
learned by recurrent networks.
",computer-science
"  The design of multi-stable RNA molecules has important applications in
biology, medicine, and biotechnology. Synthetic design approaches profit
strongly from effective in-silico methods, which can tremendously impact their
cost and feasibility. We revisit a central ingredient of most in-silico design
methods: the sampling of sequences for the design of multi-target structures,
possibly including pseudoknots. For this task, we present the efficient, tree
decomposition-based algorithm. Our fixed parameter tractable approach is
underpinned by establishing the P-hardness of uniform sampling. Modeling the
problem as a constraint network, our program supports generic
Boltzmann-weighted sampling for arbitrary additive RNA energy models; this
enables the generation of RNA sequences meeting specific goals like expected
free energies or \GCb-content. Finally, we empirically study general properties
of the approach and generate biologically relevant multi-target
Boltzmann-weighted designs for a common design benchmark. Generating seed
sequences with our program, we demonstrate significant improvements over the
previously best multi-target sampling strategy (uniform sampling).Our software
is freely available at: this https URL .
",quantitative-biology
"  In this study, we provide mathematical and practice-driven justification for
using $[0,1]$ normalization of inconsistency indicators in pairwise
comparisons. The need for normalization, as well as problems with the lack of
normalization, are presented. A new type of paradox of infinity is described.
",computer-science
"  In this paper, we deform the thermodynamics of a BTZ black hole from rainbow
functions in gravity's rainbow. The rainbow functions will be motivated from
results in loop quantum gravity and Noncommutative geometry. It will be
observed that the thermodynamics gets deformed due to these rainbow functions,
indicating the existence of a remnant. However, the Gibbs free energy does not
get deformed due to these rainbow functions, and so the critical behaviour from
Gibbs does not change by this deformation.This is because the deformation in
the entropy cancel's out the temperature deformation.
",physics
"  Fully realizing the potential of acceleration for Deep Neural Networks (DNNs)
requires understanding and leveraging algorithmic properties. This paper builds
upon the algorithmic insight that bitwidth of operations in DNNs can be reduced
without compromising their classification accuracy. However, to prevent
accuracy loss, the bitwidth varies significantly across DNNs and it may even be
adjusted for each layer. Thus, a fixed-bitwidth accelerator would either offer
limited benefits to accommodate the worst-case bitwidth requirements, or lead
to a degradation in final accuracy. To alleviate these deficiencies, this work
introduces dynamic bit-level fusion/decomposition as a new dimension in the
design of DNN accelerators. We explore this dimension by designing Bit Fusion,
a bit-flexible accelerator, that constitutes an array of bit-level processing
elements that dynamically fuse to match the bitwidth of individual DNN layers.
This flexibility in the architecture enables minimizing the computation and the
communication at the finest granularity possible with no loss in accuracy. We
evaluate the benefits of BitFusion using eight real-world feed-forward and
recurrent DNNs. The proposed microarchitecture is implemented in Verilog and
synthesized in 45 nm technology. Using the synthesis results and cycle accurate
simulation, we compare the benefits of Bit Fusion to two state-of-the-art DNN
accelerators, Eyeriss and Stripes. In the same area, frequency, and process
technology, BitFusion offers 3.9x speedup and 5.1x energy savings over Eyeriss.
Compared to Stripes, BitFusion provides 2.6x speedup and 3.9x energy reduction
at 45 nm node when BitFusion area and frequency are set to those of Stripes.
Scaling to GPU technology node of 16 nm, BitFusion almost matches the
performance of a 250-Watt Titan Xp, which uses 8-bit vector instructions, while
BitFusion merely consumes 895 milliwatts of power.
",computer-science
"  We present the DRYVR framework for verifying hybrid control systems that are
described by a combination of a black-box simulator for trajectories and a
white-box transition graph specifying mode switches. The framework includes (a)
a probabilistic algorithm for learning sensitivity of the continuous
trajectories from simulation data, (b) a bounded reachability analysis
algorithm that uses the learned sensitivity, and (c) reasoning techniques based
on simulation relations and sequential composition, that enable verification of
complex systems under long switching sequences, from the reachability analysis
of a simpler system under shorter sequences. We demonstrate the utility of the
framework by verifying a suite of automotive benchmarks that include powertrain
control, automatic transmission, and several autonomous and ADAS features like
automatic emergency braking, lane-merge, and auto-passing controllers.
",computer-science
"  We conduct a comprehensive set of tests of performance of surface coils used
for nuclear magnetic resonance (NMR) study of quasi 2-dimensional samples. We
report ${^{115} \rm{In}}$ and ${^{31} \rm{P}}$ NMR measurements on InP,
semi-conducting thin substrate samples. Surface coils of both zig-zag
meander-line and concentric spiral geometries were used. We compare reception
sensitivity and signal-to-noise ratio (SNR) of NMR signal obtained by using
surface-type coils to that obtained by standard solenoid-type coils. As
expected, we find that surface-type coils provide better sensitivity for NMR
study of thin films samples. Moreover, we compare the reception sensitivity of
different types of the surface coils. We identify the optimal geometry of the
surface coils for a given application and/or direction of the applied magnetic
field.
",physics
"  Blind deconvolution is a ubiquitous problem of recovering two unknown signals
from their convolution. Unfortunately, this is an ill-posed problem in general.
This paper focuses on the {\em short and sparse} blind deconvolution problem,
where the one unknown signal is short and the other one is sparsely and
randomly supported. This variant captures the structure of the unknown signals
in several important applications. We assume the short signal to have unit
$\ell^2$ norm and cast the blind deconvolution problem as a nonconvex
optimization problem over the sphere. We demonstrate that (i) in a certain
region of the sphere, every local optimum is close to some shift truncation of
the ground truth, and (ii) for a generic short signal of length $k$, when the
sparsity of activation signal $\theta\lesssim k^{-2/3}$ and number of
measurements $m\gtrsim poly(k)$, a simple initialization method together with a
descent algorithm which escapes strict saddle points recovers a near shift
truncation of the ground truth kernel.
",statistics
"  We analyze the motion of a rod floating in a weightless environment in space
when a force is applied at some point on the rod in a direction perpendicular
to its length. If the force applied is at the centre of mass, then the rod gets
a linear motion perpendicular to its length. However, if the same force is
applied at a point other than the centre of mass, say, near one end of the rod,
thereby giving rise to a torque, then there will also be a rotation of the rod
about its centre of mass, in addition to the motion of the centre of mass
itself. If the force applied is for a very short duration, but imparting
nevertheless a finite impulse, like in a sudden (quick) hit at one end of the
rod, then the centre of mass will move with a constant linear speed and
superimposed on it will be a rotation of the rod with constant angular speed
about the centre of mass. However, if force is applied continuously, say by
strapping a tiny rocket at one end of the rod, then the rod will spin faster
and faster about the centre of mass, with angular speed increasing linearly
with time. As the direction of the applied force, as seen by an external
(inertial) observer, will be changing continuously with the rotation of the
rod, the acceleration of the centre of mass would also be not in one fixed
direction. However, it turns out that the locus of the velocity vector of the
centre of mass will describe a Cornu spiral, with the velocity vector reaching
a final constant value with time. The mean motion of the centre of mass will be
in a straight line, with superposed initial oscillations that soon die down.
",physics
"  High-order harmonic generation (HHG) from aligned acetylene molecules
interacting with mid infra-red (IR), linearly polarized laser pulses is studied
theoretically using a mixed quantum-classical approach in which the electrons
are described using time-dependent density functional theory while the ions are
treated classically. We find that for molecules aligned perpendicular to the
laser polarization axis, HHG arises from the highest-occupied molecular orbital
(HOMO) while for molecules aligned along the laser polarization axis, HHG is
dominated by the HOMO-1. In the parallel orientation we observe a double
plateau with an inner plateau that is produced by ionization from and
recombination back to an autoionizing state. Two pieces of evidence support
this idea. Firstly, by choosing a suitably tuned vacuum ultraviolet pump pulse
that directly excites the autoionizing state we observe a dramatic enhancement
of all harmonics in the inner plateau. Secondly, in certain circumstances, the
position of the inner plateau cut-off does not agree with the classical
three-step model. We show that this discrepancy can be understood in terms of a
minimum in the dipole recombination matrix element from the continuum to the
autoionizing state. As far as we are aware, this represents the first
observation of harmonic enhancement over a wide range of frequencies arising
from autoionizing states in molecules.
",physics
"  With the aim of getting closer to the performance of the animal
muscleskeletal system, elastic elements are purposefully introduced in the
mechanical structure of soft robots. Indeed, previous works have extensively
shown that elasticity can endow robots with the ability of performing tasks
with increased efficiency, peak performances, and mechanical robustness.
However, despite the many achievements, a general theory of efficient motions
in soft robots is still lacking. Most of the literature focuses on specific
examples, or imposes a prescribed behavior through dynamic cancellations, thus
defeating the purpose of introducing elasticity in the first place. This paper
aims at making a step towards establishing such a general framework. To this
end, we leverage on the theory of oscillations in nonlinear dynamical systems,
and we take inspiration from state of the art theories about how the human
central nervous system manages the muscleskeletal system. We propose to
generate regular and efficient motions in soft robots by stabilizing
sub-manifolds of the state space on which the system would naturally evolve. We
select these sub-manifolds as the nonlinear continuation of linear eigenspaces,
called nonlinear normal modes. In such a way, efficient oscillatory behaviors
can be excited. We show the effectiveness of the methods in simulations on an
elastic inverted pendulum, and experimentally on a segmented elastic leg.
",computer-science
"  By detecting light from extrasolar planets,we can measure their compositions
and bulk physical properties. The technologies used to make these measurements
are still in their infancy, and a lack of self-consistency suggests that
previous observations have underestimated their systemic errors.We demonstrate
a statistical method, newly applied to exoplanet characterization, which uses a
Bayesian formalism to account for underestimated errorbars. We use this method
to compare photometry of a substellar companion, GJ 758b, with custom
atmospheric models. Our method produces a probability distribution of
atmospheric model parameters including temperature, gravity, cloud model
(fsed), and chemical abundance for GJ 758b. This distribution is less sensitive
to highly variant data, and appropriately reflects a greater uncertainty on
parameter fits.
",physics
"  Using low-temperature Magnetic Force Microscopy (MFM) we provide direct
experimental evidence for spontaneous vortex phase (SVP) formation in
EuFe$_2$(As$_{0.79}$P$_{0.21}$)$_2$ single crystal with the superconducting
$T^{\rm 0}_{\rm SC}=23.6$~K and ferromagnetic $T_{\rm FM}\sim17.7$~K transition
temperatures. Spontaneous vortex-antivortex (V-AV) pairs are imaged in the
vicinity of $T_{\rm FM}$. Also, upon cooling cycle near $T_{\rm FM}$ we observe
the first-order transition from the short period domain structure, which
appears in the Meissner state, into the long period domain structure with
spontaneous vortices. It is the first experimental observation of this scenario
in the ferromagnetic superconductors. Low-temperature phase is characterized by
much larger domains in V-AV state and peculiar branched striped structures at
the surface, which are typical for uniaxial ferromagnets with perpendicular
magnetic anisotropy (PMA). The domain wall parameters at various temperatures
are estimated.
",physics
"  We evolve binary mux-6 trees for up to 100000 generations evolving some
programs with more than a hundred million nodes. Our unbounded Long-Term
Evolution Experiment LTEE GP appears not to evolve building blocks but does
suggests a limit to bloat. We do see periods of tens even hundreds of
generations where the population is 100 percent functionally converged. The
distribution of tree sizes is not as predicted by theory.
",computer-science
"  We examine the possibility of a dark matter (DM) contribution to the recently
observed gamma-ray spectrum seen in the M31 galaxy. In particular, we apply
limits on Weakly Interacting Massive Particle DM annihilation cross-sections
derived from the Coma galaxy cluster and the Reticulum II dwarf galaxy to
determine the maximal flux contribution by DM annihilation to both the M31
gamma-ray spectrum and that of the Milky-Way galactic centre. We limit the
energy range between 1 and 12 GeV in M31 and galactic centre spectra due to the
limited range of former's data, as well as to encompass the high-energy
gamma-ray excess observed in the latter target. In so doing, we will make use
of Fermi-LAT data for all mentioned targets, as well as diffuse radio data for
the Coma cluster. The multi-target strategy using both Coma and Reticulum II to
derive cross-section limits, as well as multi-frequency data, ensures that our
results are robust against the various uncertainties inherent in modelling of
indirect DM emissions.
Our results indicate that, when a Navarro-Frenk-White (or shallower) radial
density profile is assumed, severe constraints can be imposed upon the fraction
of the M31 and galactic centre spectra that can be accounted for by DM, with
the best limits arising from cross-section constraints from Coma radio data and
Reticulum II gamma-ray limits. These particular limits force all the studied
annihilation channels to contribute 1% or less to the total integrated
gamma-ray flux within both M31 and galactic centre targets. In contrast,
considerably more, 10-100%, of the flux can be attributed to DM when a
contracted Navarro-Frenk-White profile is assumed. This demonstrates how
sensitive DM contributions to gamma-ray emissions are to the possibility of
cored profiles in galaxies.
",physics
"  In this manuscript, we generalize F-calculus to apply it on fractal Tartan
spaces. The generalized standard F-calculus is used to obtain the integral and
derivative of the functions on the fractal Tartan with different dimensions.
The generalized fractional derivatives have local properties that make it more
useful in modelling physical problems. The illustrative examples are used to
present the details.
",mathematics
"  Novel data acquisition schemes have been an emerging need for scanning
microscopy based imaging techniques to reduce the time in data acquisition and
to minimize probing radiation in sample exposure. Varies sparse sampling
schemes have been studied and are ideally suited for such applications where
the images can be reconstructed from a sparse set of measurements. Dynamic
sparse sampling methods, particularly supervised learning based iterative
sampling algorithms, have shown promising results for sampling pixel locations
on the edges or boundaries during imaging. However, dynamic sampling for
imaging skeleton-like objects such as metal dendrites remains difficult. Here,
we address a new unsupervised learning approach using Hierarchical Gaussian
Mixture Mod- els (HGMM) to dynamically sample metal dendrites. This technique
is very useful if the users are interested in fast imaging the primary and
secondary arms of metal dendrites in solidification process in materials
science.
",statistics
"  In the past few years, an action of $\mathrm{PGL}_2(\mathbb F_q)$ on the set
of irreducible polynomials in $\mathbb F_q[x]$ has been introduced and many
questions have been discussed, such as the characterization and number of
invariant elements. In this paper, we analyze some recent works on this action
and provide full generalizations of them, yielding final theoretical results on
the characterization and number of invariant elements.
",mathematics
"  The New Horizons spacecraft's nominal trajectory crosses the planet's
satellite plane at $\sim 10,000\ \rm{km}$ from the barycenter, between the
orbits of Pluto and Charon. I have investigated the risk to the spacecraft
based on observational limits of rings and dust within this region, assuming
various particle size distributions. The best limits are placed by 2011 and
2012 HST observations, which significantly improve on the limits from stellar
occultations, although they do not go as close to the planet. From the HST data
and assuming a `reasonable worst case' for the size distribution, we place a
limit of $N < 20$ damaging impacts by grains of radius $> 0.2\ \textrm{mm}$
onto the spacecraft during the encounter. The number of hits is $\approx$
200$\times$ above the NH mission requirement, and $\approx$ $2000\times$ above
the mission's desired level. Stellar occultations remain valuable because they
are able to measure $N$ closer to the Pluto surface than direct imaging,
although with a sensitivity limit several orders of magnitude higher than that
from HST imaging. Neither HST nor occultations are sensitive enough to place
limits on $N$ at or below the mission requirements.
",physics
"  Weak gravitational lensing alters the apparent separations between observed
sources, potentially affecting clustering statistics. We derive a general
expression for the lensing deflection which is valid for any three-point
statistic, and investigate its effect on the three-point clustering correlation
function. We find that deflection of the clustering correlation function is
greatest at around $z=2$. It is most prominent in regions where the correlation
function varies rapidly, in particular at the baryon acoustic oscillation scale
where it smooths out the peaks and troughs, reducing the peak-to-trough
difference by about 0.1 percent at $z=1$ and around 2.3 percent at $z=10$. The
modification due to lensing deflection is typically at the per cent level of
the expected errors in a Euclid-like survey and therefore undetectable.
",physics
"  Let $S$ be the power series ring or the polynomial ring over a field $K$ in
the variables $x_1,\ldots,x_n$, and let $R=S/I$, where $I$ is proper ideal
which we assume to be graded if $S$ is the polynomial ring. We give an explicit
description of the cycles of the Koszul complex whose homology classes generate
the Koszul homology of $R=S/I$ with respect to $x_1,\ldots,x_n$. The
description is given in terms of the data of the free $S$-resolution of $R$.
The result is used to determine classes of Golod ideals, among them proper
ordinary powers and proper symbolic powers of monomial ideals. Our theory is
also applied to stretched local rings.
",mathematics
"  In this Review we will study rigorously the notion of mixed states and their
density matrices. We mostly give complete proofs. We will also discuss the
quantum-mechanical consequences of possible variations of Planck's constant h.
This Review has been written having in mind two readerships: mathematical
physicists and quantum physicists. The mathematical rigor is maximal, but the
language and notation we use throughout should be familiar to physicists.
",mathematics
"  Headline generation is a special type of text summarization task. While the
amount of available training data for this task is almost unlimited, it still
remains challenging, as learning to generate headlines for news articles
implies that the model has strong reasoning about natural language. To overcome
this issue, we applied recent Universal Transformer architecture paired with
byte-pair encoding technique and achieved new state-of-the-art results on the
New York Times Annotated corpus with ROUGE-L F1-score 24.84 and ROUGE-2
F1-score 13.48. We also present the new RIA corpus and reach ROUGE-L F1-score
36.81 and ROUGE-2 F1-score 22.15 on it.
",computer-science
"  Incentivized advertising is a new ad format that is gaining popularity in
digital mobile advertising. In incentivized advertising, the publisher rewards
users for watching an ad. An endemic issue here is adverse selection, where
reward-seeking users select into incentivized ad placements to obtain rewards.
Adverse selection reduces the publisher's ad profit as well as poses a
difficulty to causal inference of the effectiveness of incentivized
advertising. To this end, we develop a treatment effect model that allows and
controls for unobserved adverse selection, and estimate the model using data
from a mobile gaming app that offers both incentivized and non-incentivized
ads. We find that rewarding users to watch an ad has an overall positive effect
on the ad conversion rate. A user is 27% more likely to convert when being
rewarded to watch an ad. However there is a negative offsetting effect that
reduces the effectiveness of incentivized ads. Some users are averse to delayed
rewards, they prefer to collect their rewards immediately after watching the
incentivized ads, instead of pursuing the content of the ads further. For the
subset of users who are averse to delayed rewards, the treatment effect is only
13%, while it can be as high as 47% for other users.
",statistics
"  We present the 2017 DAVIS Challenge on Video Object Segmentation, a public
dataset, benchmark, and competition specifically designed for the task of video
object segmentation. Following the footsteps of other successful initiatives,
such as ILSVRC and PASCAL VOC, which established the avenue of research in the
fields of scene classification and semantic segmentation, the DAVIS Challenge
comprises a dataset, an evaluation methodology, and a public competition with a
dedicated workshop co-located with CVPR 2017. The DAVIS Challenge follows up on
the recent publication of DAVIS (Densely-Annotated VIdeo Segmentation), which
has fostered the development of several novel state-of-the-art video object
segmentation techniques. In this paper we describe the scope of the benchmark,
highlight the main characteristics of the dataset, define the evaluation
metrics of the competition, and present a detailed analysis of the results of
the participants to the challenge.
",computer-science
"  Location-based augmented reality games have entered the mainstream with the
nearly overnight success of Niantic's Pokémon Go. Unlike traditional video
games, the fact that players of such games carry out actions in the external,
physical world to accomplish in-game objectives means that the large-scale
adoption of such games motivate people, en masse, to do things and go places
they would not have otherwise done in unprecedented ways. The social
implications of such mass-mobilisation of individual players are, in general,
difficult to anticipate or characterise, even for the short-term. In this work,
we focus on disaster relief, and the short- and long-term implications that a
proliferation of AR games like Pokémon Go, may have in disaster-prone regions
of the world. We take a distributed cognition approach and focus on one natural
disaster-prone region of New Zealand, the city of Wellington.
",computer-science
"  Giant vortices with higher phase-winding than $2\pi$ are usually
energetically unfavorable, but geometric symmetry constraints on a
superconductor in a magnetic field are known to stabilize such objects. Here,
we show via microscopic calculations that giant vortices can appear in
intrinsically non-superconducting materials, even without any applied magnetic
field. The enabling mechanism is the proximity effect to a host superconductor
where a current flows, and we also demonstrate that antivortices can appear in
this setup. Our results open the possibility to study electrically controllable
topological defects in unusual environments, which do not have to be exposed to
magnetic fields or intrinsically superconducting, but instead display other
types of order.
",physics
"  These notes constitute chapter 7 from ""l'Ecole de Physique des Houches""
Session CIII, August 2014 dedicated to Topological Aspects of Condensed matter
physics. The tenfold way in quasi-one-dimensional space is presented. The
method of chiral Abelian bosonization is reviewed. It is then applied to the
stability analysis for the edge theory in symmetry class AII, and for the
construction of two-dimensional topological phases from coupled wires.
",physics
"  Motivation: Although there is a rich literature on methods for assessing the
impact of functional predictors, the focus has been on approaches for dimension
reduction that can fail dramatically in certain applications. Examples of
standard approaches include functional linear models, functional principal
components regression, and cluster-based approaches, such as latent trajectory
analysis. This article is motivated by applications in which the dynamics in a
predictor, across times when the value is relatively extreme, are particularly
informative about the response. For example, physicians are interested in
relating the dynamics of blood pressure changes during surgery to post-surgery
adverse outcomes, and it is thought that the dynamics are more important when
blood pressure is significantly elevated or lowered.
Methods: We propose a novel class of extrema-weighted feature (XWF)
extraction models. Key components in defining XWFs include the marginal density
of the predictor, a function up-weighting values at high quantiles of this
marginal, and functionals characterizing local dynamics. Algorithms are
proposed for fitting of XWF-based regression and classification models, and are
compared with current methods for functional predictors in simulations and a
blood pressure during surgery application.
Results: XWFs find features of intraoperative blood pressure trajectories
that are predictive of postoperative mortality. By their nature, most of these
features cannot be found by previous methods.
",statistics
"  Conditional term rewriting is an intuitive yet complex extension of term
rewriting. In order to benefit from the simpler framework of unconditional
rewriting, transformations have been defined to eliminate the conditions of
conditional term rewrite systems.
Recent results provide confluence criteria for conditional term rewrite
systems via transformations, yet they are restricted to CTRSs with certain
syntactic properties like weak left-linearity. These syntactic properties imply
that the transformations are sound for the given CTRS.
This paper shows how to use transformations to prove confluence of
operationally terminating, right-stable deterministic conditional term rewrite
systems without the necessity of soundness restrictions. For this purpose, it
is shown that certain rewrite strategies, in particular almost U-eagerness and
innermost rewriting, always imply soundness.
",computer-science
"  School bus planning is usually divided into routing and scheduling due to the
complexity of solving them concurrently. However, the separation between these
two steps may lead to worse solutions with higher overall costs than that from
solving them together. When finding the minimal number of trips in the routing
problem, neglecting the importance of trip compatibility may increase the
number of buses actually needed in the scheduling problem. This paper proposes
a new formulation for the multi-school homogeneous fleet routing problem that
maximizes trip compatibility while minimizing total travel time. This
incorporates the trip compatibility for the scheduling problem in the routing
problem. Since the problem is inherently just a routing problem, finding a good
solution is not cumbersome. To compare the performance of the model with
traditional routing problems, we generate eight mid-size data sets. Through
importing the generated trips of the routing problems into the bus scheduling
(blocking) problem, it is shown that the proposed model uses up to 13% fewer
buses than the common traditional routing models.
",computer-science
"  We show that the two-weight estimate for the dyadic square function proved by
Lacey--Li in [2] is sharp.
",mathematics
"  This paper presents a distance-based discriminative framework for learning
with probability distributions. Instead of using kernel mean embeddings or
generalized radial basis kernels, we introduce embeddings based on
dissimilarity of distributions to some reference distributions denoted as
templates. Our framework extends the theory of similarity of Balcan et al.
(2008) to the population distribution case and we show that, for some learning
problems, some dissimilarity on distribution achieves low-error linear decision
functions with high probability. Our key result is to prove that the theory
also holds for empirical distributions. Algorithmically, the proposed approach
consists in computing a mapping based on pairwise dissimilarity where learning
a linear decision function is amenable. Our experimental results show that the
Wasserstein distance embedding performs better than kernel mean embeddings and
computing Wasserstein distance is far more tractable than estimating pairwise
Kullback-Leibler divergence of empirical distributions.
",statistics
"  Time series, as frequently the case in neuroscience, are rarely stationary,
but often exhibit abrupt changes due to attractor transitions or bifurcations
in the dynamical systems producing them. A plethora of methods for detecting
such change points in time series statistics have been developed over the
years, in addition to test criteria to evaluate their significance. Issues to
consider when developing change point analysis methods include computational
demands, difficulties arising from either limited amount of data or a large
number of covariates, and arriving at statistical tests with sufficient power
to detect as many changes as contained in potentially high-dimensional time
series. Here, a general method called Paired Adaptive Regressors for Cumulative
Sum is developed for detecting multiple change points in the mean of
multivariate time series. The method's advantages over alternative approaches
are demonstrated through a series of simulation experiments. This is followed
by a real data application to neural recordings from rat medial prefrontal
cortex during learning. Finally, the method's flexibility to incorporate useful
features from state-of-the-art change point detection techniques is discussed,
along with potential drawbacks and suggestions to remedy them.
",quantitative-biology
"  In deep learning, performance is strongly affected by the choice of
architecture and hyperparameters. While there has been extensive work on
automatic hyperparameter optimization for simple spaces, complex spaces such as
the space of deep architectures remain largely unexplored. As a result, the
choice of architecture is done manually by the human expert through a slow
trial and error process guided mainly by intuition. In this paper we describe a
framework for automatically designing and training deep models. We propose an
extensible and modular language that allows the human expert to compactly
represent complex search spaces over architectures and their hyperparameters.
The resulting search spaces are tree-structured and therefore easy to traverse.
Models can be automatically compiled to computational graphs once values for
all hyperparameters have been chosen. We can leverage the structure of the
search space to introduce different model search algorithms, such as random
search, Monte Carlo tree search (MCTS), and sequential model-based optimization
(SMBO). We present experiments comparing the different algorithms on CIFAR-10
and show that MCTS and SMBO outperform random search. In addition, these
experiments show that our framework can be used effectively for model
discovery, as it is possible to describe expressive search spaces and discover
competitive models without much effort from the human expert. Code for our
framework and experiments has been made publicly available.
",statistics
"  We present DeepPicar, a low-cost deep neural network based autonomous car
platform. DeepPicar is a small scale replication of a real self-driving car
called DAVE-2 by NVIDIA. DAVE-2 uses a deep convolutional neural network (CNN),
which takes images from a front-facing camera as input and produces car
steering angles as output. DeepPicar uses the same network architecture---9
layers, 27 million connections and 250K parameters---and can drive itself in
real-time using a web camera and a Raspberry Pi 3 quad-core platform. Using
DeepPicar, we analyze the Pi 3's computing capabilities to support end-to-end
deep learning based real-time control of autonomous vehicles. We also
systematically compare other contemporary embedded computing platforms using
the DeepPicar's CNN-based real-time control workload. We find that all tested
platforms, including the Pi 3, are capable of supporting the CNN-based
real-time control, from 20 Hz up to 100 Hz, depending on hardware platform.
However, we find that shared resource contention remains an important issue
that must be considered in applying CNN models on shared memory based embedded
computing platforms; we observe up to 11.6X execution time increase in the CNN
based control loop due to shared resource contention. To protect the CNN
workload, we also evaluate state-of-the-art cache partitioning and memory
bandwidth throttling techniques on the Pi 3. We find that cache partitioning is
ineffective, while memory bandwidth throttling is an effective solution.
",computer-science
"  Motivation: The scratch assay is a standard experimental protocol used to
characterize cell migration. It can be used to identify genes that regulate
migration and evaluate the efficacy of potential drugs that inhibit cancer
invasion. In these experiments, a scratch is made on a cell monolayer and
recolonisation of the scratched region is imaged to quantify cell migration
rates. A drawback of this methodology is the lack of its reproducibility
resulting in irregular cell-free areas with crooked leading edges. Existing
quantification methods deal poorly with such resulting irregularities present
in the data. Results: We introduce a new quantification method that can analyse
low quality experimental data. By considering in-silico and in-vitro data, we
show that the method provides a more accurate statistical classification of the
migration rates than two established quantification methods. The application of
this method will enable the quantification of migration rates of scratch assay
data previously unsuitable for analysis. Availability and Implementation: The
source code and the implementation of the algorithm as a GUI along with an
example dataset and user instructions, are available in
this https URL.
The datasets are available in
this https URL.
",quantitative-biology
"  In this paper, we obtain a class of Virasoro modules by taking tensor
products of the irreducible Virasoro modules $\Omega(\lambda,\alpha,h)$ defined
in \cite{CG}, with irreducible highest weight modules $V(\theta,h)$ or with
irreducible Virasoro modules Ind$_{\theta}(N)$ defined in \cite{MZ2}. We obtain
the necessary and sufficient conditions for such tensor product modules to be
irreducible, and determine the necessary and sufficient conditions for two of
them to be isomorphic. These modules are not isomorphic to any other known
irreducible Virasoro modules.
",mathematics
"  The development of needle-free injection systems utilizing high-speed
microjets is of great importance to world healthcare. It is thus crucial to
control the microjets, which are often induced by underwater shock waves. In
this contribution from fluid-mechanics point of view, we experimentally
investigate the effect of a shock wave on the velocity of a free surface
(microjet) and underwater cavitation onset in a microchannel, focusing on the
pressure impulse and peak pressure of the shock wave. The shock wave used had a
non-spherically-symmetric peak pressure distribution and a spherically
symmetric pressure impulse distribution [Tagawa et al., J. Fluid Mech., 2016,
808, 5-18]. First, we investigate the effect of the shock wave on the jet
velocity by installing a narrow tube and a hydrophone in different
configurations in a large water tank, and measuring the shock wave pressure and
the jet velocity simultaneously. The results suggest that the jet velocity
depends only on the pressure impulse of the shock wave. We then investigate the
effect of the shock wave on the cavitation onset by taking measurements in an
L-shaped microchannel. The results suggest that the probability of cavitation
onset depends only on the peak pressure of the shock wave. In addition, the jet
velocity varies according to the presence or absence of cavitation. The above
findings provide new insights for advancing a control method for high-speed
microjets.
",physics
"  In order to handle intense time pressure and survive in dynamic market,
software startups have to make crucial decisions constantly on whether to
change directions or stay on chosen courses, or in the terms of Lean Startup,
to pivot or to persevere. The existing research and knowledge on software
startup pivots are very limited. In this study, we focused on understanding the
pivoting processes of software startups, and identified the triggering factors
and pivot types. To achieve this, we employed a multiple case study approach,
and analyzed the data obtained from four software startups. The initial
findings show that different software startups make different types of pivots
related to business and technology during their product development life cycle.
The pivots are triggered by various factors including negative customer
feedback.
",computer-science
"  Many of the baryons in our Galaxy probably lie outside the well known disk
and bulge components. Despite a wealth of evidence for the presence of some gas
in galactic halos, including absorption line systems in the spectra of quasars,
high velocity neutral hydrogen clouds in our Galaxy halo, line emitting ionised
hydrogen originating from galactic winds in nearby starburst galaxies, and the
X-ray coronas surrounding the most massive galaxies, accounting for the gas in
the halo of any galaxy has been observationally challenging primarily because
of its low density in the expansive halo. The most sensitive measurements come
from detecting absorption by the intervening gas in the spectra of distant
objects such as quasars or distant halo stars, but these have typically been
limited to a few lines of sight to sufficiently bright objects. Massive
spectroscopic surveys of millions of objects provide an alternative approach to
the problem. Here, we present the first evidence for a widely distributed,
neutral, excited hydrogen component of the Galaxy's halo. It is observed as the
slight, (0.779 $\pm$ 0.006)\%, absorption of flux near the rest wavelength of
H$\alpha$ in the combined spectra of hundreds of thousands of galaxy spectra
and is ubiquitous in high latitude lines of sight. This observation provides an
avenue to tracing, both spatially and kinematically, the majority of the gas in
the halo of our Galaxy.
",physics
"  Timelimited functions and bandlimited functions play a fundamental role in
signal and image processing. But by the uncertainty principles, a signal cannot
be simultaneously time and bandlimited. A natural assumption is thus that a
signal is almost time and almost bandlimited. The aim of this paper is to prove
that the set of almost time and almost bandlimited signals is not excluded from
the uncertainty principles. The transforms under consideration are integral
operators with bounded kernels for which there is a Parseval Theorem. Then we
define the wavelet multipliers for this class of operators, and study their
boundedness and Schatten class properties. We show that the wavelet multiplier
is unitary equivalent to a scalar multiple of the phase space restriction
operator. Moreover we prove that a signal which is almost time and almost
bandlimited can be approximated by its projection on the span of the first
eigenfunctions of the phase space restriction operator, corresponding to the
largest eigenvalues which are close to one.
",mathematics
"  We propose an automatic diabetic retinopathy (DR) analysis algorithm based on
two-stages deep convolutional neural networks (DCNN). Compared to existing
DCNN-based DR detection methods, the proposed algorithm have the following
advantages: (1) Our method can point out the location and type of lesions in
the fundus images, as well as giving the severity grades of DR. Moreover, since
retina lesions and DR severity appear with different scales in fundus images,
the integration of both local and global networks learn more complete and
specific features for DR analysis. (2) By introducing imbalanced weighting map,
more attentions will be given to lesion patches for DR grading, which
significantly improve the performance of the proposed algorithm. In this study,
we label 12,206 lesion patches and re-annotate the DR grades of 23,595 fundus
images from Kaggle competition dataset. Under the guidance of clinical
ophthalmologists, the experimental results show that our local lesion detection
net achieve comparable performance with trained human observers, and the
proposed imbalanced weighted scheme also be proved to significantly improve the
capability of our DCNN-based DR grading algorithm.
",computer-science
"  Recently, cloud storage and processing have been widely adopted. Mobile users
in one family or one team may automatically backup their photos to the same
shared cloud storage space. The powerful face detector trained and provided by
a 3rd party may be used to retrieve the photo collection which contains a
specific group of persons from the cloud storage server. However, the privacy
of the mobile users may be leaked to the cloud server providers. In the
meanwhile, the copyright of the face detector should be protected. Thus, in
this paper, we propose a protocol of privacy preserving face retrieval in the
cloud for mobile users, which protects the user photos and the face detector
simultaneously. The cloud server only provides the resources of storage and
computing and can not learn anything of the user photos and the face detector.
We test our protocol inside several families and classes. The experimental
results reveal that our protocol can successfully retrieve the proper photos
from the cloud server and protect the user photos and the face detector.
",computer-science
"  This paper presents an investigation of the relation between some positivity
of the curvature and the finiteness of fundamental groups in semi-Riemannian
geometry. We consider semi-Riemannian submersions $\pi : (E, g) \rightarrow (B,
-g_{B}) $ under the condition with $(B, g_{B})$ Riemannian, the fiber closed
Riemannian, and the horizontal distribution integrable. Then we prove that, if
the lightlike geodesically complete or timelike geodesically complete
semi-Riemannian manifold $E$ has some positivity of curvature, then the
fundamental group of the fiber is finite. Moreover we construct an example of
semi-Riemannian submersions with some positivity of curvature, non-integrable
horizontal distribution, and the finiteness of the fundamental group of the
fiber.
",mathematics
"  We adapt the well-known spectral decimation technique for computing spectra
of Laplacians on certain symmetric self-similar sets to the case of magnetic
Schrodinger operators and work through this method completely for the diamond
lattice fractal. This connects results of physicists from the 1980's, who used
similar techniques to compute spectra of sequences of magnetic operators on
graph approximations to fractals but did not verify existence of a limiting
fractal operator, to recent work describing magnetic operators on fractals via
functional analytic techniques.
",mathematics
"  In this paper, we extend the Atiyah--Guillemin--Sternberg convexity theorem
and Delzant's classification of symplectic toric manifolds to presymplectic
manifolds. We also define and study the Morita equivalence of presymplectic
toric manifolds and of their corresponding framed momentum polytopes, which may
be rational or non-rational. Toric orbifolds, quasifolds and non-commutative
toric varieties may be viewed as the quotient of our presymplectic toric
manifolds by the kernel isotropy foliation of the presymplectic form.
",mathematics
"  Let $x\geq 1$ be a large number, and let $1 \leq a <q $ be integers such that
$\gcd(a,q)=1$ and $q=O(\log^c)$ with $c>0$ constant. This note proves that the
counting function for the number of primes $p \in \{p=qn+a: n \geq1 \}$ with a
fixed primitive root $u\ne \pm 1, v^2$ has the asymptotic formula
$\pi_u(x,q,a)=\delta(u,q,a)x/ \log x +O(x/\log^b x),$ where $\delta(u,q,a)>0$
is the density, and $b>c+1$ is a constant.
",mathematics
"  Noise is an inherent part of neuronal dynamics, and thus of the brain. It can
be observed in neuronal activity at different spatiotemporal scales, including
in neuronal membrane potentials, local field potentials,
electroencephalography, and magnetoencephalography. A central research topic in
contemporary neuroscience is to elucidate the functional role of noise in
neuronal information processing. Experimental studies have shown that a
suitable level of noise may enhance the detection of weak neuronal signals by
means of stochastic resonance. In response, theoretical research, based on the
theory of stochastic processes, nonlinear dynamics, and statistical physics,
has made great strides in elucidating the mechanism and the many benefits of
stochastic resonance in neuronal systems. In this perspective, we review recent
research dedicated to neuronal stochastic resonance in biophysical mathematical
models. We also explore the regulation of neuronal stochastic resonance, and we
outline important open questions and directions for future research. A deeper
understanding of neuronal stochastic resonance may afford us new insights into
the highly impressive information processing in the brain.
",quantitative-biology
"  We design a deterministic polynomial time $c^n$ approximation algorithm for
the permanent of positive semidefinite matrices where $c=e^{\gamma+1}\simeq
4.84$. We write a natural convex relaxation and show that its optimum solution
gives a $c^n$ approximation of the permanent. We further show that this factor
is asymptotically tight by constructing a family of positive semidefinite
matrices.
",computer-science
"  In this paper, we propose an optimization-based sparse learning approach to
identify the set of most influential reactions in a chemical reaction network.
This reduced set of reactions is then employed to construct a reduced chemical
reaction mechanism, which is relevant to chemical interaction network modeling.
The problem of identifying influential reactions is first formulated as a
mixed-integer quadratic program, and then a relaxation method is leveraged to
reduce the computational complexity of our approach. Qualitative and
quantitative validation of the sparse encoding approach demonstrates that the
model captures important network structural properties with moderate
computational load.
",computer-science
"  Here I introduce an extension to demixed principal component analysis (dPCA),
a linear dimensionality reduction technique for analyzing the activity of
neural populations, to the case of nonlinear dimensions. This is accomplished
using kernel methods, resulting in kernel demixed principal component analysis
(kdPCA). This extension resembles kernel-based extensions to standard principal
component analysis and canonical correlation analysis. kdPCA includes dPCA as a
special case when the kernel is linear. I present examples of simulated neural
activity that follows different low dimensional configurations and compare the
results of kdPCA to dPCA. These simulations demonstrate that nonlinear
interactions can impede the ability of dPCA to demix neural activity
corresponding to experimental parameters, but kdPCA can still recover
interpretable components. Additionally, I compare kdPCA and dPCA to a neural
population from rat orbitofrontal cortex during an odor classification task in
recovering decision-related activity.
",quantitative-biology
